<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Components/components-bundles" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Bundles | Langflow Documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://docs.langflow.org/components-bundle-components"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Bundles | Langflow Documentation"><meta data-rh="true" name="description" content="Bundled components are based on standard Langflow functionality, so you add them to your flows and configure them in much the same way as the standard components."><meta data-rh="true" property="og:description" content="Bundled components are based on standard Langflow functionality, so you add them to your flows and configure them in much the same way as the standard components."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.langflow.org/components-bundle-components"><link data-rh="true" rel="alternate" href="https://docs.langflow.org/components-bundle-components" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.langflow.org/components-bundle-components" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://UZK6BDPCVY-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Bundles","item":"https://docs.langflow.org/components-bundle-components"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SLQFLQ3KPT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SLQFLQ3KPT",{})</script>





<link rel="search" type="application/opensearchdescription+xml" title="Langflow Documentation" href="/opensearch.xml">





<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Sora:wght@550;600&amp;display=swap">
<script>!function(){window.semaphore=window.semaphore||[],window.ketch=function(){window.semaphore.push(arguments)};var e=document.createElement("script");e.type="text/javascript",e.src="https://global.ketchcdn.com/web/v3/config/datastax/langflow_org_web/boot.js",e.defer=e.async=!0,document.getElementsByTagName("head")[0].appendChild(e)}()</script>
<script defer="true">!function(){const e=e=>{if(window.gtag&&e.purposes&&"analytics"in e.purposes&&"targeted_advertising"in e.purposes){const n=!0===e.purposes.analytics?"granted":"denied",t=!0===e.purposes.targeted_advertising?"granted":"denied",a={analytics_storage:n,ad_personalization:t,ad_storage:t,ad_user_data:t};window.gtag("consent","update",a)}};window.ketch&&window.ketch("on","consent",e)}()</script><link rel="stylesheet" href="/assets/css/styles.530387df.css">
<script src="/assets/js/runtime~main.a99792c5.js" defer="defer"></script>
<script src="/assets/js/main.16a1d746.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/lf-docs-light.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/langflow-ai/langflow" target="_blank" class="navbar__item navbar__link header-github-link"></a><a href="https://twitter.com/langflow_ai" target="_blank" class="navbar__item navbar__link header-twitter-link"></a><a href="https://discord.gg/EqksyE2EX9" target="_blank" class="navbar__item navbar__link header-discord-link"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/lf-docs-light.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--dark_xIcU"></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Welcome to Langflow</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/get-started-installation">Get started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/basic-prompting">Templates</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/concepts-overview">Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/concepts-components">Components</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/concepts-components">Components overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-agents">Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/components-bundle-components">Bundles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-custom-components">Create custom Python components</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-data">Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-embedding-models">Embedding models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-helpers">Helpers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-io">Inputs and outputs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-logic">Logic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-memories">Memories</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-models">Language models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-processing">Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-prompts">Prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-tools">Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-vector-stores">Vector stores</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/agents">Agents</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/mcp-server">Model Context Protocol (MCP)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/configuration-api-keys">Configuration</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/develop-overview">Develop</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/deployment-overview">Deployment</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/api-reference-api-examples">API reference</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/integrations-apify">Integrations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/contributing-community">Contribute</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/release-notes">Release notes</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/troubleshoot">Support</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 sidebar-ad">
        <a href="https://astra.datastax.com/signup?type=langflow" target="_blank" class="menu__link">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-cloud"><path d="M17.5 19H9a7 7 0 1 1 6.71-9h1.79a4.5 4.5 0 1 1 0 9Z"/></svg>
          <div class="sidebar-ad-text-container">
            <span class="sidebar-ad-text">Use Langflow in the cloud</span>
            <span class="sidebar-ad-text sidebar-ad-text-gradient">Sign up for DataStax Langflow</span>
          </div>
        </a>
      </li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Components</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Bundles</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Bundles</h1></header><style>[data-ch-theme="github-dark"] {  --ch-t-colorScheme: dark;--ch-t-foreground: #c9d1d9;--ch-t-background: #0d1117;--ch-t-lighter-inlineBackground: #0d1117e6;--ch-t-editor-background: #0d1117;--ch-t-editor-foreground: #c9d1d9;--ch-t-editor-lineHighlightBackground: #6e76811a;--ch-t-editor-rangeHighlightBackground: #ffffff0b;--ch-t-editor-infoForeground: #3794FF;--ch-t-editor-selectionBackground: #264F78;--ch-t-focusBorder: #1f6feb;--ch-t-tab-activeBackground: #0d1117;--ch-t-tab-activeForeground: #c9d1d9;--ch-t-tab-inactiveBackground: #010409;--ch-t-tab-inactiveForeground: #8b949e;--ch-t-tab-border: #30363d;--ch-t-tab-activeBorder: #0d1117;--ch-t-editorGroup-border: #30363d;--ch-t-editorGroupHeader-tabsBackground: #010409;--ch-t-editorLineNumber-foreground: #6e7681;--ch-t-input-background: #0d1117;--ch-t-input-foreground: #c9d1d9;--ch-t-input-border: #30363d;--ch-t-icon-foreground: #8b949e;--ch-t-sideBar-background: #010409;--ch-t-sideBar-foreground: #c9d1d9;--ch-t-sideBar-border: #30363d;--ch-t-list-activeSelectionBackground: #6e768166;--ch-t-list-activeSelectionForeground: #c9d1d9;--ch-t-list-hoverBackground: #6e76811a;--ch-t-list-hoverForeground: #c9d1d9; }</style>
<!-- -->
<p>Bundled components are based on standard Langflow functionality, so you add them to your flows and configure them in much the same way as the standard components.
This documentation summarizes each bundled component and its parameters.
For details about provider-specific aspects of bundled components, this documentation provides links to relevant component provider documentation.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="agent-bundles">Agent bundles<a href="#agent-bundles" class="hash-link" aria-label="Direct link to Agent bundles" title="Direct link to Agent bundles">​</a></h2>
<p><strong>Agents</strong> use LLMs as a brain to analyze problems and select external tools.</p>
<p>For more information, see <a href="/agents">Agents</a>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="crewai-bundles">CrewAI bundles<a href="#crewai-bundles" class="hash-link" aria-label="Direct link to CrewAI bundles" title="Direct link to CrewAI bundles">​</a></h3>
<p>This bundle represents Agents of CrewAI allowing for the creation of specialized AI agents with defined roles goals and capabilities within a crew.</p>
<p>For more information, see the <a href="https://docs.crewai.com/core-concepts/Agents/" target="_blank" rel="noopener noreferrer">CrewAI agents documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>role</td><td>Role</td><td>The role of the agent.</td></tr><tr><td>goal</td><td>Goal</td><td>The objective of the agent.</td></tr><tr><td>backstory</td><td>Backstory</td><td>The backstory of the agent.</td></tr><tr><td>tools</td><td>Tools</td><td>The tools at the agent&#x27;s disposal.</td></tr><tr><td>llm</td><td>Language Model</td><td>The language model that runs the agent.</td></tr><tr><td>memory</td><td>Memory</td><td>This determines whether the agent should have memory or not.</td></tr><tr><td>verbose</td><td>Verbose</td><td>This enables verbose output.</td></tr><tr><td>allow_delegation</td><td>Allow Delegation</td><td>This determines whether the agent is allowed to delegate tasks to other agents.</td></tr><tr><td>allow_code_execution</td><td>Allow Code Execution</td><td>This determines whether the agent is allowed to execute code.</td></tr><tr><td>kwargs</td><td>kwargs</td><td>Additional keyword arguments for the agent.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>output</td><td>Agent</td><td>The constructed CrewAI Agent object.</td></tr></tbody></table></div></div></details>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hierarchical-crew">Hierarchical Crew<a href="#hierarchical-crew" class="hash-link" aria-label="Direct link to Hierarchical Crew" title="Direct link to Hierarchical Crew">​</a></h4>
<p>This component represents a group of agents managing how they should collaborate and the tasks they should perform in a hierarchical structure. This component allows for the creation of a crew with a manager overseeing the task execution.</p>
<p>For more information, see the <a href="https://docs.crewai.com/how-to/Hierarchical/" target="_blank" rel="noopener noreferrer">CrewAI hierarchical crew ocumentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>agents</td><td>Agents</td><td>The list of Agent objects representing the crew members.</td></tr><tr><td>tasks</td><td>Tasks</td><td>The list of HierarchicalTask objects representing the tasks to be executed.</td></tr><tr><td>manager_llm</td><td>Manager LLM</td><td>The language model for the manager agent.</td></tr><tr><td>manager_agent</td><td>Manager Agent</td><td>The specific agent to act as the manager.</td></tr><tr><td>verbose</td><td>Verbose</td><td>This enables verbose output for detailed logging.</td></tr><tr><td>memory</td><td>Memory</td><td>The memory configuration for the crew.</td></tr><tr><td>use_cache</td><td>Use Cache</td><td>This enables caching of results.</td></tr><tr><td>max_rpm</td><td>Max RPM</td><td>This sets the maximum requests per minute.</td></tr><tr><td>share_crew</td><td>Share Crew</td><td>This determines if the crew information is shared among agents.</td></tr><tr><td>function_calling_llm</td><td>Function Calling LLM</td><td>The language model for function calling.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>crew</td><td>Crew</td><td>The constructed Crew object with hierarchical task execution.</td></tr></tbody></table></div></div></details>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sequential-crew">Sequential crew<a href="#sequential-crew" class="hash-link" aria-label="Direct link to Sequential crew" title="Direct link to Sequential crew">​</a></h4>
<p>This component represents a group of agents with tasks that are executed sequentially. This component allows for the creation of a crew that performs tasks in a specific order.</p>
<p>For more information, see the <a href="https://docs.crewai.com/how-to/Sequential/" target="_blank" rel="noopener noreferrer">CrewAI sequential crew documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>tasks</td><td>Tasks</td><td>The list of SequentialTask objects representing the tasks to be executed.</td></tr><tr><td>verbose</td><td>Verbose</td><td>This enables verbose output for detailed logging.</td></tr><tr><td>memory</td><td>Memory</td><td>The memory configuration for the crew.</td></tr><tr><td>use_cache</td><td>Use Cache</td><td>This enables caching of results.</td></tr><tr><td>max_rpm</td><td>Max RPM</td><td>This sets the maximum requests per minute.</td></tr><tr><td>share_crew</td><td>Share Crew</td><td>This determines if the crew information is shared among agents.</td></tr><tr><td>function_calling_llm</td><td>Function Calling LLM</td><td>The language model for function calling.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>crew</td><td>Crew</td><td>The constructed Crew object with sequential task execution.</td></tr></tbody></table></div></div></details>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sequential-task-agent">Sequential task agent<a href="#sequential-task-agent" class="hash-link" aria-label="Direct link to Sequential task agent" title="Direct link to Sequential task agent">​</a></h4>
<p>This component creates a CrewAI Task and its associated Agent allowing for the definition of sequential tasks with specific agent roles and capabilities.</p>
<p>For more information, see the <a href="https://docs.crewai.com/how-to/Sequential/" target="_blank" rel="noopener noreferrer">CrewAI sequential agents documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>role</td><td>Role</td><td>The role of the agent.</td></tr><tr><td>goal</td><td>Goal</td><td>The objective of the agent.</td></tr><tr><td>backstory</td><td>Backstory</td><td>The backstory of the agent.</td></tr><tr><td>tools</td><td>Tools</td><td>The tools at the agent&#x27;s disposal.</td></tr><tr><td>llm</td><td>Language Model</td><td>The language model that runs the agent.</td></tr><tr><td>memory</td><td>Memory</td><td>This determines whether the agent should have memory or not.</td></tr><tr><td>verbose</td><td>Verbose</td><td>This enables verbose output.</td></tr><tr><td>allow_delegation</td><td>Allow Delegation</td><td>This determines whether the agent is allowed to delegate tasks to other agents.</td></tr><tr><td>allow_code_execution</td><td>Allow Code Execution</td><td>This determines whether the agent is allowed to execute code.</td></tr><tr><td>agent_kwargs</td><td>Agent kwargs</td><td>The additional kwargs for the agent.</td></tr><tr><td>task_description</td><td>Task Description</td><td>The descriptive text detailing the task&#x27;s purpose and execution.</td></tr><tr><td>expected_output</td><td>Expected Task Output</td><td>The clear definition of the expected task outcome.</td></tr><tr><td>async_execution</td><td>Async Execution</td><td>The boolean flag indicating asynchronous task execution.</td></tr><tr><td>previous_task</td><td>Previous Task</td><td>The previous task in the sequence for chaining.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>task_output</td><td>Sequential Task</td><td>The list of SequentialTask objects representing the created tasks.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="csv-agent">CSV Agent<a href="#csv-agent" class="hash-link" aria-label="Direct link to CSV Agent" title="Direct link to CSV Agent">​</a></h3>
<p>This component creates a CSV agent from a CSV file and LLM.</p>
<p>For more information, see the <a href="https://python.langchain.com/api_reference/experimental/agents/langchain_experimental.agents.agent_toolkits.csv.base.create_csv_agent.html" target="_blank" rel="noopener noreferrer">Langchain CSV agent documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use for the agent.</td></tr><tr><td>path</td><td>File</td><td>The path to the CSV file.</td></tr><tr><td>agent_type</td><td>String</td><td>The type of agent to create.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The CSV agent instance.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openai-tools-agent">OpenAI Tools Agent<a href="#openai-tools-agent" class="hash-link" aria-label="Direct link to OpenAI Tools Agent" title="Direct link to OpenAI Tools Agent">​</a></h3>
<p>This component creates an OpenAI Tools Agent.</p>
<p>For more information, see the <a href="https://api.python.langchain.com/en/latest/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html" target="_blank" rel="noopener noreferrer">Langchain OpenAI agent documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use.</td></tr><tr><td>tools</td><td>List of Tools</td><td>The tools to give the agent access to.</td></tr><tr><td>system_prompt</td><td>String</td><td>The system prompt to provide context to the agent.</td></tr><tr><td>input_value</td><td>String</td><td>The user&#x27;s input to the agent.</td></tr><tr><td>memory</td><td>Memory</td><td>The memory for the agent to use for context persistence.</td></tr><tr><td>max_iterations</td><td>Integer</td><td>The maximum number of iterations to allow the agent to execute.</td></tr><tr><td>verbose</td><td>Boolean</td><td>This determines whether to print out the agent&#x27;s intermediate steps.</td></tr><tr><td>handle_parsing_errors</td><td>Boolean</td><td>This determines whether to handle parsing errors in the agent.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The OpenAI Tools agent instance.</td></tr><tr><td>output</td><td>String</td><td>The output from executing the agent on the input.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openapi-agent">OpenAPI Agent<a href="#openapi-agent" class="hash-link" aria-label="Direct link to OpenAPI Agent" title="Direct link to OpenAPI Agent">​</a></h3>
<p>This component creates an agent for interacting with OpenAPI services.</p>
<p>For more information, see the <a href="https://python.langchain.com/docs/integrations/tools/openapi/" target="_blank" rel="noopener noreferrer">Langchain OpenAPI toolkit documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use.</td></tr><tr><td>openapi_spec</td><td>String</td><td>The OpenAPI specification for the service.</td></tr><tr><td>base_url</td><td>String</td><td>The base URL for the API.</td></tr><tr><td>headers</td><td>Dict</td><td>The optional headers for API requests.</td></tr><tr><td>agent_executor_kwargs</td><td>Dict</td><td>The optional parameters for the agent executor.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The OpenAPI agent instance.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sql-agent">SQL Agent<a href="#sql-agent" class="hash-link" aria-label="Direct link to SQL Agent" title="Direct link to SQL Agent">​</a></h3>
<p>This component creates an agent for interacting with SQL databases.</p>
<p>For more information, see the <a href="https://python.langchain.com/docs/tutorials/sql_qa/" target="_blank" rel="noopener noreferrer">Langchain SQL agent documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use.</td></tr><tr><td>database</td><td>Database</td><td>The SQL database connection.</td></tr><tr><td>top_k</td><td>Integer</td><td>The number of results to return from a SELECT query.</td></tr><tr><td>use_tools</td><td>Boolean</td><td>This determines whether to use tools for query execution.</td></tr><tr><td>return_intermediate_steps</td><td>Boolean</td><td>This determines whether to return the agent&#x27;s intermediate steps.</td></tr><tr><td>max_iterations</td><td>Integer</td><td>The maximum number of iterations to run the agent.</td></tr><tr><td>max_execution_time</td><td>Integer</td><td>The maximum execution time in seconds.</td></tr><tr><td>early_stopping_method</td><td>String</td><td>The method to use for early stopping.</td></tr><tr><td>verbose</td><td>Boolean</td><td>This determines whether to print the agent&#x27;s thoughts.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The SQL agent instance.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="tool-calling-agent">Tool Calling Agent<a href="#tool-calling-agent" class="hash-link" aria-label="Direct link to Tool Calling Agent" title="Direct link to Tool Calling Agent">​</a></h3>
<p>This component creates an agent for structured tool calling with various language models.</p>
<p>For more information, see the <a href="https://python.langchain.com/docs/concepts/tool_calling/" target="_blank" rel="noopener noreferrer">Langchain tool calling documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use.</td></tr><tr><td>tools</td><td>List[Tool]</td><td>The list of tools available to the agent.</td></tr><tr><td>system_message</td><td>String</td><td>The system message to use for the agent.</td></tr><tr><td>return_intermediate_steps</td><td>Boolean</td><td>This determines whether to return the agent&#x27;s intermediate steps.</td></tr><tr><td>max_iterations</td><td>Integer</td><td>The maximum number of iterations to run the agent.</td></tr><tr><td>max_execution_time</td><td>Integer</td><td>The maximum execution time in seconds.</td></tr><tr><td>early_stopping_method</td><td>String</td><td>The method to use for early stopping.</td></tr><tr><td>verbose</td><td>Boolean</td><td>This determines whether to print the agent&#x27;s thoughts.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The tool calling agent instance.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="xml-agent">XML Agent<a href="#xml-agent" class="hash-link" aria-label="Direct link to XML Agent" title="Direct link to XML Agent">​</a></h3>
<p>This component creates an XML Agent using LangChain.</p>
<p>The agent uses XML formatting for tool instructions to the Language Model.</p>
<p>For more information, see the <a href="https://python.langchain.com/api_reference/langchain/agents/langchain.agents.xml.base.XMLAgent.html" target="_blank" rel="noopener noreferrer">Langchain XML Agent documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>llm</td><td>LanguageModel</td><td>The language model to use for the agent.</td></tr><tr><td>user_prompt</td><td>String</td><td>The custom prompt template for the agent with XML formatting instructions.</td></tr><tr><td>tools</td><td>List[Tool]</td><td>The list of tools available to the agent.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>agent</td><td>AgentExecutor</td><td>The XML Agent instance.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="embedding-models-bundles">Embedding models bundles<a href="#embedding-models-bundles" class="hash-link" aria-label="Direct link to Embedding models bundles" title="Direct link to Embedding models bundles">​</a></h2>
<p>Embedding model components in Langflow generate text embeddings using the selected Large Language Model.</p>
<p>For more information, see <a href="/components-embedding-models">Embedding models</a>.</p>
<p>For more information on a specific embedding model bundle, see the provider&#x27;s documentation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="aiml">AI/ML<a href="#aiml" class="hash-link" aria-label="Direct link to AI/ML" title="Direct link to AI/ML">​</a></h3>
<p>This component generates embeddings using the <a href="https://docs.aimlapi.com/api-overview/embeddings" target="_blank" rel="noopener noreferrer">AI/ML API</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_name</td><td>String</td><td>The name of the AI/ML embedding model to use.</td></tr><tr><td>aiml_api_key</td><td>SecretString</td><td>The API key required for authenticating with the AI/ML service.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance of <code>AIMLEmbeddingsImpl</code> for generating embeddings.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="amazon-bedrock-embeddings">Amazon Bedrock Embeddings<a href="#amazon-bedrock-embeddings" class="hash-link" aria-label="Direct link to Amazon Bedrock Embeddings" title="Direct link to Amazon Bedrock Embeddings">​</a></h3>
<p>This component is used to load embedding models from <a href="https://aws.amazon.com/bedrock/" target="_blank" rel="noopener noreferrer">Amazon Bedrock</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>credentials_profile_name</td><td>String</td><td>The name of the AWS credentials profile in <code>~/.aws/credentials</code> or <code>~/.aws/config</code>, which has access keys or role information.</td></tr><tr><td>model_id</td><td>String</td><td>The ID of the model to call, such as <code>amazon.titan-embed-text-v1</code>. This is equivalent to the <code>modelId</code> property in the <code>list-foundation-models</code> API.</td></tr><tr><td>endpoint_url</td><td>String</td><td>The URL to set a specific service endpoint other than the default AWS endpoint.</td></tr><tr><td>region_name</td><td>String</td><td>The AWS region to use, such as <code>us-west-2</code>. Falls back to the <code>AWS_DEFAULT_REGION</code> environment variable or region specified in <code>~/.aws/config</code> if not provided.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Amazon Bedrock.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="astra-db-vectorize">Astra DB vectorize<a href="#astra-db-vectorize" class="hash-link" aria-label="Direct link to Astra DB vectorize" title="Direct link to Astra DB vectorize">​</a></h3>
<div class="theme-admonition theme-admonition-important admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>important</div><div class="admonitionContent_BuS1"><p>This component is deprecated as of Langflow version 1.1.2.
Instead, use the <a href="/components-vector-stores#astra-db-vector-store">Astra DB vector store component</a>.</p></div></div>
<p>Connect this component to the <strong>Embeddings</strong> port of the <a href="/components-vector-stores#astra-db-vector-store">Astra DB vector store component</a> to generate embeddings.</p>
<p>This component requires that your Astra DB database has a collection that uses a vectorize embedding provider integration.
For more information and instructions, see <a href="https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html" target="_blank" rel="noopener noreferrer">Embedding Generation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>provider</td><td>Embedding Provider</td><td>The embedding provider to use.</td></tr><tr><td>model_name</td><td>Model Name</td><td>The embedding model to use.</td></tr><tr><td>authentication</td><td>Authentication</td><td>The name of the API key in Astra that stores your <a href="https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#embedding-provider-authentication" target="_blank" rel="noopener noreferrer">vectorize embedding provider credentials</a>. (Not required if using an <a href="https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html#supported-embedding-providers" target="_blank" rel="noopener noreferrer">Astra-hosted embedding provider</a>.)</td></tr><tr><td>provider_api_key</td><td>Provider API Key</td><td>As an alternative to <code>authentication</code>, directly provide your embedding provider credentials.</td></tr><tr><td>model_parameters</td><td>Model Parameters</td><td>Additional model parameters.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Astra vectorize.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="azure-openai-embeddings">Azure OpenAI Embeddings<a href="#azure-openai-embeddings" class="hash-link" aria-label="Direct link to Azure OpenAI Embeddings" title="Direct link to Azure OpenAI Embeddings">​</a></h3>
<p>This component generates embeddings using Azure OpenAI models.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Model</td><td>String</td><td>The name of the model to use. Default: <code>text-embedding-3-small</code>.</td></tr><tr><td>Azure Endpoint</td><td>String</td><td>Your Azure endpoint, including the resource, such as <code>https://example-resource.azure.openai.com/</code>.</td></tr><tr><td>Deployment Name</td><td>String</td><td>The name of the deployment.</td></tr><tr><td>API Version</td><td>String</td><td>The API version to use, with options including various dates.</td></tr><tr><td>API Key</td><td>String</td><td>The API key required to access the Azure OpenAI service.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Azure OpenAI.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cloudflare-workers-ai-embeddings">Cloudflare Workers AI Embeddings<a href="#cloudflare-workers-ai-embeddings" class="hash-link" aria-label="Direct link to Cloudflare Workers AI Embeddings" title="Direct link to Cloudflare Workers AI Embeddings">​</a></h3>
<p>This component generates embeddings using <a href="https://developers.cloudflare.com/workers-ai/" target="_blank" rel="noopener noreferrer">Cloudflare Workers AI models</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>account_id</td><td>Cloudflare account ID</td><td><a href="https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages" target="_blank" rel="noopener noreferrer">Find your Cloudflare account ID</a>.</td></tr><tr><td>api_token</td><td>Cloudflare API token</td><td><a href="https://developers.cloudflare.com/fundamentals/api/get-started/create-token/" target="_blank" rel="noopener noreferrer">Create an API token</a>.</td></tr><tr><td>model_name</td><td>Model Name</td><td><a href="https://developers.cloudflare.com/workers-ai/models/#text-embeddings" target="_blank" rel="noopener noreferrer">List of supported models</a>.</td></tr><tr><td>strip_new_lines</td><td>Strip New Lines</td><td>Whether to strip new lines from the input text.</td></tr><tr><td>batch_size</td><td>Batch Size</td><td>The number of texts to embed in each batch.</td></tr><tr><td>api_base_url</td><td>Cloudflare API base URL</td><td>The base URL for the Cloudflare API.</td></tr><tr><td>headers</td><td>Headers</td><td>Additional request headers.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Cloudflare Workers.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cohere-embeddings">Cohere Embeddings<a href="#cohere-embeddings" class="hash-link" aria-label="Direct link to Cohere Embeddings" title="Direct link to Cohere Embeddings">​</a></h3>
<p>This component is used to load embedding models from <a href="https://cohere.com/" target="_blank" rel="noopener noreferrer">Cohere</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>cohere_api_key</td><td>String</td><td>The API key required to authenticate with the Cohere service.</td></tr><tr><td>model</td><td>String</td><td>The language model used for embedding text documents and performing queries. Default: <code>embed-english-v2.0</code>.</td></tr><tr><td>truncate</td><td>Boolean</td><td>Whether to truncate the input text to fit within the model&#x27;s constraints. Default: <code>False</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Cohere.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="embedding-similarity">Embedding similarity<a href="#embedding-similarity" class="hash-link" aria-label="Direct link to Embedding similarity" title="Direct link to Embedding similarity">​</a></h3>
<p>This component computes selected forms of similarity between two embedding vectors.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embedding_vectors</td><td>Embedding Vectors</td><td>A list containing exactly two data objects with embedding vectors to compare.</td></tr><tr><td>similarity_metric</td><td>Similarity Metric</td><td>Select the similarity metric to use. Options: &quot;Cosine Similarity&quot;, &quot;Euclidean Distance&quot;, &quot;Manhattan Distance&quot;.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>similarity_data</td><td>Similarity Data</td><td>A data object containing the computed similarity score and additional information.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="google-generative-ai-embeddings">Google generative AI embeddings<a href="#google-generative-ai-embeddings" class="hash-link" aria-label="Direct link to Google generative AI embeddings" title="Direct link to Google generative AI embeddings">​</a></h3>
<p>This component connects to Google&#x27;s generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the <code>langchain-google-genai</code> package.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>api_key</td><td>API Key</td><td>The secret API key for accessing Google&#x27;s generative AI service. Required.</td></tr><tr><td>model_name</td><td>Model Name</td><td>The name of the embedding model to use. Default: &quot;models/text-embedding-004&quot;.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>The built GoogleGenerativeAIEmbeddings object.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hugging-face-embeddings">Hugging Face Embeddings<a href="#hugging-face-embeddings" class="hash-link" aria-label="Direct link to Hugging Face Embeddings" title="Direct link to Hugging Face Embeddings">​</a></h3>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_BuS1"><p>This component is deprecated as of Langflow version 1.0.18.
Instead, use the <a href="#hugging-face-embeddings-inference">Hugging Face Embeddings Inference component</a>.</p></div></div>
<p>This component loads embedding models from HuggingFace.</p>
<p>Use this component to generate embeddings using locally downloaded Hugging Face models. Ensure you have sufficient computational resources to run the models.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>Cache Folder</td><td>Cache Folder</td><td>The folder path to cache HuggingFace models.</td></tr><tr><td>Encode Kwargs</td><td>Encoding Arguments</td><td>Additional arguments for the encoding process.</td></tr><tr><td>Model Kwargs</td><td>Model Arguments</td><td>Additional arguments for the model.</td></tr><tr><td>Model Name</td><td>Model Name</td><td>The name of the HuggingFace model to use.</td></tr><tr><td>Multi Process</td><td>Multi-Process</td><td>Whether to use multiple processes.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>The generated embeddings.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hugging-face-embeddings-inference">Hugging Face embeddings inference<a href="#hugging-face-embeddings-inference" class="hash-link" aria-label="Direct link to Hugging Face embeddings inference" title="Direct link to Hugging Face embeddings inference">​</a></h3>
<p>This component generates embeddings using <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">Hugging Face Inference API models</a> and requires a <a href="https://huggingface.co/docs/hub/security-tokens" target="_blank" rel="noopener noreferrer">Hugging Face API token</a> to authenticate. Local inference models do not require an API key.</p>
<p>Use this component to create embeddings with Hugging Face&#x27;s hosted models, or to connect to your own locally hosted models.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>API Key</td><td>API Key</td><td>The API key for accessing the Hugging Face Inference API.</td></tr><tr><td>API URL</td><td>API URL</td><td>The URL of the Hugging Face Inference API.</td></tr><tr><td>Model Name</td><td>Model Name</td><td>The name of the model to use for embeddings.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>The generated embeddings.</td></tr></tbody></table></div></div></details>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="connect-the-hugging-face-component-to-a-local-embeddings-model">Connect the Hugging Face component to a local embeddings model<a href="#connect-the-hugging-face-component-to-a-local-embeddings-model" class="hash-link" aria-label="Direct link to Connect the Hugging Face component to a local embeddings model" title="Direct link to Connect the Hugging Face component to a local embeddings model">​</a></h4>
<p>To run an embeddings inference locally, see the <a href="https://huggingface.co/docs/text-embeddings-inference/local_cpu" target="_blank" rel="noopener noreferrer">HuggingFace documentation</a>.</p>
<p>To connect the local Hugging Face model to the <strong>Hugging Face embeddings inference</strong> component and use it in a flow, follow these steps:</p>
<ol>
<li>Create a <a href="/vector-store-rag">Vector store RAG flow</a>.
There are two embeddings models in this flow that you can replace with <strong>Hugging Face</strong> embeddings inference components.</li>
<li>Replace both <strong>OpenAI</strong> embeddings model components with <strong>Hugging Face</strong> model components.</li>
<li>Connect both <strong>Hugging Face</strong> components to the <strong>Embeddings</strong> ports of the <strong>Astra DB vector store</strong> components.</li>
<li>In the <strong>Hugging Face</strong> components, set the <strong>Inference Endpoint</strong> field to the URL of your local inference model. <strong>The <strong>API Key</strong> field is not required for local inference.</strong></li>
<li>Run the flow. The local inference models generate embeddings for the input text.</li>
</ol>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ibm-watsonx-embeddings">IBM watsonx embeddings<a href="#ibm-watsonx-embeddings" class="hash-link" aria-label="Direct link to IBM watsonx embeddings" title="Direct link to IBM watsonx embeddings">​</a></h3>
<p>This component generates text using <a href="https://www.ibm.com/watsonx" target="_blank" rel="noopener noreferrer">IBM watsonx.ai</a> foundation models.</p>
<p>To use <strong>IBM watsonx.ai</strong> embeddings components, replace an embeddings component with the IBM watsonx.ai component in a flow.</p>
<p>An example document processing flow looks like the following:</p>
<p><img decoding="async" loading="lazy" alt="IBM watsonx embeddings model loading a chroma-db with split text" src="/assets/images/component-watsonx-embeddings-chroma-591e45d59ab635d1e1e68ab8036cfed7.png" width="1714" height="1486" class="img_ev3q"></p>
<p>This flow loads a PDF file from local storage and splits the text into chunks.</p>
<p>The <strong>IBM watsonx</strong> embeddings component converts the text chunks into embeddings, which are then stored in a Chroma DB vector store.</p>
<p>The values for <strong>API endpoint</strong>, <strong>Project ID</strong>, <strong>API key</strong>, and <strong>Model Name</strong> are found in your IBM watsonx.ai deployment.
For more information, see the <a href="https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/" target="_blank" rel="noopener noreferrer">Langchain documentation</a>.</p>
<h4 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="default-models">Default models<a href="#default-models" class="hash-link" aria-label="Direct link to Default models" title="Direct link to Default models">​</a></h4>
<p>The component supports several default models with the following vector dimensions:</p>
<ul>
<li><code>sentence-transformers/all-minilm-l12-v2</code>: 384-dimensional embeddings</li>
<li><code>ibm/slate-125m-english-rtrvr-v2</code>: 768-dimensional embeddings</li>
<li><code>ibm/slate-30m-english-rtrvr-v2</code>: 768-dimensional embeddings</li>
<li><code>intfloat/multilingual-e5-large</code>: 1024-dimensional embeddings</li>
</ul>
<p>The component automatically fetches and updates the list of available models from your watsonx.ai instance when you provide your API endpoint and credentials.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>url</td><td>watsonx API Endpoint</td><td>The base URL of the API.</td></tr><tr><td>project_id</td><td>watsonx project id</td><td>The project ID for your watsonx.ai instance.</td></tr><tr><td>api_key</td><td>API Key</td><td>The API Key to use for the model.</td></tr><tr><td>model_name</td><td>Model Name</td><td>The name of the embedding model to use.</td></tr><tr><td>truncate_input_tokens</td><td>Truncate Input Tokens</td><td>The maximum number of tokens to process. Default: <code>200</code>.</td></tr><tr><td>input_text</td><td>Include the original text in the output</td><td>Determines if the original text is included in the output. Default: <code>True</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using watsonx.ai.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lm-studio-embeddings">LM Studio Embeddings<a href="#lm-studio-embeddings" class="hash-link" aria-label="Direct link to LM Studio Embeddings" title="Direct link to LM Studio Embeddings">​</a></h3>
<p>This component generates embeddings using <a href="https://lmstudio.ai/docs" target="_blank" rel="noopener noreferrer">LM Studio</a> models.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>model</td><td>Model</td><td>The LM Studio model to use for generating embeddings.</td></tr><tr><td>base_url</td><td>LM Studio Base URL</td><td>The base URL for the LM Studio API.</td></tr><tr><td>api_key</td><td>LM Studio API Key</td><td>The API key for authentication with LM Studio.</td></tr><tr><td>temperature</td><td>Model Temperature</td><td>The temperature setting for the model.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>The generated embeddings.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mistralai">MistralAI<a href="#mistralai" class="hash-link" aria-label="Direct link to MistralAI" title="Direct link to MistralAI">​</a></h3>
<p>This component generates embeddings using <a href="https://docs.mistral.ai/" target="_blank" rel="noopener noreferrer">MistralAI</a> models.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>String</td><td>The MistralAI model to use. Default: &quot;mistral-embed&quot;.</td></tr><tr><td>mistral_api_key</td><td>SecretString</td><td>The API key for authenticating with MistralAI.</td></tr><tr><td>max_concurrent_requests</td><td>Integer</td><td>The maximum number of concurrent API requests. Default: 64.</td></tr><tr><td>max_retries</td><td>Integer</td><td>The maximum number of retry attempts for failed requests. Default: 5.</td></tr><tr><td>timeout</td><td>Integer</td><td>The request timeout in seconds. Default: 120.</td></tr><tr><td>endpoint</td><td>String</td><td>The custom API endpoint URL. Default: <code>https://api.mistral.ai/v1/</code>).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>A MistralAIEmbeddings instance for generating embeddings.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="nvidia">NVIDIA<a href="#nvidia" class="hash-link" aria-label="Direct link to NVIDIA" title="Direct link to NVIDIA">​</a></h3>
<p>This component generates embeddings using <a href="https://docs.nvidia.com" target="_blank" rel="noopener noreferrer">NVIDIA models</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>String</td><td>The NVIDIA model to use for embeddings, such as <code>nvidia/nv-embed-v1</code>.</td></tr><tr><td>base_url</td><td>String</td><td>The base URL for the NVIDIA API. Default: <code>https://integrate.api.nvidia.com/v1</code>.</td></tr><tr><td>nvidia_api_key</td><td>SecretString</td><td>The API key for authenticating with NVIDIA&#x27;s service.</td></tr><tr><td>temperature</td><td>Float</td><td>The model temperature for embedding generation. Default: <code>0.1</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>A NVIDIAEmbeddings instance for generating embeddings.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ollama-embeddings">Ollama embeddings<a href="#ollama-embeddings" class="hash-link" aria-label="Direct link to Ollama embeddings" title="Direct link to Ollama embeddings">​</a></h3>
<p>This component generates embeddings using <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama models</a>.</p>
<p>For a list of Ollama embeddings models, see the <a href="https://ollama.com/search?c=embedding" target="_blank" rel="noopener noreferrer">Ollama documentation</a>.</p>
<p>To use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model.</p>
<ol>
<li>In the Ollama component, in the <strong>Ollama Base URL</strong> field, enter the address for your locally running Ollama server.
This value is set as the <code>OLLAMA_HOST</code> environment variable in Ollama. The default base URL is <code>http://127.0.0.1:11434</code>.</li>
<li>To refresh the server&#x27;s list of models, click <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw" aria-label="Refresh"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg>.</li>
<li>In the <strong>Ollama Model</strong> field, select an embeddings model. This example uses <code>all-minilm:latest</code>.</li>
<li>Connect the <strong>Ollama</strong> embeddings component to a flow.
For example, this flow connects a local Ollama server running a <code>all-minilm:latest</code> embeddings model to a <a href="/components-vector-stores#chroma-db">Chroma DB</a> vector store to generate embeddings for split text.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Ollama embeddings connected to Chroma DB" src="/assets/images/component-ollama-embeddings-chromadb-c02d6ef9e753b61c274778d90f2a6eec.png" width="1098" height="811" class="img_ev3q"></p>
<p>For more information, see the <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Ollama Model</td><td>String</td><td>The name of the Ollama model to use. Default: <code>llama2</code>.</td></tr><tr><td>Ollama Base URL</td><td>String</td><td>The base URL of the Ollama API. Default: <code>http://localhost:11434</code>.</td></tr><tr><td>Model Temperature</td><td>Float</td><td>The temperature parameter for the model. Adjusts the randomness in the generated embeddings.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using Ollama.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openai-embeddings">OpenAI Embeddings<a href="#openai-embeddings" class="hash-link" aria-label="Direct link to OpenAI Embeddings" title="Direct link to OpenAI Embeddings">​</a></h3>
<p>This component is used to load embedding models from <a href="https://openai.com/" target="_blank" rel="noopener noreferrer">OpenAI</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>OpenAI API Key</td><td>String</td><td>The API key to use for accessing the OpenAI API.</td></tr><tr><td>Default Headers</td><td>Dict</td><td>The default headers for the HTTP requests.</td></tr><tr><td>Default Query</td><td>NestedDict</td><td>The default query parameters for the HTTP requests.</td></tr><tr><td>Allowed Special</td><td>List</td><td>The special tokens allowed for processing. Default: <code>[]</code>.</td></tr><tr><td>Disallowed Special</td><td>List</td><td>The special tokens disallowed for processing. Default: <code>[&quot;all&quot;]</code>.</td></tr><tr><td>Chunk Size</td><td>Integer</td><td>The chunk size for processing. Default: <code>1000</code>.</td></tr><tr><td>Client</td><td>Any</td><td>The HTTP client for making requests.</td></tr><tr><td>Deployment</td><td>String</td><td>The deployment name for the model. Default: <code>text-embedding-3-small</code>.</td></tr><tr><td>Embedding Context Length</td><td>Integer</td><td>The length of embedding context. Default: <code>8191</code>.</td></tr><tr><td>Max Retries</td><td>Integer</td><td>The maximum number of retries for failed requests. Default: <code>6</code>.</td></tr><tr><td>Model</td><td>String</td><td>The name of the model to use. Default: <code>text-embedding-3-small</code>.</td></tr><tr><td>Model Kwargs</td><td>NestedDict</td><td>Additional keyword arguments for the model.</td></tr><tr><td>OpenAI API Base</td><td>String</td><td>The base URL of the OpenAI API.</td></tr><tr><td>OpenAI API Type</td><td>String</td><td>The type of the OpenAI API.</td></tr><tr><td>OpenAI API Version</td><td>String</td><td>The version of the OpenAI API.</td></tr><tr><td>OpenAI Organization</td><td>String</td><td>The organization associated with the API key.</td></tr><tr><td>OpenAI Proxy</td><td>String</td><td>The proxy server for the requests.</td></tr><tr><td>Request Timeout</td><td>Float</td><td>The timeout for the HTTP requests.</td></tr><tr><td>Show Progress Bar</td><td>Boolean</td><td>Whether to show a progress bar for processing. Default: <code>False</code>.</td></tr><tr><td>Skip Empty</td><td>Boolean</td><td>Whether to skip empty inputs. Default: <code>False</code>.</td></tr><tr><td>TikToken Enable</td><td>Boolean</td><td>Whether to enable TikToken. Default: <code>True</code>.</td></tr><tr><td>TikToken Model Name</td><td>String</td><td>The name of the TikToken model.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using OpenAI.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="text-embedder">Text embedder<a href="#text-embedder" class="hash-link" aria-label="Direct link to Text embedder" title="Direct link to Text embedder">​</a></h3>
<p>This component generates embeddings for a given message using a specified embedding model.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embedding_model</td><td>Embedding Model</td><td>The embedding model to use for generating embeddings.</td></tr><tr><td>message</td><td>Message</td><td>The message for which to generate embeddings.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>embeddings</td><td>Embedding Data</td><td>A data object containing the original text and its embedding vector.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vertexai-embeddings">VertexAI Embeddings<a href="#vertexai-embeddings" class="hash-link" aria-label="Direct link to VertexAI Embeddings" title="Direct link to VertexAI Embeddings">​</a></h3>
<p>This component is a wrapper around <a href="https://cloud.google.com/vertex-ai" target="_blank" rel="noopener noreferrer">Google Vertex AI</a> <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings" target="_blank" rel="noopener noreferrer">Embeddings API</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>credentials</td><td>Credentials</td><td>The default custom credentials to use.</td></tr><tr><td>location</td><td>String</td><td>The default location to use when making API calls. Default: <code>us-central1</code>.</td></tr><tr><td>max_output_tokens</td><td>Integer</td><td>The token limit determines the maximum amount of text output from one prompt. Default: <code>128</code>.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Vertex AI large language model. Default: <code>text-bison</code>.</td></tr><tr><td>project</td><td>String</td><td>The default GCP project to use when making Vertex API calls.</td></tr><tr><td>request_parallelism</td><td>Integer</td><td>The amount of parallelism allowed for requests issued to VertexAI models. Default: <code>5</code>.</td></tr><tr><td>temperature</td><td>Float</td><td>Tunes the degree of randomness in text generations. Should be a non-negative value. Default: <code>0</code>.</td></tr><tr><td>top_k</td><td>Integer</td><td>How the model selects tokens for output. The next token is selected from the top <code>k</code> tokens. Default: <code>40</code>.</td></tr><tr><td>top_p</td><td>Float</td><td>Tokens are selected from the most probable to least until the sum of their probabilities exceeds the top <code>p</code> value. Default: <code>0.95</code>.</td></tr><tr><td>tuned_model_name</td><td>String</td><td>The name of a tuned model. If provided, <code>model_name</code> is ignored.</td></tr><tr><td>verbose</td><td>Boolean</td><td>This parameter controls the level of detail in the output. When set to <code>True</code>, it prints internal states of the chain to help debug. Default: <code>False</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>embeddings</td><td>Embeddings</td><td>An instance for generating embeddings using VertexAI.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="language-model-bundles">Language model bundles<a href="#language-model-bundles" class="hash-link" aria-label="Direct link to Language model bundles" title="Direct link to Language model bundles">​</a></h2>
<p>Language model components in Langflow generate text using the selected Large Language Model.</p>
<p>For more information, see <a href="/components-models">Language models</a>.</p>
<p>For more information on a specific model bundle, see the provider&#x27;s documentation.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="aiml-1">AIML<a href="#aiml-1" class="hash-link" aria-label="Direct link to AIML" title="Direct link to AIML">​</a></h3>
<p>This component creates a ChatOpenAI model instance using the AIML API.</p>
<p>For more information, see <a href="https://docs.aimlapi.com/" target="_blank" rel="noopener noreferrer">AIML documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the AIML model to use. Options are predefined in <code>AIML_CHAT_MODELS</code>.</td></tr><tr><td>aiml_api_base</td><td>String</td><td>The base URL of the AIML API. Defaults to <code>https://api.aimlapi.com</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The AIML API Key to use for the model.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="amazon-bedrock">Amazon Bedrock<a href="#amazon-bedrock" class="hash-link" aria-label="Direct link to Amazon Bedrock" title="Direct link to Amazon Bedrock">​</a></h3>
<p>This component generates text using Amazon Bedrock LLMs.</p>
<p>For more information, see <a href="https://docs.aws.amazon.com/bedrock" target="_blank" rel="noopener noreferrer">Amazon Bedrock documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_id</td><td>String</td><td>The ID of the Amazon Bedrock model to use. Options include various models.</td></tr><tr><td>aws_access_key</td><td>SecretString</td><td>AWS Access Key for authentication.</td></tr><tr><td>aws_secret_key</td><td>SecretString</td><td>AWS Secret Key for authentication.</td></tr><tr><td>aws_session_token</td><td>SecretString</td><td>The session key for your AWS account.</td></tr><tr><td>credentials_profile_name</td><td>String</td><td>Name of the AWS credentials profile to use.</td></tr><tr><td>region_name</td><td>String</td><td>AWS region name. Default: <code>us-east-1</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>endpoint_url</td><td>String</td><td>Custom endpoint URL for the Bedrock service.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatBedrock configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="anthropic">Anthropic<a href="#anthropic" class="hash-link" aria-label="Direct link to Anthropic" title="Direct link to Anthropic">​</a></h3>
<p>This component allows the generation of text using Anthropic Chat and Language models.</p>
<p>For more information, see the <a href="https://docs.anthropic.com/en/docs/welcome" target="_blank" rel="noopener noreferrer">Anthropic documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: <code>4096</code>.</td></tr><tr><td>model</td><td>String</td><td>The name of the Anthropic model to use. Options include various Claude 3 models.</td></tr><tr><td>anthropic_api_key</td><td>SecretString</td><td>Your Anthropic API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>anthropic_api_url</td><td>String</td><td>Endpoint of the Anthropic API. Defaults to <code>https://api.anthropic.com</code> if not specified (advanced).</td></tr><tr><td>prefill</td><td>String</td><td>Prefill text to guide the model&#x27;s response (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatAnthropic configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="azure-openai">Azure OpenAI<a href="#azure-openai" class="hash-link" aria-label="Direct link to Azure OpenAI" title="Direct link to Azure OpenAI">​</a></h3>
<p>This component generates text using Azure OpenAI LLM.</p>
<p>For more information, see the <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/" target="_blank" rel="noopener noreferrer">Azure OpenAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Model Name</td><td>String</td><td>Specifies the name of the Azure OpenAI model to be used for text generation.</td></tr><tr><td>Azure Endpoint</td><td>String</td><td>Your Azure endpoint, including the resource.</td></tr><tr><td>Deployment Name</td><td>String</td><td>Specifies the name of the deployment.</td></tr><tr><td>API Version</td><td>String</td><td>Specifies the version of the Azure OpenAI API to be used.</td></tr><tr><td>API Key</td><td>SecretString</td><td>Your Azure OpenAI API key.</td></tr><tr><td>Temperature</td><td>Float</td><td>Specifies the sampling temperature. Defaults to <code>0.7</code>.</td></tr><tr><td>Max Tokens</td><td>Integer</td><td>Specifies the maximum number of tokens to generate. Defaults to <code>1000</code>.</td></tr><tr><td>Input Value</td><td>String</td><td>Specifies the input text for text generation.</td></tr><tr><td>Stream</td><td>Boolean</td><td>Specifies whether to stream the response from the model. Defaults to <code>False</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of AzureOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cohere">Cohere<a href="#cohere" class="hash-link" aria-label="Direct link to Cohere" title="Direct link to Cohere">​</a></h3>
<p>This component generates text using Cohere&#x27;s language models.</p>
<p>For more information, see the <a href="https://cohere.ai/" target="_blank" rel="noopener noreferrer">Cohere documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Cohere API Key</td><td>SecretString</td><td>Your Cohere API key.</td></tr><tr><td>Max Tokens</td><td>Integer</td><td>Specifies the maximum number of tokens to generate. Defaults to <code>256</code>.</td></tr><tr><td>Temperature</td><td>Float</td><td>Specifies the sampling temperature. Defaults to <code>0.75</code>.</td></tr><tr><td>Input Value</td><td>String</td><td>Specifies the input text for text generation.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of the Cohere model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deepseek">DeepSeek<a href="#deepseek" class="hash-link" aria-label="Direct link to DeepSeek" title="Direct link to DeepSeek">​</a></h3>
<p>This component generates text using DeepSeek&#x27;s language models.</p>
<p>For more information, see the <a href="https://api-docs.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Set to <code>0</code> for unlimited. Range: <code>0-128000</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>json_mode</td><td>Boolean</td><td>If <code>True</code>, outputs JSON regardless of passing a schema.</td></tr><tr><td>model_name</td><td>String</td><td>The DeepSeek model to use. Default: <code>deepseek-chat</code>.</td></tr><tr><td>api_base</td><td>String</td><td>Base URL for API requests. Default: <code>https://api.deepseek.com</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your DeepSeek API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in responses. Range: <code>[0.0, 2.0]</code>. Default: <code>1.0</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="google-generative-ai">Google Generative AI<a href="#google-generative-ai" class="hash-link" aria-label="Direct link to Google Generative AI" title="Direct link to Google Generative AI">​</a></h3>
<p>This component generates text using Google&#x27;s Generative AI models.</p>
<p>For more information, see the <a href="https://cloud.google.com/vertex-ai/docs/" target="_blank" rel="noopener noreferrer">Google Generative AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Google API Key</td><td>SecretString</td><td>Your Google API key to use for the Google Generative AI.</td></tr><tr><td>Model</td><td>String</td><td>The name of the model to use, such as <code>&quot;gemini-pro&quot;</code>.</td></tr><tr><td>Max Output Tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>Temperature</td><td>Float</td><td>Run inference with this temperature.</td></tr><tr><td>Top K</td><td>Integer</td><td>Consider the set of top K most probable tokens.</td></tr><tr><td>Top P</td><td>Float</td><td>The maximum cumulative probability of tokens to consider when sampling.</td></tr><tr><td>N</td><td>Integer</td><td>Number of chat completions to generate for each prompt.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatGoogleGenerativeAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="groq">Groq<a href="#groq" class="hash-link" aria-label="Direct link to Groq" title="Direct link to Groq">​</a></h3>
<p>This component generates text using Groq&#x27;s language models.</p>
<ol>
<li>To use this component in a flow, connect it as a <strong>Model</strong> in a flow like the <a href="/basic-prompting">Basic prompting flow</a>, or select it as the <strong>Model Provider</strong> if you&#x27;re using an <strong>Agent</strong> component.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Groq component in a basic prompting flow" src="/assets/images/component-groq-d3df19923f67805fd483632d537af9f4.png" width="1982" height="1194" class="img_ev3q"></p>
<ol start="2">
<li>In the <strong>Groq API Key</strong> field, paste your Groq API key.
The Groq model component automatically retrieves a list of the latest models.
To refresh your list of models, click <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw" aria-label="Refresh"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg>.</li>
<li>In the <strong>Model</strong> field, select the model you want to use for your LLM.
This example uses <a href="https://console.groq.com/docs/model/llama-3.1-8b-instant" target="_blank" rel="noopener noreferrer">llama-3.1-8b-instant</a>, which Groq recommends for real-time conversational interfaces.</li>
<li>In the <strong>Prompt</strong> component, enter:</li>
</ol>
<div class="ch-codeblock not-prose" data-ch-theme="github-dark"><div class="ch-code-wrapper ch-code" data-ch-measured="false"><code class="ch-code-scroll-parent"><br><div><span class="ch-code-line-number">_<!-- -->10</span><div style="display:inline-block;margin-left:16px"><span>You are a helpful assistant who supports their claims with sources.</span></div></div><br></code></div></div>
<ol start="5">
<li>Click <strong>Playground</strong> and ask your Groq LLM a question.
The responses include a list of sources.</li>
</ol>
<p>For more information, see the <a href="https://groq.com/" target="_blank" rel="noopener noreferrer">Groq documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>groq_api_key</td><td>SecretString</td><td>API key for the Groq API.</td></tr><tr><td>groq_api_base</td><td>String</td><td>Base URL path for API requests. Default: <code>https://api.groq.com</code>.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 1.0]</code>. Default: <code>0.1</code>.</td></tr><tr><td>n</td><td>Integer</td><td>Number of chat completions to generate for each prompt.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Groq model to use. Options are dynamically fetched from the Groq API.</td></tr><tr><td>tool_mode_enabled</td><td>Bool</td><td>If enabled, the component only displays models that work with tools.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatGroq configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hugging-face-api">Hugging Face API<a href="#hugging-face-api" class="hash-link" aria-label="Direct link to Hugging Face API" title="Direct link to Hugging Face API">​</a></h3>
<p>This component sends requests to the Hugging Face API to generate text using the model specified in the <strong>Model ID</strong> field.</p>
<p>The Hugging Face API is a hosted inference API for models hosted on Hugging Face, and requires a <a href="https://huggingface.co/docs/hub/security-tokens" target="_blank" rel="noopener noreferrer">Hugging Face API token</a> to authenticate.</p>
<p>In this example based on the <a href="/basic-prompting">Basic prompting flow</a>, the <strong>Hugging Face API</strong> model component replaces the <strong>Open AI</strong> model. By selecting different hosted models, you can see how different models return different results.</p>
<ol>
<li>
<p>Create a <a href="/basic-prompting">Basic prompting flow</a>.</p>
</li>
<li>
<p>Replace the <strong>OpenAI</strong> model component with a <strong>Hugging Face API</strong> model component.</p>
</li>
<li>
<p>In the <strong>Hugging Face API</strong> component, add your Hugging Face API token to the <strong>API Token</strong> field.</p>
</li>
<li>
<p>Open the <strong>Playground</strong> and ask a question to the model, and see how it responds.</p>
</li>
<li>
<p>Try different models, and see how they perform differently.</p>
</li>
</ol>
<p>For more information, see the <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">Hugging Face documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_id</td><td>String</td><td>The model ID from Hugging Face Hub. For example, &quot;gpt2&quot;, &quot;facebook/bart-large&quot;.</td></tr><tr><td>huggingfacehub_api_token</td><td>SecretString</td><td>Your Hugging Face API token for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>max_new_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Default: 512.</td></tr><tr><td>top_p</td><td>Float</td><td>Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95.</td></tr><tr><td>top_k</td><td>Integer</td><td>Top-k sampling parameter. Default: 50.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments to pass to the model.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of HuggingFaceHub configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ibm-watsonxai">IBM watsonx.ai<a href="#ibm-watsonxai" class="hash-link" aria-label="Direct link to IBM watsonx.ai" title="Direct link to IBM watsonx.ai">​</a></h3>
<p>This component generates text using <a href="https://www.ibm.com/watsonx" target="_blank" rel="noopener noreferrer">IBM watsonx.ai</a> foundation models.</p>
<p>To use <strong>IBM watsonx.ai</strong> model components, replace a model component with the IBM watsonx.ai component in a flow.</p>
<p>An example flow looks like the following:</p>
<p><img decoding="async" loading="lazy" alt="IBM watsonx model component in a basic prompting flow" src="/assets/images/component-watsonx-model-2f388a824b49f1c49287f07bc8738d0f.png" width="2364" height="1562" class="img_ev3q"></p>
<p>The values for <strong>API endpoint</strong>, <strong>Project ID</strong>, <strong>API key</strong>, and <strong>Model Name</strong> are found in your IBM watsonx.ai deployment.
For more information, see the <a href="https://python.langchain.com/docs/integrations/chat/ibm_watsonx/" target="_blank" rel="noopener noreferrer">Langchain documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>url</td><td>String</td><td>The base URL of the watsonx API.</td></tr><tr><td>project_id</td><td>String</td><td>Your watsonx Project ID.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your IBM watsonx API Key.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the watsonx model to use. Options are dynamically fetched from the API.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Default: <code>1000</code>.</td></tr><tr><td>stop_sequence</td><td>String</td><td>The sequence where generation should stop.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls nucleus sampling, which limits the model to tokens whose probability is below the <code>top_p</code> value. Range: Default: <code>0.9</code>.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: <code>0.5</code>.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: <code>0.3</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>A random seed for the model. Default: <code>8</code>.</td></tr><tr><td>logprobs</td><td>Boolean</td><td>Whether to return log probabilities of output tokens or not. Default: <code>True</code>.</td></tr><tr><td>top_logprobs</td><td>Integer</td><td>The number of most likely tokens to return at each position. Default: <code>3</code>.</td></tr><tr><td>logit_bias</td><td>String</td><td>A JSON string of token IDs to bias or suppress.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of <a href="https://python.langchain.com/docs/integrations/chat/ibm_watsonx/" target="_blank" rel="noopener noreferrer">ChatWatsonx</a> configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lmstudio">LMStudio<a href="#lmstudio" class="hash-link" aria-label="Direct link to LMStudio" title="Direct link to LMStudio">​</a></h3>
<p>This component generates text using LM Studio&#x27;s local language models.</p>
<p>For more information, see <a href="https://lmstudio.ai/" target="_blank" rel="noopener noreferrer">LM Studio documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>base_url</td><td>String</td><td>The URL where LM Studio is running. Default: <code>&quot;http://localhost:1234&quot;</code>.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate in the response. Default: <code>512</code>.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 2.0]</code>. Default: <code>0.7</code>.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls diversity via nucleus sampling. Range: <code>[0.0, 1.0]</code>. Default: <code>1.0</code>.</td></tr><tr><td>stop</td><td>List[String]</td><td>List of strings that stop generation when encountered.</td></tr><tr><td>stream</td><td>Boolean</td><td>Whether to stream the response. Default: <code>False</code>.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Penalizes repeated tokens. Range: <code>[-2.0, 2.0]</code>. Default: <code>0.0</code>.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Penalizes frequent tokens. Range: <code>[-2.0, 2.0]</code>. Default: <code>0.0</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of LMStudio configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="maritalk">Maritalk<a href="#maritalk" class="hash-link" aria-label="Direct link to Maritalk" title="Direct link to Maritalk">​</a></h3>
<p>This component generates text using Maritalk LLMs.</p>
<p>For more information, see <a href="https://www.maritalk.com/" target="_blank" rel="noopener noreferrer">Maritalk documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to <code>0</code> for unlimited tokens. Default: <code>512</code>.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Maritalk model to use. Options: <code>sabia-2-small</code>, <code>sabia-2-medium</code>. Default: <code>sabia-2-small</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Maritalk API Key to use for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 1.0]</code>. Default: <code>0.5</code>.</td></tr><tr><td>endpoint_url</td><td>String</td><td>The Maritalk API endpoint. Default: <code>https://api.maritalk.com</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatMaritalk configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mistral">Mistral<a href="#mistral" class="hash-link" aria-label="Direct link to Mistral" title="Direct link to Mistral">​</a></h3>
<p>This component generates text using MistralAI LLMs.</p>
<p>For more information, see <a href="https://docs.mistral.ai/" target="_blank" rel="noopener noreferrer">Mistral AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens (advanced).</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Mistral AI model to use. Options include <code>open-mixtral-8x7b</code>, <code>open-mixtral-8x22b</code>, <code>mistral-small-latest</code>, <code>mistral-medium-latest</code>, <code>mistral-large-latest</code>, and <code>codestral-latest</code>. Default: <code>codestral-latest</code>.</td></tr><tr><td>mistral_api_base</td><td>String</td><td>The base URL of the Mistral API. Defaults to <code>https://api.mistral.ai/v1</code> (advanced).</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Mistral API Key to use for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.5.</td></tr><tr><td>max_retries</td><td>Integer</td><td>Maximum number of retries for API calls. Default: 5 (advanced).</td></tr><tr><td>timeout</td><td>Integer</td><td>Timeout for API calls in seconds. Default: 60 (advanced).</td></tr><tr><td>max_concurrent_requests</td><td>Integer</td><td>Maximum number of concurrent API requests. Default: 3 (advanced).</td></tr><tr><td>top_p</td><td>Float</td><td>Nucleus sampling parameter. Default: 1 (advanced).</td></tr><tr><td>random_seed</td><td>Integer</td><td>Seed for random number generation. Default: 1 (advanced).</td></tr><tr><td>safe_mode</td><td>Boolean</td><td>Enables safe mode for content generation (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatMistralAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="novita-ai">Novita AI<a href="#novita-ai" class="hash-link" aria-label="Direct link to Novita AI" title="Direct link to Novita AI">​</a></h3>
<p>This component generates text using Novita AI&#x27;s language models.</p>
<p>For more information, see <a href="https://novita.ai/docs/model-api/reference/llm/llm.html?utm_source=github_langflow&amp;utm_medium=github_readme&amp;utm_campaign=link" target="_blank" rel="noopener noreferrer">Novita AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your Novita AI API Key.</td></tr><tr><td>model</td><td>String</td><td>The id of the Novita AI model to use.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of Novita AI model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="nvidia-1">NVIDIA<a href="#nvidia-1" class="hash-link" aria-label="Direct link to NVIDIA" title="Direct link to NVIDIA">​</a></h3>
<p>This component generates text using NVIDIA LLMs.</p>
<p>For more information, see <a href="https://developer.nvidia.com/generative-ai" target="_blank" rel="noopener noreferrer">NVIDIA AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to <code>0</code> for unlimited tokens (advanced).</td></tr><tr><td>model_name</td><td>String</td><td>The name of the NVIDIA model to use. Default: <code>mistralai/mixtral-8x7b-instruct-v0.1</code>.</td></tr><tr><td>base_url</td><td>String</td><td>The base URL of the NVIDIA API. Default: <code>https://integrate.api.nvidia.com/v1</code>.</td></tr><tr><td>nvidia_api_key</td><td>SecretString</td><td>The NVIDIA API Key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>The seed controls the reproducibility of the job (advanced). Default: <code>1</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatNVIDIA configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ollama">Ollama<a href="#ollama" class="hash-link" aria-label="Direct link to Ollama" title="Direct link to Ollama">​</a></h3>
<p>This component generates text using Ollama&#x27;s language models.</p>
<p>To use this component in a flow, connect Langflow to your locally running Ollama server and select a model.</p>
<ol>
<li>In the Ollama component, in the <strong>Base URL</strong> field, enter the address for your locally running Ollama server.
This value is set as the <code>OLLAMA_HOST</code> environment variable in Ollama.
The default base URL is <code>http://127.0.0.1:11434</code>.</li>
<li>To refresh the server&#x27;s list of models, click <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw" aria-label="Refresh"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg>.</li>
<li>In the <strong>Model Name</strong> field, select a model. This example uses <code>llama3.2:latest</code>.</li>
<li>Connect the <strong>Ollama</strong> model component to a flow. For example, this flow connects a local Ollama server running a Llama 3.2 model as the custom model for an <a href="/components-agents">Agent</a> component.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Ollama model as Agent custom model" src="/assets/images/component-ollama-model-5755eab19c67fb10ee0533b3f7ade726.png" width="4000" height="2668" class="img_ev3q"></p>
<p>For more information, see the <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Base URL</td><td>String</td><td>Endpoint of the Ollama API.</td></tr><tr><td>Model Name</td><td>String</td><td>The model name to use.</td></tr><tr><td>Temperature</td><td>Float</td><td>Controls the creativity of model responses.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of an Ollama model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openai">OpenAI<a href="#openai" class="hash-link" aria-label="Direct link to OpenAI" title="Direct link to OpenAI">​</a></h3>
<p>This component generates text using OpenAI&#x27;s language models.</p>
<p>For more information, see <a href="https://beta.openai.com/docs/" target="_blank" rel="noopener noreferrer">OpenAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your OpenAI API Key.</td></tr><tr><td>model</td><td>String</td><td>The name of the OpenAI model to use. Options include &quot;gpt-3.5-turbo&quot; and &quot;gpt-4&quot;.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of OpenAI model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openrouter">OpenRouter<a href="#openrouter" class="hash-link" aria-label="Direct link to OpenRouter" title="Direct link to OpenRouter">​</a></h3>
<p>This component generates text using OpenRouter&#x27;s unified API for multiple AI models from different providers.</p>
<p>For more information, see <a href="https://openrouter.ai/docs" target="_blank" rel="noopener noreferrer">OpenRouter documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your OpenRouter API key for authentication.</td></tr><tr><td>site_url</td><td>String</td><td>Your site URL for OpenRouter rankings (advanced).</td></tr><tr><td>app_name</td><td>String</td><td>Your app name for OpenRouter rankings (advanced).</td></tr><tr><td>provider</td><td>String</td><td>The AI model provider to use.</td></tr><tr><td>model_name</td><td>String</td><td>The specific model to use for chat completion.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="perplexity">Perplexity<a href="#perplexity" class="hash-link" aria-label="Direct link to Perplexity" title="Direct link to Perplexity">​</a></h3>
<p>This component generates text using Perplexity&#x27;s language models.</p>
<p>For more information, see <a href="https://perplexity.ai/" target="_blank" rel="noopener noreferrer">Perplexity documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_name</td><td>String</td><td>The name of the Perplexity model to use. Options include various Llama 3.1 models.</td></tr><tr><td>max_output_tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Perplexity API Key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.75.</td></tr><tr><td>top_p</td><td>Float</td><td>The maximum cumulative probability of tokens to consider when sampling (advanced).</td></tr><tr><td>n</td><td>Integer</td><td>Number of chat completions to generate for each prompt (advanced).</td></tr><tr><td>top_k</td><td>Integer</td><td>Number of top tokens to consider for top-k sampling. Must be positive (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatPerplexity configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="qianfan">Qianfan<a href="#qianfan" class="hash-link" aria-label="Direct link to Qianfan" title="Direct link to Qianfan">​</a></h3>
<p>This component generates text using Qianfan&#x27;s language models.</p>
<p>For more information, see <a href="https://github.com/baidubce/bce-qianfan-sdk" target="_blank" rel="noopener noreferrer">Qianfan documentation</a>.</p>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sambanova">SambaNova<a href="#sambanova" class="hash-link" aria-label="Direct link to SambaNova" title="Direct link to SambaNova">​</a></h3>
<p>This component generates text using SambaNova LLMs.</p>
<p>For more information, see <a href="https://cloud.sambanova.ai/" target="_blank" rel="noopener noreferrer">Sambanova Cloud documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>sambanova_url</td><td>String</td><td>Base URL path for API requests. Default: <code>https://api.sambanova.ai/v1/chat/completions</code>.</td></tr><tr><td>sambanova_api_key</td><td>SecretString</td><td>Your SambaNova API Key.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Sambanova model to use. Options include various Llama models.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of SambaNova model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vertexai">VertexAI<a href="#vertexai" class="hash-link" aria-label="Direct link to VertexAI" title="Direct link to VertexAI">​</a></h3>
<p>This component generates text using Vertex AI LLMs.</p>
<p>For more information, see <a href="https://cloud.google.com/vertex-ai" target="_blank" rel="noopener noreferrer">Google Vertex AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>credentials</td><td>File</td><td>JSON credentials file. Leave empty to fall back to environment variables. File type: JSON.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Vertex AI model to use. Default: &quot;gemini-1.5-pro&quot;.</td></tr><tr><td>project</td><td>String</td><td>The project ID (advanced).</td></tr><tr><td>location</td><td>String</td><td>The location for the Vertex AI API. Default: &quot;us-central1&quot; (advanced).</td></tr><tr><td>max_output_tokens</td><td>Integer</td><td>The maximum number of tokens to generate (advanced).</td></tr><tr><td>max_retries</td><td>Integer</td><td>Maximum number of retries for API calls. Default: 1 (advanced).</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.0.</td></tr><tr><td>top_k</td><td>Integer</td><td>The number of highest probability vocabulary tokens to keep for top-k-filtering (advanced).</td></tr><tr><td>top_p</td><td>Float</td><td>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Default: 0.95 (advanced).</td></tr><tr><td>verbose</td><td>Boolean</td><td>Whether to print verbose output. Default: False (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatVertexAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="xai">xAI<a href="#xai" class="hash-link" aria-label="Direct link to xAI" title="Direct link to xAI">​</a></h3>
<p>This component generates text using xAI models like <a href="https://x.ai/grok" target="_blank" rel="noopener noreferrer">Grok</a>.</p>
<p>For more information, see the <a href="https://x.ai/" target="_blank" rel="noopener noreferrer">xAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Set to <code>0</code> for unlimited. Range: <code>0-128000</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>json_mode</td><td>Boolean</td><td>If <code>True</code>, outputs JSON regardless of passing a schema.</td></tr><tr><td>model_name</td><td>String</td><td>The xAI model to use. Default: <code>grok-2-latest</code>.</td></tr><tr><td>base_url</td><td>String</td><td>Base URL for API requests. Default: <code>https://api.x.ai/v1</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your xAI API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 2.0]</code>. Default: <code>0.1</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>Controls reproducibility of the job.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="memory-bundles">Memory bundles<a href="#memory-bundles" class="hash-link" aria-label="Direct link to Memory bundles" title="Direct link to Memory bundles">​</a></h2>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="astradbchatmemory-component">AstraDBChatMemory Component<a href="#astradbchatmemory-component" class="hash-link" aria-label="Direct link to AstraDBChatMemory Component" title="Direct link to AstraDBChatMemory Component">​</a></h3>
<p>This component creates an <code>AstraDBChatMessageHistory</code> instance, which stores and retrieves chat messages using Astra DB, a cloud-native database service.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>collection_name</td><td>String</td><td>The name of the Astra DB collection for storing messages. Required.</td></tr><tr><td>token</td><td>SecretString</td><td>The authentication token for Astra DB access. Required.</td></tr><tr><td>api_endpoint</td><td>SecretString</td><td>The API endpoint URL for the Astra DB service. Required.</td></tr><tr><td>namespace</td><td>String</td><td>The optional namespace within Astra DB for the collection.</td></tr><tr><td>session_id</td><td>MessageText</td><td>The unique identifier for the chat session. Uses the current session ID if not provided.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>message_history</td><td>BaseChatMessageHistory</td><td>An instance of AstraDBChatMessageHistory for the session.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cassandrachatmemory-component">CassandraChatMemory Component<a href="#cassandrachatmemory-component" class="hash-link" aria-label="Direct link to CassandraChatMemory Component" title="Direct link to CassandraChatMemory Component">​</a></h3>
<p>This component creates a <code>CassandraChatMessageHistory</code> instance, enabling storage and retrieval of chat messages using Apache Cassandra or DataStax Astra DB.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>database_ref</td><td>MessageText</td><td>The contact points for the Cassandra database or Astra DB database ID. Required.</td></tr><tr><td>username</td><td>MessageText</td><td>The username for Cassandra. Leave empty for Astra DB.</td></tr><tr><td>token</td><td>SecretString</td><td>The password for Cassandra or the token for Astra DB. Required.</td></tr><tr><td>keyspace</td><td>MessageText</td><td>The keyspace in Cassandra or namespace in Astra DB. Required.</td></tr><tr><td>table_name</td><td>MessageText</td><td>The name of the table or collection for storing messages. Required.</td></tr><tr><td>session_id</td><td>MessageText</td><td>The unique identifier for the chat session. Optional.</td></tr><tr><td>cluster_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the Cassandra cluster configuration. Optional.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>message_history</td><td>BaseChatMessageHistory</td><td>An instance of CassandraChatMessageHistory for the session.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mem0-chat-memory">Mem0 Chat Memory<a href="#mem0-chat-memory" class="hash-link" aria-label="Direct link to Mem0 Chat Memory" title="Direct link to Mem0 Chat Memory">​</a></h3>
<p>The Mem0 Chat Memory component retrieves and stores chat messages using Mem0 memory storage.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>mem0_config</td><td>Mem0 Configuration</td><td>The configuration dictionary for initializing the Mem0 memory instance.</td></tr><tr><td>ingest_message</td><td>Message to Ingest</td><td>The message content to be ingested into Mem0 memory.</td></tr><tr><td>existing_memory</td><td>Existing Memory Instance</td><td>An optional existing Mem0 memory instance.</td></tr><tr><td>user_id</td><td>User ID</td><td>The identifier for the user associated with the messages.</td></tr><tr><td>search_query</td><td>Search Query</td><td>The input text for searching related memories in Mem0.</td></tr><tr><td>mem0_api_key</td><td>Mem0 API Key</td><td>The API key for the Mem0 platform. Leave empty to use the local version.</td></tr><tr><td>metadata</td><td>Metadata</td><td>The additional metadata to associate with the ingested message.</td></tr><tr><td>openai_api_key</td><td>OpenAI API Key</td><td>The API key for OpenAI. Required when using OpenAI embeddings without a provided configuration.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>memory</td><td>Mem0 Memory</td><td>The resulting Mem0 Memory object after ingesting data.</td></tr><tr><td>search_results</td><td>Search Results</td><td>The search results from querying Mem0 memory.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="redis-chat-memory">Redis Chat Memory<a href="#redis-chat-memory" class="hash-link" aria-label="Direct link to Redis Chat Memory" title="Direct link to Redis Chat Memory">​</a></h3>
<p>This component retrieves and stores chat messages from Redis.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>host</td><td>hostname</td><td>The IP address or hostname.</td></tr><tr><td>port</td><td>port</td><td>The Redis Port Number.</td></tr><tr><td>database</td><td>database</td><td>The Redis database.</td></tr><tr><td>username</td><td>Username</td><td>The Redis username.</td></tr><tr><td>password</td><td>Password</td><td>The password for the username.</td></tr><tr><td>key_prefix</td><td>Key prefix</td><td>The key prefix.</td></tr><tr><td>session_id</td><td>Session ID</td><td>The unique session identifier for the message.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Display Name</th><th>Info</th></tr></thead><tbody><tr><td>memory</td><td>Memory</td><td>The Redis chat message history object.</td></tr></tbody></table></div></div></details>
<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="zepchatmemory-component">ZepChatMemory Component<a href="#zepchatmemory-component" class="hash-link" aria-label="Direct link to ZepChatMemory Component" title="Direct link to ZepChatMemory Component">​</a></h3>
<p>This component creates a <code>ZepChatMessageHistory</code> instance, enabling storage and retrieval of chat messages using Zep, a memory server for Large Language Models (LLMs).</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>url</td><td>MessageText</td><td>The URL of the Zep instance. Required.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The API Key for authentication with the Zep instance.</td></tr><tr><td>api_base_path</td><td>Dropdown</td><td>The API version to use. Options include api/v1 or api/v2.</td></tr><tr><td>session_id</td><td>MessageText</td><td>The unique identifier for the chat session. Optional.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>message_history</td><td>BaseChatMessageHistory</td><td>An instance of ZepChatMessageHistory for the session.</td></tr></tbody></table></div></div></details></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/components-agents"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Agents</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/components-custom-components"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Create custom Python components</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#agent-bundles" class="table-of-contents__link toc-highlight">Agent bundles</a><ul><li><a href="#crewai-bundles" class="table-of-contents__link toc-highlight">CrewAI bundles</a></li><li><a href="#csv-agent" class="table-of-contents__link toc-highlight">CSV Agent</a></li><li><a href="#openai-tools-agent" class="table-of-contents__link toc-highlight">OpenAI Tools Agent</a></li><li><a href="#openapi-agent" class="table-of-contents__link toc-highlight">OpenAPI Agent</a></li><li><a href="#sql-agent" class="table-of-contents__link toc-highlight">SQL Agent</a></li><li><a href="#tool-calling-agent" class="table-of-contents__link toc-highlight">Tool Calling Agent</a></li><li><a href="#xml-agent" class="table-of-contents__link toc-highlight">XML Agent</a></li></ul></li><li><a href="#embedding-models-bundles" class="table-of-contents__link toc-highlight">Embedding models bundles</a><ul><li><a href="#aiml" class="table-of-contents__link toc-highlight">AI/ML</a></li><li><a href="#amazon-bedrock-embeddings" class="table-of-contents__link toc-highlight">Amazon Bedrock Embeddings</a></li><li><a href="#astra-db-vectorize" class="table-of-contents__link toc-highlight">Astra DB vectorize</a></li><li><a href="#azure-openai-embeddings" class="table-of-contents__link toc-highlight">Azure OpenAI Embeddings</a></li><li><a href="#cloudflare-workers-ai-embeddings" class="table-of-contents__link toc-highlight">Cloudflare Workers AI Embeddings</a></li><li><a href="#cohere-embeddings" class="table-of-contents__link toc-highlight">Cohere Embeddings</a></li><li><a href="#embedding-similarity" class="table-of-contents__link toc-highlight">Embedding similarity</a></li><li><a href="#google-generative-ai-embeddings" class="table-of-contents__link toc-highlight">Google generative AI embeddings</a></li><li><a href="#hugging-face-embeddings" class="table-of-contents__link toc-highlight">Hugging Face Embeddings</a></li><li><a href="#hugging-face-embeddings-inference" class="table-of-contents__link toc-highlight">Hugging Face embeddings inference</a></li><li><a href="#ibm-watsonx-embeddings" class="table-of-contents__link toc-highlight">IBM watsonx embeddings</a></li><li><a href="#lm-studio-embeddings" class="table-of-contents__link toc-highlight">LM Studio Embeddings</a></li><li><a href="#mistralai" class="table-of-contents__link toc-highlight">MistralAI</a></li><li><a href="#nvidia" class="table-of-contents__link toc-highlight">NVIDIA</a></li><li><a href="#ollama-embeddings" class="table-of-contents__link toc-highlight">Ollama embeddings</a></li><li><a href="#openai-embeddings" class="table-of-contents__link toc-highlight">OpenAI Embeddings</a></li><li><a href="#text-embedder" class="table-of-contents__link toc-highlight">Text embedder</a></li><li><a href="#vertexai-embeddings" class="table-of-contents__link toc-highlight">VertexAI Embeddings</a></li></ul></li><li><a href="#language-model-bundles" class="table-of-contents__link toc-highlight">Language model bundles</a><ul><li><a href="#aiml-1" class="table-of-contents__link toc-highlight">AIML</a></li><li><a href="#amazon-bedrock" class="table-of-contents__link toc-highlight">Amazon Bedrock</a></li><li><a href="#anthropic" class="table-of-contents__link toc-highlight">Anthropic</a></li><li><a href="#azure-openai" class="table-of-contents__link toc-highlight">Azure OpenAI</a></li><li><a href="#cohere" class="table-of-contents__link toc-highlight">Cohere</a></li><li><a href="#deepseek" class="table-of-contents__link toc-highlight">DeepSeek</a></li><li><a href="#google-generative-ai" class="table-of-contents__link toc-highlight">Google Generative AI</a></li><li><a href="#groq" class="table-of-contents__link toc-highlight">Groq</a></li><li><a href="#hugging-face-api" class="table-of-contents__link toc-highlight">Hugging Face API</a></li><li><a href="#ibm-watsonxai" class="table-of-contents__link toc-highlight">IBM watsonx.ai</a></li><li><a href="#lmstudio" class="table-of-contents__link toc-highlight">LMStudio</a></li><li><a href="#maritalk" class="table-of-contents__link toc-highlight">Maritalk</a></li><li><a href="#mistral" class="table-of-contents__link toc-highlight">Mistral</a></li><li><a href="#novita-ai" class="table-of-contents__link toc-highlight">Novita AI</a></li><li><a href="#nvidia-1" class="table-of-contents__link toc-highlight">NVIDIA</a></li><li><a href="#ollama" class="table-of-contents__link toc-highlight">Ollama</a></li><li><a href="#openai" class="table-of-contents__link toc-highlight">OpenAI</a></li><li><a href="#openrouter" class="table-of-contents__link toc-highlight">OpenRouter</a></li><li><a href="#perplexity" class="table-of-contents__link toc-highlight">Perplexity</a></li><li><a href="#qianfan" class="table-of-contents__link toc-highlight">Qianfan</a></li><li><a href="#sambanova" class="table-of-contents__link toc-highlight">SambaNova</a></li><li><a href="#vertexai" class="table-of-contents__link toc-highlight">VertexAI</a></li><li><a href="#xai" class="table-of-contents__link toc-highlight">xAI</a></li></ul></li><li><a href="#memory-bundles" class="table-of-contents__link toc-highlight">Memory bundles</a><ul><li><a href="#astradbchatmemory-component" class="table-of-contents__link toc-highlight">AstraDBChatMemory Component</a></li><li><a href="#cassandrachatmemory-component" class="table-of-contents__link toc-highlight">CassandraChatMemory Component</a></li><li><a href="#mem0-chat-memory" class="table-of-contents__link toc-highlight">Mem0 Chat Memory</a></li><li><a href="#redis-chat-memory" class="table-of-contents__link toc-highlight">Redis Chat Memory</a></li><li><a href="#zepchatmemory-component" class="table-of-contents__link toc-highlight">ZepChatMemory Component</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><div class="footer-links">
                  <span>© 2025 Langflow</span>
                  <span id="preferenceCenterContainer"> ·&nbsp; <a href="https://langflow.org/preferences">Manage Privacy Choices</a></span>
                  </div></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><img src="/img/lf-docs-light.svg" alt="Langflow" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE" width="160" height="40"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU" width="160" height="40"></div></div></div></footer><div style="position:fixed;right:20px;bottom:20px;z-index:100;display:flex;align-items:center;gap:10px;cursor:pointer"><div style="background-color:#f6f6f6;border-radius:50%;width:48px;height:48px;display:flex;align-items:center;justify-content:center;box-shadow:0 2px 4px rgba(0,0,0,0.1)"><img src="/img/langflow-icon-black-transparent.svg" style="width:40px" alt="Search"></div></div></div>
</body>
</html>