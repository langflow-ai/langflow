---
title: OpenAI Responses API
slug: /api-openai-responses
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/Tabs';

Langflow includes an OpenAI-compatible API endpoint at `POST /api/v1/responses`.
This endpoint allows you to run your flows using OpenAI-compatible request and response formats.

This enables you to use existing OpenAI client libraries with minimal code changes - just change the `model` name to your `flow_id` instead of using OpenAI model names like "gpt-4".
Flow IDs can be found on the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.

## Prerequisites

Your Langflow flow must contain a **ChatInput** component to be compatible with the OpenAI Responses API.
Flows without this component will return an error. The component types `ChatInput` and `Chat Input` are recognized as chat inputs.

- **Tools**: The `tools` parameter is not yet supported and will return an error if provided.
- **Flow Requirements**: Flows must contain a ChatInput component.
- **Model Names**: The `model` field must contain a valid flow ID or endpoint name.
- **Authentication**: All requests require an API key passed in the `x-api-key` header.
For more information, see [API keys and authentication](/api-keys-and-authentication).

## Request

```
POST /api/v1/responses
```

## Headers

| Header | Required | Description | Example |
|--------|----------|-------------|---------|
| Content-Type | Yes | Specifies the JSON format | "application/json" |
| x-api-key | Yes | Your Langflow API key for authentication | "sk-..." |
| X-LANGFLOW-GLOBAL-VAR-* | No | Global variables for the flow | "X-LANGFLOW-GLOBAL-VAR-API_KEY: sk-..." |


## Example request

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$YOUR_FLOW_ID",
    "input": "Hello, how are you?",
    "stream": false
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "e5e8ef8a-7efd-4090-a110-6aca082bceb7",
  "object": "response",
  "created_at": 1756837941,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_e5e8ef8a-7efd-4090-a110-6aca082bceb7",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello! I'm here and ready to help. How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {"effort": null, "summary": null},
  "store": true,
  "temperature": 1.0,
  "text": {"format": {"type": "text"}},
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": null,
  "user": null,
  "metadata": {}
}
```

</details>

### Request body

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| model | string | Yes | - | The flow ID to execute. |
| input | string | Yes | - | The input text to process. |
| stream | boolean | No | false | Whether to stream the response. |
| background | boolean | No | false | Whether to process in background. |
| tools | list[Any] | No | null | Tools are not supported yet. |
| previous_response_id | string | No | null | ID of previous response to continue conversation. For more, see [Continue conversations with response and session IDs](#response-id). |
| include | list[string] | No | null | Additional response data to include, such as `['tool_call.results']`. For more, see [Retrieve tool call results](#tool-call-results). |

### Response body

The response contains fields that Langflow sets dynamically and fields that use OpenAI-compatible defaults.

The OpenAI-compatible default values shown above are currently fixed and cannot be modified via the request.
They are included to maintain API compatibility and provide a consistent response format.

For your requests, you will only be setting the dynamic fields.
The default values are documented here for completeness and to show the full response structure.

Fields set dynamically by Langflow:

| Field | Type | Description |
|-------|------|-------------|
| id | string | Unique response identifier. |
| created_at | int | Unix timestamp of response creation. |
| model | string | The flow ID that was executed. |
| output | list[dict] | Array of output items (messages, tool calls, etc.). |
| previous_response_id | string | ID of previous response if continuing conversation. |

<details closed>
<summary>Fields with OpenAI-compatible default values</summary>

| Field | Type | Default Value | Description |
|-------|------|---------------|-------------|
| object | string | "response" | Always "response". |
| status | string | "completed" | Response status: "completed", "in_progress", or "failed". |
| error | dict | null | Error details (if any). |
| incomplete_details | dict | null | Incomplete response details (if any). |
| instructions | string | null | Response instructions (if any). |
| max_output_tokens | int | null | Maximum output tokens (if any). |
| parallel_tool_calls | boolean | true | Whether parallel tool calls are enabled. |
| reasoning | dict | `{"effort": null, "summary": null}` | Reasoning information with effort and summary. |
| store | boolean | true | Whether response is stored. |
| temperature | float | 1.0 | Temperature setting. |
| text | dict | `{"format": {"type": "text"}}` | Text format configuration. |
| tool_choice | string | "auto" | Tool choice setting. |
| tools | list[dict] | [] | Available tools. |
| top_p | float | 1.0 | Top-p setting. |
| truncation | string | "disabled" | Truncation setting. |
| usage | dict | null | Usage statistics (if any). |
| user | string | null | User identifier (if any). |
| metadata | dict | {} | Additional metadata. |

</details>

### Example streaming request

When you set `"stream": true` with your request, the API returns a stream where each chunk contains a small piece of the response as it's generated. This provides a real-time experience where users can see the AI's output appear word by word, similar to ChatGPT's typing effect.

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$FLOW_ID",
    "input": "Tell me a story about a robot",
    "stream": true
  }'
```

<details closed>
<summary>Streaming response</summary>

```json
{
  "id": "f7fcea36-f128-41c4-9ac1-e683137375d5",
  "object": "response.chunk",
  "created": 1756838094,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "delta": {
    "content": "Once"
  },
  "status": null
}
```

</details>
### Streaming response body

| Field | Type | Description |
|-------|------|-------------|
| id | string | Unique response identifier. |
| object | string | Always "response.chunk". |
| created | int | Unix timestamp of chunk creation. |
| model | string | The flow ID that was executed. |
| delta | dict | The new content chunk. |
| status | string | Response status: "completed", "in_progress", or "failed" (optional). |

The stream continues until a final chunk with `"status": "completed"` indicates the response is finished.

<details closed>
<summary>Final completion chunk</summary>

```
{
  "id": "f7fcea36-f128-41c4-9ac1-e683137375d5",
  "object": "response.chunk",
  "created": 1756838094,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "delta": {},
  "status": "completed"
}
```
</details>

## Retrieve tool call results {#tool-call-results}

When you send a request to the `/api/v1/responses` endpoint to run a flow that includes tools or function calls, you can retrieve the raw tool execution details by adding `"include": ["tool_call.results"]` to the request payload.

Without the `include` parameter, tool calls return basic function call information, but not the raw tool results, like this example:

```json
{
  "id": "fc_1",
  "type": "function_call",
  "status": "completed",
  "name": "evaluate_expression",
  "arguments": "{\"expression\": \"15*23\"}"
},
```

To get the raw `results` of each tool execution, add the line  `include: ["tool_call.results"]` to the request payload.

```bash
curl -X POST \
  "http://LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "model": "FLOW_ID",
    "input": "Calculate 25 * 13 and show me the result",
    "stream": false,
    "include": ["tool_call.results"]
  }'
```
<details closed>
<summary>Response with results</summary>

```json
{
  "id": "a6e5511e-71f8-457a-88d2-7d8c6ea34e36",
  "object": "response",
  "created_at": 1756835379,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "id": "evaluate_expression_1",
      "queries": [
        "45+67"
      ],
      "status": "completed",
      "tool_name": "evaluate_expression",
      "type": "tool_call",
      "results": {
        "result": "112"
      }
    },
    {
      "type": "message",
      "id": "msg_a6e5511e-71f8-457a-88d2-7d8c6ea34e36",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "The result of 45 + 67 is 112.",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": null,
  "user": null,
  "metadata": {}
}
```
</details>

The response now includes the tool call's results.

```json
{
  "id": "evaluate_expression_1",
  "type": "tool_call",
  "tool_name": "evaluate_expression",
  "queries": ["15*23"],
  "results": {"result": "112"}
}
```

## Pass global variables to your flows in headers

Global variables allow you to pass dynamic values to your flows that can be used by components within that flow run.
This is useful for passing API keys, user IDs, or any other configuration that might change between requests.

The `/responses` endpoint accepts global variables as custom HTTP headers with the format `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}`.
Variable names are automatically converted to uppercase.

Variables passed with `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` supersede database variables for a single flow run.
If no variable is found in the database, the flow will fail, unless the `FALLBACK_TO_ENV_VARS` environment variable is `true`, in which case the flow will use the variable set in the `.env` file.

This example shows how global variables work for a single flow run. The variables are only available during that specific request execution and are not persisted.

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -H "X-LANGFLOW-GLOBAL-VAR-OPENAI_API_KEY: sk-..." \
  -H "X-LANGFLOW-GLOBAL-VAR-USER_ID: user123" \
  -H "X-LANGFLOW-GLOBAL-VAR-ENVIRONMENT: production" \
  -d '{
    "model": "your-flow-id",
    "input": "Hello"
  }'
```

<details>
<summary>Response</summary>

```json
{
  "id": "4a4d2f24-bb45-4a55-a499-0191305264be",
  "object": "response",
  "created_at": 1756839935,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_4a4d2f24-bb45-4a55-a499-0191305264be",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello! How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": null
}
```

</details>

## Continue conversations with response and session IDs {#response-id}

Conversation continuity allows you to maintain context across multiple API calls, enabling multi-turn conversations with your flows. This is essential for building chat applications where users can have ongoing conversations.

When you make a request, the API returns a response with an `id` field. You can use this `id` as the `previous_response_id` in your next request to continue the conversation from where it left off.

First Message:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$FLOW_ID",
    "input": "Hello, my name is Alice"
  }'
```

<details>
<summary>Response</summary>

```json
{
  "id": "c45f4ac8-772b-4675-8551-c560b1afd590",
  "object": "response",
  "created_at": 1756839042,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_c45f4ac8-772b-4675-8551-c560b1afd590",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello, Alice! How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": null
}
```

</details>

Follow-up message:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
    "input": "What's my name?",
    "previous_response_id": "c45f4ac8-772b-4675-8551-c560b1afd590"
  }'
```

<details>
<summary>Response</summary>

```json
{
  "id": "c45f4ac8-772b-4675-8551-c560b1afd590",
  "object": "response",
  "created_at": 1756839043,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_c45f4ac8-772b-4675-8551-c560b1afd590",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Your name is Alice. How can I help you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": "c45f4ac8-772b-4675-8551-c560b1afd590"
}
```

</details>

You can optionally use your own session ID values in place of `previous_response_id`.

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
    "input": "What's my name?",
    "previous_response_id": "session-alice-1756839048"
  }'
```

<details>
<summary>Response</summary>

```json
{
  "id": "session-alice-1756839048",
  "object": "response",
  "created_at": 1756839048,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_session-alice-1756839048",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "I don't have access to your name unless you tell me. If you'd like, you can share your name, and I'll remember it for this conversation!",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": "session-alice-1756839048"
}
```

</details>