---
title: Language Model
slug: /components-models
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Language model components in Langflow generate text using a specified Large Language Model (LLM).
These components accept inputs like chat messages, files, and instructions in order to generate a text response.

Langflow includes a **Language Model** core component that has built-in support for many LLMs.
Alternatively, you can use any [additional language model](#additional-language-models) in place of the **Language Model** core component.

## Use language model components in flows

Use language model components anywhere you would use an LLM in a flow.

<Tabs>
<TabItem value="chat" label="Chat" default>

One of the most common use cases of language model components is to chat with LLMs in your flows.

The following example uses a language model component in a chatbot flow similar to the **Basic Prompting** template.

1. Add the **Language Model** core component to your flow, and then enter your OpenAI API key.

    This example uses the **Language Model** core component's default OpenAI model.
    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

    :::tip My preferred provider or model isn't listed
    If you want to use a provider or model that isn't built-in to the **Language Model** core component, you can replace this component with any [additional language model](#additional-language-models).

    Browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred provider to find additional language models.
    :::

3. In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable the **System Message** parameter, and then click **Close**.

4. Add a [**Prompt Template** component](/components-prompts) to your flow.

5. In the **Template** field, enter some instructions for the LLM, such as `You are an expert in geography who is tutoring high school students`.

6. Connect the **Prompt Template** component's output to the **Language Model** component's **System Message** input.

7. Add [**Chat Input** and **Chat Output** components](/components-io#chat-io) to your flow.
These components are required for direct chat interaction with an LLM.

8. Connect the **Chat Input** component to the **Language Model** component's **Input**, and then connect the **Language Model** component's **Message** output to the **Chat Output** component.

    ![A basic prompting flow with Language Model, Prompt Template, Chat Input, and Chat Output components](/img/component-language-model.png)

9. Open the **Playground**, and ask a question to chat with the LLM and test the flow, such as `What is the capital of Utah?`.

    <details>
    <summary>Result</summary>

    The following response is an example of an OpenAI model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    ```
    The capital of Utah is Salt Lake City. It is not only the largest city in the state but also serves as the cultural and economic center of Utah. Salt Lake City was founded in 1847 by Mormon pioneers and is known for its proximity to the Great Salt Lake and its role in the history of the Church of Jesus Christ of Latter-day Saints. For more information, you can refer to sources such as the U.S. Geological Survey or the official state website of Utah.
    ```

    </details>

10. Optional: Try a different model or provider to see how the response changes.
For example, if you are using the **Language Model** core component, you could try an Anthropic model.

    Then, open the **Playground**, ask the same question as you did before, and then compare the content and format of the responses.

    This helps you understand how different models handle the same request so you can choose the best model for your use case.
    You can also learn more about different models in each model provider's documentation.

    <details>
    <summary>Result</summary>

    The following response is an example of an Anthropic model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    Note that this response is shorter and includes sources, whereas the previous OpenAI response was more encyclopedic and didn't cite sources.

    ```
    The capital of Utah is Salt Lake City. It is also the most populous city in the state. Salt Lake City has been the capital of Utah since 1896, when Utah became a state.
    Sources:
    Utah State Government Official Website (utah.gov)
    U.S. Census Bureau
    Encyclopedia Britannica
    ```

    </details>

</TabItem>
<TabItem value="drivers" label="Drivers">

Some components use a language model component to perform LLM-driven actions.
Typically, these components prepare data for further processing by downstream components, rather than emitting direct chat output.
For an example, see the [**Smart Function** component](/components-processing#smart-transform).

A component must accept a `LanguageModel` input to use a language model component as a driver, and you must set the language model component's output type to `LanguageModel`.
For more information, see [Language Model output types](#language-model-output-types).

</TabItem>
<TabItem value="agents" label="Agents">

If you don't want to use the **Agent** component's built-in LLMs, you can use a language model component to connect your preferred model:

1. Add a language model component to your flow.

    You can use the **Language Model** core component or browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) to find additional language models.
    Components in bundles may not have `language model` in the name.
    For example, Azure OpenAI LLMs are provided through the [**Azure OpenAI** component](/bundles-azure#azure-openai).

2. Configure the language model component as needed to connect to your preferred model.

3. Change the language model component's output type from **Model Response** to **Language Model**.
The output port changes to a `LanguageModel` port.
This is required to connect the language model component to the **Agent** component.
For more information, see [Language Model output types](#language-model-output-types).

4. Add an **Agent** component to the flow, and then set **Model Provider** to **Connect other models**.

    The **Model Provider** field changes to a **Language Model** (`LanguageModel`) input.

5. Connect the language model component's output to the **Agent** component's **Language Model** input.
The **Agent** component now inherits the language model settings from the connected language model component instead of using any of the built-in models.

</TabItem>
</Tabs>

## Language model parameters

The following parameters are for the **Language Model** core component.
Other language model components can have additional or different parameters.

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| provider | String | Input parameter. The model provider to use. |
| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |
| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |
| input_value | String | Input parameter. The input text to send to the model. |
| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |
| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |
| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |
| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |

## Language model output types

Language model components, including the core component and bundled components, can produce two types of output:

* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).
Use this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.

* **Language Model**: Change the language model component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow, such as an **Agent** or **Smart Function** component.

    With this configuration, the language model component supports an action completed by another component, rather than a direct chat interaction.
    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.

## Additional language models

If your provider or model isn't supported by the **Language Model** core component, additional language model components are available in <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

You can use these components in the same way that you use the core **Language Model** component, as explained in [Use language model components in flows](#use-language-model-components-in-flows).

## Pair models with vector stores

import PartialVectorRagBlurb from '@site/docs/_partial-vector-rag-blurb.mdx';

<PartialVectorRagBlurb />

<details>
<summary>Example: Vector search flow</summary>

import PartialVectorRagFlow from '@site/docs/_partial-vector-rag-flow.mdx';

<PartialVectorRagFlow />

</details>