---
title: Cleanlab
slug: /bundles-cleanlab
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.

Integrate the **Cleanlab** component with Langflow and unlock trustworthy agentic, RAG, and LLM pipelines by utilizing Cleanlab's evaluation and remediation suite.

This component evaluates the quality of an LLM response using Cleanlab, optionally remediating the response with fallback text or expert-provided answers, provided in the [Cleanlab AI Platform](https://help.cleanlab.ai/codex/).

Authentication is required with a Cleanlab API key. You can submit a request for one [here](https://cleanlab.ai/contact/).

## Cleanlab Component

The **Cleanlab** component validates an AI-generated response against the same inputs your LLM saw (system message, user input, query, and optional context). It can automatically replace untrustworthy/unhelpful responses with fallback or expert answers.

Currently, this component only supports single-turn conversations. The component will still function with multi-turn conversations, but it will only validate the last message in the conversation with no context of prior message history. If you need support for multi-turn conversations with message history, please reach out to Cleanlab at [support@cleanlab.ai](mailto:support@cleanlab.ai).

### Cleanlab Inputs

<PartialParams />

| Name            | Type     | Description                                                                 |
|-----------------|----------|-----------------------------------------------------------------------------|
| api_key         | Secret   | Your Cleanlab API (project access) key.                                                      |
| input           | Message  | The complete input message passed into the LLM to produce the response.          |
| query           | Message  | The user's core query (latest user message). This must be present in the `input`.                                |
| response        | Message  | The AI-generated response to validate.                                      |
| context         | Message  | (Optional) Retrieved context for the query. Defaults to empty string if not provided. This must be present in the `input`.   |
| system_message  | Message  | (Optional) A system message prepended as the first message.                  |
| fallback_text   | Prompt   | Fallback message if the response is flagged bad and no expert answer exists. |

### Cleanlab Outputs

The **Cleanlab** component outputs a single message:

| Name           | Type     | Description                                                                 |
|----------------|----------|-----------------------------------------------------------------------------|
| final_response | Message  | Either the original response, an expert remediated answer, or the fallback text. |


## Quickstart

You can download the [Cleanlab Quickstart flow](/files/eval_and_remediate_cleanlab.json), and then [import the flow](/concepts-flows-import) to your Langflow instance. This flow takes in a user's query, does a web search to retrieve relevant context, then serves an answer.

![Cleanlab Example Flow](/img/cleanlab-flow-example.png)

This example flow shows how to insert **Cleanlab** at the end of the generation to validate and remediate the model response:

* Connect the `Model Response` from your LLM or Agent to the **Model Response** input.
* Connect the original input `query` to the `User Query` input. This should be the core user query that `Model Response` is answering. For most cases this is just the output from the `Chat Input` component.
* (Optional) Connect the retrieved `context` to the `Context` input. This can be any information added into the prompt after the user query.
* (Optional) Connect your system instructions or messages to the `System Message` input. This should include any instructions or system messages provided to the LLM.

You will also need to create a [Project](https://help.cleanlab.ai/codex/concepts/projects/) within the Cleanlab Platform and add the [Access Key](https://help.cleanlab.ai/codex/web_tutorials/create_project/#access-keys) from the Settings page of the Project into the `API Key` field of the Component.

When you run the flow, **Cleanlab** will either validate and return the original response, replace it with an expert-provided answer, or fall back to your configured fallback text. This is enabled by Cleanlab's novel uncertainty estimation algorithms, developed to reliably evaluate LLM inputs and outputs.

Here's an example of a response to the query `"How can Cleanlab improve my AI app?"` that passed validation.

![Response Passed Validation](/img/cleanlab-response-passed-validation.png)

You can view the respective query and all the associated scores that Cleanlab provides in the Project you created earlier. Here is what the query looks like in the Cleanlab AI Platform. Notice how the trustworthiness score has passed validation (trustworthiness = 0.985)!

![Cleanlab AI Platform](/img/cleanlab-ai-platform.png)

Let's try something more fun (and complex): `"How many basketballs fit between the earth and sun?"` In this case, we see that the flow returns the provided fallback answer, which means Cleanlab has flagged the original response.

![Cleanlab Fallback example](/img/cleanlab-fallback.png)

If we take a closer look at this query in the Cleanlab AI Platform, we see that the original response given was entirely incorrect (100 basketballs fit between the earth and sun) and was flagged correctly (trustworthiness = 0.269)!

![Cleanlab flags wrong response](/img/cleanlab-wrong-response.png)

Instead of spending hours sifting through thousands of responses, you can focus only on the small percentage that were flagged as potentially problematic. This targeted approach dramatically reduces the time and effort needed for quality assurance while ensuring critical issues don't slip through the cracks.

In the next section, we will cover how to create `custom` guardrails and evaluations to flag issues that are important to your application or business use case.

## Guardrails and Evaluations

At the core of the Cleanlab AI Platform lies Guardrails and Evaluations.

**Guardrails** are safety mechanisms that block AI outputs when they fail to meet specified criteria. When a guardrail is triggered, the AI response is prevented from reaching the user, and an alternative response (such as an SME-provided answer) is served instead.

**Evaluations** are monitoring tools used for analytics and visibility. They score AI responses based on specific criteria but do not block outputs. Evaluations help you understand AI performance patterns and identify areas for improvement.

For detailed configuration of guardrails and evaluations, see the [Guardrails & Evaluations](https://help.cleanlab.ai/codex/web_tutorials/codex_guardrails_evals/) documentation.

You have the ability to create custom guardrails and evaluations in a few minutes. Imagine you want prevent your AI from discussing politics in any way. Let's add the following guardrail to the Project to prevent this:

```markdown
Name: Politics
Eval Key: politics

Criteria:
Determine if the Response discusses politics in any way.

A good Response:
- Contains no references to politics, political figures,
parties, elections, governments, or policies

A bad Response:
- Mentions politics, political figures, parties, elections,
or governments (e.g., “The president said…” or “Democrats vs Republicans”).
- Discusses political ideologies or systems (e.g., socialism, capitalism,
communism, democracy vs authoritarianism).
- Engages with political events, controversies, or news coverage in any way.
```

In just a few sentences, we can prevent the AI from discussing politics!

![Cleanlab add guardrail](/img/cleanlab-politics-guardrail.png)

Now, when the AI responds with something political, the guardrail is triggered, and the fallback response is returned in its place. Notice how the (original) AI Response in the image below does discuss politics. Instead of returning this response, the fallback response configured in the component is returned.

![Cleanlab politics guardrailed](/img/cleanlab-politics-guardrailed.png)
![Cleanlab politics guardrailed chat](/img/cleanlab-politics-guardrailed-chat.png)

## Remediations

The Cleanlab AI Platform also enables Subject Matter Experts (SMEs) to directly improve your AI application by providing Expert Answers for issues flagged by the Guardrails and Evaluations. Simply navigate to the `Issues` tab where all flagged user queries will be aggregated and prioritized for easy remediation.

For the basketball question earlier, we can directly add the correct answer here.

![Cleanlab add remediation](/img/cleanlab-add-remediation.png)

Then, when a user asks the same or semantically similar query in the future, instead of returning the unhelpful fallback answer, Cleanlab serves the newly added Expert Answer automatically.

![Cleanlab expert answer](/img/cleanlab-expert-answer.png)

## Wrap Up

The Cleanlab component in Langflow provides a powerful safety layer for your AI applications. By integrating Cleanlab's validation capabilities, you can:

- Monitor and evaluate every AI interaction in real-time
- Automatically detect and block problematic responses
- Provide expert-curated answers for flagged queries
- Continuously improve your AI system through SME remediations

Getting started is straightforward:
1. [Sign up](https://cleanlab.ai/contact/) for a Cleanlab account
2. Add the Cleanlab component to your flow (quickstart flow [here](#quickstart))
3. Configure your guardrails and evaluations
4. Start monitoring and improving your AI responses

With Cleanlab's comprehensive AI Platform and Langflow's intuitive interface, you can build more reliable and trustworthy AI applications that deliver consistently high-quality responses to your users.

If you have any questions or want help with integrating Cleanlab into your application, please reach out to Cleanlab at [support@cleanlab.ai](mailto:support@cleanlab.ai).