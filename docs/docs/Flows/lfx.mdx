---
title: Run flows with Langflow Executor (LFX)
slug: /lfx-stateless-flows
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PartialLfxDependencies from '@site/docs/_partial-lfx-dependencies.mdx';

The Langflow Executor (LFX) is a command-line tool that serves and runs flows statelessly from [flow JSON files](/concepts-flows-import) with minimal dependencies.

Running a flow with LFX is similar to running flows with the [`--backend-only` environment variable](/environment-variables#server) enabled, but even more lightweight because the Langflow package and all of its dependencies don't need to be installed.

Use LFX to share flows with other developers, test flows in different environments, and run flows in production applications without requiring the full Langflow UI or database setup.

LFX includes two commands for executing flows:

* [`lfx serve`](#serve): This command starts a FastAPI server hosting a Langflow API endpoint with your flow available at `/flows/{flow_id}/run`.
The flow graph is stored in memory at all times, so there is less overhead for loading the graph from a database.
* [`lfx run`](#run): This command executes a flow locally and returns the results to `stdout`.

## Prerequisites

- Install [Python](https://www.python.org/downloads/release/python-3100/).
- Install [uv](https://docs.astral.sh/uv/getting-started/installation/).
- Create or download a [flow JSON file](/concepts-flows). For example, download the Simple Agent flow from the repository:
  ```bash
  curl -o simple-agent-flow.json "https://raw.githubusercontent.com/langflow-ai/langflow/main/src/backend/base/langflow/initial_setup/starter_projects/Simple%20Agent.json"
  ```
- Create an [OpenAI API key](https://platform.openai.com/api-keys).
- Create a Langflow API key. For LFX, you can [generate a secure token locally](#serve), or create one through the [Langflow server UI or CLI](/api-keys-and-authentication).

## Install LFX

LFX can be installed in multiple ways.
If you have [installed Langflow OSS version >=1.6](/get-started-installation#install-and-run-the-langflow-oss-python-package), `lfx` is already included.

<Tabs>
<TabItem value="source" label="Clone repository" default>

1. Clone the Langflow repository:
   ```bash
   git clone https://github.com/langflow-ai/langflow
   ```

2. Change directory to `langflow/src/lfx`:
   ```bash
   cd langflow/src/lfx
   ```

   From this directory, you can run `lfx` commands using `uv run lfx` as shown in [`lfx serve`](#serve) or [`lfx run`](#run).

</TabItem>
<TabItem value="pypi" label="Install from PyPI">

1. Create and activate a virtual environment.

   ```bash
   uv venv lfx-venv
   source lfx-venv/bin/activate
   ```

2. Install the LFX package from PyPI:

   ```bash
   uv pip install lfx
   ```

   To install the latest nightly version of LFX:

   ```bash
   uv pip install lfx-nightly
   ```

   To run `lfx` commands, continue to [`lfx serve`](#serve) or [`lfx run`](#run).
</TabItem>
<TabItem value="uvx" label="Run without installing">

Run LFX without installing it locally using `uvx`.

1. Create a [Langflow API key](#serve), and set `LANGFLOW_API_KEY` in the same terminal session as `lfx`:

   ```bash
   export LANGFLOW_API_KEY="sk..."
   ```

2. Run `lfx serve` using `uvx`:

   ```bash
   uvx lfx serve simple-agent-flow.json
   ```

   This command downloads and runs LFX in a temporary environment without permanent installation.
   From the same environment, you can also run flows directly with [`lfx run`](#run).

</TabItem>
</Tabs>

## Serve the simple agent starter flow with `lfx serve` {#serve}

To serve a flow as a REST API endpoint, set a `LANGFLOW_API_KEY` and run the flow JSON.

The API key is required for security because `lfx serve` can create a publicly accessible FastAPI server.

This example uses the **Agent** component's built-in OpenAI model, which requires an OpenAI API key.
If you want to use a different provider, edit the model provider, model name, and credentials accordingly.

1. Generate a Langflow API key.

   For LFX, you can generate a secure token locally to use as your `LANGFLOW_API_KEY`:

   ```bash
   uv run python -c "import secrets; print(secrets.token_urlsafe(32))"
   ```

   This is different from creating a Langflow API key through the Langflow server UI or CLI, which stores the key in the Langflow database.
   For LFX, you only need a secure token string to authenticate requests to your LFX server.
   To create a Langflow API key for use with a Langflow server, see [API keys and authentication](/api-keys-and-authentication).

2. Set up your environment variables.

    <Tabs>
    <TabItem value="env-file" label=".env file" default>

    Create a `.env` file and populate it with your flow's variables.
    The `LANGFLOW_API_KEY` is required.
    This example assumes the flow requires an OpenAI API key.

    ```bash
    LANGFLOW_API_KEY="sk..."
    OPENAI_API_KEY="sk-..."
    ```

    </TabItem>
    <TabItem value="export" label="Export variables">

    Export your variables in the same terminal session where you'll start the server.
    You must declare your variables before the server starts for the server to pick them up.

    ```bash
    export LANGFLOW_API_KEY="sk..."
    export OPENAI_API_KEY="sk-..."
    ```

    </TabItem>
    </Tabs>

3. Install dependencies.

   <PartialLfxDependencies />

4. Start the server with your variable values.

    <Tabs>
    <TabItem value="env-file" label=".env file" default>

    This example assumes your flow file and `.env` file are in the current directory:

    ```
    uv run lfx serve simple-agent-flow.json --env-file .env
    ```

    If your `.env` file is in a different location, provide the full or relative path:

    ```
    uv run lfx serve simple-agent-flow.json --env-file /path/to/.env
    ```

    </TabItem>
    <TabItem value="export" label="Export variables">

    If you exported your variables, the command to start the server automatically picks up the values when it starts.

    ```
    uv run lfx serve simple-agent-flow.json
    ```

    To export new values, stop the server, export the variables, and then start the server again.

    </TabItem>
    </Tabs>
5. The startup process displays a `flow_id` value in the output.
   Copy the `flow_id` to use in the test API call in the next step.
   In this example, the `flow_id` is `c1dab29d-3364-58ef-8fef-99311d32ee42`.

    ```bash
     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LFX Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
     â”‚ ðŸŽ¯ Single Flow Served Successfully!                                  â”‚
     â”‚                                                                      â”‚
     â”‚ Source: /Users/mendonkissling/Downloads/simple-agent-flow.json       â”‚
     â”‚ Server: http://127.0.0.1:8000                                        â”‚
     â”‚ API Key: sk-...                                                 â”‚
     â”‚                                                                      â”‚
     â”‚ Send POST requests to:                                               â”‚
     â”‚ http://127.0.0.1:8000/flows/c1dab29d-3364-58ef-8fef-99311d32ee42/run â”‚
     â”‚                                                                      â”‚
     â”‚ With headers:                                                        â”‚
     â”‚ x-api-key: sk-...                                               â”‚
     â”‚                                                                      â”‚
     â”‚ Or query parameter:                                                  â”‚
     â”‚ ?x-api-key=sk-...                                               â”‚
     â”‚                                                                      â”‚
     â”‚ Request body:                                                        â”‚
     â”‚ {'input_value': 'Your input message'}                                â”‚
     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    ```

6. To send a test request to the server, open a new terminal and export your `flow_id` and Langflow API key values as variables:
    ```bash
    export LANGFLOW_API_KEY="sk..."
    export FLOW_ID="c1dab29d-3364-58ef-8fef-99311d32ee42"
    ```

7. Test the server with an API call to the `/flows/flow_id/run` endpoint:

    ```bash
    curl -X POST http://localhost:8000/flows/$FLOW_ID/run \
      -H "Content-Type: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -d '{"input_value": "Hello, world!"}'
    ```

    Successful response example:
    ```json
    {
      "result": "Hello world! ðŸ‘‹\n\nHow can I help you today? If you have any questions or need assistance, just let me know!",
      "success": true,
      "logs": "\n\n\u001b[1m> Entering new None chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mHello world! ðŸ‘‹\n\nHow can I help you today? If you have any questions or need assistance, just let me know!\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n",
      "type": "message",
      "component": "Chat Output"
    }
    ```

Your flow is now running as a lightweight API endpoint, with only the flow's required dependencies and no visual builder installed.
Users who call your endpoint don't need to install Langflow or configure their own LLM provider keys.

To make your server publicly accessible, use a [tunneling service like ngrok](/deployment-public-server), or deploy to a public cloud provider such as [DigitalOcean](/deployment-nginx-ssl).

### LFX response schema

The LFX server's response schema is different from the Langflow API `/run` endpoint's schema.
Requests to the LFX server's `/flows/{flow_id}/run` endpoint return the following fields:

```json
{
  "result": "string",      // Output result from the flow execution
  "success": true,         // Whether execution was successful
  "logs": "string",        // Captured logs from execution
  "type": "message",       // Type of result
  "component": "string"    // The component that generated the result (for example, "Chat Output")
}
```

To view the LFX server's API docs and schema, see the `/docs` endpoint at `http://localhost:8000/docs`.

### LFX serve options

| Option                                      | Description                                                                                                          |
|---------------------------------------------|----------------------------------------------------------------------------------------------------------------------|
| `--check-variables` / `--no-check-variables` | Check global variables for environment compatibility. Default: `--check-variables`.                                 |
| `--env-file`                                | Path to the `.env` file containing environment variables.                                                           |
| `--host`, `-h`                              | Host to bind the server to. Default: `127.0.0.1`.                                                                   |
| `--log-level`                               | Logging level. One of: `debug`, `info`, `warning`, `error`, `critical`. Default: `warning`.                         |
| `--port`, `-p`                              | Port to bind the server to. Default: `8000`.                                                                        |
| `--verbose`, `-v`                           | Show diagnostic output and execution details.                                                                       |
| `--flow-json`                               | Read inline flow JSON content as a string. For example: `uv run lfx serve --flow-json '{...}'`, where `{...}` is the flow JSON definition. |
| `--stdin`                                   | Read JSON flow content from `stdin`. For example: `cat flow.json \| uv run lfx serve --stdin`.                              |

## Run the simple agent flow with `lfx run` {#run}

The `lfx run` command runs a flow from a JSON file without serving it, and the output is sent to `stdout`.
Input to `lfx run` can be a path to the JSON file, inline JSON passed with `--input-value`, or read from `stdin`.
No Langflow API key is required.

This example uses the **Agent** component's built-in OpenAI model, which requires an OpenAI API key.
If you want to use a different provider, edit the model provider, model name, and credentials accordingly.

1. Export your variables in the same terminal session where you'll run the flow:
    ```bash
    export OPENAI_API_KEY="sk-..."
    ```

2. Install dependencies.

   <PartialLfxDependencies />

3. Run the flow from a flow JSON file.
    ```bash
    uv run lfx run simple-agent-flow.json "Hello world"
    ```

    This flow expects a [Message](/data-types#message) input, which is a simple text string.

    You can also use the `--input-value` flag instead of a positional argument:
    ```bash
    uv run lfx run simple-agent-flow.json --input-value "Hello world"
    ```

    The `--input-value` flag is required when using `--stdin` or `--flow-json` options, since those options use the positional argument for the flow definition instead of the input value.

In addition to running flows from JSON files, `lfx run` supports other input methods, which are described in the following sections.

### Run flows from stdin

The `--stdin` option allows you to run flows that come from dynamic sources such as APIs or databases, or when you want to modify a flow before execution.
The command reads the flow's JSON definition from `stdin`, validates the JSON structure, and runs the flow.
The `--input-value` flag is required when using `--stdin`.

This example reads a flow JSON from stdin.
```bash
cat simple-agent-flow.json | uv run lfx run --stdin \
  --input-value "Hello world" \
  --format json | jq '.result'
```

This example fetches a flow JSON from a remote API endpoint and runs it:
```bash
curl https://api.example.com/flows/my-agent-flow | uv run lfx run --stdin \
  --input-value "Hello world"
```

Running a flow with `stdin` allows you to modify flows created in the visual builder before execution.
This example demonstrates changing the OpenAI model to `gpt-4o` before running the flow:
```bash
cat simple-agent-flow.json | jq '(.data.nodes[] | select(.data.node.template.model_name.value) | .data.node.template.model_name.value) = "gpt-4o"' | \
  uv run lfx run --stdin \
  --input-value "Hello world" \
  --format json | jq '.result'
```

### Run flows with inline JSON

Instead of piping from `stdin` or reading from a JSON file, you can pass the flow JSON directly as a string argument.
The `--input-value` flag is required when using `--flow-json`.

```bash
uv run lfx run --flow-json '{"data": {"nodes": [...], "edges": [...]}}' \
  --input-value "Hello world"
```

### LFX run options

| Option                                         | Description                                                                                      |
|------------------------------------------------|--------------------------------------------------------------------------------------------------|
| `--check-variables`/`--no-check-variables`     | Validates the flow's global variables. Default: check.                           |
| `--flow-json`                                  | Loads inline JSON flow content as a string.                             |
| `--format`, `-f`                               | Output format. Accepts `json`, `text`, `message`, or `result`. Default: `json`.                |
| `--input-value`                                | Input value to pass to the graph.                         |
| `--stdin`                                      | Read JSON flow content from `stdin`.                                 |
| `--timing`                                     | Include detailed timing information in output.                                                  |
| `--verbose`, `-v`                              | Show basic progress information and diagnostic output.                                         |
| `-vv`                                          | Show detailed progress and debug information.                                                   |
| `-vvv`                                         | Show full debugging output including component logs.                                            |

### Use LFX run to create an application

:::info
This feature is still a work in progress.
:::

In addition to running flows from JSON files, you can use `lfx run` with Python scripts that define flows programmatically.
This approach allows you to create flows directly in Python code without the visual builder.

For a complete example of creating an agent flow programmatically using LFX components, see the [Complete Agent Example on PyPI](https://pypi.org/project/lfx).