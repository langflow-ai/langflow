---
title: Run flows with Langflow Executor (LFX)
slug: /lfx-stateless-flows
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Langflow Executor (LFX) is a command-line tool that serves and runs flows statelessly from [flow JSON files](/concepts-flows-import) with minimal dependencies.

Flows are run without the flow builder UI or database, and any flow dependencies are automatically added to complete the run.
The flow graph is stored in memory at all times, so there is less overhead for loading the graph from a database.
Running a flow with LFX is similar to running flows with the `--backend-only` environment variable enabled, but even more lightweight, because the Langflow package and all of its dependencies don't need to be installed.

Use LFX to share flows with other developers, test flows in different environments, and run flows in production applications without requiring the full Langflow UI or database setup.

LFX includes two commands for executing flows:

* [`lfx serve`](#serve): This command starts a FastAPI server hosting a Langflow API endpoint with your flow available at `/flows/{flow_id}/run`. The actual value for `flow_id` is displayed when the server starts. See the [API reference](/api-flows-run) for more details.
* [`lfx run`](#run): This command executes a flow locally and returns the results to `stdout`.

## Prerequisites

- Install [Python](https://www.python.org/downloads/release/python-3100/)
- Install [uv](https://docs.astral.sh/uv/getting-started/installation/)
- Create or download a [flow JSON file](/concepts-flows)
- Create an [OpenAI API key](https://platform.openai.com/api-keys)
- Create a [Langflow API key](/api-keys-and-authentication)

## Install LFX

<Tabs>
<TabItem value="source" label="Clone Repository" default>

1. Clone the Langflow repository:
   ```bash
   git clone https://github.com/langflow-ai/langflow
   ```

2. Change directory to `langflow/src/lfx`:
   ```bash
   cd langflow/src/lfx
   ```

3. Run LFX commands using `uv run`:
   ```bash
   uv run lfx serve my_flow.json
   ```

</TabItem>
<TabItem value="pypi" label="Install from PyPI">

1. Create and activate a virtual environment.

   ```bash
   uv venv lfx-venv
   source lfx-venv/bin/activate  # On Windows: lfx-venv\Scripts\activate
   ```

2. Install the LFX package from PyPI:

   ```bash
   uv pip install lfx
   ```

3. Run LFX commands directly:

   ```bash
   uv lfx serve simple-agent-flow.json
   ```

</TabItem>
<TabItem value="uvx" label="Run without installing">

Run LFX without installing it using `uvx`:

```bash
uvx lfx serve simple-agent-flow.json
```

This automatically downloads and runs LFX in a temporary environment without permanent installation.

</TabItem>
</Tabs>

## Serve the simple agent starter flow with `lfx serve` {#serve}

To serve a flow as a REST API endpoint, set a `LANGFLOW_API_KEY` and run the flow JSON.
The API key is required for security because `lfx serve` can create a publicly accessible FastAPI server.
For more information, see [API keys and authentication](/api-keys-and-authentication).

This example uses the **Agent** component's built-in OpenAI model, which requires an OpenAI API key.
If you want to use a different provider, edit the model provider, model name, and credentials accordingly.

1. Set up your environment variables.

    <Tabs>
    <TabItem value="env-file" label=".env file" default>

    Create a `.env` file and populate it with your flow's variables.
    The `LANGFLOW_API_KEY` is required.
    This example assumes the flow requires an OpenAI API key.

    ```bash
    LANGFLOW_API_KEY="sk..."
    OPENAI_API_KEY="sk-..."
    ```

    </TabItem>
    <TabItem value="export" label="Export variables">

    Export your variables in the same terminal session where you'll start the server.
    You must declare your variables before the server starts for the server to pick them up.

    ```bash
    export LANGFLOW_API_KEY="sk..."
    export OPENAI_API_KEY="sk-..."
    ```

    </TabItem>
    </Tabs>

2. Start the server with your variable values.

    <Tabs>
    <TabItem value="env-file" label=".env file" default>

    This example assumes your flow file and `.env` file are in the current directory:

    ```
    uv run lfx serve simple-agent-flow.json --env-file .env
    ```

    If your `.env` file is in a different location, provide the full or relative path:

    ```
    uv run lfx serve simple-agent-flow.json --env-file /path/to/.env
    ```

    </TabItem>
    <TabItem value="export" label="Export variables">

    If you exported your variables, the command to start the server automatically picks up the values when it starts.

    ```
    uv run lfx serve simple-agent-flow.json
    ```

    To export new values, stop the server, export the variables, and start the server again.

    </TabItem>
    </Tabs>



3. The startup process displays a `flow_id` value in the output.
   Copy the `flow_id` to use in the test API call in the next step.
   In this example, the `flow_id` is `c1dab29d-3364-58ef-8fef-99311d32ee42`.

    ```bash
     â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LFX Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
     â”‚ ðŸŽ¯ Single Flow Served Successfully!                                  â”‚
     â”‚                                                                      â”‚
     â”‚ Source: /Users/mendonkissling/Downloads/simple-agent-flow.json       â”‚
     â”‚ Server: http://127.0.0.1:8000                                        â”‚
     â”‚ API Key: sk-...                                                 â”‚
     â”‚                                                                      â”‚
     â”‚ Send POST requests to:                                               â”‚
     â”‚ http://127.0.0.1:8000/flows/c1dab29d-3364-58ef-8fef-99311d32ee42/run â”‚
     â”‚                                                                      â”‚
     â”‚ With headers:                                                        â”‚
     â”‚ x-api-key: sk-...                                               â”‚
     â”‚                                                                      â”‚
     â”‚ Or query parameter:                                                  â”‚
     â”‚ ?x-api-key=sk-...                                               â”‚
     â”‚                                                                      â”‚
     â”‚ Request body:                                                        â”‚
     â”‚ {'input_value': 'Your input message'}                                â”‚
     â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
    ```

4. In a new terminal, export your `flow_id` and Langflow API key values as variables.
    ```bash
    export LANGFLOW_API_KEY="sk..."
    export FLOW_ID="c1dab29d-3364-58ef-8fef-99311d32ee42"
    ```

5. Test the server with an API call to the `/flows/flow_id/run` endpoint.

    ```bash
    curl -X POST http://localhost:8000/flows/$FLOW_ID/run \
      -H "Content-Type: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -d '{"input_value": "Hello, world!"}'
    ```

    Successful response:
    ```
    {"result":"Hello world! ðŸ‘‹\n\nHow can I help you today? If you have any questions or need assistance, just let me know!","success":true,"logs":"\n\n\u001b[1m> Entering new None chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mHello world! ðŸ‘‹\n\nHow can I help you today? If you have any questions or need assistance, just let me know!\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","type":"message","component":"Chat Output"}%
    ```

Your flow is now running as a lightweight API endpoint, with only the flow's required dependencies and no visual builder installed.
Users who call your endpoint don't need to install Langflow or configure their own LLM provider keys.

To make your server publicly accessible, use a [tunneling service like ngrok](/deployment-public-server), or deploy to a public cloud provider such as [DigitalOcean](/deployment-nginx-ssl).

### LFX serve options

**Table: Command-line options for `lfx serve`**
| Option                                 | Description                                                                                   |
|-----------------------------------------|-----------------------------------------------------------------------------------------------|
| `--host`, `-h`                         | Host to bind server. Default: `127.0.0.1` (localhost only). Use `0.0.0.0` to make it publicly accessible from other machines. |
| `--port`, `-p`                         | Port to bind server. Default:`8000`.                                                         |
| `--verbose`, `-v`                      | Display diagnostic output.                                                                        |
| `--env-file`                           | The path to the `.env` file.                                                                           |
| `--log-level`                          | Set logging level. Options are `debug`, `info`, `warning`, `error`, or `critical`.                           |
| `--check-variables`/`--no-check-variables` | Check global variables for environment variables.

## Run a flow with `lfx run` {#run}

The `lfx run` command runs a flow from a JSON file without serving it, and the output is sent to `stdout`.
Input to `lfx run` can be a path to the JSON file, inline JSON passed with `--input-value`, or read from `stdin`.

To run the flow from a JSON file with an input message, run:
    ```
    uv run lfx run my_flow.json "your input message"
    ```

If the flow expects structured input, pass it as a JSON string.
You can provide the input as a positional argument or with the `--input-value` flag.
The `--input-value` flag is especially useful for structured input, as it avoids ambiguity.
    ```
    uv run lfx run my_flow.json --input-value '{"question": "What is the weather in Paris?", "context": "weather"}'
    ```

To run a flow from `stdin` and extract the `result` field from the output, run the `lfx run` command with additional options:
    ```
    cat my_flow.json | uv run lfx run --stdin --format json | jq '.result'
    ```

### LFX run options

**Table: Command-line options for `lfx run`**

| Option                                         | Description                                                                                      |
|------------------------------------------------|--------------------------------------------------------------------------------------------------|
| `--format`, `-f`                               | Output format (`json`, `text`, `message`, `result`). Default: `json`                             |
| `--verbose`                                    | Show diagnostic output                                                                           |
| `--input-value`                                | Input value to pass to the graph (alternative to positional argument)                            |
| `--flow-json`                                  | Inline JSON flow content as a string                                                             |
| `--stdin`                                      | Read JSON flow from stdin                                                                        |
| `--check-variables`/`--no-check-variables`     | Check global variables for environment variables