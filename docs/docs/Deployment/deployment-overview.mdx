---
title: Langflow deployment overview
slug: /deployment-overview
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

This section includes the different ways to bring your locally-built flows to the world.

* To self-host your local server through an Ngrok gateway, see [Deploy a public Langflow server](/deployment-public-server).
This approach uses [ngrok](https://ngrok.com/docs/getting-started/) to forward traffic and share your local Langflow server over the internet, without deploying to a cloud provider or exposing your network directly.

* To build and deploy a Langflow container that includes your flow files, see [Containerize a Langflow application](develop-application).
This approach bundles your flows and dependencies into a portable, reproducible Docker image for easy deployment across different environments.

* To deploy a Langflow server on a remote server with Docker and Caddy, see [Deploy Langflow on a remote server](/deployment-caddyfile).
This approach is good for hosting your own Langflow instance on a remote server with secure web access, using Docker containers and Caddy as a reverse proxy for HTTPS support.

* To deploy Langflow on Kubernetes, see [Langflow Kubernetes architecture and best practices](/deployment-prod-best-practices)
This approach creates production-grade deployments with high availability, scalability, and robust orchestration.

For cloud provider-specific deployment guides, see the individual provider pages.

<!--TODO: Verify these scenarios-->
<!--You can host a Langflow server 24x7 and have all of your apps call that fixed server to run your flows. You can bundle the Langflow package as a dependency of a larger application and run the whole thing all together. You can containerize Langflow and serve it as a microservice for a larger application comprised of microservices like a website.-->