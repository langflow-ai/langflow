---
title: Langflow architecture and best practices on Kubernetes
slug: /deployment-prod-best-practices
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Langflow can be deployed in two distinct environments.

* [**Langflow IDE**](/deployment-kubernetes-dev): The **Langflow IDE** includes the frontend for visual development of your flow. The default [docker-compose.yml](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file hosted in the Langflow repository builds the Langflow IDE image. The Langflow IDE can be deployed on [Docker](/deployment-docker) or [Kubernetes](/deployment-kubernetes-dev).
* [**Langflow runtime**](/deployment-kubernetes-prod): The **Langflow runtime** is a headless or backend-only mode. The server exposes your flow as an endpoint, and runs only the processes necessary to serve your flow, with PostgreSQL as the database for improved scalability. Use the Langflow **runtime** to deploy your flows if you don't require the frontend for visual development. The Langflow runtime can be deployed on [Docker](/deployment-docker) or [Kubernetes](/deployment-kubernetes-prod).

:::tip
You can start Langflow in headless mode with the [LANGFLOW_BACKEND_ONLY](/environment-variables#LANGFLOW_BACKEND_ONLY) environment variable.
:::

Deploying on Kubernetes offers the following advantages:

* **Scalability**: Kubernetes allows you to scale the Langflow service to meet the demands of your workload.
* **Availability and resilience**: Kubernetes provides built-in resilience features, such as automatic failover and self-healing, to ensure that the Langflow service is always available.
* **Security**: Kubernetes provides security features, such as role-based access control and network isolation, to protect the Langflow service and its data.
* **Portability**: Kubernetes is a portable platform, which means that you can deploy the Langflow service to any Kubernetes cluster, on-premises or in the cloud.

Langflow can be deployed on cloud deployments like **AWS EKS, Google GKE, or Azure AKS**. For more information about deploying Langflow on AWS EKS, Google GKE, or Azure AKS, see the [Langflow Helm charts repository](https://github.com/langflow-ai/langflow-helm-charts).

## Langflow deployment

A typical Langflow deployment includes:

* **Langflow API and UI**: The Langflow service is the core component of the Langflow platform. It provides a RESTful API for executing flows.
* **Kubernetes cluster**: The Kubernetes cluster provides a platform for deploying and managing the Langflow service and its supporting components.
* **Persistent storage**: Persistent storage is used to store the Langflow service's data, such as models and training data.
* **Ingress controller**: The ingress controller provides a single entry point for traffic to the Langflow service.
* **Load balancer**: Balances traffic across multiple Langflow replicas.
* **Vector database**: If you are using Langflow for RAG, you can integrate with the vector database in Astra Serverless.

![Langflow reference architecture on Kubernetes](/img/langflow-reference-architecture.png)

## Environment isolation

It is recommended to deploy and run two separate environments for Langflow, with one environment reserved for development use and another for production use.

![Langflow environments](/img/langflow-env.png)

* **The Langflow development environment** must include the Integrated Development Environment (IDE) for the full experience of Langflow, optimized for prototyping and testing new flows.
* **The Langflow production environment** executes the flow logic in production and enables Langflow flows as standalone services.

## Why is it important to have separate deployments?

This separation is designed to enhance security, optimize resource allocation, and streamline management.

* **Security**
  * **Isolation**: By separating the development and production environments, you can better isolate different phases of the application lifecycle. This isolation minimizes the risk of development-related issues impacting the production environments.
  * **Access control**: Different security policies and access controls can be applied to each environment. Developers may require broader access in the IDE for testing and debugging, while the runtime environment can be locked down with stricter security measures.
  * **Reduced attack surface**: The runtime environment is configured to include only essential components, reducing the attack surface and potential vulnerabilities.
* **Resource allocation**
  * **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.
  * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.

## Scaling Resources

Langflow's resource requirements vary depending on whether you're deploying the IDE for development or the runtime for production flows. Below are detailed strategies for scaling RAM, disk, instances, and users per instance.

**IDE vs. Runtime**:

* **IDE**: Deploy for developers using the UI. Requires frontend (512Mi RAM, 0.3 CPU) and backend (1Gi RAM, 0.5 CPU) services.
* **Runtime**: Deploy for production flows. Headless, requiring 2Gi RAM and 1 CPU per instance, focused on API endpoints.

### Example Resource Configuration

| Component | RAM Request | CPU Request | Replica Count | Notes |
|-----------|-------------|-------------|---------------|-------|
| IDE Backend | 1Gi | 0.5 | 1 | Scale replicas for more developers. |
| IDE Frontend | 512Mi | 0.3 | 1 | Adjust based on UI load. |
| Runtime | 2Gi | 1000m | 3 | Use HPA for dynamic scaling. |
| PostgreSQL | 4Gi | 2 | 1+ | Use replication for high availability. |

### RAM

**Overview**: RAM usage in Langflow depends on flow complexity, the size of language models, and concurrent request volume. The runtime, used for production flows, has a baseline requirement, while the IDE (for developers) requires additional resources for the frontend and backend.

**Base Requirements**:

* **Runtime**: 2Gi per instance (Langflow Helm Charts - Runtime).
* **IDE Backend**: 1Gi per instance.
* **IDE Frontend**: 512Mi per instance (Langflow Helm Charts - IDE).

**Factors Affecting RAM Usage**:

* **Flow Complexity**: Flows with many nodes or large datasets (e.g., RAG pipelines) increase memory needs.
* **Language Models**: Larger LLMs or embeddings loaded in-memory consume significant RAM.
* **Concurrent Requests**: More simultaneous API calls or UI sessions require additional memory.

**Scaling Recommendations**:

* Start with 2Gi for runtime instances and 1.5Gi total (1Gi backend + 512Mi frontend) for IDE instances.
* Monitor memory usage with tools like Prometheus to identify bottlenecks.
* Adjust memory requests and limits in Kubernetes. Example configuration:

```yaml
resources:
  requests:
    memory: "2Gi"
    cpu: "1000m"
  limits:
    memory: "4Gi"
    cpu: "2000m"
```

* For intensive use (e.g., multi-core CPU setups), allocate 4Gi or more per instance.

### Disk

**Overview**: Disk storage is used for the database (SQLite or PostgreSQL) and file storage for large files (e.g., documents for RAG). The database stores flow configurations, user data, and settings, while files are managed on disk.

**Usage**:

* **Database**: Stores flow definitions, user profiles, and logs. Size depends on the number of flows and users.
* **File Storage**: Large files are stored in directories like `/opt/langflow/data/` or `.cache/langflow/`.

**Scaling Recommendations**:

* Use an external PostgreSQL database for production to improve scalability and reliability.
* Configure shared file storage (e.g., NFS or cloud storage) for multi-instance setups to ensure file access across replicas.
* Estimate initial database size based on expected flows and users (e.g., 10GB for moderate use) and monitor growth.
* Use Persistent Volumes in Kubernetes for database and file storage, with dynamic provisioning for scalability.

### Instances

**Overview**: Scaling instances involves adding more replicas to handle increased load, applicable to both IDE and runtime deployments. Horizontal scaling is preferred for production environments.

**Base Configuration**:

* **Runtime**: Default replica count of 3 (Langflow Helm Charts - Runtime).
* **IDE**: Default replica count of 1 for both backend and frontend (Langflow Helm Charts - IDE).

**Scaling Recommendations**:

* Implement Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically adjust replicas based on CPU or memory usage. Example HPA configuration:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langflow-runtime-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langflow-runtime
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
```

* For bursty workloads (e.g., 100,000s of tokens per second), ensure sufficient replicas to handle spikes.
* Vertically scale by increasing CPU/memory requests for complex flows, but prioritize horizontal scaling for reliability.

### Users per Instance

**Overview**: The number of users or requests per instance depends on whether you're supporting developers via the IDE or API clients via the runtime.

**UI (IDE)**:

* Each developer using the UI generates requests to the backend, requiring resources for session handling.
* Scale backend replicas based on concurrent developers (e.g., 1 replica per 10-20 developers, adjusted via load testing).

**Production Flows (Runtime)**:

* Users are typically API clients making requests to flow endpoints.
* Capacity depends on request rate and flow complexity. For example, a single instance with 2vCPUs and 2GB RAM can handle ~30 concurrent connections for simple flows.
* Perform load testing to determine the number of requests per instance and scale replicas accordingly.

**Scaling Recommendations**:

* Use load balancers to distribute requests across replicas.
* Monitor API request rates and response times to adjust replica counts.
* For IDE, ensure frontend and backend replicas are balanced to avoid bottlenecks.

## Failure Points

Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability.

**Database Failure**:

* **Impact**: Disrupts flow retrieval, saving, user authentication, user management, project collection access, configuration updates, and log writing.
* **Mitigation**: Use a replicated PostgreSQL setup with high availability and regular backups. Flows already loaded in memory may continue to function.

**File System Issues**:

* **Impact**: Concurrency issues in file caching (e.g., `/app/data/.cache`) can cause IO errors in multi-instance setups.
* **Mitigation**: Use a shared, POSIX-compliant file system or cloud storage. Avoid ramdisk solutions due to data loss on container shutdown.

**Instance Failures**:

* **Impact**: A single instance failure can disrupt service if not replicated.
* **Mitigation**: Deploy multiple replicas with Kubernetes to ensure availability. Use health checks to detect and replace failed pods.

**Network and Dependency Failures**:

* **Impact**: External APIs or services used in flows may fail, causing flow errors.
* **Mitigation**: Implement retry logic and error handling in flows. Monitor network latency and dependency health.

## Monitoring Recommendations

Effective monitoring ensures Langflow operates reliably and performs well under varying loads.

**Database Health**:

* Monitor availability, query performance, and resource usage (CPU, memory, disk).
* Use tools like pgAdmin or cloud-native monitoring for PostgreSQL.

**Application Logs**:

* Collect and analyze logs for errors, warnings, and flow execution issues.
* Centralize logs using tools like ELK Stack or Fluentd.

**Resource Usage**:

* Track CPU, memory, and disk usage of Langflow instances.
* Use Prometheus and Grafana for real-time monitoring in Kubernetes.

**API Performance**:

* Monitor response times, error rates, and request throughput.
* Set alerts for high latency or error spikes.

**Observability Tools**:

* Integrate with LangSmith or LangFuse for detailed flow tracing and metrics.
* Use these tools to debug flow performance and optimize execution.

**Example Monitoring Setup**:

* Deploy Prometheus for metrics collection.
* Use Grafana dashboards to visualize resource usage and API performance.
* Configure alerts for critical thresholds (e.g., 90% memory usage, 500ms API latency).

## Security Implications

Running Langflow in production requires robust security measures to protect the application, data, and users.

**Container Security**:

* Enable `readOnlyRootFilesystem: true` in runtime containers to prevent unauthorized modifications.
* Only disable if necessary and with compensating controls.

**Secret Management**:

* Store sensitive data (e.g., API keys) in Kubernetes secrets or external secret managers.
* Avoid embedding secrets in flow JSON files.

**Authentication and Authorization**:

* Implement strong authentication for the IDE UI and runtime API (e.g., OAuth, API tokens).
* Enforce role-based access control to limit user permissions.

**Data Privacy**:

* Ensure compliance with regulations like GDPR if handling personal data.
* Encrypt sensitive data at rest and in transit.

**Encryption**:

* Use HTTPS for all communications to secure data in transit.
* Configure TLS for PostgreSQL connections.

**Additional Security Measures**:

* Conduct regular security audits and apply software updates.
* Restrict network access to Langflow services using firewalls or network policies.
* Monitor for suspicious activity using intrusion detection systems.
