---
title: Langflow in Production Best Practices
slug: /deployment-langflow-prod-best-practices
---
 

# Langflow in Production Best Practices

[**Langflow deployment Best Practices	1**](#langflow-deployment-best-practices)

[**Langflow Components	2**](#langflow-components)

[**Environment Segregation	2**](#environment-segregation)

[Why is it important to have separate deployments?	3](#why-is-it-important-to-have-separate-deployments?)

[**Setting up Langflow Development environment	4**](#setting-up-langflow-development-environment)

[Sample Deployment	4](#sample-deployment)

[Prerequisites	4](#prerequisites)

[Prepare a Kubernetes cluster	4](#prepare-a-kubernetes-cluster)

[Install the Langflow IDE Helm chart	4](#install-the-langflow-ide-helm-chart)

[Configure port forwarding to access Langflow	5](#configure-port-forwarding-to-access-langflow)

[Configure the Langflow version	6](#configure-the-langflow-version)

[Configure External storage for Development environment	6](#configure-external-storage-for-development-environment)

[Scaling Development Environment	7](#scaling-development-environment)

[**Promoting flows from Langflow Development to Langflow Production	8**](#promoting-flows-from-langflow-development-to-langflow-production)

[Flow Lifecycle	8](#flow-lifecycle)

[Download flows options	9](#download-flows-options)

[**Setting up Langflow Production environment	9**](#setting-up-langflow-production-environment)

[Sample Deployment	9](#sample-deployment-1)

[Prerequisites	9](#prerequisites-1)

[Prepare a Kubernetes cluster	10](#prepare-a-kubernetes-cluster-1)

[Install the Langflow runtime Helm chart	10](#install-the-langflow-runtime-helm-chart)

[Access the Langflow app API	11](#access-the-langflow-app-api)

[Deploy the flow	11](#deploy-the-flow)

[Upgrade Langflow version	13](#upgrade-langflow-version)

[Secrets Management in Production	13](#secrets-management-in-production)

[Using values.yaml	13](#using-values.yaml)

[Using Helm Commands	13](#using-helm-commands)

[Configure Logging	14](#configure-logging)

[Scaling Production Environment	14](#scaling-production-environment)

[Scale horizontally	14](#scale-horizontally)

[Scale vertically	14](#scale-vertically)

[**RAG/Agent Application Architecture with Langflow	15**](#rag/agent-application-architecture-with-langflow)

[**Flow lifecycle in Production	16**](#flow-lifecycle-in-production)

# Langflow deployment Best Practices {#langflow-deployment-best-practices}

While Langflow offers flexible deployment options, **deploying on a Kubernetes cluster is highly recommended for production environments.**

Deploying on Kubernetes offers following advantages:

* **Scalability**: Kubernetes allows you to scale the Langflow service to meet the demands of your workload.  
* **Availability and Resilience**: Kubernetes provides built-in resilience features, such as automatic failover and self-healing, to ensure that the Langflow service is always available.  
* **Security**: Kubernetes provides security features, such as role-based access control and network isolation, to protect the Langflow service and its data.  
* **Portability**: Kubernetes is a portable platform, which means that you can deploy the Langflow service to any Kubernetes cluster, on-premises or in the cloud.

Langflow can be easily deployed on Cloud deployments like **AWS EKS, Google GKE, or Azure AKS**.

# Langflow Components {#langflow-components}

A typical Langflow deployment includes:  
 

* **Langflow API & UI** – The Langflow service is the core component of the Langflow platform. It provides a RESTful API for executing flows.  
* **Kubernetes Cluster** – The Kubernetes cluster provides a platform for deploying and managing the Langflow service and its supporting components.  
* **Persistent Storage:** Persistent storage is used to store the Langflow service's data, such as models and training data.  
* **Ingress Controller** – The ingress controller provides a single entry point for traffic to the Langflow service.  
* **Load Balancer** – Balances traffic across multiple Langflow replicas.  
* **Vector Database** – If using Langflow for RAG, Vector DB (e.g., Astra DB) can be integrated.

# Environment Segregation {#environment-segregation}

It is recommended to deploy and run two separate environments for Langflow. 

One environment to be reserved for Development use and another for Production use. 

* **The Langflow Development environment** must include the Integrated Development Environment (IDE) for full experience of Langflow, optimized for prototyping and testing new flows.  
* **The Langflow Production environment** executes the flow logic in production and productionizes Langflow flows as standalone services.

## Why is it important to have separate deployments? {#why-is-it-important-to-have-separate-deployments?}

This separation is designed to enhance security, optimize resource allocation, and streamline management. Understanding the rationale behind these deployment options will help you make informed decisions about how to best deploy and manage your applications.

* **Security**  
  * **Isolation**: By separating the development and production environments, we can better isolate different phases of the application lifecycle. This isolation minimizes the risk of development-related issues impacting the production environment.  
  * **Access Control**: Different security policies and access controls can be applied to each environment. Developers may require broader access in the IDE for testing and debugging, whereas the runtime environment can be locked down with stricter security measures.  
  * **Reduced Attack Surface**: The runtime environment is configured to only include essential components, reducing the attack surface and potential vulnerabilities.  
* **Resource Allocation**  
  * **Optimized Resource Usage and cost efficiency**: By separating the two, we can allocate resources more effectively. Additionally, each flow can be deployed independently, providing fine-grained resource control.  
  * **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.  
    

# Setting up Langflow Development environment  {#setting-up-langflow-development-environment}

Helm chart is available for Langflow IDE and should be used for setting up the Langflow Development  environment.

[Langflow Integrated Development Environment (IDE)](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-ide) chart is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the API and the UI.

```
helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
helm repo update
helm install langflow-ide langflow/langflow-ide -n langflow --create-namespace
```

You can install the Langflow IDE Helm chart for Langflow as an IDE with persistent storage or an external database (for example PostgreSQL).

### Sample Deployment {#sample-deployment}

##### Prerequisites {#prerequisites}

* A Kubernetes cluster  
* kubectl  
* Helm

##### Prepare a Kubernetes cluster {#prepare-a-kubernetes-cluster}

This example uses Minikube, but you can use any Kubernetes cluster.

Create a Kubernetes cluster on Minikube.

```
minikube start
```

Set kubectl to use Minikube.

```
kubectl config use-context minikube
```

##### Install the Langflow IDE Helm chart {#install-the-langflow-ide-helm-chart}

1. Add the repository to Helm and update it.

```
helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
helm repo update
```

   

2. Install Langflow with the default options in the **langflow-dev** namespace.

```
(base) nidhimishra@Nidhi-Mishra-C0W2CD4XLQ ~ % helm install langflow langflow/langflow-ide -n langflow-dev --create-namespace 
NAME: langflow
LAST DEPLOYED: Thu Mar 27 15:29:37 2025
NAMESPACE: langflow-dev
STATUS: deployed
REVISION: 1
TEST SUITE: None

```

3. Check the status of the pods

```
(base) nidhimishra@Nidhi-Mishra-C0W2CD4XLQ ~ % kubectl get pods -n langflow-dev
NAME                                         READY   STATUS    RESTARTS   AGE
langflow-service-0                           1/1     Running   0          3m46s
langflow-service-frontend-84cf44b678-5rdjj   1/1     Running   0          3m46s

```

##### Configure port forwarding to access Langflow {#configure-port-forwarding-to-access-langflow}

Enable local port forwarding to access Langflow from your local machine.

1. To make the Langflow API accessible from your local machine at port 7860:  
   

```
(base) nidhimishra@Nidhi-Mishra-C0W2CD4XLQ ~ % kubectl port-forward -n langflow-dev svc/langflow-service-backend 7860:7860
Forwarding from 127.0.0.1:7860 -> 7860
Forwarding from [::1]:7860 -> 7860
```

   

2. To make the Langflow UI accessible from your local machine at port 8080:

```
(base) nidhimishra@Nidhi-Mishra-C0W2CD4XLQ ~ % kubectl port-forward -n langflow-dev svc/langflow-service 8080:8080
Forwarding from 127.0.0.1:8080 -> 8080
Forwarding from [::1]:8080 -> 8080
Handling connection for 8080
Handling connection for 8080
Handling connection for 8080
```

Now you can access:

1. The Langflow API at [http://localhost:7860](http://localhost:7860)  
2. The Langflow UI at [http://localhost:8080](http://localhost:8080)

### Configure the Langflow version {#configure-the-langflow-version}

Langflow is deployed with the latest version by default. 

To specify a different Langflow version, set the langflow.backend.image.tag and langflow.frontend.image.tag values in the [values.yaml](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-ide/values.yaml) file.

```
langflow:
  backend:
    image:
      tag: "v1.2.0.dev29"
  frontend:
    image:
      tag: "v1.2.0.dev29"
```

### Configure External storage for Development environment {#configure-external-storage-for-development-environment}

By default, the chart deploys a SQLite database stored in a local persistent disk. If you want to use an external PostgreSQL database, you can configure it in two ways:

* Use the built-in PostgreSQL chart:

```
postgresql:
  enabled: true
  auth:
    username: "langflow"
    password: "langflow-postgres"
    database: "langflow-db"
```


* Use an external database:

```
postgresql:
  enabled: false

langflow:
  backend:
    externalDatabase:
      enabled: true
      driver:
        value: "postgresql"
      port:
        value: "5432"
      user:
        value: "langflow"
      password:
        valueFrom:
          secretKeyRef:
            key: "password"
            name: "your-secret-name"
      database:
        value: "langflow-db"
    sqlite:
      enabled: false
```

### Scaling Development Environment {#scaling-development-environment}

Scale the number of replicas and resources for both frontend and backend services:

```

langflow:
  backend:
    replicaCount: 1
    resources:
      requests:
        cpu: 0.5
        memory: 1Gi
      # limits:
      #   cpu: 0.5
      #   memory: 1Gi

  frontend:
    enabled: true
    replicaCount: 1
    resources:
      requests:
        cpu: 0.3
        memory: 512Mi
      # limits:
      #   cpu: 0.3
      #   memory: 512Mi

```

More Examples of langflow-ide deployment are available [here](https://github.com/langflow-ai/langflow-helm-charts/tree/main/examples/langflow-ide) in the examples directory.

# Promoting flows from Langflow Development to Langflow Production {#promoting-flows-from-langflow-development-to-langflow-production}

## Flow Lifecycle {#flow-lifecycle}

1. The developer designs the business logic in the form of a Langflow Flow by arranging langglow components in a logical manner and connecting them.  
   1. Langflow’s Visual Editor   
   2. Templates  
   3. Langflow Store  
2. Components  can be built and verified  
3. Flow can be functionally tested in Langflow Playground and evaluated using integration ecosystem.  
4. Once good to go, they can be promoted to the Production environment.

**As best practice, all the flows must be versioned and maintained in a Git repository.**

There are two ways to move a flow to production environment:

1. **From a remote location:** You can reference a flow stored in a remote location, such as a URL or a Git repository by customizing the [values.yaml](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file in the section downloadFlows:

```
downloadFlows:
  flows:
    - url: https://raw.githubusercontent.com/datastax/langflow-charts/main/examples/flows/basic-prompting-hello-world.json
      endpoint: hello-world
```

##### Download flows options {#download-flows-options}

The downloadFlows section in the [values.yaml](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file allows you to download flows from remote locations. You can specify the following options:

1. url: The URL of the flow. Must point to a JSON file.  
   2. endpoint: Override the endpoint of the flow. By default, the endpoint is the UUID of the flow or anything you set in the flow (endpoint\_name key).  
   3. uuid: Override the UUID of the flow. If not specified, the UUID will be extracted from the flow file.  
   4. basicAuth: Basic authentication credentials in the form username:password.  
   5. headers: Custom headers to add to the request. For example, to add Authorization header for downloading from private repositories.  
        
2. **Packaging the flow as docker image:** You can add a flow from to a docker image based on Langflow runtime and refer to it in the chart. 

# Setting up Langflow Production environment {#setting-up-langflow-production-environment}

Helm chart is available for [Langflow Runtime](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) and should be used for setting up the Langflow Production  environment.

The [Langflow Runtime](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) chart is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation and security to ensure that applications run reliably and efficiently.

### Sample Deployment {#sample-deployment-1}

##### Prerequisites {#prerequisites-1}

* A Kubernetes cluster  
* kubectl  
* Helm

##### Prepare a Kubernetes cluster {#prepare-a-kubernetes-cluster-1}

This example uses Minikube, but you can use any Kubernetes cluster.

1. Create a Kubernetes cluster on Minikube.

```
minikube start
```

2. Set kubectl to use Minikube.

```
kubectl config use-context minikube
```

##### Install the Langflow runtime Helm chart {#install-the-langflow-runtime-helm-chart}

1. Add the repository to Helm.

```
helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
helm repo update
```

2. Install the Langflow app with the default options in the langflow namespace.  
   1. If you have created a custom image with packaged flows, you can deploy Langflow by overriding the default values.yaml file with the \--set flag.  
      1. Use a custom image with bundled flows:

```
helm install my-langflow-app langflow/langflow-runtime -n langflow --create-namespace --set image.repository=myuser/langflow-hello-world --set image.tag=1.0.0
```

      2. Alternatively, install the chart and download the flows from a URL with the \--set flag:

```
helm install my-langflow-app-with-flow langflow/langflow-runtime \
-n langflow-prod \
--create-namespace \
--set'downloadFlows.flows[0].url=https://raw.githubusercontent.com/langflow-ai/langflow/dev/tests/data/basic_example.json'
```

3. Check the status of the pods.

```
(base) nidhimishra@Nidhi-Mishra-C0W2CD4XLQ ~ % kubectl get pods -n langflow-prod
NAME                                         READY   STATUS             RESTARTS      AGE
langflow-langflow-runtime-7c89d4f69f-qf8np   0/1     CrashLoopBackOff   1 (15s ago)   21s

```

##### Access the Langflow app API {#access-the-langflow-app-api}

1. Get your service name.

```
kubectl get svc -n langflow
```

The service name is your release name followed by \-langflow-runtime. For example, if you had used helm install,  my-langflow-app-with-flow the service name is my-langflow-app-with-flow-langflow-runtime.

2. Enable port forwarding to access Langflow from your local machine:

```
kubectl port-forward -n langflow svc/my-langflow-app-with-flow-langflow-runtime 7860:7860
```

3. Confirm you can access the API at [http://localhost:7860/api/v1/flows/](http://localhost:7860/api/v1/flows/) and view a list of flows.

```
curl -v http://localhost:7860/api/v1/flows/
```

##### 

##### Deploy the flow {#deploy-the-flow}

Since the basic prompting needs an OpenAI Key, we need to create a secret with the key:

```
kubectl create secret generic langflow-secrets --from-literal=openai-key=sk-xxxx
```

This command will create a secret named langflow-secrets with the key openai-key containing your secret value.

We need to create a custom values.yaml file to:

1. Refer to the flow we want to deploy  
2. Plug the secret we created to the Langflow deployment

(custom-values.yaml)

```
downloadFlows:
  flows:
  - url: https://raw.githubusercontent.com/datastax/langflow-charts/main/examples/flows/basic-prompting-hello-world.json
    endpoint: hello-world

env:
  - name: LANGFLOW_LOG_LEVEL
    value: "INFO"
  - name: OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: langflow-secrets
        key: openai-key
```

See the full file at [basic-prompting-hello-world.yaml](https://raw.githubusercontent.com/datastax/langflow-charts/main/examples/flows/langflow-runtime/basic-prompting-hello-world.yaml)

Now we can deploy the chart (using option 1):

```
helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
helm repo update
helm install langflow-runtime langflow/langflow-runtime --values custom-values.yaml
```

Tunnel the service to localhost:

```
kubectl port-forward svc/langflow-langflow-runtime 7860:7860
```

Call the flow API endpoint using hello-world as flow name:

```
curl -X POST \
    "http://localhost:7860/api/v1/run/hello-world?stream=false" \
    -H 'Content-Type: application/json'\
    -d '{
      "input_value": "Hello there!",
      "output_type": "chat",
      "input_type": "chat"
    }'
```

### Upgrade Langflow version {#upgrade-langflow-version}

To change the Langflow version or use a custom docker image, you can modify the image parameter in the chart.

```
image:
  repository: "langflowai/langflow-backend"
  tag: 1.x.y
```

### Secrets Management in Production {#secrets-management-in-production}

**The recommended way to set sensitive information is to use Kubernetes secrets.** 

The env section in the [values.yaml](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file allows you to set environment variables for the Langflow deployment. 

###### *Using values.yaml* {#using-values.yaml}

You can reference a secret in the values.yaml file by using the valueFrom key.

```
env:
  - name: OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: langflow-secrets
        key: openai-key
  - name: ASTRA_DB_APPLICATION_TOKEN
    valueFrom:
      secretKeyRef:
        name: langflow-secrets
        key: astra-token
```

where:

* name: refer to the environment variable name used by your flow.  
* valueFrom.secretKeyRef.name: refers to the kubernetes secret name.  
* valueFrom.secretKeyRef.key: refers to the key in the secret. 

###### *Using Helm Commands* {#using-helm-commands}

1. To create a matching secret with the above example you can use the following command:

```
kubectl create secret generic openai-credentials \
  --namespace langflow \
  --from-literal=OPENAI_API_KEY=sk...
```

2. Verify the secret exists. The result is encrypted.

```
kubectl get secrets -n langflow openai-credentials
```

3. Upgrade the Helm release to use the secret.

```
helm upgrade my-langflow-app-image langflow/langflow-runtime -n langflow \
  --reuse-values \
  --set "extraEnv[0].name=OPENAI_API_KEY" \
  --set "extraEnv[0].valueFrom.secretKeyRef.name=openai-credentials" \
  --set "extraEnv[0].valueFrom.secretKeyRef.key=OPENAI_API_KEY"
```

### Configure Logging {#configure-logging}

Set the log level and other Langflow configurations in the [values.yaml](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file.

```
env:
  - name: LANGFLOW_LOG_LEVEL
```

### Scaling Production Environment {#scaling-production-environment}

You have the option to scale the Langflow production environment either Horizontally or Vertically to add more resources to the flows container.

* Horizontal scaling adds more replicas of the deployment  
* Vertical scaling adds more cpu/memory resources to the deployment

###### *Scale horizontally* {#scale-horizontally}

To scale horizontally you only need to modify the replicaCount parameter in the chart.

```
replicaCount: 5
```

Please note that if your flow relies on shared state (e.g. builtin chat memory), you will need to set up a shared database.

###### *Scale vertically* {#scale-vertically}

By default the deployment doesn't have any limits and it could consume all the node resources. In order to limit the available resources, you can modify the resources value:

```
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
```

```
    value: "INFO"
```

# RAG/Agent Application Architecture with Langflow {#rag/agent-application-architecture-with-langflow}

Thus Langflow acts as the execution engine for GenAI applications. Thus the server logic for execution of GenAI applications like RAG apps or Agent apps is offloaded by Langflow.

The GenAI application frontend can trigger these pre-built flows by simply making a call to Langlow REST endpoint exposed for that flow logic.

# Flow lifecycle in Production  {#flow-lifecycle-in-production}

