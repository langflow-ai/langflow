By design, vector data is essential for LLM applications, such as chatbots and agents.

While you can use an LLM alone for generic chat interactions and common tasks, you can take your application to the next level with context sensitivity (such as RAG) and custom datasets (such as internal business data).
This often requires integrating vector databases and vector searches that provide the additional context and define meaningful queries.

Langflow includes vector store components that can read and write vector data, including embedding storage, similarity search, Graph RAG traversals, and dedicated search instances like OpenSearch.
Because of their interdependent functionality, it is common to use vector store, language model, and embedding model components in the same flow or in a series of dependent flows.

To find available vector store components, browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred vector database provider.

:::tip
In agentic flows, you can use the Knowledge Base components to create co-located vector stores for improved context and retrieval by **Agent** components.
Rather than retrieve context from a remote vector store with each flow run, your flows can read and write to knowledge bases in Langflow storage.
You can use the same knowledge base across multiple flows, and you can create additional knowledge bases as needed.
:::