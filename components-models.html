<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Components/components-models" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Models | Langflow Documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://docs.langflow.org/components-models"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Models | Langflow Documentation"><meta data-rh="true" name="description" content="Model components generate text using large language models."><meta data-rh="true" property="og:description" content="Model components generate text using large language models."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://docs.langflow.org/components-models"><link data-rh="true" rel="alternate" href="https://docs.langflow.org/components-models" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.langflow.org/components-models" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://UZK6BDPCVY-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Models","item":"https://docs.langflow.org/components-models"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SLQFLQ3KPT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SLQFLQ3KPT",{})</script>





<link rel="search" type="application/opensearchdescription+xml" title="Langflow Documentation" href="/opensearch.xml">





<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Sora:wght@550;600&amp;display=swap">
<script>!function(){window.semaphore=window.semaphore||[],window.ketch=function(){window.semaphore.push(arguments)};var e=document.createElement("script");e.type="text/javascript",e.src="https://global.ketchcdn.com/web/v3/config/datastax/langflow_org_web/boot.js",e.defer=e.async=!0,document.getElementsByTagName("head")[0].appendChild(e)}()</script>
<script defer="true">!function(){const e=e=>{if(window.gtag&&e.purposes&&"analytics"in e.purposes&&"targeted_advertising"in e.purposes){const n=!0===e.purposes.analytics?"granted":"denied",t=!0===e.purposes.targeted_advertising?"granted":"denied",a={analytics_storage:n,ad_personalization:t,ad_storage:t,ad_user_data:t};window.gtag("consent","update",a)}};window.ketch&&window.ketch("on","consent",e)}()</script><link rel="stylesheet" href="/assets/css/styles.530387df.css">
<script src="/assets/js/runtime~main.444decc7.js" defer="defer"></script>
<script src="/assets/js/main.5efa91a8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top navbarHideable_m1mJ"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/lf-docs-light.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/langflow-ai/langflow" target="_blank" class="navbar__item navbar__link header-github-link"></a><a href="https://twitter.com/langflow_ai" target="_blank" class="navbar__item navbar__link header-twitter-link"></a><a href="https://discord.gg/EqksyE2EX9" target="_blank" class="navbar__item navbar__link header-discord-link"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd sidebarWithHideableNavbar_wUlq"><a tabindex="-1" class="sidebarLogo_isFc" href="/"><img src="/img/lf-docs-light.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="themedComponent_mlkZ themedComponent--dark_xIcU"></a><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/">Welcome to Langflow</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/get-started-installation">Get started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/basic-prompting">Templates</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/concepts-overview">Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/concepts-components">Components</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/concepts-components">Components overview</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-agents">Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-bundle-components">Bundles</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-custom-components">Create custom Python components</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-data">Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-embedding-models">Embeddings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-helpers">Helpers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-io">Inputs and outputs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-logic">Logic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-memories">Memories</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/components-models">Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-processing">Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-prompts">Prompts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-tools">Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/components-vector-stores">Vector stores</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/agents">Agents</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/mcp-server">Model Context Protocol (MCP)</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/configuration-api-keys">Configuration</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/develop-overview">Develop</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/deployment-overview">Deployment</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/api-reference-api-examples">API reference</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/integrations-apify">Integrations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/contributing-community">Contribute</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a href="https://github.com/langflow-ai/langflow/releases/latest" target="_blank" rel="noopener noreferrer" class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false">Changelog</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/troubleshoot">Support</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 sidebar-ad">
        <a href="https://astra.datastax.com/signup?type=langflow" target="_blank" class="menu__link">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-cloud"><path d="M17.5 19H9a7 7 0 1 1 6.71-9h1.79a4.5 4.5 0 1 1 0 9Z"/></svg>
          <div class="sidebar-ad-text-container">
            <span class="sidebar-ad-text">Use Langflow in the cloud</span>
            <span class="sidebar-ad-text sidebar-ad-text-gradient">Sign up for DataStax Langflow</span>
          </div>
        </a>
      </li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Components</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Models</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><style>[data-ch-theme="github-dark"] {  --ch-t-colorScheme: dark;--ch-t-foreground: #c9d1d9;--ch-t-background: #0d1117;--ch-t-lighter-inlineBackground: #0d1117e6;--ch-t-editor-background: #0d1117;--ch-t-editor-foreground: #c9d1d9;--ch-t-editor-lineHighlightBackground: #6e76811a;--ch-t-editor-rangeHighlightBackground: #ffffff0b;--ch-t-editor-infoForeground: #3794FF;--ch-t-editor-selectionBackground: #264F78;--ch-t-focusBorder: #1f6feb;--ch-t-tab-activeBackground: #0d1117;--ch-t-tab-activeForeground: #c9d1d9;--ch-t-tab-inactiveBackground: #010409;--ch-t-tab-inactiveForeground: #8b949e;--ch-t-tab-border: #30363d;--ch-t-tab-activeBorder: #0d1117;--ch-t-editorGroup-border: #30363d;--ch-t-editorGroupHeader-tabsBackground: #010409;--ch-t-editorLineNumber-foreground: #6e7681;--ch-t-input-background: #0d1117;--ch-t-input-foreground: #c9d1d9;--ch-t-input-border: #30363d;--ch-t-icon-foreground: #8b949e;--ch-t-sideBar-background: #010409;--ch-t-sideBar-foreground: #c9d1d9;--ch-t-sideBar-border: #30363d;--ch-t-list-activeSelectionBackground: #6e768166;--ch-t-list-activeSelectionForeground: #c9d1d9;--ch-t-list-hoverBackground: #6e76811a;--ch-t-list-hoverForeground: #c9d1d9; }</style>
<!-- -->
<header><h1>Model components in Langflow</h1></header>
<p>Model components generate text using large language models.</p>
<p>Refer to your specific component&#x27;s documentation for more information on parameters.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="use-a-model-component-in-a-flow">Use a model component in a flow<a href="#use-a-model-component-in-a-flow" class="hash-link" aria-label="Direct link to Use a model component in a flow" title="Direct link to Use a model component in a flow">​</a></h2>
<p>Model components receive inputs and prompts for generating text, and the generated text is sent to an output component.</p>
<p>The model output can also be sent to the <strong>Language Model</strong> port and on to a <strong>Parse Data</strong> component, where the output can be parsed into structured <a href="/concepts-objects">Data</a> objects.</p>
<p>This example has the OpenAI model in a chatbot flow. For more information, see the <a href="/basic-prompting">Basic prompting flow</a>.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/starter-flow-basic-prompting-09331815d7282bd6a3feedf84838ba20.png" width="2500" height="1528" class="img_ev3q"></p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="aiml">AIML<a href="#aiml" class="hash-link" aria-label="Direct link to AIML" title="Direct link to AIML">​</a></h2>
<p>This component creates a ChatOpenAI model instance using the AIML API.</p>
<p>For more information, see <a href="https://docs.aimlapi.com/" target="_blank" rel="noopener noreferrer">AIML documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the AIML model to use. Options are predefined in <code>AIML_CHAT_MODELS</code>.</td></tr><tr><td>aiml_api_base</td><td>String</td><td>The base URL of the AIML API. Defaults to <code>https://api.aimlapi.com</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The AIML API Key to use for the model.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="amazon-bedrock">Amazon Bedrock<a href="#amazon-bedrock" class="hash-link" aria-label="Direct link to Amazon Bedrock" title="Direct link to Amazon Bedrock">​</a></h2>
<p>This component generates text using Amazon Bedrock LLMs.</p>
<p>For more information, see <a href="https://docs.aws.amazon.com/bedrock" target="_blank" rel="noopener noreferrer">Amazon Bedrock documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_id</td><td>String</td><td>The ID of the Amazon Bedrock model to use. Options include various models.</td></tr><tr><td>aws_access_key</td><td>SecretString</td><td>AWS Access Key for authentication.</td></tr><tr><td>aws_secret_key</td><td>SecretString</td><td>AWS Secret Key for authentication.</td></tr><tr><td>aws_session_token</td><td>SecretString</td><td>The session key for your AWS account.</td></tr><tr><td>credentials_profile_name</td><td>String</td><td>Name of the AWS credentials profile to use.</td></tr><tr><td>region_name</td><td>String</td><td>AWS region name. Default: <code>us-east-1</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>endpoint_url</td><td>String</td><td>Custom endpoint URL for the Bedrock service.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatBedrock configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="anthropic">Anthropic<a href="#anthropic" class="hash-link" aria-label="Direct link to Anthropic" title="Direct link to Anthropic">​</a></h2>
<p>This component allows the generation of text using Anthropic Chat and Language models.</p>
<p>For more information, see the <a href="https://docs.anthropic.com/en/docs/welcome" target="_blank" rel="noopener noreferrer">Anthropic documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: <code>4096</code>.</td></tr><tr><td>model</td><td>String</td><td>The name of the Anthropic model to use. Options include various Claude 3 models.</td></tr><tr><td>anthropic_api_key</td><td>SecretString</td><td>Your Anthropic API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>anthropic_api_url</td><td>String</td><td>Endpoint of the Anthropic API. Defaults to <code>https://api.anthropic.com</code> if not specified (advanced).</td></tr><tr><td>prefill</td><td>String</td><td>Prefill text to guide the model&#x27;s response (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatAnthropic configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="azure-openai">Azure OpenAI<a href="#azure-openai" class="hash-link" aria-label="Direct link to Azure OpenAI" title="Direct link to Azure OpenAI">​</a></h2>
<p>This component generates text using Azure OpenAI LLM.</p>
<p>For more information, see the <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/" target="_blank" rel="noopener noreferrer">Azure OpenAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Model Name</td><td>String</td><td>Specifies the name of the Azure OpenAI model to be used for text generation.</td></tr><tr><td>Azure Endpoint</td><td>String</td><td>Your Azure endpoint, including the resource.</td></tr><tr><td>Deployment Name</td><td>String</td><td>Specifies the name of the deployment.</td></tr><tr><td>API Version</td><td>String</td><td>Specifies the version of the Azure OpenAI API to be used.</td></tr><tr><td>API Key</td><td>SecretString</td><td>Your Azure OpenAI API key.</td></tr><tr><td>Temperature</td><td>Float</td><td>Specifies the sampling temperature. Defaults to <code>0.7</code>.</td></tr><tr><td>Max Tokens</td><td>Integer</td><td>Specifies the maximum number of tokens to generate. Defaults to <code>1000</code>.</td></tr><tr><td>Input Value</td><td>String</td><td>Specifies the input text for text generation.</td></tr><tr><td>Stream</td><td>Boolean</td><td>Specifies whether to stream the response from the model. Defaults to <code>False</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of AzureOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="cohere">Cohere<a href="#cohere" class="hash-link" aria-label="Direct link to Cohere" title="Direct link to Cohere">​</a></h2>
<p>This component generates text using Cohere&#x27;s language models.</p>
<p>For more information, see the <a href="https://cohere.ai/" target="_blank" rel="noopener noreferrer">Cohere documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Cohere API Key</td><td>SecretString</td><td>Your Cohere API key.</td></tr><tr><td>Max Tokens</td><td>Integer</td><td>Specifies the maximum number of tokens to generate. Defaults to <code>256</code>.</td></tr><tr><td>Temperature</td><td>Float</td><td>Specifies the sampling temperature. Defaults to <code>0.75</code>.</td></tr><tr><td>Input Value</td><td>String</td><td>Specifies the input text for text generation.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of the Cohere model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="deepseek">DeepSeek<a href="#deepseek" class="hash-link" aria-label="Direct link to DeepSeek" title="Direct link to DeepSeek">​</a></h2>
<p>This component generates text using DeepSeek&#x27;s language models.</p>
<p>For more information, see the <a href="https://api-docs.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Set to <code>0</code> for unlimited. Range: <code>0-128000</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>json_mode</td><td>Boolean</td><td>If <code>True</code>, outputs JSON regardless of passing a schema.</td></tr><tr><td>model_name</td><td>String</td><td>The DeepSeek model to use. Default: <code>deepseek-chat</code>.</td></tr><tr><td>api_base</td><td>String</td><td>Base URL for API requests. Default: <code>https://api.deepseek.com</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your DeepSeek API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in responses. Range: <code>[0.0, 2.0]</code>. Default: <code>1.0</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="google-generative-ai">Google Generative AI<a href="#google-generative-ai" class="hash-link" aria-label="Direct link to Google Generative AI" title="Direct link to Google Generative AI">​</a></h2>
<p>This component generates text using Google&#x27;s Generative AI models.</p>
<p>For more information, see the <a href="https://cloud.google.com/vertex-ai/docs/" target="_blank" rel="noopener noreferrer">Google Generative AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Google API Key</td><td>SecretString</td><td>Your Google API key to use for the Google Generative AI.</td></tr><tr><td>Model</td><td>String</td><td>The name of the model to use, such as <code>&quot;gemini-pro&quot;</code>.</td></tr><tr><td>Max Output Tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>Temperature</td><td>Float</td><td>Run inference with this temperature.</td></tr><tr><td>Top K</td><td>Integer</td><td>Consider the set of top K most probable tokens.</td></tr><tr><td>Top P</td><td>Float</td><td>The maximum cumulative probability of tokens to consider when sampling.</td></tr><tr><td>N</td><td>Integer</td><td>Number of chat completions to generate for each prompt.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatGoogleGenerativeAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="groq">Groq<a href="#groq" class="hash-link" aria-label="Direct link to Groq" title="Direct link to Groq">​</a></h2>
<p>This component generates text using Groq&#x27;s language models.</p>
<ol>
<li>To use this component in a flow, connect it as a <strong>Model</strong> in a flow like the <a href="/basic-prompting">Basic prompting flow</a>, or select it as the <strong>Model Provider</strong> if you&#x27;re using an <strong>Agent</strong> component.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Groq component in a basic prompting flow" src="/assets/images/component-groq-d3df19923f67805fd483632d537af9f4.png" width="1982" height="1194" class="img_ev3q"></p>
<ol start="2">
<li>In the <strong>Groq API Key</strong> field, paste your Groq API key.
The Groq model component automatically retrieves a list of the latest models.
To refresh your list of models, click <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw" aria-hidden="true"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg> <strong>Refresh</strong>.</li>
<li>In the <strong>Model</strong> field, select the model you want to use for your LLM.
This example uses <a href="https://console.groq.com/docs/model/llama-3.1-8b-instant" target="_blank" rel="noopener noreferrer">llama-3.1-8b-instant</a>, which Groq recommends for real-time conversational interfaces.</li>
<li>In the <strong>Prompt</strong> component, enter:</li>
</ol>
<div class="ch-codeblock not-prose" data-ch-theme="github-dark"><div class="ch-code-wrapper ch-code" data-ch-measured="false"><code class="ch-code-scroll-parent"><br><div><span class="ch-code-line-number">_<!-- -->10</span><div style="display:inline-block;margin-left:16px"><span>You are a helpful assistant who supports their claims with sources.</span></div></div><br></code></div></div>
<ol start="5">
<li>Click <strong>Playground</strong> and ask your Groq LLM a question.
The responses include a list of sources.</li>
</ol>
<p>For more information, see the <a href="https://groq.com/" target="_blank" rel="noopener noreferrer">Groq documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>groq_api_key</td><td>SecretString</td><td>API key for the Groq API.</td></tr><tr><td>groq_api_base</td><td>String</td><td>Base URL path for API requests. Default: <code>https://api.groq.com</code>.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 1.0]</code>. Default: <code>0.1</code>.</td></tr><tr><td>n</td><td>Integer</td><td>Number of chat completions to generate for each prompt.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Groq model to use. Options are dynamically fetched from the Groq API.</td></tr><tr><td>tool_mode_enabled</td><td>Bool</td><td>If enabled, the component only displays models that work with tools.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatGroq configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hugging-face-api">Hugging Face API<a href="#hugging-face-api" class="hash-link" aria-label="Direct link to Hugging Face API" title="Direct link to Hugging Face API">​</a></h2>
<p>This component sends requests to the Hugging Face API to generate text using the model specified in the <strong>Model ID</strong> field.</p>
<p>The Hugging Face API is a hosted inference API for models hosted on Hugging Face, and requires a <a href="https://huggingface.co/docs/hub/security-tokens" target="_blank" rel="noopener noreferrer">Hugging Face API token</a> to authenticate.</p>
<p>In this example based on the <a href="/basic-prompting">Basic prompting flow</a>, the <strong>Hugging Face API</strong> model component replaces the <strong>Open AI</strong> model. By selecting different hosted models, you can see how different models return different results.</p>
<ol>
<li>
<p>Create a <a href="/basic-prompting">Basic prompting flow</a>.</p>
</li>
<li>
<p>Replace the <strong>OpenAI</strong> model component with a <strong>Hugging Face API</strong> model component.</p>
</li>
<li>
<p>In the <strong>Hugging Face API</strong> component, add your Hugging Face API token to the <strong>API Token</strong> field.</p>
</li>
<li>
<p>Open the <strong>Playground</strong> and ask a question to the model, and see how it responds.</p>
</li>
<li>
<p>Try different models, and see how they perform differently.</p>
</li>
</ol>
<p>For more information, see the <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">Hugging Face documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_id</td><td>String</td><td>The model ID from Hugging Face Hub. For example, &quot;gpt2&quot;, &quot;facebook/bart-large&quot;.</td></tr><tr><td>huggingfacehub_api_token</td><td>SecretString</td><td>Your Hugging Face API token for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>max_new_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Default: 512.</td></tr><tr><td>top_p</td><td>Float</td><td>Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95.</td></tr><tr><td>top_k</td><td>Integer</td><td>Top-k sampling parameter. Default: 50.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments to pass to the model.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of HuggingFaceHub configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ibm-watsonxai">IBM watsonx.ai<a href="#ibm-watsonxai" class="hash-link" aria-label="Direct link to IBM watsonx.ai" title="Direct link to IBM watsonx.ai">​</a></h2>
<p>This component generates text using <a href="https://www.ibm.com/watsonx" target="_blank" rel="noopener noreferrer">IBM watsonx.ai</a> foundation models.</p>
<p>To use <strong>IBM watsonx.ai</strong> model components, replace a model component with the IBM watsonx.ai component in a flow.</p>
<p>An example flow looks like the following:</p>
<p><img decoding="async" loading="lazy" alt="IBM watsonx model component in a basic prompting flow" src="/assets/images/component-watsonx-model-2f388a824b49f1c49287f07bc8738d0f.png" width="2364" height="1562" class="img_ev3q"></p>
<p>The values for <strong>API endpoint</strong>, <strong>Project ID</strong>, <strong>API key</strong>, and <strong>Model Name</strong> are found in your IBM watsonx.ai deployment.
For more information, see the <a href="https://python.langchain.com/docs/integrations/chat/ibm_watsonx/" target="_blank" rel="noopener noreferrer">Langchain documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>url</td><td>String</td><td>The base URL of the watsonx API.</td></tr><tr><td>project_id</td><td>String</td><td>Your watsonx Project ID.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your IBM watsonx API Key.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the watsonx model to use. Options are dynamically fetched from the API.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Default: <code>1000</code>.</td></tr><tr><td>stop_sequence</td><td>String</td><td>The sequence where generation should stop.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls nucleus sampling, which limits the model to tokens whose probability is below the <code>top_p</code> value. Range: Default: <code>0.9</code>.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: <code>0.5</code>.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: <code>0.3</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>A random seed for the model. Default: <code>8</code>.</td></tr><tr><td>logprobs</td><td>Boolean</td><td>Whether to return log probabilities of output tokens or not. Default: <code>True</code>.</td></tr><tr><td>top_logprobs</td><td>Integer</td><td>The number of most likely tokens to return at each position. Default: <code>3</code>.</td></tr><tr><td>logit_bias</td><td>String</td><td>A JSON string of token IDs to bias or suppress.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of <a href="https://python.langchain.com/docs/integrations/chat/ibm_watsonx/" target="_blank" rel="noopener noreferrer">ChatWatsonx</a> configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="language-model">Language model<a href="#language-model" class="hash-link" aria-label="Direct link to Language model" title="Direct link to Language model">​</a></h2>
<p>This component generates text using either OpenAI or Anthropic language models.</p>
<p>Use this component as a drop-in replacement for LLM models to switch between different model providers and models.</p>
<p>Instead of swapping out model components when you want to try a different provider, like switching between OpenAI and Anthropic components, change the provider dropdown in this single component. This makes it easier to experiment with and compare different models while keeping the rest of your flow intact.</p>
<p>For more information, see the <a href="https://platform.openai.com/docs" target="_blank" rel="noopener noreferrer">OpenAI documentation</a> and <a href="https://docs.anthropic.com/" target="_blank" rel="noopener noreferrer">Anthropic documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>provider</td><td>String</td><td>The model provider to use. Options: &quot;OpenAI&quot;, &quot;Anthropic&quot;. Default: &quot;OpenAI&quot;.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the model to use. Options depend on the selected provider.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The API Key for authentication with the selected provider.</td></tr><tr><td>input_value</td><td>String</td><td>The input text to send to the model.</td></tr><tr><td>system_message</td><td>String</td><td>A system message that helps set the behavior of the assistant (advanced).</td></tr><tr><td>stream</td><td>Boolean</td><td>Whether to stream the response. Default: <code>False</code> (advanced).</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in responses. Range: <code>[0.0, 1.0]</code>. Default: <code>0.1</code> (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI or ChatAnthropic configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="lmstudio">LMStudio<a href="#lmstudio" class="hash-link" aria-label="Direct link to LMStudio" title="Direct link to LMStudio">​</a></h2>
<p>This component generates text using LM Studio&#x27;s local language models.</p>
<p>For more information, see <a href="https://lmstudio.ai/" target="_blank" rel="noopener noreferrer">LM Studio documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>base_url</td><td>String</td><td>The URL where LM Studio is running. Default: <code>&quot;http://localhost:1234&quot;</code>.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate in the response. Default: <code>512</code>.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 2.0]</code>. Default: <code>0.7</code>.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls diversity via nucleus sampling. Range: <code>[0.0, 1.0]</code>. Default: <code>1.0</code>.</td></tr><tr><td>stop</td><td>List[String]</td><td>List of strings that stop generation when encountered.</td></tr><tr><td>stream</td><td>Boolean</td><td>Whether to stream the response. Default: <code>False</code>.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Penalizes repeated tokens. Range: <code>[-2.0, 2.0]</code>. Default: <code>0.0</code>.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Penalizes frequent tokens. Range: <code>[-2.0, 2.0]</code>. Default: <code>0.0</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of LMStudio configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="maritalk">Maritalk<a href="#maritalk" class="hash-link" aria-label="Direct link to Maritalk" title="Direct link to Maritalk">​</a></h2>
<p>This component generates text using Maritalk LLMs.</p>
<p>For more information, see <a href="https://www.maritalk.com/" target="_blank" rel="noopener noreferrer">Maritalk documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to <code>0</code> for unlimited tokens. Default: <code>512</code>.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Maritalk model to use. Options: <code>sabia-2-small</code>, <code>sabia-2-medium</code>. Default: <code>sabia-2-small</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Maritalk API Key to use for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 1.0]</code>. Default: <code>0.5</code>.</td></tr><tr><td>endpoint_url</td><td>String</td><td>The Maritalk API endpoint. Default: <code>https://api.maritalk.com</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatMaritalk configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="mistral">Mistral<a href="#mistral" class="hash-link" aria-label="Direct link to Mistral" title="Direct link to Mistral">​</a></h2>
<p>This component generates text using MistralAI LLMs.</p>
<p>For more information, see <a href="https://docs.mistral.ai/" target="_blank" rel="noopener noreferrer">Mistral AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens (advanced).</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Mistral AI model to use. Options include <code>open-mixtral-8x7b</code>, <code>open-mixtral-8x22b</code>, <code>mistral-small-latest</code>, <code>mistral-medium-latest</code>, <code>mistral-large-latest</code>, and <code>codestral-latest</code>. Default: <code>codestral-latest</code>.</td></tr><tr><td>mistral_api_base</td><td>String</td><td>The base URL of the Mistral API. Defaults to <code>https://api.mistral.ai/v1</code> (advanced).</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Mistral API Key to use for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.5.</td></tr><tr><td>max_retries</td><td>Integer</td><td>Maximum number of retries for API calls. Default: 5 (advanced).</td></tr><tr><td>timeout</td><td>Integer</td><td>Timeout for API calls in seconds. Default: 60 (advanced).</td></tr><tr><td>max_concurrent_requests</td><td>Integer</td><td>Maximum number of concurrent API requests. Default: 3 (advanced).</td></tr><tr><td>top_p</td><td>Float</td><td>Nucleus sampling parameter. Default: 1 (advanced).</td></tr><tr><td>random_seed</td><td>Integer</td><td>Seed for random number generation. Default: 1 (advanced).</td></tr><tr><td>safe_mode</td><td>Boolean</td><td>Enables safe mode for content generation (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatMistralAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="novita-ai">Novita AI<a href="#novita-ai" class="hash-link" aria-label="Direct link to Novita AI" title="Direct link to Novita AI">​</a></h2>
<p>This component generates text using Novita AI&#x27;s language models.</p>
<p>For more information, see <a href="https://novita.ai/docs/model-api/reference/llm/llm.html?utm_source=github_langflow&amp;utm_medium=github_readme&amp;utm_campaign=link" target="_blank" rel="noopener noreferrer">Novita AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your Novita AI API Key.</td></tr><tr><td>model</td><td>String</td><td>The id of the Novita AI model to use.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of Novita AI model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="nvidia">NVIDIA<a href="#nvidia" class="hash-link" aria-label="Direct link to NVIDIA" title="Direct link to NVIDIA">​</a></h2>
<p>This component generates text using NVIDIA LLMs.</p>
<p>For more information, see <a href="https://developer.nvidia.com/generative-ai" target="_blank" rel="noopener noreferrer">NVIDIA AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to <code>0</code> for unlimited tokens (advanced).</td></tr><tr><td>model_name</td><td>String</td><td>The name of the NVIDIA model to use. Default: <code>mistralai/mixtral-8x7b-instruct-v0.1</code>.</td></tr><tr><td>base_url</td><td>String</td><td>The base URL of the NVIDIA API. Default: <code>https://integrate.api.nvidia.com/v1</code>.</td></tr><tr><td>nvidia_api_key</td><td>SecretString</td><td>The NVIDIA API Key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: <code>0.1</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>The seed controls the reproducibility of the job (advanced). Default: <code>1</code>.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatNVIDIA configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="ollama">Ollama<a href="#ollama" class="hash-link" aria-label="Direct link to Ollama" title="Direct link to Ollama">​</a></h2>
<p>This component generates text using Ollama&#x27;s language models.</p>
<p>To use this component in a flow, connect Langflow to your locally running Ollama server and select a model.</p>
<ol>
<li>In the Ollama component, in the <strong>Base URL</strong> field, enter the address for your locally running Ollama server.
This value is set as the <code>OLLAMA_HOST</code> environment variable in Ollama.
The default base URL is <code>http://localhost:11434</code>.</li>
<li>To refresh the server&#x27;s list of models, click <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-refresh-cw" aria-hidden="true"><path d="M3 12a9 9 0 0 1 9-9 9.75 9.75 0 0 1 6.74 2.74L21 8"></path><path d="M21 3v5h-5"></path><path d="M21 12a9 9 0 0 1-9 9 9.75 9.75 0 0 1-6.74-2.74L3 16"></path><path d="M8 16H3v5"></path></svg> <strong>Refresh</strong>.</li>
<li>In the <strong>Model Name</strong> field, select a model. This example uses <code>llama3.2:latest</code>.</li>
<li>Connect the <strong>Ollama</strong> model component to a flow. For example, this flow connects a local Ollama server running a Llama 3.2 model as the custom model for an <a href="/components-agents">Agent</a> component.</li>
</ol>
<p><img decoding="async" loading="lazy" alt="Ollama model as Agent custom model" src="/assets/images/component-ollama-model-5755eab19c67fb10ee0533b3f7ade726.png" width="4000" height="2668" class="img_ev3q"></p>
<p>For more information, see the <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">Ollama documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>Base URL</td><td>String</td><td>Endpoint of the Ollama API.</td></tr><tr><td>Model Name</td><td>String</td><td>The model name to use.</td></tr><tr><td>Temperature</td><td>Float</td><td>Controls the creativity of model responses.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of an Ollama model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openai">OpenAI<a href="#openai" class="hash-link" aria-label="Direct link to OpenAI" title="Direct link to OpenAI">​</a></h2>
<p>This component generates text using OpenAI&#x27;s language models.</p>
<p>For more information, see <a href="https://beta.openai.com/docs/" target="_blank" rel="noopener noreferrer">OpenAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your OpenAI API Key.</td></tr><tr><td>model</td><td>String</td><td>The name of the OpenAI model to use. Options include &quot;gpt-3.5-turbo&quot; and &quot;gpt-4&quot;.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7.</td></tr><tr><td>top_p</td><td>Float</td><td>Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0.</td></tr><tr><td>frequency_penalty</td><td>Float</td><td>Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr><tr><td>presence_penalty</td><td>Float</td><td>Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of OpenAI model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openrouter">OpenRouter<a href="#openrouter" class="hash-link" aria-label="Direct link to OpenRouter" title="Direct link to OpenRouter">​</a></h2>
<p>This component generates text using OpenRouter&#x27;s unified API for multiple AI models from different providers.</p>
<p>For more information, see <a href="https://openrouter.ai/docs" target="_blank" rel="noopener noreferrer">OpenRouter documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>api_key</td><td>SecretString</td><td>Your OpenRouter API key for authentication.</td></tr><tr><td>site_url</td><td>String</td><td>Your site URL for OpenRouter rankings (advanced).</td></tr><tr><td>app_name</td><td>String</td><td>Your app name for OpenRouter rankings (advanced).</td></tr><tr><td>provider</td><td>String</td><td>The AI model provider to use.</td></tr><tr><td>model_name</td><td>String</td><td>The specific model to use for chat completion.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="perplexity">Perplexity<a href="#perplexity" class="hash-link" aria-label="Direct link to Perplexity" title="Direct link to Perplexity">​</a></h2>
<p>This component generates text using Perplexity&#x27;s language models.</p>
<p>For more information, see <a href="https://perplexity.ai/" target="_blank" rel="noopener noreferrer">Perplexity documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model_name</td><td>String</td><td>The name of the Perplexity model to use. Options include various Llama 3.1 models.</td></tr><tr><td>max_output_tokens</td><td>Integer</td><td>The maximum number of tokens to generate.</td></tr><tr><td>api_key</td><td>SecretString</td><td>The Perplexity API Key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.75.</td></tr><tr><td>top_p</td><td>Float</td><td>The maximum cumulative probability of tokens to consider when sampling (advanced).</td></tr><tr><td>n</td><td>Integer</td><td>Number of chat completions to generate for each prompt (advanced).</td></tr><tr><td>top_k</td><td>Integer</td><td>Number of top tokens to consider for top-k sampling. Must be positive (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatPerplexity configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="qianfan">Qianfan<a href="#qianfan" class="hash-link" aria-label="Direct link to Qianfan" title="Direct link to Qianfan">​</a></h2>
<p>This component generates text using Qianfan&#x27;s language models.</p>
<p>For more information, see <a href="https://github.com/baidubce/bce-qianfan-sdk" target="_blank" rel="noopener noreferrer">Qianfan documentation</a>.</p>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="sambanova">SambaNova<a href="#sambanova" class="hash-link" aria-label="Direct link to SambaNova" title="Direct link to SambaNova">​</a></h2>
<p>This component generates text using SambaNova LLMs.</p>
<p>For more information, see <a href="https://cloud.sambanova.ai/" target="_blank" rel="noopener noreferrer">Sambanova Cloud documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>sambanova_url</td><td>String</td><td>Base URL path for API requests. Default: <code>https://api.sambanova.ai/v1/chat/completions</code>.</td></tr><tr><td>sambanova_api_key</td><td>SecretString</td><td>Your SambaNova API Key.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Sambanova model to use. Options include various Llama models.</td></tr><tr><td>max_tokens</td><td>Integer</td><td>The maximum number of tokens to generate. Set to 0 for unlimited tokens.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of SambaNova model configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="vertexai">VertexAI<a href="#vertexai" class="hash-link" aria-label="Direct link to VertexAI" title="Direct link to VertexAI">​</a></h2>
<p>This component generates text using Vertex AI LLMs.</p>
<p>For more information, see <a href="https://cloud.google.com/vertex-ai" target="_blank" rel="noopener noreferrer">Google Vertex AI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>credentials</td><td>File</td><td>JSON credentials file. Leave empty to fall back to environment variables. File type: JSON.</td></tr><tr><td>model_name</td><td>String</td><td>The name of the Vertex AI model to use. Default: &quot;gemini-1.5-pro&quot;.</td></tr><tr><td>project</td><td>String</td><td>The project ID (advanced).</td></tr><tr><td>location</td><td>String</td><td>The location for the Vertex AI API. Default: &quot;us-central1&quot; (advanced).</td></tr><tr><td>max_output_tokens</td><td>Integer</td><td>The maximum number of tokens to generate (advanced).</td></tr><tr><td>max_retries</td><td>Integer</td><td>Maximum number of retries for API calls. Default: 1 (advanced).</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Default: 0.0.</td></tr><tr><td>top_k</td><td>Integer</td><td>The number of highest probability vocabulary tokens to keep for top-k-filtering (advanced).</td></tr><tr><td>top_p</td><td>Float</td><td>The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Default: 0.95 (advanced).</td></tr><tr><td>verbose</td><td>Boolean</td><td>Whether to print verbose output. Default: False (advanced).</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatVertexAI configured with the specified parameters.</td></tr></tbody></table></div></div></details>
<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="xai">xAI<a href="#xai" class="hash-link" aria-label="Direct link to xAI" title="Direct link to xAI">​</a></h2>
<p>This component generates text using xAI models like <a href="https://x.ai/grok" target="_blank" rel="noopener noreferrer">Grok</a>.</p>
<p>For more information, see the <a href="https://x.ai/" target="_blank" rel="noopener noreferrer">xAI documentation</a>.</p>
<details class="details_lb9f alert alert--info details_b_Ee" data-collapsed="true"><summary>Parameters</summary><div><div class="collapsibleContent_i85q"><p><strong>Inputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>max_tokens</td><td>Integer</td><td>Maximum number of tokens to generate. Set to <code>0</code> for unlimited. Range: <code>0-128000</code>.</td></tr><tr><td>model_kwargs</td><td>Dictionary</td><td>Additional keyword arguments for the model.</td></tr><tr><td>json_mode</td><td>Boolean</td><td>If <code>True</code>, outputs JSON regardless of passing a schema.</td></tr><tr><td>model_name</td><td>String</td><td>The xAI model to use. Default: <code>grok-2-latest</code>.</td></tr><tr><td>base_url</td><td>String</td><td>Base URL for API requests. Default: <code>https://api.x.ai/v1</code>.</td></tr><tr><td>api_key</td><td>SecretString</td><td>Your xAI API key for authentication.</td></tr><tr><td>temperature</td><td>Float</td><td>Controls randomness in the output. Range: <code>[0.0, 2.0]</code>. Default: <code>0.1</code>.</td></tr><tr><td>seed</td><td>Integer</td><td>Controls reproducibility of the job.</td></tr></tbody></table><p><strong>Outputs</strong></p><table><thead><tr><th>Name</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td>model</td><td>LanguageModel</td><td>An instance of ChatOpenAI configured with the specified parameters.</td></tr></tbody></table></div></div></details></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/components-memories"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Memories</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/components-processing"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Processing</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#use-a-model-component-in-a-flow" class="table-of-contents__link toc-highlight">Use a model component in a flow</a></li><li><a href="#aiml" class="table-of-contents__link toc-highlight">AIML</a></li><li><a href="#amazon-bedrock" class="table-of-contents__link toc-highlight">Amazon Bedrock</a></li><li><a href="#anthropic" class="table-of-contents__link toc-highlight">Anthropic</a></li><li><a href="#azure-openai" class="table-of-contents__link toc-highlight">Azure OpenAI</a></li><li><a href="#cohere" class="table-of-contents__link toc-highlight">Cohere</a></li><li><a href="#deepseek" class="table-of-contents__link toc-highlight">DeepSeek</a></li><li><a href="#google-generative-ai" class="table-of-contents__link toc-highlight">Google Generative AI</a></li><li><a href="#groq" class="table-of-contents__link toc-highlight">Groq</a></li><li><a href="#hugging-face-api" class="table-of-contents__link toc-highlight">Hugging Face API</a></li><li><a href="#ibm-watsonxai" class="table-of-contents__link toc-highlight">IBM watsonx.ai</a></li><li><a href="#language-model" class="table-of-contents__link toc-highlight">Language model</a></li><li><a href="#lmstudio" class="table-of-contents__link toc-highlight">LMStudio</a></li><li><a href="#maritalk" class="table-of-contents__link toc-highlight">Maritalk</a></li><li><a href="#mistral" class="table-of-contents__link toc-highlight">Mistral</a></li><li><a href="#novita-ai" class="table-of-contents__link toc-highlight">Novita AI</a></li><li><a href="#nvidia" class="table-of-contents__link toc-highlight">NVIDIA</a></li><li><a href="#ollama" class="table-of-contents__link toc-highlight">Ollama</a></li><li><a href="#openai" class="table-of-contents__link toc-highlight">OpenAI</a></li><li><a href="#openrouter" class="table-of-contents__link toc-highlight">OpenRouter</a></li><li><a href="#perplexity" class="table-of-contents__link toc-highlight">Perplexity</a></li><li><a href="#qianfan" class="table-of-contents__link toc-highlight">Qianfan</a></li><li><a href="#sambanova" class="table-of-contents__link toc-highlight">SambaNova</a></li><li><a href="#vertexai" class="table-of-contents__link toc-highlight">VertexAI</a></li><li><a href="#xai" class="table-of-contents__link toc-highlight">xAI</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><div class="footer-links">
                  <span>© 2025 Langflow</span>
                  <span id="preferenceCenterContainer"> ·&nbsp; <a href="https://langflow.org/preferences">Manage Privacy Choices</a></span>
                  </div></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><img src="/img/lf-docs-light.svg" alt="Langflow" class="footer__logo themedComponent_mlkZ themedComponent--light_NVdE" width="160" height="40"><img src="/img/lf-docs-dark.svg" alt="Langflow" class="footer__logo themedComponent_mlkZ themedComponent--dark_xIcU" width="160" height="40"></div></div></div></footer><div style="position:fixed;right:20px;bottom:20px;z-index:100;display:flex;align-items:center;gap:10px;cursor:pointer"><div style="background-color:#f6f6f6;border-radius:50%;width:48px;height:48px;display:flex;align-items:center;justify-content:center;box-shadow:0 2px 4px rgba(0,0,0,0.1)"><img src="/img/langflow-icon-black-transparent.svg" style="width:40px" alt="Search"></div></div></div>
</body>
</html>