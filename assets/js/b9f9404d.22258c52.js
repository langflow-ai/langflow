"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[2323],{9187:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/cleanlab_remediator_example-5d7e2d1782b3fe5e7ef8abd73d46e628.png"},17886:(e,n,t)=>{t.d(n,{A:()=>r});t(96540);var a=t(64058),s=t(74848);function r({name:e,...n}){const t=a[e];return t?(0,s.jsx)(t,{...n}):null}},22908:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/files/eval_and_remediate_cleanlab-aac380677836d5237ad3db4405aae6f7.json"},31411:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/eval_summary_rag-a48e121abf7d5f2761fc1022c61be17d.png"},46099:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/eval_response-5947d6e0eb45b0feec6a36d4b489bf99.png"},57064:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>d,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>h});const a=JSON.parse('{"id":"Integrations/Cleanlab/integrations-cleanlab","title":"Cleanlab","description":"Cleanlab adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.","source":"@site/docs/Integrations/Cleanlab/integrations-cleanlab.mdx","sourceDirName":"Integrations/Cleanlab","slug":"/integrations-cleanlab","permalink":"/integrations-cleanlab","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Cleanlab","slug":"/integrations-cleanlab"},"sidebar":"docs","previous":{"title":"Bing","permalink":"/bundles-bing"},"next":{"title":"Cloudflare","permalink":"/bundles-cloudflare"}}');var s=t(74848),r=t(28453),l=t(17886);const o={title:"Cleanlab",slug:"/integrations-cleanlab"},d=void 0,i={},h=[{value:"Cleanlab Evaluator",id:"cleanlab-evaluator",level:2},{value:"Cleanlab Evaluator parameters",id:"cleanlab-evaluator-parameters",level:3},{value:"Cleanlab Evaluator outputs",id:"cleanlab-evaluator-outputs",level:3},{value:"Cleanlab Remediator",id:"cleanlab-remediator",level:2},{value:"Cleanlab Remediator parameters",id:"cleanlab-remediator-parameters",level:3},{value:"Cleanlab RAG Evaluator",id:"cleanlab-rag-evaluator",level:2},{value:"Cleanlab RAG Evaluator parameters",id:"cleanlab-rag-evaluator-parameters",level:3},{value:"Cleanlab RAG Evaluator outputs",id:"cleanlab-rag-evaluator-outputs",level:3},{value:"Example Cleanlab flows",id:"example-cleanlab-flows",level:2},{value:"Evaluate and remediate responses from an LLM",id:"evaluate-and-remediate-responses-from-an-llm",level:3},{value:"Evaluate RAG pipeline",id:"evaluate-rag-pipeline",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.cleanlab.ai/",children:"Cleanlab"})," adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions."]}),"\n",(0,s.jsx)(n.p,{children:"Use the Cleanlab components to integrate Cleanlab Evaluations with Langflow and unlock trustworthy Agentic, RAG, and LLM pipelines with Cleanlab's evaluation and remediation suite."}),"\n",(0,s.jsxs)(n.p,{children:["You can use these components to quantify the trustworthiness of any LLM response with a score between ",(0,s.jsx)(n.code,{children:"0"})," and ",(0,s.jsx)(n.code,{children:"1"}),", and explain why a response may be good or bad. For RAG/Agentic pipelines with context, you can evaluate context sufficiency, groundedness, helpfulness, and query clarity with quantitative scores. Additionally, you can remediate low-trust responses with warnings or fallback answers."]}),"\n",(0,s.jsx)(n.p,{children:"Authentication is required with a Cleanlab API key."}),"\n",(0,s.jsx)(n.h2,{id:"cleanlab-evaluator",children:"Cleanlab Evaluator"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Cleanlab Evaluator"})," component evaluates and explains the trustworthiness of a prompt and response pair using Cleanlab. For more information on how the score works, see the ",(0,s.jsx)(n.a,{href:"https://help.cleanlab.ai/tlm/",children:"Cleanlab documentation"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"cleanlab-evaluator-parameters",children:"Cleanlab Evaluator parameters"}),"\n",(0,s.jsxs)(n.p,{children:["Some ",(0,s.jsx)(n.strong,{children:"Cleanlab Evaluator"})," component input parameters are hidden by default in the visual editor.\nYou can toggle parameters through the ",(0,s.jsx)(l.A,{name:"SlidersHorizontal","aria-hidden":"true"})," ",(0,s.jsx)(n.strong,{children:"Controls"})," in the ",(0,s.jsx)(n.a,{href:"/concepts-components#component-menus",children:"component's header menu"}),"."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"system_prompt"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The system message prepended to the prompt. Optional."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"prompt"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The user-facing input to the LLM."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"response"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The model's response to evaluate."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"cleanlab_api_key"}),(0,s.jsx)(n.td,{children:"Secret"}),(0,s.jsx)(n.td,{children:"Input parameter. Your Cleanlab API key."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"cleanlab_evaluation_model"}),(0,s.jsx)(n.td,{children:"Dropdown"}),(0,s.jsx)(n.td,{children:"Input parameter. Evaluation model used by Cleanlab, such as GPT-4 or Claude. This does not need to be the same model that generated the response."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"quality_preset"}),(0,s.jsx)(n.td,{children:"Dropdown"}),(0,s.jsx)(n.td,{children:"Input parameter. Tradeoff between evaluation speed and accuracy."})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"cleanlab-evaluator-outputs",children:"Cleanlab Evaluator outputs"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Cleanlab Evaluator"})," component has three possible outputs."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"score"}),(0,s.jsx)(n.td,{children:"number, float"}),(0,s.jsx)(n.td,{children:"Displays the trust score between 0 and 1."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"explanation"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Message"})}),(0,s.jsx)(n.td,{children:"Provides an explanation of the trust score."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"response"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Message"})}),(0,s.jsxs)(n.td,{children:["Returns the original response for easy chaining to the ",(0,s.jsx)(n.strong,{children:"Cleanlab Remediator"})," component."]})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"cleanlab-remediator",children:"Cleanlab Remediator"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Cleanlab Remediator"})," component uses the trust score from the ",(0,s.jsxs)(n.a,{href:"#cleanlab-evaluator",children:[(0,s.jsx)(n.strong,{children:"Cleanlab Evaluator"})," component"]})," to determine whether to show, warn about, or replace an LLM response."]}),"\n",(0,s.jsx)(n.p,{children:"This component has parameters for the score threshold, warning text, and fallback message that you can customize as needed."}),"\n",(0,s.jsxs)(n.p,{children:["The output is ",(0,s.jsx)(n.strong,{children:"Remediated Response"})," (",(0,s.jsx)(n.code,{children:"remediated_response"}),"), which is a ",(0,s.jsx)(n.code,{children:"Message"})," containing the final message shown to the user after remediation logic is applied."]}),"\n",(0,s.jsx)(n.h3,{id:"cleanlab-remediator-parameters",children:"Cleanlab Remediator parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"response"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The response to potentially remediate."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"score"}),(0,s.jsx)(n.td,{children:"Number"}),(0,s.jsxs)(n.td,{children:["Input parameter. The trust score from ",(0,s.jsx)(n.code,{children:"CleanlabEvaluator"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"explanation"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The explanation to append if a warning is shown. Optional."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"threshold"}),(0,s.jsx)(n.td,{children:"Float"}),(0,s.jsx)(n.td,{children:"Input parameter. The minimum trust score to pass a response unchanged."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"show_untrustworthy_response"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsx)(n.td,{children:"Input parameter. Whether to display or hide the original response with a warning if a response is deemed untrustworthy."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"untrustworthy_warning_text"}),(0,s.jsx)(n.td,{children:"Prompt"}),(0,s.jsx)(n.td,{children:"Input parameter. The warning text for untrustworthy responses."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"fallback_text"}),(0,s.jsx)(n.td,{children:"Prompt"}),(0,s.jsx)(n.td,{children:"Input parameter. The fallback message if the response is hidden."})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"cleanlab-rag-evaluator",children:"Cleanlab RAG Evaluator"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Cleanlab RAG Evaluator"})," component evaluates RAG and LLM pipeline outputs for trustworthiness, context sufficiency, response groundedness, helpfulness, and query ease using ",(0,s.jsx)(n.a,{href:"https://help.cleanlab.ai/tlm/use-cases/tlm_rag/",children:"Cleanlab's evaluation metrics"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["You can pair this component with the ",(0,s.jsxs)(n.a,{href:"#cleanlab-remediator",children:[(0,s.jsx)(n.strong,{children:"Cleanlab Remediator"})," component"]})," to remediate low-trust responses coming from the RAG pipeline."]}),"\n",(0,s.jsx)(n.h3,{id:"cleanlab-rag-evaluator-parameters",children:"Cleanlab RAG Evaluator parameters"}),"\n",(0,s.jsxs)(n.p,{children:["Some ",(0,s.jsx)(n.strong,{children:"Cleanlab RAG Evaluator"})," component input parameters are hidden by default in the visual editor.\nYou can toggle parameters through the ",(0,s.jsx)(l.A,{name:"SlidersHorizontal","aria-hidden":"true"})," ",(0,s.jsx)(n.strong,{children:"Controls"})," in the ",(0,s.jsx)(n.a,{href:"/concepts-components#component-menus",children:"component's header menu"}),"."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"cleanlab_api_key"}),(0,s.jsx)(n.td,{children:"Secret"}),(0,s.jsx)(n.td,{children:"Input parameter. Your Cleanlab API key."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"cleanlab_evaluation_model"}),(0,s.jsx)(n.td,{children:"Dropdown"}),(0,s.jsx)(n.td,{children:"Input parameter. The evaluation model used by Cleanlab, such as GPT-4, or Claude. This does not need to be the same model that generated the response."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"quality_preset"}),(0,s.jsx)(n.td,{children:"Dropdown"}),(0,s.jsx)(n.td,{children:"Input parameter. The tradeoff between evaluation speed and accuracy."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"context"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The retrieved context from your RAG system."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"query"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The original user query."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"response"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"Input parameter. The model's response based on the context and query."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"run_context_sufficiency"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsx)(n.td,{children:"Input parameter. Evaluate whether context supports answering the query."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"run_response_groundedness"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsx)(n.td,{children:"Input parameter. Evaluate whether the response is grounded in the context."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"run_response_helpfulness"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsx)(n.td,{children:"Input parameter. Evaluate how helpful the response is."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"run_query_ease"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsx)(n.td,{children:"Input parameter. Evaluate if the query is vague, complex, or adversarial."})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"cleanlab-rag-evaluator-outputs",children:"Cleanlab RAG Evaluator outputs"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Cleanlab RAG Evaluator"})," component has the following output options:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"trust_score"}),(0,s.jsx)(n.td,{children:"Number"}),(0,s.jsx)(n.td,{children:"The overall trust score."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"trust_explanation"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"The explanation for the trust score."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"other_scores"}),(0,s.jsx)(n.td,{children:"Dictionary"}),(0,s.jsx)(n.td,{children:"A dictionary of optional enabled RAG evaluation metrics."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"evaluation_summary"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsx)(n.td,{children:"A Markdown summary of query, context, response, and evaluation results."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"response"}),(0,s.jsx)(n.td,{children:"Message"}),(0,s.jsxs)(n.td,{children:["Returns the original response for easy chaining to the ",(0,s.jsx)(n.strong,{children:"Cleanlab Remediator"})," component."]})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"example-cleanlab-flows",children:"Example Cleanlab flows"}),"\n",(0,s.jsxs)(n.p,{children:["The following example flows show how to use the ",(0,s.jsx)(n.strong,{children:"CleanlabEvaluator"})," and ",(0,s.jsx)(n.strong,{children:"CleanlabRemediator"})," components to evaluate and remediate responses from any LLM, and how to use the ",(0,s.jsx)(n.code,{children:"CleanlabRAGEvaluator"})," component to evaluate RAG pipeline outputs."]}),"\n",(0,s.jsx)(n.h3,{id:"evaluate-and-remediate-responses-from-an-llm",children:"Evaluate and remediate responses from an LLM"}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["You can ",(0,s.jsx)(n.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:t(22908).A+"",children:"download the the Evaluate and Remediate flow"}),", and then import it to your Langflow instance to follow along."]})}),"\n",(0,s.jsxs)(n.p,{children:["This flow evaluates and remediates the trustworthiness of a response from any LLM using the ",(0,s.jsx)(n.strong,{children:"CleanlabEvaluator"})," and ",(0,s.jsx)(n.strong,{children:"CleanlabRemediator"})," components."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Evaluate response trustworthiness",src:t(46099).A+"",width:"3092",height:"1468"})}),"\n",(0,s.jsxs)(n.p,{children:["Connect the ",(0,s.jsx)(n.code,{children:"Message"})," output from any LLM component to the ",(0,s.jsx)(n.code,{children:"response"})," input of the ",(0,s.jsx)(n.strong,{children:"CleanlabEvaluator"})," component, and then connect the Prompt component to its ",(0,s.jsx)(n.code,{children:"prompt"})," input."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"CleanlabEvaluator"})," component returns a trust score and explanation from the flow."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"CleanlabRemediator"})," component uses this trust score to determine whether to output the original response, warn about it, or replace it with a fallback answer."]}),"\n",(0,s.jsxs)(n.p,{children:["This example shows a response that was determined to be untrustworthy (a score of ",(0,s.jsx)(n.code,{children:".09"}),") and flagged with a warning by the ",(0,s.jsx)(n.strong,{children:"CleanlabRemediator"})," component."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"CleanlabRemediator Example",src:t(9187).A+"",width:"1890",height:"1322"})}),"\n",(0,s.jsxs)(n.p,{children:["To hide untrustworthy responses, configure the ",(0,s.jsx)(n.strong,{children:"CleanlabRemediator"})," component to replace the response with a fallback message."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"CleanlabRemediator Example",src:t(87490).A+"",width:"1654",height:"416"})}),"\n",(0,s.jsx)(n.h3,{id:"evaluate-rag-pipeline",children:"Evaluate RAG pipeline"}),"\n",(0,s.jsxs)(n.p,{children:["This example flow includes the ",(0,s.jsx)(n.a,{href:"/vector-store-rag",children:"Vector Store RAG"})," template with the ",(0,s.jsx)(n.strong,{children:"CleanlabRAGEvaluator"})," component added to evaluate the flow's context, query, and response."]}),"\n",(0,s.jsxs)(n.p,{children:["To use the ",(0,s.jsx)(n.strong,{children:"CleanlabRAGEvaluator"})," component in a flow, connect the ",(0,s.jsx)(n.code,{children:"context"}),", ",(0,s.jsx)(n.code,{children:"query"}),", and ",(0,s.jsx)(n.code,{children:"response"})," outputs from any RAG pipeline to the ",(0,s.jsx)(n.strong,{children:"CleanlabRAGEvaluator"})," component."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Evaluate RAG pipeline",src:t(72186).A+"",width:"3058",height:"1274"})}),"\n",(0,s.jsxs)(n.p,{children:["Here is an example of the ",(0,s.jsx)(n.code,{children:"Evaluation Summary"})," output from the ",(0,s.jsx)(n.strong,{children:"CleanlabRAGEvaluator"})," component."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Evaluate RAG pipeline",src:t(31411).A+"",width:"1666",height:"1568"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"Evaluation Summary"})," includes the query, context, response, and all evaluation results. In this example, the ",(0,s.jsx)(n.code,{children:"Context Sufficiency"})," and ",(0,s.jsx)(n.code,{children:"Response Groundedness"})," scores are low (a score of ",(0,s.jsx)(n.code,{children:"0.002"}),") because the context doesn't contain information about the query, and the response is not grounded in the context."]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},72186:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/eval_rag-a8bfd2d0d1ac01168ca183f856c14545.png"},87490:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/cleanlab_remediator_example_fallback-7c585c112d908654873c27eb21de1702.png"}}]);