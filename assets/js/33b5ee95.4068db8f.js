"use strict";(globalThis.webpackChunklangflow_docs=globalThis.webpackChunklangflow_docs||[]).push([[750],{7685:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/vector-store-document-ingestion-6157311fb4d16e7f944d55254f0cc0e2.png"},11470:(e,n,t)=>{t.d(n,{A:()=>L});var o=t(96540),s=t(18215),r=t(17559),a=t(23104),i=t(56347),l=t(205),d=t(57485),c=t(31682),h=t(70679);function u(e){return o.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:t}=e;return(0,o.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:o}})=>({value:e,label:n,attributes:t,default:o}))}(t);return function(e){const n=(0,c.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,d.aZ)(s),(0,o.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(t.location.search);n.set(s,e),t.replace({...t.location,search:n.toString()})},[s,t])]}function x(e){const{defaultValue:n,queryString:t=!1,groupId:s}=e,r=p(e),[a,i]=(0,o.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:r})),[d,c]=g({queryString:t,groupId:s}),[u,x]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,s]=(0,h.Dv)(n);return[t,(0,o.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),f=(()=>{const e=d??u;return m({value:e,tabValues:r})?e:null})();(0,l.A)(()=>{f&&i(f)},[f]);return{selectedValue:a,selectValue:(0,o.useCallback)(e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);i(e),c(e),x(e)},[c,x,r]),tabValues:r}}var f=t(92303);const j={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(74848);function b({className:e,block:n,selectedValue:t,selectValue:o,tabValues:r}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,a.a_)(),d=e=>{const n=e.currentTarget,s=i.indexOf(n),a=r[s].value;a!==t&&(l(n),o(a))},c=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:r.map(({value:e,label:n,attributes:o})=>(0,y.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:c,onClick:d,...o,className:(0,s.A)("tabs__item",j.tabItem,o?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:t}){const r=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=r.find(e=>e.props.value===t);return e?(0,o.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:r.map((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function w(e){const n=x(e);return(0,y.jsxs)("div",{className:(0,s.A)(r.G.tabs.container,"tabs-container",j.tabList),children:[(0,y.jsx)(b,{...n,...e}),(0,y.jsx)(v,{...n,...e})]})}function L(e){const n=(0,f.A)();return(0,y.jsx)(w,{...e,children:u(e.children)},String(n))}},19365:(e,n,t)=>{t.d(n,{A:()=>a});t(96540);var o=t(18215);const s={tabItem:"tabItem_Ymn6"};var r=t(74848);function a({children:e,hidden:n,className:t}){return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(s.tabItem,t),hidden:n,children:e})}},22444:(e,n,t)=>{t.d(n,{Ay:()=>l,RM:()=>a});var o=t(74848),s=t(28453),r=t(40619);const a=[];function i(e){const n={a:"a",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"By design, vector data is essential for LLM applications, such as chatbots and agents."}),"\n",(0,o.jsx)(n.p,{children:"While you can use an LLM alone for generic chat interactions and common tasks, you can take your application to the next level with context sensitivity (such as RAG) and custom datasets (such as internal business data).\nThis often requires integrating vector databases and vector searches that provide the additional context and define meaningful queries."}),"\n",(0,o.jsx)(n.p,{children:"Langflow includes vector store components that can read and write vector data, including embedding storage, similarity search, Graph RAG traversals, and dedicated search instances like OpenSearch.\nBecause of their interdependent functionality, it is common to use vector store, language model, and embedding model components in the same flow or in a series of dependent flows."}),"\n",(0,o.jsxs)(n.p,{children:["To find available vector store components, browse ",(0,o.jsx)(r.A,{name:"Blocks","aria-hidden":"true"})," ",(0,o.jsx)(n.a,{href:"/components-bundle-components",children:(0,o.jsx)(n.strong,{children:"Bundles"})})," or ",(0,o.jsx)(r.A,{name:"Search","aria-hidden":"true"})," ",(0,o.jsx)(n.strong,{children:"Search"})," for your preferred vector database provider."]})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(i,{...e})}):i(e)}},27027:(e,n,t)=>{t.d(n,{Ay:()=>l,RM:()=>a});var o=t(74848),s=t(28453),r=t(40619);const a=[];function i(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsxs)(n.p,{children:["For a tutorial that uses vector data in a flow, see ",(0,o.jsx)(n.a,{href:"/chat-with-rag",children:"Create a vector RAG chatbot"}),"."]})}),"\n",(0,o.jsx)(n.p,{children:"The following example demonstrates how to use vector store components in flows alongside related components like embedding model and language model components.\nThese steps walk through important configuration details, functionality, and best practices for using these components effectively.\nThis is only one example; it isn't a prescriptive guide to all possible use cases or configurations."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Create a flow with the ",(0,o.jsx)(n.strong,{children:"Vector Store RAG"})," template."]}),"\n",(0,o.jsxs)(n.p,{children:["This template has two subflows.\nThe ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow loads embeddings and content into a vector database, and the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow runs a vector search to retrieve relevant context based on a user's query."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Configure the database connection for both ",(0,o.jsxs)(n.a,{href:"/bundles-datastax#astra-db",children:[(0,o.jsx)(n.strong,{children:"Astra DB"})," components"]}),", or replace them with another pair of vector store components of your choice.\nMake sure the components connect to the same vector store, and that the component in the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow is able to run a similarity search."]}),"\n",(0,o.jsxs)(n.p,{children:["The parameters you set in each vector store component depend on the component's role in your flow.\nIn this example, the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow ",(0,o.jsx)(n.em,{children:"writes"})," to the vector store, whereas the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow ",(0,o.jsx)(n.em,{children:"reads"})," from the vector store.\nTherefore, search-related parameters are only relevant to the ",(0,o.jsx)(n.strong,{children:"Vector Search"})," component in the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow."]}),"\n",(0,o.jsx)(n.p,{children:"For information about specific parameters, see the documentation for your chosen vector store component."}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"To configure the embedding model, do one of the following:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Use an OpenAI model"}),": In both ",(0,o.jsx)(n.strong,{children:"OpenAI Embeddings"})," components, enter your OpenAI API key.\nYou can use the default model or select a different OpenAI embedding model."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Use another provider"}),": Replace the ",(0,o.jsx)(n.strong,{children:"OpenAI Embeddings"})," components with another pair of ",(0,o.jsx)(n.a,{href:"/components-embedding-models",children:"embedding model components"})," of your choice, and then configure the parameters and credentials accordingly."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Use Astra DB vectorize"}),": If you are using an Astra DB vector store that has a vectorize integration, you can remove both ",(0,o.jsx)(n.strong,{children:"OpenAI Embeddings"})," components.\nIf you do this, the vectorize integration automatically generates embeddings from the ",(0,o.jsx)(n.strong,{children:"Ingest Data"})," (in the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow) and ",(0,o.jsx)(n.strong,{children:"Search Query"})," (in the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow)."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.admonition,{type:"tip",children:(0,o.jsx)(n.p,{children:"If your vector store already contains embeddings, make sure your embedding model components use the same model as your previous embeddings.\nMixing embedding models in the same vector store can produce inaccurate search results."})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Recommended: In the ",(0,o.jsxs)(n.a,{href:"/split-text",children:[(0,o.jsx)(n.strong,{children:"Split Text"})," component"]}),", optimize the chunking settings for your embedding model.\nFor example, if your embedding model has a token limit of 512, then the ",(0,o.jsx)(n.strong,{children:"Chunk Size"})," parameter must not exceed that limit."]}),"\n",(0,o.jsxs)(n.p,{children:["Additionally, because the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow passes the chat input directly to the vector store component for vector search, make sure that your chat input string doesn't exceed your embedding model's limits.\nFor this example, you can enter a query that is within the limits; however, in a production environment, you might need to implement additional checks or preprocessing steps to ensure compliance.\nFor example, use additional components to prepare the chat input before running the vector search, or enforce chat input limits in your application code."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["In the ",(0,o.jsx)(n.strong,{children:"Language Model"})," component, enter your OpenAI API key, or select a different provider and model to use for the chat portion of the flow."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Run the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow to populate your vector store.\nIn the ",(0,o.jsx)(n.strong,{children:"Read File"})," component, select one or more files, and then click ",(0,o.jsx)(r.A,{name:"Play","aria-hidden":"true"})," ",(0,o.jsx)(n.strong,{children:"Run component"})," on the vector store component in the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow loads files from your local machine, chunks them, generates embeddings for the chunks, and then stores the chunks and their embeddings in the vector database."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Embedding data into a vector store",src:t(7685).A+"",width:"4000",height:"2512"})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow is separate from the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow because you probably won't run it every time you use the chat.\nYou can run the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow as needed to preload or update the data in your vector store.\nThen, your chat interactions only use the components that are necessary for chat."]}),"\n",(0,o.jsxs)(n.p,{children:["If your vector store already contains data that you want to use for vector search, then you don't need to run the ",(0,o.jsx)(n.strong,{children:"Load Data"})," subflow."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Open the ",(0,o.jsx)(n.strong,{children:"Playground"})," and start chatting to run the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow."]}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow generates an embedding from chat input, runs a vector search to retrieve similar content from your vector store, parses the search results into supplemental context for the LLM, and then uses the LLM to generate a natural language response to your query.\nThe LLM uses the vector search results along with its internal training data and tools, such as basic web search and datetime information, to produce the response."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Retrieval from a vector store",src:t(67719).A+"",width:"4000",height:"1324"})}),"\n",(0,o.jsxs)(n.p,{children:["To avoid passing the entire block of raw search results to the LLM, the ",(0,o.jsx)(n.strong,{children:"Parser"})," component extracts ",(0,o.jsx)(n.code,{children:"text"})," strings from the search results ",(0,o.jsx)(n.code,{children:"Data"})," object, and then passes them to the ",(0,o.jsx)(n.strong,{children:"Prompt Template"})," component in ",(0,o.jsx)(n.code,{children:"Message"})," format.\nFrom there, the strings and other template content are compiled into natural language instructions for the LLM."]}),"\n",(0,o.jsxs)(n.p,{children:["You can use other components for this transformation, such as the ",(0,o.jsx)(n.strong,{children:"Data Operations"})," component, depending on how you want to use the search results."]}),"\n",(0,o.jsxs)(n.p,{children:["To view the raw search results, click ",(0,o.jsx)(r.A,{name:"TextSearch","aria-hidden":"true"})," ",(0,o.jsx)(n.strong,{children:"Inspect output"})," on the vector store component after running the ",(0,o.jsx)(n.strong,{children:"Retriever"})," subflow."]}),"\n"]}),"\n"]})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(i,{...e})}):i(e)}},31929:(e,n,t)=>{t.d(n,{Ay:()=>l,RM:()=>a});var o=t(74848),s=t(28453),r=t(40619);const a=[];function i(e){const n={a:"a",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,o.jsxs)(n.p,{children:["Some parameters are hidden by default in the visual editor.\nYou can modify all parameters through the ",(0,o.jsx)(r.A,{name:"SlidersHorizontal","aria-hidden":"true"})," ",(0,o.jsx)(n.strong,{children:"Controls"})," in the ",(0,o.jsx)(n.a,{href:"/concepts-components#component-menus",children:"component's header menu"}),"."]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(i,{...e})}):i(e)}},45591:(e,n,t)=>{t.r(n),t.d(n,{CH:()=>x,assets:()=>g,chCodeConfig:()=>f,contentTitle:()=>m,default:()=>b,frontMatter:()=>p,metadata:()=>o,toc:()=>j});const o=JSON.parse('{"id":"Components/components-models","title":"Language Model","description":"Language model components in Langflow generate text using a specified Large Language Model (LLM).","source":"@site/docs/Components/components-models.mdx","sourceDirName":"Components","slug":"/components-models","permalink":"/components-models","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Language Model","slug":"/components-models"},"sidebar":"docs","previous":{"title":"Structured Output","permalink":"/structured-output"},"next":{"title":"Prompt Template","permalink":"/components-prompts"}}');var s=t(74848),r=t(28453),a=t(24754),i=t(40619),l=t(11470),d=t(19365),c=t(31929),h=t(22444),u=t(27027);const p={title:"Language Model",slug:"/components-models"},m=void 0,g={},x={annotations:a.hk,Code:a.Cy},f={staticMediaQuery:"not screen, (max-width: 768px)",lineNumbers:!0,showCopyButton:!0,themeName:"github-dark"},j=[{value:"Use language model components in flows",id:"use-language-model-components-in-flows",level:2},{value:"Language model parameters",id:"language-model-parameters",level:2},...c.RM,{value:"Language model output types",id:"language-model-output-types",level:2},{value:"Additional language models",id:"additional-language-models",level:2},{value:"Pair models with vector stores",id:"pair-models-with-vector-stores",level:2},...h.RM,...u.RM];function y(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components},{Details:o}=n;return x||v("CH",!1),x.Code||v("CH.Code",!0),o||v("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("style",{dangerouslySetInnerHTML:{__html:'[data-ch-theme="github-dark"] {  --ch-t-colorScheme: dark;--ch-t-foreground: #c9d1d9;--ch-t-background: #0d1117;--ch-t-lighter-inlineBackground: #0d1117e6;--ch-t-editor-background: #0d1117;--ch-t-editor-foreground: #c9d1d9;--ch-t-editor-lineHighlightBackground: #6e76811a;--ch-t-editor-rangeHighlightBackground: #ffffff0b;--ch-t-editor-infoForeground: #3794FF;--ch-t-editor-selectionBackground: #264F78;--ch-t-focusBorder: #1f6feb;--ch-t-tab-activeBackground: #0d1117;--ch-t-tab-activeForeground: #c9d1d9;--ch-t-tab-inactiveBackground: #010409;--ch-t-tab-inactiveForeground: #8b949e;--ch-t-tab-border: #30363d;--ch-t-tab-activeBorder: #0d1117;--ch-t-editorGroup-border: #30363d;--ch-t-editorGroupHeader-tabsBackground: #010409;--ch-t-editorLineNumber-foreground: #6e7681;--ch-t-input-background: #0d1117;--ch-t-input-foreground: #c9d1d9;--ch-t-input-border: #30363d;--ch-t-icon-foreground: #8b949e;--ch-t-sideBar-background: #010409;--ch-t-sideBar-foreground: #c9d1d9;--ch-t-sideBar-border: #30363d;--ch-t-list-activeSelectionBackground: #6e768166;--ch-t-list-activeSelectionForeground: #c9d1d9;--ch-t-list-hoverBackground: #6e76811a;--ch-t-list-hoverForeground: #c9d1d9; }'}}),"\n","\n",(0,s.jsx)(n.p,{children:"Language model components in Langflow generate text using a specified Large Language Model (LLM).\nThese components accept inputs like chat messages, files, and instructions in order to generate a text response."}),"\n",(0,s.jsxs)(n.p,{children:["Langflow includes a ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component that has built-in support for many LLMs.\nAlternatively, you can use any ",(0,s.jsx)(n.a,{href:"#additional-language-models",children:"additional language model"})," in place of the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component."]}),"\n",(0,s.jsx)(n.h2,{id:"use-language-model-components-in-flows",children:"Use language model components in flows"}),"\n",(0,s.jsx)(n.p,{children:"Use language model components anywhere you would use an LLM in a flow."}),"\n",(0,s.jsxs)(l.A,{children:[(0,s.jsxs)(d.A,{value:"chat",label:"Chat",default:!0,children:[(0,s.jsx)(n.p,{children:"One of the most common use cases of language model components is to chat with LLMs in your flows."}),(0,s.jsxs)(n.p,{children:["The following example uses a language model component in a chatbot flow similar to the ",(0,s.jsx)(n.strong,{children:"Basic Prompting"})," template."]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component to your flow, and then enter your OpenAI API key."]}),"\n",(0,s.jsxs)(n.p,{children:["This example uses the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component's default OpenAI model.\nIf you want to use a different provider or model, edit the ",(0,s.jsx)(n.strong,{children:"Model Provider"}),", ",(0,s.jsx)(n.strong,{children:"Model Name"}),", and ",(0,s.jsx)(n.strong,{children:"API Key"})," fields accordingly."]}),"\n",(0,s.jsxs)(n.admonition,{title:"My preferred provider or model isn't listed",type:"tip",children:[(0,s.jsxs)(n.p,{children:["If you want to use a provider or model that isn't built-in to the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component, you can replace this component with any ",(0,s.jsx)(n.a,{href:"#additional-language-models",children:"additional language model"}),"."]}),(0,s.jsxs)(n.p,{children:["Browse ",(0,s.jsx)(i.A,{name:"Blocks","aria-hidden":"true"})," ",(0,s.jsx)(n.a,{href:"/components-bundle-components",children:(0,s.jsx)(n.strong,{children:"Bundles"})})," or ",(0,s.jsx)(i.A,{name:"Search","aria-hidden":"true"})," ",(0,s.jsx)(n.strong,{children:"Search"})," for your preferred provider to find additional language models."]})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.a,{href:"/concepts-components#component-menus",children:"component's header menu"}),", click ",(0,s.jsx)(i.A,{name:"SlidersHorizontal","aria-hidden":"true"})," ",(0,s.jsx)(n.strong,{children:"Controls"}),", enable the ",(0,s.jsx)(n.strong,{children:"System Message"})," parameter, and then click ",(0,s.jsx)(n.strong,{children:"Close"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add a ",(0,s.jsxs)(n.a,{href:"/components-prompts",children:[(0,s.jsx)(n.strong,{children:"Prompt Template"})," component"]})," to your flow."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.strong,{children:"Template"})," field, enter some instructions for the LLM, such as ",(0,s.jsx)(n.code,{children:"You are an expert in geography who is tutoring high school students"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Connect the ",(0,s.jsx)(n.strong,{children:"Prompt Template"})," component's output to the ",(0,s.jsx)(n.strong,{children:"Language Model"})," component's ",(0,s.jsx)(n.strong,{children:"System Message"})," input."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add ",(0,s.jsxs)(n.a,{href:"/chat-input-and-output",children:[(0,s.jsx)(n.strong,{children:"Chat Input"})," and ",(0,s.jsx)(n.strong,{children:"Chat Output"})," components"]})," to your flow.\nThese components are required for direct chat interaction with an LLM."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Connect the ",(0,s.jsx)(n.strong,{children:"Chat Input"})," component to the ",(0,s.jsx)(n.strong,{children:"Language Model"})," component's ",(0,s.jsx)(n.strong,{children:"Input"}),", and then connect the ",(0,s.jsx)(n.strong,{children:"Language Model"})," component's ",(0,s.jsx)(n.strong,{children:"Message"})," output to the ",(0,s.jsx)(n.strong,{children:"Chat Output"})," component."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"A basic prompting flow with Language Model, Prompt Template, Chat Input, and Chat Output components",src:t(80625).A+"",width:"2000",height:"1060"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Open the ",(0,s.jsx)(n.strong,{children:"Playground"}),", and ask a question to chat with the LLM and test the flow, such as ",(0,s.jsx)(n.code,{children:"What is the capital of Utah?"}),"."]}),"\n",(0,s.jsxs)(o,{children:[(0,s.jsx)("summary",{children:"Result"}),(0,s.jsx)(n.p,{children:"The following response is an example of an OpenAI model's response.\nYour actual response may vary based on the model version at the time of your request, your template, and input."}),(0,s.jsx)(x.Code,{codeConfig:f,northPanel:{tabs:[""],active:"",heightRatio:1},files:[{name:"",focus:"",code:{lines:[{tokens:[{content:"The capital of Utah is Salt Lake City. It is not only the largest city in the state but also serves as the cultural and economic center of Utah. Salt Lake City was founded in 1847 by Mormon pioneers and is known for its proximity to the Great Salt Lake and its role in the history of the Church of Jesus Christ of Latter-day Saints. For more information, you can refer to sources such as the U.S. Geological Survey or the official state website of Utah.",props:{}}]}],lang:"text"},annotations:[]}]})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Optional: Try a different model or provider to see how the response changes.\nFor example, if you are using the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component, you could try an Anthropic model."]}),"\n",(0,s.jsxs)(n.p,{children:["Then, open the ",(0,s.jsx)(n.strong,{children:"Playground"}),", ask the same question as you did before, and then compare the content and format of the responses."]}),"\n",(0,s.jsx)(n.p,{children:"This helps you understand how different models handle the same request so you can choose the best model for your use case.\nYou can also learn more about different models in each model provider's documentation."}),"\n",(0,s.jsxs)(o,{children:[(0,s.jsx)("summary",{children:"Result"}),(0,s.jsx)(n.p,{children:"The following response is an example of an Anthropic model's response.\nYour actual response may vary based on the model version at the time of your request, your template, and input."}),(0,s.jsx)(n.p,{children:"Note that this response is shorter and includes sources, whereas the previous OpenAI response was more encyclopedic and didn't cite sources."}),(0,s.jsx)(x.Code,{codeConfig:f,northPanel:{tabs:[""],active:"",heightRatio:1},files:[{name:"",focus:"",code:{lines:[{tokens:[{content:"The capital of Utah is Salt Lake City. It is also the most populous city in the state. Salt Lake City has been the capital of Utah since 1896, when Utah became a state.",props:{}}]},{tokens:[{content:"Sources:",props:{}}]},{tokens:[{content:"Utah State Government Official Website (utah.gov)",props:{}}]},{tokens:[{content:"U.S. Census Bureau",props:{}}]},{tokens:[{content:"Encyclopedia Britannica",props:{}}]}],lang:"text"},annotations:[]}]})]}),"\n"]}),"\n"]})]}),(0,s.jsxs)(d.A,{value:"drivers",label:"Drivers",children:[(0,s.jsxs)(n.p,{children:["Some components use a language model component to perform LLM-driven actions.\nTypically, these components prepare data for further processing by downstream components, rather than emitting direct chat output.\nFor an example, see the ",(0,s.jsxs)(n.a,{href:"/smart-transform",children:[(0,s.jsx)(n.strong,{children:"Smart Transform"})," component"]}),"."]}),(0,s.jsxs)(n.p,{children:["A component must accept a ",(0,s.jsx)(n.code,{children:"LanguageModel"})," input to use a language model component as a driver, and you must set the language model component's output type to ",(0,s.jsx)(n.code,{children:"LanguageModel"}),".\nFor more information, see ",(0,s.jsx)(n.a,{href:"#language-model-output-types",children:"Language Model output types"}),"."]})]}),(0,s.jsxs)(d.A,{value:"agents",label:"Agents",children:[(0,s.jsxs)(n.p,{children:["If you don't want to use the ",(0,s.jsx)(n.strong,{children:"Agent"})," component's built-in LLMs, you can use a language model component to connect your preferred model:"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Add a language model component to your flow."}),"\n",(0,s.jsxs)(n.p,{children:["You can use the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component or browse ",(0,s.jsx)(i.A,{name:"Blocks","aria-hidden":"true"})," ",(0,s.jsx)(n.a,{href:"/components-bundle-components",children:(0,s.jsx)(n.strong,{children:"Bundles"})})," to find additional language models.\nComponents in bundles may not have ",(0,s.jsx)(n.code,{children:"language model"})," in the name.\nFor example, Azure OpenAI LLMs are provided through the ",(0,s.jsxs)(n.a,{href:"/bundles-azure#azure-openai",children:[(0,s.jsx)(n.strong,{children:"Azure OpenAI"})," component"]}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Configure the language model component as needed to connect to your preferred model."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Change the language model component's output type from ",(0,s.jsx)(n.strong,{children:"Model Response"})," to ",(0,s.jsx)(n.strong,{children:"Language Model"}),".\nThe output port changes to a ",(0,s.jsx)(n.code,{children:"LanguageModel"})," port.\nThis is required to connect the language model component to the ",(0,s.jsx)(n.strong,{children:"Agent"})," component.\nFor more information, see ",(0,s.jsx)(n.a,{href:"#language-model-output-types",children:"Language Model output types"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Add an ",(0,s.jsx)(n.strong,{children:"Agent"})," component to the flow, and then set ",(0,s.jsx)(n.strong,{children:"Model Provider"})," to ",(0,s.jsx)(n.strong,{children:"Connect other models"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Model Provider"})," field changes to a ",(0,s.jsx)(n.strong,{children:"Language Model"})," (",(0,s.jsx)(n.code,{children:"LanguageModel"}),") input."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Connect the language model component's output to the ",(0,s.jsx)(n.strong,{children:"Agent"})," component's ",(0,s.jsx)(n.strong,{children:"Language Model"})," input.\nThe ",(0,s.jsx)(n.strong,{children:"Agent"})," component now inherits the language model settings from the connected language model component instead of using any of the built-in models."]}),"\n"]}),"\n"]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"language-model-parameters",children:"Language model parameters"}),"\n",(0,s.jsxs)(n.p,{children:["The following parameters are for the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component.\nOther language model components can have additional or different parameters."]}),"\n","\n",(0,s.jsx)(c.Ay,{}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Name"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"provider"}),(0,s.jsx)(n.td,{children:"String"}),(0,s.jsx)(n.td,{children:"Input parameter. The model provider to use."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"model_name"}),(0,s.jsx)(n.td,{children:"String"}),(0,s.jsx)(n.td,{children:"Input parameter. The name of the model to use. Options depend on the selected provider."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"api_key"}),(0,s.jsx)(n.td,{children:"SecretString"}),(0,s.jsx)(n.td,{children:"Input parameter. The API Key for authentication with the selected provider."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"input_value"}),(0,s.jsx)(n.td,{children:"String"}),(0,s.jsx)(n.td,{children:"Input parameter. The input text to send to the model."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"system_message"}),(0,s.jsx)(n.td,{children:"String"}),(0,s.jsx)(n.td,{children:"Input parameter. A system message that helps set the behavior of the assistant."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"stream"}),(0,s.jsx)(n.td,{children:"Boolean"}),(0,s.jsxs)(n.td,{children:["Input parameter. Whether to stream the response. Default: ",(0,s.jsx)(n.code,{children:"false"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"temperature"}),(0,s.jsx)(n.td,{children:"Float"}),(0,s.jsxs)(n.td,{children:["Input parameter. Controls randomness in responses. Range: ",(0,s.jsx)(n.code,{children:"[0.0, 1.0]"}),". Default: ",(0,s.jsx)(n.code,{children:"0.1"}),"."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"model"}),(0,s.jsx)(n.td,{children:"LanguageModel"}),(0,s.jsxs)(n.td,{children:["Output parameter. Alternative output type to the default ",(0,s.jsx)(n.code,{children:"Message"})," output. Produces an instance of Chat configured with the specified parameters. See ",(0,s.jsx)(n.a,{href:"#language-model-output-types",children:"Language Model output types"}),"."]})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"language-model-output-types",children:"Language model output types"}),"\n",(0,s.jsx)(n.p,{children:"Language model components, including the core component and bundled components, can produce two types of output:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Response"}),": The default output type emits the model's generated response as ",(0,s.jsxs)(n.a,{href:"/data-types#message",children:[(0,s.jsx)(n.code,{children:"Message"})," data"]}),".\nUse this output type when you want the typical LLM interaction where the LLM produces a text response based on given input."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Language Model"}),": Change the language model component's output type to ",(0,s.jsx)(n.a,{href:"/data-types#languagemodel",children:(0,s.jsx)(n.code,{children:"LanguageModel"})})," when you need to attach an LLM to another component in your flow, such as an ",(0,s.jsx)(n.strong,{children:"Agent"})," or ",(0,s.jsx)(n.strong,{children:"Smart Transform"})," component."]}),"\n",(0,s.jsxs)(n.p,{children:["With this configuration, the language model component supports an action completed by another component, rather than a direct chat interaction.\nFor an example, the ",(0,s.jsx)(n.strong,{children:"Smart Transform"})," component uses an LLM to create a function from natural language input."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"additional-language-models",children:"Additional language models"}),"\n",(0,s.jsxs)(n.p,{children:["If your provider or model isn't supported by the ",(0,s.jsx)(n.strong,{children:"Language Model"})," core component, additional language model components are available in ",(0,s.jsx)(i.A,{name:"Blocks","aria-hidden":"true"})," ",(0,s.jsx)(n.a,{href:"/components-bundle-components",children:(0,s.jsx)(n.strong,{children:"Bundles"})}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["You can use these components in the same way that you use the core ",(0,s.jsx)(n.strong,{children:"Language Model"})," component, as explained in ",(0,s.jsx)(n.a,{href:"#use-language-model-components-in-flows",children:"Use language model components in flows"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"pair-models-with-vector-stores",children:"Pair models with vector stores"}),"\n","\n",(0,s.jsx)(h.Ay,{}),"\n",(0,s.jsxs)(o,{children:[(0,s.jsx)("summary",{children:"Example: Vector search flow"}),(0,s.jsx)(u.Ay,{})]})]})}function b(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(y,{...e})}):y(e)}function v(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}},67719:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/vector-store-retrieval-af7257d77ff0259ab1a0980641d464ce.png"},80625:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/component-language-model-1ddc1bd5726d5ea55ac91f7813a024d7.png"}}]);