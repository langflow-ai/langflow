"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[5304],{21832:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Integrations/Nvidia/integrations-nvidia-nim-wsl2","title":"Integrate NVIDIA NIMs with Langflow","description":"Connect Langflow with NVIDIA NIM on an RTX Windows system with Windows Subsystem for Linux 2 (WSL2) installed.","source":"@site/docs/Integrations/Nvidia/integrations-nvidia-nim-wsl2.md","sourceDirName":"Integrations/Nvidia","slug":"/integrations-nvidia-ingest-wsl2","permalink":"/integrations-nvidia-ingest-wsl2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Integrate NVIDIA NIMs with Langflow","slug":"/integrations-nvidia-ingest-wsl2"},"sidebar":"docs","previous":{"title":"Integrate NVIDIA Ingest with Langflow","permalink":"/integrations-nvidia-ingest"},"next":{"title":"Join the Langflow community","permalink":"/contributing-community"}}');var s=t(74848),o=t(28453);const r={title:"Integrate NVIDIA NIMs with Langflow",slug:"/integrations-nvidia-ingest-wsl2"},l=void 0,d={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Use the NVIDIA NIM in a flow",id:"use-the-nvidia-nim-in-a-flow",level:2}];function a(n){const e={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(e.p,{children:["Connect ",(0,s.jsx)(e.strong,{children:"Langflow"})," with ",(0,s.jsx)(e.strong,{children:"NVIDIA NIM"})," on an RTX Windows system with ",(0,s.jsx)(e.a,{href:"https://learn.microsoft.com/en-us/windows/wsl/install",children:"Windows Subsystem for Linux 2 (WSL2)"})," installed."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA NIM (NVIDIA Inference Microservices)"})," provides containers to self-host GPU-accelerated inferencing microservices.\nIn this example, you connect a model component in ",(0,s.jsx)(e.strong,{children:"Langflow"})," to a deployed ",(0,s.jsx)(e.code,{children:"mistral-nemo-12b-instruct"})," NIM on an ",(0,s.jsx)(e.strong,{children:"RTX Windows system"})," with ",(0,s.jsx)(e.strong,{children:"WSL2"}),"."]}),"\n",(0,s.jsxs)(e.p,{children:["For more information on NVIDIA NIM, see the ",(0,s.jsx)(e.a,{href:"https://docs.nvidia.com/nim/index.html",children:"NVIDIA documentation"}),"."]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://docs.nvidia.com/nim/wsl2/latest/getting-started.html",children:"NVIDIA NIM WSL2 installed"})}),"\n",(0,s.jsxs)(e.li,{children:["A NIM container deployed according to the model's instructions. Prerequisites vary between models.\nFor example, to deploy the ",(0,s.jsx)(e.code,{children:"mistral-nemo-12b-instruct"})," NIM, follow the instructions for ",(0,s.jsx)(e.strong,{children:"Windows on RTX AI PCs (Beta)"})," on your ",(0,s.jsx)(e.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"model's deployment overview"})]}),"\n",(0,s.jsx)(e.li,{children:"Windows 11 build 23H2 or later"}),"\n",(0,s.jsx)(e.li,{children:"At least 12 GB of RAM"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"use-the-nvidia-nim-in-a-flow",children:"Use the NVIDIA NIM in a flow"}),"\n",(0,s.jsxs)(e.p,{children:["To connect the NIM you've deployed with Langflow, add the ",(0,s.jsx)(e.strong,{children:"NVIDIA"})," model component to a flow."]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["Create a ",(0,s.jsx)(e.a,{href:"/get-started-quickstart",children:"basic prompting flow"}),"."]}),"\n",(0,s.jsxs)(e.li,{children:["Replace the ",(0,s.jsx)(e.strong,{children:"OpenAI"})," model component with the ",(0,s.jsx)(e.strong,{children:"NVIDIA"})," component."]}),"\n",(0,s.jsxs)(e.li,{children:["In the ",(0,s.jsx)(e.strong,{children:"NVIDIA"})," component's ",(0,s.jsx)(e.strong,{children:"Base URL"})," field, add the URL where your NIM is accessible. If you followed your model's ",(0,s.jsx)(e.a,{href:"https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md",children:"deployment instructions"}),", the value is ",(0,s.jsx)(e.code,{children:"http://localhost:8000/v1"}),"."]}),"\n",(0,s.jsxs)(e.li,{children:["In the ",(0,s.jsx)(e.strong,{children:"NVIDIA"})," component's ",(0,s.jsx)(e.strong,{children:"NVIDIA API Key"})," field, add your NVIDIA API Key."]}),"\n",(0,s.jsxs)(e.li,{children:["Select your model from the ",(0,s.jsx)(e.strong,{children:"Model Name"})," dropdown."]}),"\n",(0,s.jsxs)(e.li,{children:["Open the ",(0,s.jsx)(e.strong,{children:"Playground"})," and chat with your ",(0,s.jsx)(e.strong,{children:"NIM"})," model."]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(a,{...n})}):a(n)}},28453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>l});var i=t(96540);const s={},o=i.createContext(s);function r(n){const e=i.useContext(o);return i.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);