"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[548],{17886:(e,n,o)=>{o.d(n,{A:()=>i});o(96540);var s=o(64058),t=o(74848);function i({name:e,...n}){const o=s[e];return o?(0,t.jsx)(o,{...n}):null}},28453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>c});var s=o(96540);const t={},i=s.createContext(t);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(i.Provider,{value:n},e.children)}},38223:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>c,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"Concepts/concepts-voice-mode","title":"Use voice mode","description":"You can use Langflow\'s voice mode to interact with your flows verbally through a microphone and speakers.","source":"@site/docs/Concepts/concepts-voice-mode.mdx","sourceDirName":"Concepts","slug":"/concepts-voice-mode","permalink":"/concepts-voice-mode","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Use voice mode","slug":"/concepts-voice-mode"},"sidebar":"docs","previous":{"title":"Use the Playground","permalink":"/concepts-playground"},"next":{"title":"Langflow data types","permalink":"/data-types"}}');var t=o(74848),i=o(28453),r=o(17886);const c={title:"Use voice mode",slug:"/concepts-voice-mode"},a=void 0,l={},d=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Test voice mode in the Playground",id:"test-voice-mode-in-the-playground",level:2},{value:"Develop applications with websockets endpoints",id:"develop-applications-with-websockets-endpoints",level:2},{value:"Voice-to-voice audio streaming",id:"voice-to-voice-audio-streaming",level:3},{value:"Speech-to-text audio transcription",id:"speech-to-text-audio-transcription",level:3},{value:"Session IDs for websockets endpoints",id:"session-ids-for-websockets-endpoints",level:3}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"You can use Langflow's voice mode to interact with your flows verbally through a microphone and speakers."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Voice mode requires the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["A flow with ",(0,t.jsx)(n.strong,{children:"Chat Input"}),", ",(0,t.jsx)(n.strong,{children:"Language Model"}),", and ",(0,t.jsx)(n.strong,{children:"Chat Output"})," components."]}),"\n",(0,t.jsxs)(n.p,{children:["If your flow has an ",(0,t.jsx)(n.strong,{children:"Agent"})," component, make sure the tools in your flow have accurate names and descriptions to help the agent choose which tools to use."]}),"\n",(0,t.jsxs)(n.p,{children:["Additionally, be aware that voice mode overrides typed instructions in the ",(0,t.jsx)(n.strong,{children:"Agent"})," component's ",(0,t.jsx)(n.strong,{children:"Agent Instructions"})," field."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["An ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/",children:"OpenAI"})," account and an OpenAI API key because Langflow uses the OpenAI API to process voice input and generate responses."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Optional: An ",(0,t.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," API key to enable voice options for the LLM's response."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"A microphone and speakers."}),"\n",(0,t.jsx)(n.p,{children:"A high quality microphone and minimal background noise are recommended for optimal voice comprehension."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"test-voice-mode-in-the-playground",children:"Test voice mode in the Playground"}),"\n",(0,t.jsxs)(n.p,{children:["In the ",(0,t.jsx)(n.strong,{children:"Playground"}),", click the ",(0,t.jsx)(r.A,{name:"Mic","aria-hidden":"true"})," ",(0,t.jsx)(n.strong,{children:"Microphone"})," to enable voice mode and verbally interact with your flows through a microphone and speakers."]}),"\n",(0,t.jsxs)(n.p,{children:["The following steps use the ",(0,t.jsxs)(n.a,{href:"/simple-agent",children:[(0,t.jsx)(n.strong,{children:"Simple Agent"})," template"]})," to demonstrate how to enable voice mode:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Create a flow based on the ",(0,t.jsx)(n.strong,{children:"Simple Agent"})," template."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Add your ",(0,t.jsx)(n.strong,{children:"OpenAI API key"})," credentials to the ",(0,t.jsx)(n.strong,{children:"Agent"})," component."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Click ",(0,t.jsx)(n.strong,{children:"Playground"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Click the ",(0,t.jsx)(r.A,{name:"Mic","aria-hidden":"true"})," ",(0,t.jsx)(n.strong,{children:"Microphone"})," icon to open the ",(0,t.jsx)(n.strong,{children:"Voice mode"})," dialog."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Enter your OpenAI API key, and then click ",(0,t.jsx)(n.strong,{children:"Save"}),". Langflow saves the key as a ",(0,t.jsx)(n.a,{href:"/configuration-global-variables",children:"global variable"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"If you are prompted to grant microphone access, you must allow microphone access to use voice mode.\nIf microphone access is blocked, you won't be able to provide verbal input."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["For ",(0,t.jsx)(n.strong,{children:"Audio Input"}),", select the input device to use with voice mode."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Optional: Add an ElevenLabs API key to enable more voices for the LLM's response.\nLangflow saves this key as a global variable."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["For ",(0,t.jsx)(n.strong,{children:"Preferred Language"}),", select the language you want to use for your conversations with the LLM.\nThis option changes both the expected input language and the response language."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Speak into your microphone to start the chat."}),"\n",(0,t.jsxs)(n.p,{children:["If configured correctly, the waveform in the voice mode dialog registers your input, and then the agent's logic and response are described verbally and in the ",(0,t.jsx)(n.strong,{children:"Playground"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"develop-applications-with-websockets-endpoints",children:"Develop applications with websockets endpoints"}),"\n",(0,t.jsxs)(n.p,{children:["Langflow exposes two OpenAI Realtime API-compatible websocket endpoints for your flows.\nYou can build applications against these endpoints the same way you would build against ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime#connect-with-websockets",children:"OpenAI Realtime API websockets"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The Langflow API's websockets endpoints require an ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/overview",children:"OpenAI API key"})," for authentication, and they support an optional ",(0,t.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," integration with an ElevenLabs API key."]}),"\n",(0,t.jsx)(n.p,{children:"Additionally, both endpoints require that you provide the flow ID in the endpoint path."}),"\n",(0,t.jsx)(n.h3,{id:"voice-to-voice-audio-streaming",children:"Voice-to-voice audio streaming"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"/ws/flow_as_tool/$FLOW_ID"})," endpoint establishes a connection to OpenAI Realtime voice, and then invokes the specified flow as a tool according to the ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets",children:"OpenAI Realtime model"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"This approach is ideal for low latency applications, but it is less deterministic because the OpenAI voice-to-voice model determines when to call your flow."}),"\n",(0,t.jsx)(n.h3,{id:"speech-to-text-audio-transcription",children:"Speech-to-text audio transcription"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"/ws/flow_tts/$FLOW_ID"})," endpoint converts audio to text using ",(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-transcription",children:"OpenAI Realtime voice transcription"}),", and then directly invokes the specified flow for each transcript."]}),"\n",(0,t.jsx)(n.p,{children:"This approach is more deterministic but has higher latency."}),"\n",(0,t.jsxs)(n.p,{children:["This is the mode used in the Langflow ",(0,t.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"session-ids-for-websockets-endpoints",children:"Session IDs for websockets endpoints"}),"\n",(0,t.jsxs)(n.p,{children:["Both endpoints accept an optional ",(0,t.jsx)(n.code,{children:"/$SESSION_ID"})," path parameter to provide a unique ID for the conversation.\nIf omitted, Langflow uses the flow ID as the ",(0,t.jsx)(n.a,{href:"/session-id",children:"session ID"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["However, be aware that voice mode only maintains context within the current conversation instance.\nWhen you close the ",(0,t.jsx)(n.strong,{children:"Playground"})," or end a chat, verbal chat history is discarded and not available for future chat sessions."]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);