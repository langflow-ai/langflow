"use strict";(globalThis.webpackChunklangflow_docs=globalThis.webpackChunklangflow_docs||[]).push([[4173],{31929:(e,t,n)=>{n.d(t,{Ay:()=>l,RM:()=>d});var r=n(74848),o=n(28453),s=n(40619);const d=[];function i(e){const t={a:"a",p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,r.jsxs)(t.p,{children:["Some parameters are hidden by default in the visual editor.\nYou can modify all parameters through the ",(0,r.jsx)(s.A,{name:"SlidersHorizontal","aria-hidden":"true"})," ",(0,r.jsx)(t.strong,{children:"Controls"})," in the ",(0,r.jsx)(t.a,{href:"/concepts-components#component-menus",children:"component's header menu"}),"."]})}function l(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(i,{...e})}):i(e)}},59345:(e,t,n)=>{n.r(t),n.d(t,{CH:()=>p,assets:()=>h,chCodeConfig:()=>u,contentTitle:()=>c,default:()=>x,frontMatter:()=>a,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"Components/bundles-vllm","title":"vLLM","description":"Bundles contain custom components that support specific third-party integrations with Langflow.","source":"@site/docs/Components/bundles-vllm.mdx","sourceDirName":"Components","slug":"/bundles-vllm","permalink":"/bundles-vllm","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"vLLM","slug":"/bundles-vllm"},"sidebar":"docs","previous":{"title":"Upstash","permalink":"/bundles-upstash"},"next":{"title":"Vectara","permalink":"/bundles-vectara"}}');var o=n(74848),s=n(28453),d=n(24754),i=n(40619),l=n(31929);const a={title:"vLLM",slug:"/bundles-vllm"},c=void 0,h={},p={annotations:d.hk,Code:d.Cy},u={staticMediaQuery:"not screen, (max-width: 768px)",lineNumbers:!0,showCopyButton:!0,themeName:"github-dark"},m=[{value:"vLLM text generation",id:"vllm-text-generation",level:2},{value:"vLLM text generation parameters",id:"vllm-text-generation-parameters",level:3},...l.RM,{value:"Setting up vLLM",id:"setting-up-vllm",level:2},{value:"See also",id:"see-also",level:2}];function g(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return p||j("CH",!1),p.Code||j("CH.Code",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("style",{dangerouslySetInnerHTML:{__html:'[data-ch-theme="github-dark"] {  --ch-t-colorScheme: dark;--ch-t-foreground: #c9d1d9;--ch-t-background: #0d1117;--ch-t-lighter-inlineBackground: #0d1117e6;--ch-t-editor-background: #0d1117;--ch-t-editor-foreground: #c9d1d9;--ch-t-editor-lineHighlightBackground: #6e76811a;--ch-t-editor-rangeHighlightBackground: #ffffff0b;--ch-t-editor-infoForeground: #3794FF;--ch-t-editor-selectionBackground: #264F78;--ch-t-focusBorder: #1f6feb;--ch-t-tab-activeBackground: #0d1117;--ch-t-tab-activeForeground: #c9d1d9;--ch-t-tab-inactiveBackground: #010409;--ch-t-tab-inactiveForeground: #8b949e;--ch-t-tab-border: #30363d;--ch-t-tab-activeBorder: #0d1117;--ch-t-editorGroup-border: #30363d;--ch-t-editorGroupHeader-tabsBackground: #010409;--ch-t-editorLineNumber-foreground: #6e7681;--ch-t-input-background: #0d1117;--ch-t-input-foreground: #c9d1d9;--ch-t-input-border: #30363d;--ch-t-icon-foreground: #8b949e;--ch-t-sideBar-background: #010409;--ch-t-sideBar-foreground: #c9d1d9;--ch-t-sideBar-border: #30363d;--ch-t-list-activeSelectionBackground: #6e768166;--ch-t-list-activeSelectionForeground: #c9d1d9;--ch-t-list-hoverBackground: #6e76811a;--ch-t-list-hoverForeground: #c9d1d9; }'}}),"\n","\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(i.A,{name:"Blocks","aria-hidden":"true"})," ",(0,o.jsx)(t.a,{href:"/components-bundle-components",children:(0,o.jsx)(t.strong,{children:"Bundles"})})," contain custom components that support specific third-party integrations with Langflow."]}),"\n",(0,o.jsxs)(t.p,{children:["This page describes the components that are available in the ",(0,o.jsx)(t.strong,{children:"vLLM"})," bundle."]}),"\n",(0,o.jsxs)(t.p,{children:["For more information about vLLM features and functionality used by vLLM components, see the ",(0,o.jsx)(t.a,{href:"https://docs.vllm.ai/",children:"vLLM documentation"}),"."]}),"\n",(0,o.jsx)(t.h2,{id:"vllm-text-generation",children:"vLLM text generation"}),"\n",(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.strong,{children:"vLLM"})," component generates text using ",(0,o.jsx)(t.a,{href:"https://docs.vllm.ai/en/latest/models/supported_models.html",children:"vLLM models"})," via an OpenAI-compatible API."]}),"\n",(0,o.jsx)(t.p,{children:"vLLM is a fast and easy-to-use library for LLM inference and serving. It provides high-throughput serving with efficient attention and PagedAttention, making it ideal for self-hosted model deployments."}),"\n",(0,o.jsx)(t.p,{children:"The component connects to a vLLM server running locally or remotely and uses the OpenAI-compatible API endpoint to generate text responses."}),"\n",(0,o.jsxs)(t.p,{children:["It can output either a ",(0,o.jsx)(t.strong,{children:"Model Response"})," (",(0,o.jsx)(t.a,{href:"/data-types#message",children:(0,o.jsx)(t.code,{children:"Message"})}),") or a ",(0,o.jsx)(t.strong,{children:"Language Model"})," (",(0,o.jsx)(t.a,{href:"/data-types#languagemodel",children:(0,o.jsx)(t.code,{children:"LanguageModel"})}),")."]}),"\n",(0,o.jsxs)(t.p,{children:["Use the ",(0,o.jsx)(t.strong,{children:"Language Model"})," output when you want to use a vLLM model as the LLM for another LLM-driven component, such as an ",(0,o.jsx)(t.strong,{children:"Agent"})," or ",(0,o.jsx)(t.strong,{children:"Smart Function"})," component."]}),"\n",(0,o.jsxs)(t.p,{children:["For more information, see ",(0,o.jsx)(t.a,{href:"/components-models",children:"Language model components"}),"."]}),"\n",(0,o.jsx)(t.h3,{id:"vllm-text-generation-parameters",children:"vLLM text generation parameters"}),"\n",(0,o.jsx)(l.Ay,{}),"\n",(0,o.jsxs)(t.table,{children:[(0,o.jsx)(t.thead,{children:(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.th,{children:"Name"}),(0,o.jsx)(t.th,{children:"Type"}),(0,o.jsx)(t.th,{children:"Description"})]})}),(0,o.jsxs)(t.tbody,{children:[(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"api_key"}),(0,o.jsx)(t.td,{children:"SecretString"}),(0,o.jsx)(t.td,{children:"Input parameter. The API Key to use for the vLLM model (optional for local servers)."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"model_name"}),(0,o.jsx)(t.td,{children:"String"}),(0,o.jsx)(t.td,{children:"Input parameter. The name of the vLLM model to use (e.g., 'ibm-granite/granite-3.3-8b-instruct')."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"api_base"}),(0,o.jsx)(t.td,{children:"String"}),(0,o.jsxs)(t.td,{children:["Input parameter. The base URL of the vLLM API server. Defaults to ",(0,o.jsx)(t.a,{href:"http://localhost:8000/v1",children:"http://localhost:8000/v1"})," for local vLLM server."]})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"temperature"}),(0,o.jsx)(t.td,{children:"Float"}),(0,o.jsx)(t.td,{children:"Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.1."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"max_tokens"}),(0,o.jsx)(t.td,{children:"Integer"}),(0,o.jsx)(t.td,{children:"Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"seed"}),(0,o.jsx)(t.td,{children:"Integer"}),(0,o.jsx)(t.td,{children:"Input parameter. The seed controls the reproducibility of the job. Default: 1."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"max_retries"}),(0,o.jsx)(t.td,{children:"Integer"}),(0,o.jsx)(t.td,{children:"Input parameter. The maximum number of retries to make when generating. Default: 5."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"timeout"}),(0,o.jsx)(t.td,{children:"Integer"}),(0,o.jsx)(t.td,{children:"Input parameter. The timeout for requests to vLLM completion API. Default: 700."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"model_kwargs"}),(0,o.jsx)(t.td,{children:"Dict"}),(0,o.jsx)(t.td,{children:"Input parameter. Additional keyword arguments to pass to the model."})]}),(0,o.jsxs)(t.tr,{children:[(0,o.jsx)(t.td,{children:"json_mode"}),(0,o.jsx)(t.td,{children:"Boolean"}),(0,o.jsx)(t.td,{children:"Input parameter. If True, it will output JSON regardless of passing a schema."})]})]})]}),"\n",(0,o.jsx)(t.h2,{id:"setting-up-vllm",children:"Setting up vLLM"}),"\n",(0,o.jsx)(t.p,{children:"To use the vLLM component, you need to have a vLLM server running. Here are the basic steps:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Install vLLM"}),": ",(0,o.jsx)(t.code,{children:"pip install vllm"})]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Start a vLLM server"}),":","\n",(0,o.jsx)(p.Code,{codeConfig:u,northPanel:{tabs:[""],active:"",heightRatio:1},files:[{name:"",focus:"",code:{lines:[{tokens:[{content:"python ",props:{style:{color:"#FFA657"}}},{content:"-m ",props:{style:{color:"#79C0FF"}}},{content:"vllm.entrypoints.openai.api_server ",props:{style:{color:"#A5D6FF"}}},{content:"--model ",props:{style:{color:"#79C0FF"}}},{content:"<",props:{style:{color:"#FF7B72"}}},{content:"model_nam",props:{style:{color:"#A5D6FF"}}},{content:"e",props:{style:{color:"#C9D1D9"}}},{content:"> ",props:{style:{color:"#FF7B72"}}},{content:"--port 8000",props:{style:{color:"#79C0FF"}}}]}],lang:"bash"},annotations:[]}]}),"\n"]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Configure the component"}),": Set the ",(0,o.jsx)(t.code,{children:"api_base"})," to your vLLM server URL (e.g., ",(0,o.jsx)(t.code,{children:"http://localhost:8000/v1"}),")"]}),"\n"]}),"\n",(0,o.jsxs)(t.p,{children:["For more detailed setup instructions, see the ",(0,o.jsx)(t.a,{href:"https://docs.vllm.ai/en/latest/getting_started/quickstart.html",children:"vLLM documentation"}),"."]}),"\n",(0,o.jsx)(t.h2,{id:"see-also",children:"See also"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsxs)(t.a,{href:"/components-agents",children:[(0,o.jsx)(t.strong,{children:"Agent"})," component"]})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsxs)(t.a,{href:"/components-models",children:[(0,o.jsx)(t.strong,{children:"Language Model"})," component"]})}),"\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM GitHub repository"})}),"\n"]})]})}function x(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(g,{...e})}):g(e)}function j(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);