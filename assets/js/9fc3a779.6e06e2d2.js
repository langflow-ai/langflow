"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[8716],{3612:(e,n,t)=>{t.d(n,{Z:()=>p});var o=t(7294),s=t(6010),r=t(5281),i=t(5999);const l={admonition:"admonition_LlT9",admonitionHeading:"admonitionHeading_tbUL",admonitionIcon:"admonitionIcon_kALy",admonitionContent:"admonitionContent_S0QG"};var a=t(5108);const d={note:{infimaClassName:"secondary",iconComponent:function(){return o.createElement("svg",{viewBox:"0 0 14 16"},o.createElement("path",{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))},label:o.createElement(i.Z,{id:"theme.admonition.note",description:"The default label used for the Note admonition (:::note)"},"note")},tip:{infimaClassName:"success",iconComponent:function(){return o.createElement("svg",{viewBox:"0 0 12 16"},o.createElement("path",{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))},label:o.createElement(i.Z,{id:"theme.admonition.tip",description:"The default label used for the Tip admonition (:::tip)"},"tip")},danger:{infimaClassName:"danger",iconComponent:function(){return o.createElement("svg",{viewBox:"0 0 12 16"},o.createElement("path",{fillRule:"evenodd",d:"M5.05.31c.81 2.17.41 3.38-.52 4.31C3.55 5.67 1.98 6.45.9 7.98c-1.45 2.05-1.7 6.53 3.53 7.7-2.2-1.16-2.67-4.52-.3-6.61-.61 2.03.53 3.33 1.94 2.86 1.39-.47 2.3.53 2.27 1.67-.02.78-.31 1.44-1.13 1.81 3.42-.59 4.78-3.42 4.78-5.56 0-2.84-2.53-3.22-1.25-5.61-1.52.13-2.03 1.13-1.89 2.75.09 1.08-1.02 1.8-1.86 1.33-.67-.41-.66-1.19-.06-1.78C8.18 5.31 8.68 2.45 5.05.32L5.03.3l.02.01z"}))},label:o.createElement(i.Z,{id:"theme.admonition.danger",description:"The default label used for the Danger admonition (:::danger)"},"danger")},info:{infimaClassName:"info",iconComponent:function(){return o.createElement("svg",{viewBox:"0 0 14 16"},o.createElement("path",{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))},label:o.createElement(i.Z,{id:"theme.admonition.info",description:"The default label used for the Info admonition (:::info)"},"info")},caution:{infimaClassName:"warning",iconComponent:function(){return o.createElement("svg",{viewBox:"0 0 16 16"},o.createElement("path",{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))},label:o.createElement(i.Z,{id:"theme.admonition.caution",description:"The default label used for the Caution admonition (:::caution)"},"caution")}},c={secondary:"note",important:"info",success:"tip",warning:"danger"};function h(e){const{mdxAdmonitionTitle:n,rest:t}=function(e){const n=o.Children.toArray(e),t=n.find((e=>o.isValidElement(e)&&"mdxAdmonitionTitle"===e.props?.mdxType)),s=o.createElement(o.Fragment,null,n.filter((e=>e!==t)));return{mdxAdmonitionTitle:t,rest:s}}(e.children);return{...e,title:e.title??n,children:t}}function p(e){const{children:n,type:t,title:i,icon:p}=h(e),u=function(e){const n=c[e]??e,t=d[n];return t||(a.warn(`No admonition config found for admonition type "${n}". Using Info as fallback.`),d.info)}(t),m=i??u.label,{iconComponent:x}=u,g=p??o.createElement(x,null);return o.createElement("div",{className:(0,s.Z)(r.k.common.admonition,r.k.common.admonitionType(e.type),"alert",`alert--${u.infimaClassName}`,l.admonition)},o.createElement("div",{className:l.admonitionHeading},o.createElement("span",{className:l.admonitionIcon},g),m),o.createElement("div",{className:l.admonitionContent},n))}},1729:(e,n,t)=>{t.r(n),t.d(n,{CH:()=>h,assets:()=>c,chCodeConfig:()=>p,contentTitle:()=>a,default:()=>x,frontMatter:()=>l,metadata:()=>d,toc:()=>u});t(7294);var o=t(5893),s=t(1151),r=t(9794),i=t(3612);const l={},a="LLMs",d={unversionedId:"components/llms",id:"components/llms",title:"LLMs",description:"We appreciate your understanding as we polish our documentation \u2013 it may contain some rough edges. Share your feedback or report issues to help us improve! \ud83d\udee0\ufe0f\ud83d\udcdd",source:"@site/docs/components/llms.mdx",sourceDirName:"components",slug:"/components/llms",permalink:"/components/llms",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Embeddings",permalink:"/components/embeddings"},next:{title:"Loaders",permalink:"/components/loaders"}},c={},h={annotations:r.ds,Code:r.EK},p={staticMediaQuery:"not screen, (max-width: 768px)",lineNumbers:!0,showCopyButton:!0,themeName:"github-light"},u=[{value:"Anthropic",id:"anthropic",level:3},{value:"ChatAnthropic",id:"chatanthropic",level:3},{value:"CTransformers",id:"ctransformers",level:3},{value:"ChatOpenAI",id:"chatopenai",level:3},{value:"Cohere",id:"cohere",level:3},{value:"HuggingFaceHub",id:"huggingfacehub",level:3},{value:"LlamaCpp",id:"llamacpp",level:3},{value:"OpenAI",id:"openai",level:3},{value:"VertexAI",id:"vertexai",level:3},{value:"ChatVertexAI",id:"chatvertexai",level:3}];function m(e){const n=Object.assign({h1:"h1",p:"p",hr:"hr",h3:"h3",a:"a",ul:"ul",li:"li",strong:"strong",code:"code"},(0,s.ah)(),e.components);return h||g("CH",!1),h.Code||g("CH.Code",!0),(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("style",{dangerouslySetInnerHTML:{__html:'[data-ch-theme="github-light"] {  --ch-t-colorScheme: light;--ch-t-foreground: #24292f;--ch-t-background: #ffffff;--ch-t-lighter-inlineBackground: #ffffffe6;--ch-t-editor-background: #ffffff;--ch-t-editor-foreground: #24292f;--ch-t-editor-lineHighlightBackground: #eaeef280;--ch-t-editor-rangeHighlightBackground: #fdff0033;--ch-t-editor-infoForeground: #1a85ff;--ch-t-editor-selectionBackground: #ADD6FF;--ch-t-focusBorder: #0969da;--ch-t-tab-activeBackground: #ffffff;--ch-t-tab-activeForeground: #24292f;--ch-t-tab-inactiveBackground: #f6f8fa;--ch-t-tab-inactiveForeground: #57606a;--ch-t-tab-border: #d0d7de;--ch-t-tab-activeBorder: #ffffff;--ch-t-editorGroup-border: #d0d7de;--ch-t-editorGroupHeader-tabsBackground: #f6f8fa;--ch-t-editorLineNumber-foreground: #8c959f;--ch-t-input-background: #ffffff;--ch-t-input-foreground: #24292f;--ch-t-input-border: #d0d7de;--ch-t-icon-foreground: #57606a;--ch-t-sideBar-background: #f6f8fa;--ch-t-sideBar-foreground: #24292f;--ch-t-sideBar-border: #d0d7de;--ch-t-list-activeSelectionBackground: #afb8c133;--ch-t-list-activeSelectionForeground: #24292f;--ch-t-list-hoverBackground: #eaeef280;--ch-t-list-hoverForeground: #24292f; }'}}),"\n","\n","\n",(0,o.jsx)(n.h1,{id:"llms",children:"LLMs"}),"\n",(0,o.jsx)(i.Z,{type:"caution",icon:"\ud83d\udea7",title:"ZONE UNDER CONSTRUCTION",children:(0,o.jsx)("p",{children:(0,o.jsx)(n.p,{children:"We appreciate your understanding as we polish our documentation \u2013 it may contain some rough edges. Share your feedback or report issues to help us improve! \ud83d\udee0\ufe0f\ud83d\udcdd"})})}),"\n",(0,o.jsx)(n.p,{children:"An LLM stands for Large Language Model. It is a core component of Langflow and provides a standard interface for interacting with different LLMs from various providers such as OpenAI, Cohere, and HuggingFace. LLMs are used widely throughout Langflow, including in chains and agents. They can be used to generate text based on a given prompt (or input)."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"anthropic",children:"Anthropic"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around Anthropic's large language models. Find out more at ",(0,o.jsx)(n.a,{href:"https://www.anthropic.com",children:"Anthropic"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"anthropic_api_key:"})," Used to authenticate and authorize access to the Anthropic API."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"anthropic_api_url:"})," Specifies the URL of the Anthropic API to connect to."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"chatanthropic",children:"ChatAnthropic"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around Anthropic's large language model used for chat-based interactions. Find out more at ",(0,o.jsx)(n.a,{href:"https://www.anthropic.com",children:"Anthropic"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"anthropic_api_key:"})," Used to authenticate and authorize access to the Anthropic API."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"anthropic_api_url:"})," Specifies the URL of the Anthropic API to connect to."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"ctransformers",children:"CTransformers"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"CTransformers"})," component provides access to the Transformer models implemented in C/C++ using the\xa0",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/ggml",children:"GGML"}),"\xa0library."]}),"\n",(0,o.jsxs)(n.p,{children:[":::info\nMake sure to have the ",(0,o.jsx)(n.code,{children:"ctransformers"})," python package installed. Learn more about installation, supported models, and usage ",(0,o.jsx)(n.a,{href:"https://github.com/marella/ctransformers",children:"here"}),".\n:::"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"config:"})," Configuration for the Transformer models. Check out ",(0,o.jsx)(n.a,{href:"https://github.com/marella/ctransformers#config",children:"config"}),". Defaults to:"]}),"\n",(0,o.jsx)(h.Code,{codeConfig:p,northPanel:{tabs:[""],active:"",heightRatio:1},files:[{name:"",focus:"",code:{lines:[{tokens:[{content:"{",props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"top_k": 40,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"top_p": 0.95,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"temperature": 0.8,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"repetition_penalty": 1.1,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"last_n_tokens": 64,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"seed": -1,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"max_new_tokens": 256,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"stop": null,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"stream": false,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"reset": true,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"batch_size": 8,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"threads": -1,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"context_length": -1,',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:'"gpu_layers": 0',props:{}}]},{tokens:[{content:"",props:{}}]},{tokens:[{content:"}",props:{}}]}],lang:"text"},annotations:[]}]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"model:"})," The path to a model file or directory or the name of a Hugging Face Hub model repo."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"model_file:"})," The name of the model file in the repo or directory."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"model_type:"})," Transformer model to be used. Learn more ",(0,o.jsx)(n.a,{href:"https://github.com/marella/ctransformers",children:"here"}),"."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"chatopenai",children:"ChatOpenAI"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://openai.com",children:"OpenAI's"})," chat large language models. This component supports some of the LLMs (Large Language Models) available by OpenAI and is used for tasks such as chatbots, Generative Question-Answering (GQA), and summarization."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens:"})," The maximum number of tokens to generate in the completion. ",(0,o.jsx)(n.code,{children:"-1"})," returns as many tokens as possible, given the prompt and the model's maximal context size \u2013 defaults to ",(0,o.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_kwargs:"})," Holds any model parameters valid for creating non-specified calls."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_name:"})," Defines the OpenAI chat model to be used."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"openai_api_base:"})," Used to specify the base URL for the OpenAI API. It is typically set to the API endpoint provided by the OpenAI service."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"openai_api_key:"}),"  Key used to authenticate and access the OpenAI API."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0.7"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"cohere",children:"Cohere"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://cohere.com",children:"Cohere's"})," large language models."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"cohere_api_key:"})," Holds the API key required to authenticate with the Cohere service."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens:"})," Maximum number of tokens to predict per generation \u2013 defaults to ",(0,o.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0.75"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"huggingfacehub",children:"HuggingFaceHub"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://www.huggingface.co/models",children:"HuggingFace"})," models."]}),"\n",(0,o.jsxs)(n.p,{children:[":::info\nThe HuggingFace Hub is an online platform that hosts over 120k models, 20k datasets, and 50k demo apps, all of which are open-source and publicly available. Discover more at ",(0,o.jsx)(n.a,{href:"http://www.huggingface.co",children:"HuggingFace"}),".\n:::"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"huggingfacehub_api_token:"})," Token needed to authenticate the API."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_kwargs:"})," Keyword arguments to pass to the model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"repo_id:"})," Model name to use \u2013 defaults to ",(0,o.jsx)(n.code,{children:"gpt2"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"task:"})," Task to call the model with. Should be a task that returns ",(0,o.jsx)(n.code,{children:"generated_text"})," or ",(0,o.jsx)(n.code,{children:"summary_text"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"llamacpp",children:"LlamaCpp"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"LlamaCpp"})," component provides access to the ",(0,o.jsx)(n.code,{children:"llama.cpp"})," models."]}),"\n",(0,o.jsxs)(n.p,{children:[":::info\nMake sure to have the ",(0,o.jsx)(n.code,{children:"llama.cpp"})," python package installed. Learn more about installation, supported models, and usage ",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp",children:"here"}),".\n:::"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"echo:"})," Whether to echo the prompt \u2013 defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"f16_kv:"})," Use half-precision for key/value cache \u2013 defaults to ",(0,o.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"last_n_tokens_size:"})," The number of tokens to look back at when applying the repeat_penalty. Defaults to ",(0,o.jsx)(n.code,{children:"64"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"logits_all:"})," Return logits for all tokens, not just the last token Defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"logprobs:"})," The number of logprobs to return. If None, no logprobs are returned."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"lora_base:"})," The path to the Llama LoRA base model."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"lora_path:"})," The path to the Llama LoRA. If None, no LoRa is loaded."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens:"})," The maximum number of tokens to generate. Defaults to ",(0,o.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_path:"})," The path to the Llama model file."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_batch:"})," Number of tokens to process in parallel. Should be a number between 1 and n_ctx. Defaults to ",(0,o.jsx)(n.code,{children:"8"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_ctx:"})," Token context window. Defaults to ",(0,o.jsx)(n.code,{children:"512"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_gpu_layers:"})," Number of layers to be loaded into GPU memory. Default None."]}),"\n",(0,o.jsxs)(n.li,{children:["**n_parts:**Number of parts to split the model into. If -1, the number of parts is automatically determined. Defaults to ",(0,o.jsx)(n.code,{children:"-1"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"n_threads:"})," Number of threads to use. If None, the number of threads is automatically determined."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"repeat_penalty:"})," The penalty to apply to repeated tokens. Defaults to ",(0,o.jsx)(n.code,{children:"1.1"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"seed:"})," Seed. If -1, a random seed is used. Defaults to ",(0,o.jsx)(n.code,{children:"-1"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"stop:"})," A list of strings to stop generation when encountered."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"streaming:"})," Whether to stream the results, token by token. Defaults to ",(0,o.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"suffix:"})," A suffix to append to the generated text. If None, no suffix is appended."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tags:"})," Tags to add to the run trace."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," The temperature to use for sampling. Defaults to ",(0,o.jsx)(n.code,{children:"0.8"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_k:"})," The top-k value to use for sampling. Defaults to ",(0,o.jsx)(n.code,{children:"40"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_p:"})," The top-p value to use for sampling. Defaults to ",(0,o.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"use_mlock:"})," Force the system to keep the model in RAM. Defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"use_mmap:"})," Whether to keep the model loaded in RAM. Defaults to ",(0,o.jsx)(n.code,{children:"True"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"verbose:"})," This parameter is used to control the level of detail in the output of the chain. When set to True, it will print out some internal states of the chain while it is being run, which can help debug and understand the chain's behavior. If set to False, it will suppress the verbose output. Defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"vocab_only:"})," Only load the vocabulary, no weights. Defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"openai",children:"OpenAI"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://openai.com",children:"OpenAI's"})," large language models."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_tokens:"})," The maximum number of tokens to generate in the completion. ",(0,o.jsx)(n.code,{children:"-1"})," returns as many tokens as possible, given the prompt and the model's maximal context size \u2013 defaults to ",(0,o.jsx)(n.code,{children:"256"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_kwargs:"})," Holds any model parameters valid for creating non-specified calls."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_name:"})," Defines the OpenAI model to be used."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"openai_api_base:"})," Used to specify the base URL for the OpenAI API. It is typically set to the API endpoint provided by the OpenAI service."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"openai_api_key:"}),"  Key used to authenticate and access the OpenAI API."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0.7"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"vertexai",children:"VertexAI"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai",children:"Google Vertex AI"})," large language models."]}),"\n",(0,o.jsxs)(n.p,{children:[":::info\nVertex AI is a cloud computing platform offered by Google Cloud Platform (GCP). It provides access, management, and development of applications and services through global data centers. To use Vertex AI PaLM, you need to have the ",(0,o.jsx)(n.a,{href:"https://pypi.org/project/google-cloud-aiplatform/",children:"google-cloud-aiplatform"})," Python package installed and credentials configured for your environment.\n:::"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"credentials:"})," The default custom credentials (google.auth.credentials.Credentials) to use."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"location:"})," The default location to use when making API calls \u2013 defaults to ",(0,o.jsx)(n.code,{children:"us-central1"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_output_tokens:"})," Token limit determines the maximum amount of text output from one prompt \u2013 defaults to ",(0,o.jsx)(n.code,{children:"128"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_name:"})," The name of the Vertex AI large language model \u2013 defaults to ",(0,o.jsx)(n.code,{children:"text-bison"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"project:"})," The default GCP project to use when making Vertex API calls."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"request_parallelism:"})," The amount of parallelism allowed for requests issued to VertexAI models \u2013 defaults to ",(0,o.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_k:"})," How the model selects tokens for output, the next token is selected from \u2013 defaults to ",(0,o.jsx)(n.code,{children:"40"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_p:"})," Tokens are selected from most probable to least until the sum of their \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tuned_model_name:"})," The name of a tuned model. If provided, model_name is ignored."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"verbose:"})," This parameter is used to control the level of detail in the output of the chain. When set to True, it will print out some internal states of the chain while it is being run, which can help debug and understand the chain's behavior. If set to False, it will suppress the verbose output \u2013 defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h3,{id:"chatvertexai",children:"ChatVertexAI"}),"\n",(0,o.jsxs)(n.p,{children:["Wrapper around ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai",children:"Google Vertex AI"})," large language models."]}),"\n",(0,o.jsxs)(n.p,{children:[":::info\nVertex AI is a cloud computing platform offered by Google Cloud Platform (GCP). It provides access, management, and development of applications and services through global data centers. To use Vertex AI PaLM, you need to have the ",(0,o.jsx)(n.a,{href:"https://pypi.org/project/google-cloud-aiplatform/",children:"google-cloud-aiplatform"})," Python package installed and credentials configured for your environment.\n:::"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"credentials:"})," The default custom credentials (google.auth.credentials.Credentials) to use."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"location:"})," The default location to use when making API calls \u2013 defaults to ",(0,o.jsx)(n.code,{children:"us-central1"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"max_output_tokens:"})," Token limit determines the maximum amount of text output from one prompt \u2013 defaults to ",(0,o.jsx)(n.code,{children:"128"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"model_name:"})," The name of the Vertex AI large language model \u2013 defaults to ",(0,o.jsx)(n.code,{children:"text-bison"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"project:"})," The default GCP project to use when making Vertex API calls."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"request_parallelism:"})," The amount of parallelism allowed for requests issued to VertexAI models \u2013 defaults to ",(0,o.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"temperature:"})," Tunes the degree of randomness in text generations. Should be a non-negative value \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_k:"})," How the model selects tokens for output, the next token is selected from \u2013 defaults to ",(0,o.jsx)(n.code,{children:"40"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"top_p:"})," Tokens are selected from most probable to least until the sum of their \u2013 defaults to ",(0,o.jsx)(n.code,{children:"0.95"}),"."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tuned_model_name:"})," The name of a tuned model. If provided, model_name is ignored."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"verbose:"})," This parameter is used to control the level of detail in the output of the chain. When set to True, it will print out some internal states of the chain while it is being run, which can help debug and understand the chain's behavior. If set to False, it will suppress the verbose output \u2013 defaults to ",(0,o.jsx)(n.code,{children:"False"}),"."]}),"\n"]})]})}const x=function(e){void 0===e&&(e={});const{wrapper:n}=Object.assign({},(0,s.ah)(),e.components);return n?(0,o.jsx)(n,Object.assign({},e,{children:(0,o.jsx)(m,e)})):m(e)};function g(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);