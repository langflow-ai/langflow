{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-86Rvw",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "diagnosis_description",
            "id": "Prompt-miaxU",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-TextInput-86Rvw{œdataTypeœ:œTextInputœ,œidœ:œTextInput-86Rvwœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-miaxU{œfieldNameœ:œdiagnosis_descriptionœ,œidœ:œPrompt-miaxUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-86Rvw",
        "sourceHandle": "{œdataTypeœ: œTextInputœ, œidœ: œTextInput-86Rvwœ, œnameœ: œtextœ, œoutput_typesœ: [œMessageœ]}",
        "target": "Prompt-miaxU",
        "targetHandle": "{œfieldNameœ: œdiagnosis_descriptionœ, œidœ: œPrompt-miaxUœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-ua4NV",
            "name": "text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "procedure_description",
            "id": "Prompt-miaxU",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-TextInput-ua4NV{œdataTypeœ:œTextInputœ,œidœ:œTextInput-ua4NVœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-miaxU{œfieldNameœ:œprocedure_descriptionœ,œidœ:œPrompt-miaxUœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "TextInput-ua4NV",
        "sourceHandle": "{œdataTypeœ: œTextInputœ, œidœ: œTextInput-ua4NVœ, œnameœ: œtextœ, œoutput_typesœ: [œMessageœ]}",
        "target": "Prompt-miaxU",
        "targetHandle": "{œfieldNameœ: œprocedure_descriptionœ, œidœ: œPrompt-miaxUœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-miaxU",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "search_query",
            "id": "KnowledgeHubSearch-2yCHC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-miaxU{œdataTypeœ:œPromptœ,œidœ:œPrompt-miaxUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-KnowledgeHubSearch-2yCHC{œfieldNameœ:œsearch_queryœ,œidœ:œKnowledgeHubSearch-2yCHCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-miaxU",
        "sourceHandle": "{œdataTypeœ: œPromptœ, œidœ: œPrompt-miaxUœ, œnameœ: œpromptœ, œoutput_typesœ: [œMessageœ]}",
        "target": "KnowledgeHubSearch-2yCHC",
        "targetHandle": "{œfieldNameœ: œsearch_queryœ, œidœ: œKnowledgeHubSearch-2yCHCœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "KnowledgeHubSearch",
            "id": "KnowledgeHubSearch-2yCHC",
            "name": "query_results",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "ParserComponent-BxCXL",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-KnowledgeHubSearch-2yCHC{œdataTypeœ:œKnowledgeHubSearchœ,œidœ:œKnowledgeHubSearch-2yCHCœ,œnameœ:œquery_resultsœ,œoutput_typesœ:[œDataœ]}-ParserComponent-BxCXL{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-BxCXLœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "KnowledgeHubSearch-2yCHC",
        "sourceHandle": "{œdataTypeœ: œKnowledgeHubSearchœ, œidœ: œKnowledgeHubSearch-2yCHCœ, œnameœ: œquery_resultsœ, œoutput_typesœ: [œDataœ]}",
        "target": "ParserComponent-BxCXL",
        "targetHandle": "{œfieldNameœ: œinput_dataœ, œidœ: œParserComponent-BxCXLœ, œinputTypesœ: [œDataFrameœ, œDataœ], œtypeœ: œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ParserComponent",
            "id": "ParserComponent-BxCXL",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-kGSQZ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ParserComponent-BxCXL{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-BxCXLœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-kGSQZ{œfieldNameœ:œcontextœ,œidœ:œPrompt-kGSQZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ParserComponent-BxCXL",
        "sourceHandle": "{œdataTypeœ: œParserComponentœ, œidœ: œParserComponent-BxCXLœ, œnameœ: œparsed_textœ, œoutput_typesœ: [œMessageœ]}",
        "target": "Prompt-kGSQZ",
        "targetHandle": "{œfieldNameœ: œcontextœ, œidœ: œPrompt-kGSQZœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-miaxU",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-kGSQZ",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-miaxU{œdataTypeœ:œPromptœ,œidœ:œPrompt-miaxUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-kGSQZ{œfieldNameœ:œquestionœ,œidœ:œPrompt-kGSQZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-miaxU",
        "sourceHandle": "{œdataTypeœ: œPromptœ, œidœ: œPrompt-miaxUœ, œnameœ: œpromptœ, œoutput_typesœ: [œMessageœ]}",
        "target": "Prompt-kGSQZ",
        "targetHandle": "{œfieldNameœ: œquestionœ, œidœ: œPrompt-kGSQZœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-kGSQZ",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "AzureOpenAIModel-TegVj",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-kGSQZ{œdataTypeœ:œPromptœ,œidœ:œPrompt-kGSQZœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-AzureOpenAIModel-TegVj{œfieldNameœ:œinput_valueœ,œidœ:œAzureOpenAIModel-TegVjœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt-kGSQZ",
        "sourceHandle": "{œdataTypeœ: œPromptœ, œidœ: œPrompt-kGSQZœ, œnameœ: œpromptœ, œoutput_typesœ: [œMessageœ]}",
        "target": "AzureOpenAIModel-TegVj",
        "targetHandle": "{œfieldNameœ: œinput_valueœ, œidœ: œAzureOpenAIModel-TegVjœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "AzureOpenAIModel",
            "id": "AzureOpenAIModel-TegVj",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "TextOutput-eIziv",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-AzureOpenAIModel-TegVj{œdataTypeœ:œAzureOpenAIModelœ,œidœ:œAzureOpenAIModel-TegVjœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-eIziv{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-eIzivœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "AzureOpenAIModel-TegVj",
        "sourceHandle": "{œdataTypeœ: œAzureOpenAIModelœ, œidœ: œAzureOpenAIModel-TegVjœ, œnameœ: œtext_outputœ, œoutput_typesœ: [œMessageœ]}",
        "target": "TextOutput-eIziv",
        "targetHandle": "{œfieldNameœ: œinput_valueœ, œidœ: œTextOutput-eIzivœ, œinputTypesœ: [œMessageœ], œtypeœ: œstrœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "Prompt-miaxU",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "procedure_description",
                "diagnosis_description"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "prompts",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt Message",
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "diagnosis_description": {
                "advanced": false,
                "display_name": "diagnosis_description",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "diagnosis_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "procedure_description": {
                "advanced": false,
                "display_name": "procedure_description",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "procedure_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "{procedure_description} is considered medically necessary for {diagnosis_description} under which scenarios/indications?"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-miaxU",
        "measured": {
          "height": 419,
          "width": 320
        },
        "position": {
          "x": 561.1698099779404,
          "y": 399.2108494686537
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-86Rvw",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Acute laryngitis"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-86Rvw",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": 142.34278464270332,
          "y": 382.3294394399319
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextInput-ua4NV",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get user text inputs.",
            "display_name": "Text Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get user text inputs.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-input\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        return Message(\n            text=self.input_value,\n        )\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Text to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Debridement of eczematous skin, each additional 10% of body surface"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextInput"
        },
        "dragging": false,
        "id": "TextInput-ua4NV",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": 136.00779349953325,
          "y": 632.038951073729
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "KnowledgeHubSearch-2yCHC",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "This component is used to search for information in the knowledge hub.",
            "display_name": "Knowledge Hub Search",
            "documentation": "http://docs.langflow.org/components/custom",
            "edited": false,
            "field_order": [
              "search_query",
              "selected_hubs"
            ],
            "frozen": false,
            "icon": "Autonomize",
            "last_updated": "2025-10-29T10:54:22.730Z",
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Query Results",
                "group_outputs": false,
                "method": "build_output",
                "name": "query_results",
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\n\nfrom typing import Any\n\nfrom langflow.custom import Component\nfrom langflow.services.deps import get_knowledge_service\nfrom langflow.io import DropdownInput, IntInput, MultilineInput, MultiselectInput, Output\nfrom langflow.schema import Data\nfrom loguru import logger\n\n# Import the service and factory directly\nfrom langflow.services.knowledge.factory import KnowledgeServiceFactory\nfrom langflow.services.knowledge.service import KnowledgeService\n\n\nclass KnowledgeHubSearchComponent(Component):\n    display_name = \"Knowledge Hub Search\"\n    description = (\n        \"This component is used to search for information in the knowledge hub.\"\n    )\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"Autonomize\"\n    name = \"KnowledgeHubSearch\"\n\n    def __init__(self, **kwargs):\n        self._hub_data: list[dict[str, str]] = []\n        self._selected_hub_names: list[str] = []  # Track selected hub names\n        self._knowledge_service: KnowledgeService | None = None  # Cache the service instance\n        super().__init__(**kwargs)\n\n    def _get_knowledge_service(self) -> KnowledgeService:\n        \"\"\"Get or create the knowledge service instance.\"\"\"\n        if self._knowledge_service is None:\n            factory = KnowledgeServiceFactory()\n            self._knowledge_service = factory.create()\n        return self._knowledge_service\n\n    async def update_build_config(\n        self, build_config: dict, field_value: Any, field_name: str | None = None\n    ):\n        \"\"\"Update the build configuration based on field changes.\"\"\"\n        logger.info(f\"update_build_config called with field_name: {field_name}\")\n\n        if field_name == \"selected_hubs\":\n            try:\n                # Get the knowledge service directly\n                service = self._get_knowledge_service()\n                if not service.ready:\n                    logger.error(\"KnowledgeHub service is not ready\")\n                    return build_config\n                \n                self._hub_data = await service.get_knowledge_hubs()\n\n                # Debug the raw response\n                logger.info(f\"Raw hub data: {self._hub_data}\")\n\n                options = [hub[\"name\"] for hub in self._hub_data]\n                logger.info(f\"Extracted hub options: {options}\")\n\n                # Debug the build_config before update\n                logger.info(\n                    f\"Build config before update: {build_config.get('selected_hubs', {})}\"\n                )\n\n                build_config[\"selected_hubs\"][\"options\"] = options\n\n                # Store selected hub names for validation during build\n                if field_value and isinstance(field_value, list):\n                    self._selected_hub_names = field_value\n                    logger.info(f\"Stored selected hub names: {self._selected_hub_names}\")\n\n                # Debug the build_config after update\n                logger.info(\n                    f\"Build config after update: {build_config.get('selected_hubs', {})}\"\n                )\n\n                return build_config\n            except Exception as e:\n                logger.exception(f\"Error in update_build_config: {e!s}\")\n                raise\n        return build_config\n\n    inputs = [\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n            tool_mode=True,\n        ),\n        MultiselectInput(\n            name=\"selected_hubs\",\n            display_name=\"Data Sources\",\n            value=[],\n            refresh_button=True,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Query Results\",\n            name=\"query_results\",\n            method=\"build_output\",\n        ),\n    ]\n\n    async def _validate_and_refresh_data_sources(self) -> tuple[bool, list[str]]:\n        \"\"\"Validate that the selected data sources are still available, if not fetch and update them\"\"\"\n        if not self._selected_hub_names:\n            logger.info(\"No data sources selected, validation skipped\")\n            return True, []\n            \n        try:\n            # Get the knowledge service directly\n            service = self._get_knowledge_service()\n            if not service.ready:\n                logger.error(\"KnowledgeHub service is not ready for validation\")\n                return True, self._selected_hub_names  # Return original selection if service not ready\n                \n            fresh_hub_data = await service.get_knowledge_hubs()\n            available_names = [hub[\"name\"] for hub in fresh_hub_data]\n            \n            logger.info(f\"Available data sources: {available_names}\")\n            logger.info(f\"Selected data sources: {self._selected_hub_names}\")\n            \n            # Check which selected hubs are still available\n            still_available = []\n            missing_hubs = []\n            refreshed_hubs = []\n            \n            for selected_name in self._selected_hub_names:\n                if selected_name in available_names:\n                    still_available.append(selected_name)\n                    logger.info(f\"Data source '{selected_name}' is still available\")\n                else:\n                    missing_hubs.append(selected_name)\n                    logger.warning(f\"Data source '{selected_name}' is no longer available\")\n            \n            # Try to find missing hubs in fresh data (in case of name changes or refresh issues)\n            for missing_name in missing_hubs:\n                # Look for exact match first\n                found_hub = next(\n                    (hub for hub in fresh_hub_data if hub[\"name\"] == missing_name), None\n                )\n                \n                if found_hub:\n                    still_available.append(found_hub[\"name\"])\n                    refreshed_hubs.append(found_hub[\"name\"])\n                    logger.info(f\"Refreshed data source '{missing_name}' found and re-added\")\n                else:\n                    # Try partial match (in case of minor name changes)\n                    partial_matches = [\n                        hub[\"name\"] for hub in fresh_hub_data \n                        if missing_name.lower() in hub[\"name\"].lower() or hub[\"name\"].lower() in missing_name.lower()\n                    ]\n                    \n                    if partial_matches:\n                        logger.info(f\"Possible matches for missing '{missing_name}': {partial_matches}\")\n                        # For now, don't auto-select partial matches, just log them\n                    else:\n                        logger.error(f\"Data source '{missing_name}' not found even after refresh\")\n            \n            # Update the hub data cache\n            self._hub_data = fresh_hub_data\n            \n            # Update selected hub names to only include available ones\n            self._selected_hub_names = still_available\n            \n            if refreshed_hubs:\n                logger.info(f\"Successfully refreshed data sources: {refreshed_hubs}\")\n            \n            if missing_hubs and not refreshed_hubs:\n                logger.warning(f\"Some data sources are no longer available: {[h for h in missing_hubs if h not in refreshed_hubs]}\")\n                return False, still_available\n            \n            return True, still_available\n                \n        except Exception as e:\n            logger.error(f\"Error validating/refreshing data sources: {e}\")\n            logger.exception(\"Full error details:\")\n            # If we can't validate, return original selection to avoid breaking the flow\n            return True, self._selected_hub_names\n\n    async def build_output(self) -> Data:\n        \"\"\"Generate the output based on selected knowledge hubs.\"\"\"\n        try:\n            # Validate and refresh data sources if needed\n            if self._selected_hub_names:\n                is_valid, validated_hubs = await self._validate_and_refresh_data_sources()\n                \n                if not is_valid and not validated_hubs:\n                    error_message = f\"Error: Selected data sources are no longer available. Please select different data sources.\"\n                    logger.error(error_message)\n                    return Data(\n                        text=error_message,\n                        data={\"error\": error_message, \"query_results\": []}\n                    )\n                \n                # Use validated hubs instead of self.selected_hubs\n                effective_selected_hubs = validated_hubs\n            else:\n                effective_selected_hubs = self.selected_hubs if hasattr(self, 'selected_hubs') else []\n\n            if not effective_selected_hubs:\n                logger.warning(\"No knowledge hubs selected or available.\")\n                return Data(value={\"query_results\": []})\n\n            # Make sure we have hub data\n            if not self._hub_data:\n                service = self._get_knowledge_service()\n                if not service.ready:\n                    logger.error(\"KnowledgeHub service is not ready\")\n                    return Data(value={\"query_results\": []})\n                self._hub_data = await service.get_knowledge_hubs()\n\n            # Map the selected names to their IDs\n            selected_hub_ids = [\n                hub[\"id\"] for hub in self._hub_data if hub[\"name\"] in effective_selected_hubs\n            ]\n\n            logger.info(f\"Using data sources: {effective_selected_hubs}\")\n            logger.info(f\"Mapped to hub IDs: {selected_hub_ids}\")\n\n            service = self._get_knowledge_service()\n            if not service.ready:\n                logger.error(\"KnowledgeHub service is not ready\")\n                return Data(value={\"query_results\": []})\n            \n            query_results = await service.query_vector_store(\n                knowledge_hub_ids=selected_hub_ids, query=self.search_query\n            )\n            logger.debug(f\"query_results: {query_results}\")\n            \n            # Concatenate content from query results\n            contents = [\n                result.get(\"metadata\", {}).get(\"content\", \"\")\n                for result in query_results\n            ]\n            plain_text = \"\\n\\n=== NEW CHUNK ===\\n\\n\".join(contents)\n\n            data = Data(\n                text=plain_text,\n                data={\n                    \"result\": query_results,\n                    \"used_data_sources\": effective_selected_hubs,  # Include which sources were actually used\n                },\n            )\n            self.status = data\n            return data\n\n        except Exception as e:\n            logger.error(f\"Error in build_output: {e!s}\")\n            return Data(value={\"query_results\": []})\n\n    async def __aenter__(self):\n        \"\"\"Async context manager entry.\"\"\"\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Async context manager exit - cleanup resources.\"\"\"\n        if self._knowledge_service:\n            await self._knowledge_service.cleanup()\n            self._knowledge_service = None\n"
              },
              "search_query": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Search Query",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "search_query",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "selected_hubs": {
                "_input_type": "MultiselectInput",
                "advanced": false,
                "combobox": false,
                "display_name": "Data Sources",
                "dynamic": false,
                "info": "",
                "list": true,
                "list_add_label": "Add More",
                "name": "selected_hubs",
                "options": [],
                "placeholder": "",
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": [
                  "Carelon Guidelines - 2023"
                ]
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "KnowledgeHubSearch"
        },
        "dragging": false,
        "id": "KnowledgeHubSearch-2yCHC",
        "measured": {
          "height": 302,
          "width": 320
        },
        "position": {
          "x": 969.4848959616387,
          "y": 324.82388552289945
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Prompt-kGSQZ",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": [
                "question",
                "context"
              ]
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "error": null,
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "prompts",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt Message",
                "hidden": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import MessageTextInput, Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "context": {
                "advanced": false,
                "display_name": "context",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "context",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "question": {
                "advanced": false,
                "display_name": "question",
                "dynamic": false,
                "field_type": "str",
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "question",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Question:\n{question}\nContext:\n{context}\nClinical Guideline Criteria are as follows:"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Prompt"
        },
        "dragging": false,
        "id": "Prompt-kGSQZ",
        "measured": {
          "height": 435,
          "width": 320
        },
        "position": {
          "x": 1723.4693089356597,
          "y": 502.92691424232413
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ParserComponent-BxCXL",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "processing",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts text using a template.",
            "display_name": "Parser",
            "documentation": "",
            "edited": false,
            "field_order": [
              "mode",
              "pattern",
              "input_data",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "key": "ParserComponent",
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.001,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.custom.custom_component.component import Component\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, HandleInput, MessageTextInput, MultilineInput, TabInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.template.field.base import Output\n\n\nclass ParserComponent(Component):\n    display_name = \"Parser\"\n    description = \"Extracts text using a template.\"\n    documentation: str = \"https://docs.langflow.org/components-processing#parser\"\n    icon = \"braces\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            formatted_text = self.pattern.format(**data.data)\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([safe_convert(item, clean_data=self.clean_data or False) for item in self.input_data])\n        else:\n            result = safe_convert(self.input_data or False)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data or DataFrame",
                "dynamic": false,
                "info": "Accepts either a DataFrame or a Data object.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "tab",
                "value": "Parser"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ParserComponent"
        },
        "dragging": false,
        "id": "ParserComponent-BxCXL",
        "measured": {
          "height": 329,
          "width": 320
        },
        "position": {
          "x": 1331.2810413787931,
          "y": 319.78535061206645
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "AzureOpenAIModel-TegVj",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "category": "models",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generate text using Azure OpenAI LLMs.",
            "display_name": "Azure OpenAI",
            "documentation": "https://python.langchain.com/docs/integrations/llms/azure_openai",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "azure_endpoint",
              "azure_deployment",
              "api_key",
              "api_version",
              "temperature",
              "max_tokens"
            ],
            "frozen": false,
            "icon": "Azure",
            "key": "AzureOpenAIModel",
            "legacy": false,
            "lf_version": "1.6.3",
            "metadata": {
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ]
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "method": "text_response",
                "name": "text_output",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "method": "build_model",
                "name": "model_output",
                "selected": null,
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.003924824467069744,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Azure Chat OpenAI API Key",
                "dynamic": false,
                "info": "",
                "input_types": [],
                "load_from_db": true,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "api_version": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "API Version",
                "dynamic": false,
                "info": "",
                "name": "api_version",
                "options": [
                  "2025-02-01-preview",
                  "2025-01-01-preview",
                  "2024-12-01-preview",
                  "2024-10-01-preview",
                  "2024-09-01-preview",
                  "2024-08-01-preview",
                  "2024-07-01-preview",
                  "2024-06-01",
                  "2024-03-01-preview",
                  "2024-02-15-preview",
                  "2023-12-01-preview",
                  "2023-05-15"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "2024-08-01-preview"
              },
              "azure_deployment": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Deployment Name",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "azure_deployment",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "GPT316k"
              },
              "azure_endpoint": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Azure Endpoint",
                "dynamic": false,
                "info": "Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "azure_endpoint",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "https://cog-54p2emd7pu2vu.openai.azure.com/"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langchain_openai import AzureChatOpenAI\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs.inputs import MessageTextInput\nfrom langflow.io import DropdownInput, IntInput, SecretStrInput, SliderInput\n\n\nclass AzureChatOpenAIComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI\"\n    description: str = \"Generate text using Azure OpenAI LLMs.\"\n    documentation: str = \"https://python.langchain.com/docs/integrations/llms/azure_openai\"\n    beta = False\n    icon = \"Azure\"\n    name = \"AzureOpenAIModel\"\n\n    AZURE_OPENAI_API_VERSIONS = [\n        \"2024-06-01\",\n        \"2024-07-01-preview\",\n        \"2024-08-01-preview\",\n        \"2024-09-01-preview\",\n        \"2024-10-01-preview\",\n        \"2023-05-15\",\n        \"2023-12-01-preview\",\n        \"2024-02-15-preview\",\n        \"2024-03-01-preview\",\n        \"2024-12-01-preview\",\n        \"2025-01-01-preview\",\n        \"2025-02-01-preview\",\n    ]\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        MessageTextInput(\n            name=\"azure_endpoint\",\n            display_name=\"Azure Endpoint\",\n            info=\"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\",\n            required=True,\n        ),\n        MessageTextInput(name=\"azure_deployment\", display_name=\"Deployment Name\", required=True),\n        SecretStrInput(name=\"api_key\", display_name=\"Azure Chat OpenAI API Key\", required=True),\n        DropdownInput(\n            name=\"api_version\",\n            display_name=\"API Version\",\n            options=sorted(AZURE_OPENAI_API_VERSIONS, reverse=True),\n            value=next(\n                (\n                    version\n                    for version in sorted(AZURE_OPENAI_API_VERSIONS, reverse=True)\n                    if not version.endswith(\"-preview\")\n                ),\n                AZURE_OPENAI_API_VERSIONS[0],\n            ),\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            range_spec=RangeSpec(min=0, max=2, step=0.01),\n            info=\"Controls randomness. Lower values are more deterministic, higher values are more creative.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        azure_endpoint = self.azure_endpoint\n        azure_deployment = self.azure_deployment\n        api_version = self.api_version\n        api_key = self.api_key\n        temperature = self.temperature\n        max_tokens = self.max_tokens\n        stream = self.stream\n\n        try:\n            output = AzureChatOpenAI(\n                azure_endpoint=azure_endpoint,\n                azure_deployment=azure_deployment,\n                api_version=api_version,\n                api_key=api_key,\n                temperature=temperature,\n                max_tokens=max_tokens or None,\n                streaming=stream,\n            )\n        except Exception as e:\n            msg = f\"Could not connect to AzureOpenAI API: {e}\"\n            raise ValueError(msg) from e\n\n        return output\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are an AI assistant known for your accuracy and helpfulness. Carefully review the following clinical guidelines that are used to approve procedures for various medical conditions. Follow these specific instructions: 1. You must generate exactly **eight (8)** guidelines — no more, no less. 2. If there are **more than eight** guidelines in the context, **combine** and merge them logically so that the final output always contains **eight guidelines**. 3. If there are **fewer than eight** guidelines, **split** or expand them appropriately, ensuring that each original guideline is still represented, and the final count remains **eight guidelines**. 4. Each guideline MUST be **standalone** and **independent** and in a single line. 5. **Every guideline** provided in the context must be considered to answer the question. **Do not skip** or omit any. Ensure your final output always contains exactly **eight comprehensive guidelines**."
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "Controls randomness. Lower values are more deterministic, higher values are more creative.",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "placeholder": "",
                "range_spec": {
                  "max": 2,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "type": "slider",
                "value": 0.7
              }
            },
            "tool_mode": false
          },
          "selected_output": "text_output",
          "showNode": true,
          "type": "AzureOpenAIModel"
        },
        "dragging": false,
        "id": "AzureOpenAIModel-TegVj",
        "measured": {
          "height": 616,
          "width": 320
        },
        "position": {
          "x": 2086.915062956044,
          "y": 499.23978889642524
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "TextOutput-eIziv",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "category": "outputs",
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Sends text output via API.",
            "display_name": "Text Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value"
            ],
            "frozen": false,
            "icon": "type",
            "key": "TextOutput",
            "legacy": false,
            "lf_version": "1.4.3",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Text",
                "group_outputs": false,
                "method": "text_response",
                "name": "text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "score": 0.003169567463043492,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Sends text output via API.\"\n    documentation: str = \"https://docs.langflow.org/components-io#text-output\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Output Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Text to be passed as output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "TextOutput"
        },
        "dragging": false,
        "id": "TextOutput-eIziv",
        "measured": {
          "height": 204,
          "width": 320
        },
        "position": {
          "x": 2466.6328831880537,
          "y": 926.6214851632049
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -791.2065297375518,
      "y": -80.52160803113443,
      "zoom": 0.5848688087039475
    }
  },
  "description": "Locates and retrieves relevant clinical guidelines from policy documents for checking medical necessity ensuring quick-access to up-to-date criteria for informed clinical decision-making.",
  "endpoint_name": null,
  "id": "46a44581-daec-486c-b7b4-44c7bcebb355",
  "is_component": false,
  "last_tested_version": "1.6.3",
  "name": "Auth Guidelines",
  "tags": [
    "prior-auth",
    "chart-review"
  ]
}