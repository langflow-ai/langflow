{
  "version": "1.0",
  "generated_at": "2025-12-22T13:59:44.570876+00:00",
  "total_pages": 146,
  "categories": {
    "api_reference": [
      "/api-build",
      "/api-files",
      "/api-flows",
      "/api-flows-run",
      "/api-logs",
      "/api-monitor",
      "/api-openai-responses",
      "/api-projects",
      "/api-reference-api-examples",
      "/api-users",
      "/typescript-client"
    ],
    "agents": [
      "/agents",
      "/agents-tools",
      "/mcp-client",
      "/mcp-component-astra",
      "/mcp-server"
    ],
    "components": [
      "/bundles-aiml",
      "/bundles-amazon",
      "/bundles-anthropic",
      "/bundles-apify",
      "/bundles-arxiv",
      "/bundles-assemblyai",
      "/bundles-azure",
      "/bundles-baidu",
      "/bundles-bing",
      "/bundles-cassandra",
      "/bundles-chroma",
      "/bundles-cleanlab",
      "/bundles-clickhouse",
      "/bundles-cloudflare",
      "/bundles-cohere",
      "/bundles-cometapi",
      "/bundles-composio",
      "/bundles-couchbase",
      "/bundles-datastax",
      "/bundles-deepseek",
      "/bundles-docling",
      "/bundles-duckduckgo",
      "/bundles-elastic",
      "/bundles-exa",
      "/bundles-faiss",
      "/bundles-glean",
      "/bundles-google",
      "/bundles-groq",
      "/bundles-huggingface",
      "/bundles-ibm",
      "/bundles-icosacomputing",
      "/bundles-langchain",
      "/bundles-lmstudio",
      "/bundles-maritalk",
      "/bundles-mem0",
      "/bundles-milvus",
      "/bundles-mistralai",
      "/bundles-mongodb",
      "/bundles-notion",
      "/bundles-novita",
      "/bundles-nvidia",
      "/bundles-ollama",
      "/bundles-openai",
      "/bundles-openrouter",
      "/bundles-perplexity",
      "/bundles-pgvector",
      "/bundles-pinecone",
      "/bundles-qdrant",
      "/bundles-queryrouter",
      "/bundles-redis",
      "/bundles-sambanova",
      "/bundles-searchapi",
      "/bundles-serper",
      "/bundles-supabase",
      "/bundles-upstash",
      "/bundles-vectara",
      "/bundles-vertexai",
      "/bundles-weaviate",
      "/bundles-wikipedia",
      "/bundles-xai",
      "/components-agents",
      "/components-bundle-components",
      "/components-custom-components",
      "/components-data",
      "/components-embedding-models",
      "/components-helpers",
      "/components-io",
      "/components-logic",
      "/components-models",
      "/components-processing",
      "/components-prompts",
      "/components-tools",
      "/concepts-components"
    ],
    "contributing": [
      "/contributing-bundles",
      "/contributing-community",
      "/contributing-component-tests",
      "/contributing-components",
      "/contributing-how-to-contribute",
      "/contributing-templates"
    ],
    "deployment": [
      "/deployment-architecture",
      "/deployment-caddyfile",
      "/deployment-docker",
      "/deployment-gcp",
      "/deployment-hugging-face-spaces",
      "/deployment-kubernetes-dev",
      "/deployment-kubernetes-prod",
      "/deployment-nginx-ssl",
      "/deployment-overview",
      "/deployment-prod-best-practices",
      "/deployment-public-server",
      "/deployment-railway",
      "/deployment-render",
      "/develop-application"
    ],
    "develop": [
      "/api-keys-and-authentication",
      "/concepts-file-management",
      "/concepts-voice-mode",
      "/configuration-cli",
      "/configuration-custom-database",
      "/configuration-global-variables",
      "/contributing-telemetry",
      "/data-types",
      "/enterprise-database-guide",
      "/environment-variables",
      "/install-custom-dependencies",
      "/integrations-arize",
      "/integrations-instana-traceloop",
      "/integrations-langfuse",
      "/integrations-langsmith",
      "/integrations-langwatch",
      "/integrations-opik",
      "/logging",
      "/memory",
      "/session-id"
    ],
    "flows": [
      "/concepts-flows",
      "/concepts-flows-import",
      "/concepts-overview",
      "/concepts-playground",
      "/concepts-publish",
      "/webhook"
    ],
    "get_started": [
      "/",
      "/get-started-installation",
      "/get-started-quickstart"
    ],
    "support": [
      "/contributing-github-issues",
      "/luna-for-langflow",
      "/release-notes",
      "/troubleshoot"
    ],
    "tutorials": [
      "/agent-tutorial",
      "/chat-with-files",
      "/chat-with-rag",
      "/mcp-tutorial"
    ]
  },
  "pages": {
    "/api-build": {
      "title": "Build endpoints",
      "slug": "/api-build",
      "category": "api_reference",
      "content": "**Note:** The `/build` endpoints are used by Langflow's frontend visual editor code.\nThese endpoints are part of the internal Langflow codebase.\n\nDon't use these endpoints to run flows in applications that use your Langflow flows.\nTo run flows in your apps, see [Flow trigger endpoints](/api-flows-run).\n\nThe `/build` endpoints support Langflow's frontend code for building flows in the Langflow visual editor.\nYou can use these endpoints to build vertices and flows, as well as execute flows with streaming event responses.\nYou might need to use or understand these endpoints when contributing to the Langflow codebase.\n\n## Build flow and stream events\n\nThis endpoint builds and executes a flow, returning a job ID that can be used to stream execution events.\n\n1. Send a POST request to the `/build/$FLOW_ID/flow` endpoint:\n\n    ```shell\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -d '{\n        \"inputs\": {\n          \"input_value\": \"Tell me a story\"\n        }\n      }'\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```json\n    {\n      \"job_id\": \"123e4567-e89b-12d3-a456-426614174000\"\n    }\n    ```\n\n    </details>\n\n2. After receiving a job ID from the build endpoint, use the `/build/$JOB_ID/events` endpoint to stream the execution results:\n\n    ```shell\n    curl -X GET \\\n      \"$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```json\n    {\"event\": \"vertices_sorted\", \"data\": {\"ids\": [\"ChatInput-XtBLx\"], \"to_run\": [\"Prompt-x74Ze\", \"ChatOutput-ylMzN\", \"ChatInput-XtBLx\", \"OpenAIModel-d1wOZ\"]}}\n\n    {\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:42:23\", \"sender\": \"User\", \"sender_name\": \"User\", \"session_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\", \"text\": \"Tell me a story\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": null, \"display_name\": null, \"source\": null}, \"icon\": \"\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"28879bd8-6a68-4dd5-b658-74d643a4dd92\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n    // ... Additional events as the flow executes ...\n\n    {\"event\": \"end\", \"data\": {}}\n    ```\n\n    </details>\n\nThe `/build/$FLOW_ID/events` endpoint has a `stream` query parameter that defaults to `true`.\nTo disable streaming and get all events at once, set `?stream=false`.\n\n```shell\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events?stream=false\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Build headers\n\n| Header | Info | Example |\n|--------|------|---------|\n| Content-Type | Required. Specifies the JSON format. | \"application/json\" |\n| accept | Optional. Specifies the response format. | \"application/json\" |\n| x-api-key | Optional. Required only if authentication is enabled. | \"sk-...\" |\n\n## Build parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| inputs | object | Optional. Input values for flow components. |\n| data | object | Optional. Flow data to override stored configuration. |\n| files | array[string] | Optional. List of file paths to use. |\n| start_component_id | string | Optional. ID of the component where the execution should start. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) |\n| stop_component_id | string | Optional. ID of the component where the execution should stop. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents).|\n| log_builds | Boolean | Whether to record build logs. Default: Enabled (`true`). |\n\n### Set start and stop points\n\nThe `/build` endpoint accepts optional values for `start_component_id` and `stop_component_id` to control where the flow run starts and stops.\nSetting `stop_component_id` for a component triggers the same behavior as clicking **Run component** on that component in the visual editor: The specified component and all dependent components leading up to that component will run.\n\nThe following example stops flow execution at an **OpenAI** component:\n\n```shell\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\"stop_component_id\": \"OpenAIModel-Uksag\"}'\n```\n\n### Override flow parameters\n\nThe `/build` endpoint also accepts inputs for `data` directly, instead of using the values stored in the Langflow database.\nThis is useful for running flows without having to pass custom values through the visual editor.\n\n```shell\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"data\": {\n      \"nodes\": [],\n      \"edges\": []\n    },\n    \"inputs\": {\n      \"input_value\": \"Your custom input here\",\n      \"session\": \"session_id\"\n    }\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{ \"job_id\": \"0bcc7f23-40b4-4bfa-9b8a-a44181fd1175\" }\n```\n\n</details>\n\n## See also\n\n- [Get Vertex builds](/api-monitor#get-vertex-builds)\n- [Delete Vertex builds](/api-monitor#delete-vertex-builds)\n- [Session ID](/session-id)",
      "summary": "**Note:** The `/build` endpoints are used by Langflow's frontend visual editor code.\nThese endpoints are part of the internal Langflow codebase.",
      "keywords": [
        "build",
        "component",
        "endpoints",
        "events",
        "flow",
        "headers",
        "note",
        "openai",
        "override",
        "parameters",
        "points",
        "run",
        "set",
        "start",
        "stop",
        "stream"
      ]
    },
    "api-build": {
      "title": "Build endpoints",
      "slug": "/api-build",
      "category": "api_reference",
      "content": "**Note:** The `/build` endpoints are used by Langflow's frontend visual editor code.\nThese endpoints are part of the internal Langflow codebase.\n\nDon't use these endpoints to run flows in applications that use your Langflow flows.\nTo run flows in your apps, see [Flow trigger endpoints](/api-flows-run).\n\nThe `/build` endpoints support Langflow's frontend code for building flows in the Langflow visual editor.\nYou can use these endpoints to build vertices and flows, as well as execute flows with streaming event responses.\nYou might need to use or understand these endpoints when contributing to the Langflow codebase.\n\n## Build flow and stream events\n\nThis endpoint builds and executes a flow, returning a job ID that can be used to stream execution events.\n\n1. Send a POST request to the `/build/$FLOW_ID/flow` endpoint:\n\n    ```shell\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -d '{\n        \"inputs\": {\n          \"input_value\": \"Tell me a story\"\n        }\n      }'\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```json\n    {\n      \"job_id\": \"123e4567-e89b-12d3-a456-426614174000\"\n    }\n    ```\n\n    </details>\n\n2. After receiving a job ID from the build endpoint, use the `/build/$JOB_ID/events` endpoint to stream the execution results:\n\n    ```shell\n    curl -X GET \\\n      \"$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```json\n    {\"event\": \"vertices_sorted\", \"data\": {\"ids\": [\"ChatInput-XtBLx\"], \"to_run\": [\"Prompt-x74Ze\", \"ChatOutput-ylMzN\", \"ChatInput-XtBLx\", \"OpenAIModel-d1wOZ\"]}}\n\n    {\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:42:23\", \"sender\": \"User\", \"sender_name\": \"User\", \"session_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\", \"text\": \"Tell me a story\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": null, \"display_name\": null, \"source\": null}, \"icon\": \"\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"28879bd8-6a68-4dd5-b658-74d643a4dd92\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n    // ... Additional events as the flow executes ...\n\n    {\"event\": \"end\", \"data\": {}}\n    ```\n\n    </details>\n\nThe `/build/$FLOW_ID/events` endpoint has a `stream` query parameter that defaults to `true`.\nTo disable streaming and get all events at once, set `?stream=false`.\n\n```shell\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events?stream=false\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Build headers\n\n| Header | Info | Example |\n|--------|------|---------|\n| Content-Type | Required. Specifies the JSON format. | \"application/json\" |\n| accept | Optional. Specifies the response format. | \"application/json\" |\n| x-api-key | Optional. Required only if authentication is enabled. | \"sk-...\" |\n\n## Build parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| inputs | object | Optional. Input values for flow components. |\n| data | object | Optional. Flow data to override stored configuration. |\n| files | array[string] | Optional. List of file paths to use. |\n| start_component_id | string | Optional. ID of the component where the execution should start. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) |\n| stop_component_id | string | Optional. ID of the component where the execution should stop. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents).|\n| log_builds | Boolean | Whether to record build logs. Default: Enabled (`true`). |\n\n### Set start and stop points\n\nThe `/build` endpoint accepts optional values for `start_component_id` and `stop_component_id` to control where the flow run starts and stops.\nSetting `stop_component_id` for a component triggers the same behavior as clicking **Run component** on that component in the visual editor: The specified component and all dependent components leading up to that component will run.\n\nThe following example stops flow execution at an **OpenAI** component:\n\n```shell\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\"stop_component_id\": \"OpenAIModel-Uksag\"}'\n```\n\n### Override flow parameters\n\nThe `/build` endpoint also accepts inputs for `data` directly, instead of using the values stored in the Langflow database.\nThis is useful for running flows without having to pass custom values through the visual editor.\n\n```shell\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"data\": {\n      \"nodes\": [],\n      \"edges\": []\n    },\n    \"inputs\": {\n      \"input_value\": \"Your custom input here\",\n      \"session\": \"session_id\"\n    }\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{ \"job_id\": \"0bcc7f23-40b4-4bfa-9b8a-a44181fd1175\" }\n```\n\n</details>\n\n## See also\n\n- [Get Vertex builds](/api-monitor#get-vertex-builds)\n- [Delete Vertex builds](/api-monitor#delete-vertex-builds)\n- [Session ID](/session-id)",
      "summary": "**Note:** The `/build` endpoints are used by Langflow's frontend visual editor code.\nThese endpoints are part of the internal Langflow codebase.",
      "keywords": [
        "build",
        "component",
        "endpoints",
        "events",
        "flow",
        "headers",
        "note",
        "openai",
        "override",
        "parameters",
        "points",
        "run",
        "set",
        "start",
        "stop",
        "stream"
      ]
    },
    "/api-files": {
      "title": "Files endpoints",
      "slug": "/api-files",
      "category": "api_reference",
      "content": "Use the `/files` endpoints to move files between your local machine and Langflow.\n\n## Differences between `/v1/files` and `/v2/files`\n\nThere are two versions of the `/files` endpoints.\n\n`/v2/files` offers the following improvements over `/v1/files`:\n\n- `/v2` files are organized by `user_id` instead of `flow_id`.\n  This means files are owned by users, and they aren't attached to specific flows.\n  You can upload a file to Langflow one time, and use it with multiple flows.\n- `/v2` files are tracked in the Langflow database.\n- `/v2` supports bulk upload and delete.\n- `/v2` responses contain more descriptive metadata.\n\nHowever, `/v2/files` doesn't support image files.\nTo send image files to your flows through the API, use [Upload image files (v1)](#upload-image-files-v1).\n\n## Files/V1 endpoints\n\nUse the `/files` endpoints to move files between your local machine and Langflow.\n\n### Upload file (v1)\n\nUpload a file to the `v1/files/upload/$FLOW_ID` endpoint:\nReplace **FILE_NAME** with the uploaded file name.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@FILE_NAME.txt\"\n```\n\nReplace `FILE_NAME.txt` with the name and extension of the file you want to upload.\nNot all file types are supported.\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"flowId\": \"92f9a4c5-cfc8-4656-ae63-1f0881163c28\",\n  \"file_path\": \"92f9a4c5-cfc8-4656-ae63-1f0881163c28/2024-12-30_15-19-43_your_file.txt\"\n}\n```\n\n</details>\n\n### Upload image files (v1)\n\nSend image files to Langflow to use them in flows.\n\nThe default file limit is 1024 MB.\nTo change this limit, set the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n1. Attach the image to a `POST /v1/files/upload/$FLOW_ID` request with `--form` (`-F`) and the file path:\n\n    ```bash\n    curl -X POST \"$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@PATH/TO/FILE.png\"\n    ```\n\n    A successful request returns the `file_path` for the image in the Langflow file management system in the format `FLOW_ID/TIMESTAMP_FILENAME.TYPE`.\n    For example:\n\n    ```json\n    {\n      \"flowId\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4\",\n      \"file_path\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png\"\n    }\n    ```\n\n2. Use the returned `file_path` to send the image file to other components that can accept file input. Where you specify the file path depends on the component type.\n\n    The following example runs the **Basic Prompting** template flow, passing the image file and the query `describe this image` as input for the **Chat Input** component.\n    In this case, the file path is specified in `tweaks`.\n\n    ```bash\n    curl -X POST \\\n        \"$LANGFLOW_URL/api/v1/run/a430cc57-06bb-4c11-be39-d3d4de68d2c4?stream=false\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n        -d '{\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"tweaks\": {\n          \"ChatInput-b67sL\": {\n            \"files\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png\",\n            \"input_value\": \"describe this image\"\n          }\n        }\n      }'\n    ```\n\n    \n**Note:** For help with tweaks, use the **Input Schema** in a flow's [**API access** pane](/concepts-publish#api-access).\n    Setting tweaks with **Input Schema** also automatically populates the required component IDs.\n    \n\n### List files (v1)\n\nList all files associated with a specific flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/files/list/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"files\": [\"2024-12-30_15-19-43_your_file.txt\"]\n}\n```\n\n</details>\n\n### Download file (v1)\n\nDownload a specific file from a flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/files/download/$FLOW_ID/2024-12-30_15-19-43_your_file.txt\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output downloaded_file.txt\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nFile contents downloaded to downloaded_file.txt\n```\n\n</details>\n\n### Delete file (v1)\n\nDelete a specific file from a flow.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/files/delete/$FLOW_ID/2024-12-30_15-19-43_your_file.txt\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"File 2024-12-30_15-19-43_your_file.txt deleted successfully\"\n}\n```\n\n</details>\n\n## Files/V2 endpoints\n\nUse the `/files` endpoints to move files between your local machine and Langflow.\n\nThe `/v2/files` endpoints can be authenticated by an API key or JWT.\nTo create a Langflow API key and export it as an environment variable, see [Get started with the Langflow API](/api-reference-api-examples).\n\n### Upload file (v2)\n\nUpload a file to your user account. The file can be used across multiple flows.\n\nThe file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, such as `07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf`.\n\n1. To retrieve your current `user_id`, call the `/whoami` endpoint:\n\n    ```bash\n    curl -X GET \\\n      \"$LANGFLOW_URL/api/v1/users/whoami\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```\n    {\"id\":\"07e5b864-e367-4f52-b647-a48035ae7e5e\",\"username\":\"langflow\",\"profile_image\":null,\"store_api_key\":null,\"is_active\":true,\"is_superuser\":true,\"create_at\":\"2025-05-08T17:59:07.855965\",\"updated_at\":\"2025-05-28T19:00:42.556460\",\"last_login_at\":\"2025-05-28T19:00:42.554338\",\"optins\":{\"github_starred\":false,\"dialog_dismissed\":true,\"discord_clicked\":false,\"mcp_dialog_dismissed\":true}}\n    ```\n\n    </details>\n\n2. In the POST request to `v2/files`, replace **@FILE_NAME.EXTENSION** with the uploaded file name and its extension.\nYou must include the ampersand (`@`) in the request to instruct curl to upload the contents of the file, not the string `FILE_NAME.EXTENSION`.\n\n    ```bash\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v2/files\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@FILE_NAME.EXTENSION\"\n    ```\n\n    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:\n\n    ```json\n    {\n      \"id\":\"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94\",\n      \"name\":\"engine_manual\",\n      \"path\":\"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf\",\n      \"size\":851160,\n      \"provider\":null\n    }\n    ```\n\n### Send files to your flows (v2)\n\n**Note:** The `/v2/files` endpoint can't send image files to flows.\nTo send image files to your flows through the API, see [Upload image files (v1)](#upload-image-files-v1).\n\nThis endpoint uploads files to your Langflow server's file management system.\nTo use an uploaded file in a flow, send the file path to a flow with a [**File** component](/components-data#file).\n\nThe default file limit is 1024 MB. To configure this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n1. To send a file to your flow with the API, POST the file to the `/api/v2/files` endpoint.\n\n    Replace **FILE_NAME.EXTENSION** with the name and extension of the file you want to upload.\n    This is the same step described in [Upload file (v2)](#upload-file-v2), but since you need the filename to upload to your flow, it is included here.\n\n    ```bash\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v2/files\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@FILE_NAME.EXTENSION\"\n    ```\n\n    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:\n\n    ```json\n    {\n      \"id\":\"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94\",\n      \"name\":\"engine_manual\",\n      \"path\":\"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf\",\n      \"size\":851160,\n      \"provider\": null\n    }\n    ```\n\n2. To use this file in your flow, add a **File** component to your flow.\nThis component loads files into flows from your local machine or Langflow file management.\n\n3. Run the flow, passing the `path` to the `File` component in the `tweaks` object:\n\n    ```text\n    curl --request POST \\\n      --url \"$LANGFLOW_URL/api/v1/run/$FLOW_ID\" \\\n      --header \"Content-Type: application/json\" \\\n      --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n      --data '{\n      \"input_value\": \"what do you see?\",\n      \"output_type\": \"chat\",\n      \"input_type\": \"text\",\n      \"tweaks\": {\n        \"File-1olS3\": {\n          \"path\": [\n            \"07e5b864-e367-4f52-b647-a48035ae7e5e/3a290013-fe1e-4d3d-a454-cacae81288f3.pdf\"\n          ]\n        }\n      }\n    }'\n    ```\n\n    To get the `File` component's ID, call the [Read flow](/api-flows#read-flow) endpoint or inspect the component in the visual editor.\n\n    If the file path is valid, the flow runs successfully.\n\n### List files (v2)\n\nList all files associated with your user account.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v2/files\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"c7b22c4c-d5e0-4ec9-af97-5d85b7657a34\",\n    \"name\": \"your_file\",\n    \"path\": \"6f17a73e-97d7-4519-a8d9-8e4c0be411bb/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34.txt\",\n    \"size\": 1234,\n    \"provider\": null\n  }\n]\n```\n\n</details>\n\n### Download file (v2)\n\nDownload a specific file by its ID and file extension.\n\nYou must specify the file type you expect in the `--output` value.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v2/files/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output downloaded_file.txt\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nFile contents downloaded to downloaded_file.txt\n```\n\n</details>\n\n### Edit file name (v2)\n\nChange a file name.\n\n```bash\ncurl -X PUT \\\n  \"$LANGFLOW_URL/api/v2/files/$FILE_ID?name=new_file_name\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"76543e40-f388-4cb3-b0ee-a1e870aca3d3\",\n  \"name\": \"new_file_name\",\n  \"path\": \"6f17a73e-97d7-4519-a8d9-8e4c0be411bb/76543e40-f388-4cb3-b0ee-a1e870aca3d3.png\",\n  \"size\": 2728251,\n  \"provider\": null\n}\n```\n\n</details>\n\n### Delete file (v2)\n\nDelete a specific file by its ID.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v2/files/$FILE_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"File deleted successfully\"\n}\n```\n\n</details>\n\n### Delete all files (v2)\n\nDelete all files associated with your user account.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v2/files\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"All files deleted successfully\"\n}\n```\n\n</details>\n\n## Create upload file (Deprecated)\n\nThis endpoint is deprecated. Use the `/files` endpoints instead.\n\n## See also\n\n* [Manage files](/concepts-file-management)",
      "summary": "Use the `/files` endpoints to move files between your local machine and Langflow.",
      "keywords": [
        "access",
        "all",
        "api",
        "basic",
        "between",
        "chat",
        "create",
        "delete",
        "deprecated",
        "differences",
        "download",
        "edit",
        "endpoints",
        "extension",
        "file",
        "files",
        "flows",
        "image",
        "input",
        "list"
      ]
    },
    "api-files": {
      "title": "Files endpoints",
      "slug": "/api-files",
      "category": "api_reference",
      "content": "Use the `/files` endpoints to move files between your local machine and Langflow.\n\n## Differences between `/v1/files` and `/v2/files`\n\nThere are two versions of the `/files` endpoints.\n\n`/v2/files` offers the following improvements over `/v1/files`:\n\n- `/v2` files are organized by `user_id` instead of `flow_id`.\n  This means files are owned by users, and they aren't attached to specific flows.\n  You can upload a file to Langflow one time, and use it with multiple flows.\n- `/v2` files are tracked in the Langflow database.\n- `/v2` supports bulk upload and delete.\n- `/v2` responses contain more descriptive metadata.\n\nHowever, `/v2/files` doesn't support image files.\nTo send image files to your flows through the API, use [Upload image files (v1)](#upload-image-files-v1).\n\n## Files/V1 endpoints\n\nUse the `/files` endpoints to move files between your local machine and Langflow.\n\n### Upload file (v1)\n\nUpload a file to the `v1/files/upload/$FLOW_ID` endpoint:\nReplace **FILE_NAME** with the uploaded file name.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@FILE_NAME.txt\"\n```\n\nReplace `FILE_NAME.txt` with the name and extension of the file you want to upload.\nNot all file types are supported.\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"flowId\": \"92f9a4c5-cfc8-4656-ae63-1f0881163c28\",\n  \"file_path\": \"92f9a4c5-cfc8-4656-ae63-1f0881163c28/2024-12-30_15-19-43_your_file.txt\"\n}\n```\n\n</details>\n\n### Upload image files (v1)\n\nSend image files to Langflow to use them in flows.\n\nThe default file limit is 1024 MB.\nTo change this limit, set the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n1. Attach the image to a `POST /v1/files/upload/$FLOW_ID` request with `--form` (`-F`) and the file path:\n\n    ```bash\n    curl -X POST \"$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@PATH/TO/FILE.png\"\n    ```\n\n    A successful request returns the `file_path` for the image in the Langflow file management system in the format `FLOW_ID/TIMESTAMP_FILENAME.TYPE`.\n    For example:\n\n    ```json\n    {\n      \"flowId\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4\",\n      \"file_path\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png\"\n    }\n    ```\n\n2. Use the returned `file_path` to send the image file to other components that can accept file input. Where you specify the file path depends on the component type.\n\n    The following example runs the **Basic Prompting** template flow, passing the image file and the query `describe this image` as input for the **Chat Input** component.\n    In this case, the file path is specified in `tweaks`.\n\n    ```bash\n    curl -X POST \\\n        \"$LANGFLOW_URL/api/v1/run/a430cc57-06bb-4c11-be39-d3d4de68d2c4?stream=false\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n        -d '{\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"tweaks\": {\n          \"ChatInput-b67sL\": {\n            \"files\": \"a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png\",\n            \"input_value\": \"describe this image\"\n          }\n        }\n      }'\n    ```\n\n    \n**Note:** For help with tweaks, use the **Input Schema** in a flow's [**API access** pane](/concepts-publish#api-access).\n    Setting tweaks with **Input Schema** also automatically populates the required component IDs.\n    \n\n### List files (v1)\n\nList all files associated with a specific flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/files/list/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"files\": [\"2024-12-30_15-19-43_your_file.txt\"]\n}\n```\n\n</details>\n\n### Download file (v1)\n\nDownload a specific file from a flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/files/download/$FLOW_ID/2024-12-30_15-19-43_your_file.txt\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output downloaded_file.txt\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nFile contents downloaded to downloaded_file.txt\n```\n\n</details>\n\n### Delete file (v1)\n\nDelete a specific file from a flow.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/files/delete/$FLOW_ID/2024-12-30_15-19-43_your_file.txt\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"File 2024-12-30_15-19-43_your_file.txt deleted successfully\"\n}\n```\n\n</details>\n\n## Files/V2 endpoints\n\nUse the `/files` endpoints to move files between your local machine and Langflow.\n\nThe `/v2/files` endpoints can be authenticated by an API key or JWT.\nTo create a Langflow API key and export it as an environment variable, see [Get started with the Langflow API](/api-reference-api-examples).\n\n### Upload file (v2)\n\nUpload a file to your user account. The file can be used across multiple flows.\n\nThe file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, such as `07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf`.\n\n1. To retrieve your current `user_id`, call the `/whoami` endpoint:\n\n    ```bash\n    curl -X GET \\\n      \"$LANGFLOW_URL/api/v1/users/whoami\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```\n    {\"id\":\"07e5b864-e367-4f52-b647-a48035ae7e5e\",\"username\":\"langflow\",\"profile_image\":null,\"store_api_key\":null,\"is_active\":true,\"is_superuser\":true,\"create_at\":\"2025-05-08T17:59:07.855965\",\"updated_at\":\"2025-05-28T19:00:42.556460\",\"last_login_at\":\"2025-05-28T19:00:42.554338\",\"optins\":{\"github_starred\":false,\"dialog_dismissed\":true,\"discord_clicked\":false,\"mcp_dialog_dismissed\":true}}\n    ```\n\n    </details>\n\n2. In the POST request to `v2/files`, replace **@FILE_NAME.EXTENSION** with the uploaded file name and its extension.\nYou must include the ampersand (`@`) in the request to instruct curl to upload the contents of the file, not the string `FILE_NAME.EXTENSION`.\n\n    ```bash\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v2/files\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@FILE_NAME.EXTENSION\"\n    ```\n\n    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:\n\n    ```json\n    {\n      \"id\":\"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94\",\n      \"name\":\"engine_manual\",\n      \"path\":\"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf\",\n      \"size\":851160,\n      \"provider\":null\n    }\n    ```\n\n### Send files to your flows (v2)\n\n**Note:** The `/v2/files` endpoint can't send image files to flows.\nTo send image files to your flows through the API, see [Upload image files (v1)](#upload-image-files-v1).\n\nThis endpoint uploads files to your Langflow server's file management system.\nTo use an uploaded file in a flow, send the file path to a flow with a [**File** component](/components-data#file).\n\nThe default file limit is 1024 MB. To configure this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n1. To send a file to your flow with the API, POST the file to the `/api/v2/files` endpoint.\n\n    Replace **FILE_NAME.EXTENSION** with the name and extension of the file you want to upload.\n    This is the same step described in [Upload file (v2)](#upload-file-v2), but since you need the filename to upload to your flow, it is included here.\n\n    ```bash\n    curl -X POST \\\n      \"$LANGFLOW_URL/api/v2/files\" \\\n      -H \"accept: application/json\" \\\n      -H \"Content-Type: multipart/form-data\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n      -F \"file=@FILE_NAME.EXTENSION\"\n    ```\n\n    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:\n\n    ```json\n    {\n      \"id\":\"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94\",\n      \"name\":\"engine_manual\",\n      \"path\":\"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf\",\n      \"size\":851160,\n      \"provider\": null\n    }\n    ```\n\n2. To use this file in your flow, add a **File** component to your flow.\nThis component loads files into flows from your local machine or Langflow file management.\n\n3. Run the flow, passing the `path` to the `File` component in the `tweaks` object:\n\n    ```text\n    curl --request POST \\\n      --url \"$LANGFLOW_URL/api/v1/run/$FLOW_ID\" \\\n      --header \"Content-Type: application/json\" \\\n      --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n      --data '{\n      \"input_value\": \"what do you see?\",\n      \"output_type\": \"chat\",\n      \"input_type\": \"text\",\n      \"tweaks\": {\n        \"File-1olS3\": {\n          \"path\": [\n            \"07e5b864-e367-4f52-b647-a48035ae7e5e/3a290013-fe1e-4d3d-a454-cacae81288f3.pdf\"\n          ]\n        }\n      }\n    }'\n    ```\n\n    To get the `File` component's ID, call the [Read flow](/api-flows#read-flow) endpoint or inspect the component in the visual editor.\n\n    If the file path is valid, the flow runs successfully.\n\n### List files (v2)\n\nList all files associated with your user account.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v2/files\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"c7b22c4c-d5e0-4ec9-af97-5d85b7657a34\",\n    \"name\": \"your_file\",\n    \"path\": \"6f17a73e-97d7-4519-a8d9-8e4c0be411bb/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34.txt\",\n    \"size\": 1234,\n    \"provider\": null\n  }\n]\n```\n\n</details>\n\n### Download file (v2)\n\nDownload a specific file by its ID and file extension.\n\nYou must specify the file type you expect in the `--output` value.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v2/files/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output downloaded_file.txt\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nFile contents downloaded to downloaded_file.txt\n```\n\n</details>\n\n### Edit file name (v2)\n\nChange a file name.\n\n```bash\ncurl -X PUT \\\n  \"$LANGFLOW_URL/api/v2/files/$FILE_ID?name=new_file_name\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"76543e40-f388-4cb3-b0ee-a1e870aca3d3\",\n  \"name\": \"new_file_name\",\n  \"path\": \"6f17a73e-97d7-4519-a8d9-8e4c0be411bb/76543e40-f388-4cb3-b0ee-a1e870aca3d3.png\",\n  \"size\": 2728251,\n  \"provider\": null\n}\n```\n\n</details>\n\n### Delete file (v2)\n\nDelete a specific file by its ID.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v2/files/$FILE_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"File deleted successfully\"\n}\n```\n\n</details>\n\n### Delete all files (v2)\n\nDelete all files associated with your user account.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v2/files\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"All files deleted successfully\"\n}\n```\n\n</details>\n\n## Create upload file (Deprecated)\n\nThis endpoint is deprecated. Use the `/files` endpoints instead.\n\n## See also\n\n* [Manage files](/concepts-file-management)",
      "summary": "Use the `/files` endpoints to move files between your local machine and Langflow.",
      "keywords": [
        "access",
        "all",
        "api",
        "basic",
        "between",
        "chat",
        "create",
        "delete",
        "deprecated",
        "differences",
        "download",
        "edit",
        "endpoints",
        "extension",
        "file",
        "files",
        "flows",
        "image",
        "input",
        "list"
      ]
    },
    "/api-flows-run": {
      "title": "Flow trigger endpoints",
      "slug": "/api-flows-run",
      "category": "api_reference",
      "content": "Use the `/run` and `/webhook` endpoints to run flows.\n\nTo create, read, update, and delete flows, see [Flow management endpoints](/api-flows).\n\n## Run flow\n\n**Note:** Langflow automatically generates Python, JavaScript, and curl code snippets for the `/v1/run/$FLOW_ID` endpoint for all flows.\nFor more information, see [Generate API code snippets](/concepts-publish#generate-api-code-snippets).\n\nExecute a specified flow by ID or name.\nFlow IDs can be found on the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.\n\nThe following example runs the **Basic Prompting** template flow with flow parameters passed in the request body.\nThis flow requires a chat input string (`input_value`), and uses default values for all other parameters.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"input_value\": \"Tell me about something interesting!\",\n    \"session_id\": \"chat-123\",\n    \"input_type\": \"chat\",\n    \"output_type\": \"chat\",\n    \"output_component\": \"\",\n    \"tweaks\": null\n  }'\n```\n\nThe response from `/v1/run/$FLOW_ID` includes metadata, inputs, and outputs for the run.\n\n<details>\n<summary>Result</summary>\n\nThe following example illustrates a response from a Basic Prompting flow:\n\n```json\n{\n  \"session_id\": \"chat-123\",\n  \"outputs\": [{\n    \"inputs\": {\n      \"input_value\": \"Tell me about something interesting!\"\n    },\n    \"outputs\": [{\n      \"results\": {\n        \"message\": {\n          \"text\": \"Sure! Have you ever heard of the phenomenon known as \\\"bioluminescence\\\"? It's a fascinating natural occurrence where living organisms produce and emit light. This ability is found in various species, including certain types of jellyfish, fireflies, and deep-sea creatures like anglerfish.\\n\\nBioluminescence occurs through a chemical reaction in which a light-emitting molecule called luciferin reacts with oxygen, catalyzed by an enzyme called luciferase. The result is a beautiful glow that can serve various purposes, such as attracting mates, deterring predators, or luring prey.\\n\\nOne of the most stunning displays of bioluminescence can be seen in the ocean, where certain plankton emit light when disturbed, creating a mesmerizing blue glow in the water. This phenomenon is often referred to as \\\"sea sparkle\\\" and can be seen in coastal areas around the world.\\n\\nBioluminescence not only captivates our imagination but also has practical applications in science and medicine, including the development of biosensors and imaging techniques. It's a remarkable example of nature's creativity and complexity!\",\n          \"sender\": \"Machine\",\n          \"sender_name\": \"AI\",\n          \"session_id\": \"chat-123\",\n          \"timestamp\": \"2025-03-03T17:17:37+00:00\",\n          \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\",\n          \"properties\": {\n            \"source\": {\n              \"id\": \"OpenAIModel-d1wOZ\",\n              \"display_name\": \"OpenAI\",\n              \"source\": \"gpt-4o-mini\"\n            },\n            \"icon\": \"OpenAI\"\n          },\n          \"component_id\": \"ChatOutput-ylMzN\"\n        }\n      }\n    }]\n  }]\n}\n```\n</details>\n\nIf you are parsing the response in an application, you most likely need to extract the relevant content from the response, rather than pass the entire response back to the user.\nFor an example of a script that extracts data from a Langflow API response, see the [Quickstart](/get-started-quickstart).\n\n### Stream LLM token responses\n\nWith `/v1/run/$FLOW_ID`, the flow is executed as a batch with optional LLM token response streaming.\n\nTo stream LLM token responses, append the `?stream=true` query parameter to the request:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"message\": \"Tell me something interesting!\",\n    \"session_id\": \"chat-123\"\n  }'\n```\n\nLLM chat responses are streamed back as `token` events, culminating in a final `end` event that closes the connection.\n\n<details>\n<summary>Result</summary>\n\nThe following example is truncated to illustrate a series of `token` events as well as the final `end` event that closes the LLM's token streaming response:\n\n```text\n{\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:20:18\", \"sender\": \"User\", \"sender_name\": \"User\", \"session_id\": \"chat-123\", \"text\": \"Tell me about something interesting!\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": null, \"display_name\": null, \"source\": null}, \"icon\": \"\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"0103a21b-ebf7-4c02-9d72-017fb297f812\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n{\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:20:18\", \"sender\": \"Machine\", \"sender_name\": \"AI\", \"session_id\": \"chat-123\", \"text\": \"\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": \"OpenAIModel-d1wOZ\", \"display_name\": \"OpenAI\", \"source\": \"gpt-4o-mini\"}, \"icon\": \"OpenAI\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" Have\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" you\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" ever\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" heard\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" of\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" the\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" phenomenon\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"end\", \"data\": {\"result\": {\"session_id\": \"chat-123\", \"message\": \"Sure! Have you ever heard of the phenomenon known as \\\"bioluminescence\\\"?...\"}}}\n```\n\n</details>\n\n### Run endpoint headers\n\n| Header | Info | Example |\n|--------|------|---------|\n| Content-Type | Required. Specifies the JSON format. | \"application/json\" |\n| accept | Optional. Specifies the response format. | \"application/json\" |\n| x-api-key | Optional. Required only if authentication is enabled. | \"sk-...\" |\n\n### Run endpoint parameters\n\n| Parameter | Type | Info |\n|-----------|------|------|\n| flow_id | UUID/string | Required. Part of URL: `/run/$FLOW_ID` |\n| stream | Boolean | Optional. Query parameter: `/run/$FLOW_ID?stream=true` |\n| input_value | string | Optional. JSON body field. Main input text/prompt. Default: `null` |\n| input_type | string | Optional. JSON body field. Input type (\"chat\" or \"text\"). Default: `\"chat\"` |\n| output_type | string | Optional. JSON body field. Output type (\"chat\", \"any\", \"debug\"). Default: `\"chat\"` |\n| output_component | string | Optional. JSON body field. Target component for output. Default: `\"\"` |\n| tweaks | object | Optional. JSON body field. Component adjustments. Default: `null` |\n| session_id | string | Optional. JSON body field. Conversation context ID. See [Session ID](/session-id). Default: `null` |\n\n### Request example with all headers and parameters\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"input_value\": \"Tell me a story\",\n    \"input_type\": \"chat\",\n    \"output_type\": \"chat\",\n    \"output_component\": \"chat_output\",\n    \"session_id\": \"chat-123\",\n    \"tweaks\": {\n      \"component_id\": {\n        \"parameter_name\": \"value\"\n      }\n    }\n  }'\n```\n\n## Webhook run flow\n\nUse the `/webhook` endpoint to start a flow by sending an HTTP `POST` request.\n\n**Note:** After you add a [**Webhook** component](/components-data#webhook) to a flow, open the [**API access** pane](/concepts-publish), and then click the **Webhook curl** tab to get an automatically generated `POST /webhook` request for your flow.\nFor more information, see [Trigger flows with webhooks](/webhook).\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\"data\": \"example-data\"}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"Task started in the background\",\n  \"status\": \"in progress\"\n}\n```\n\n</details>\n\n## Deprecated flow trigger endpoints\n\nThe following endpoints are deprecated and replaced by the `/run` endpoint:\n\n* `/process`\n* `/predict`",
      "summary": "Use the `/run` and `/webhook` endpoints to run flows.",
      "keywords": [
        "access",
        "all",
        "api",
        "basic",
        "curl",
        "deprecated",
        "endpoint",
        "endpoints",
        "flow",
        "headers",
        "llm",
        "note",
        "parameters",
        "prompting",
        "request",
        "responses",
        "run",
        "stream",
        "token",
        "trigger"
      ]
    },
    "api-flows-run": {
      "title": "Flow trigger endpoints",
      "slug": "/api-flows-run",
      "category": "api_reference",
      "content": "Use the `/run` and `/webhook` endpoints to run flows.\n\nTo create, read, update, and delete flows, see [Flow management endpoints](/api-flows).\n\n## Run flow\n\n**Note:** Langflow automatically generates Python, JavaScript, and curl code snippets for the `/v1/run/$FLOW_ID` endpoint for all flows.\nFor more information, see [Generate API code snippets](/concepts-publish#generate-api-code-snippets).\n\nExecute a specified flow by ID or name.\nFlow IDs can be found on the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.\n\nThe following example runs the **Basic Prompting** template flow with flow parameters passed in the request body.\nThis flow requires a chat input string (`input_value`), and uses default values for all other parameters.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"input_value\": \"Tell me about something interesting!\",\n    \"session_id\": \"chat-123\",\n    \"input_type\": \"chat\",\n    \"output_type\": \"chat\",\n    \"output_component\": \"\",\n    \"tweaks\": null\n  }'\n```\n\nThe response from `/v1/run/$FLOW_ID` includes metadata, inputs, and outputs for the run.\n\n<details>\n<summary>Result</summary>\n\nThe following example illustrates a response from a Basic Prompting flow:\n\n```json\n{\n  \"session_id\": \"chat-123\",\n  \"outputs\": [{\n    \"inputs\": {\n      \"input_value\": \"Tell me about something interesting!\"\n    },\n    \"outputs\": [{\n      \"results\": {\n        \"message\": {\n          \"text\": \"Sure! Have you ever heard of the phenomenon known as \\\"bioluminescence\\\"? It's a fascinating natural occurrence where living organisms produce and emit light. This ability is found in various species, including certain types of jellyfish, fireflies, and deep-sea creatures like anglerfish.\\n\\nBioluminescence occurs through a chemical reaction in which a light-emitting molecule called luciferin reacts with oxygen, catalyzed by an enzyme called luciferase. The result is a beautiful glow that can serve various purposes, such as attracting mates, deterring predators, or luring prey.\\n\\nOne of the most stunning displays of bioluminescence can be seen in the ocean, where certain plankton emit light when disturbed, creating a mesmerizing blue glow in the water. This phenomenon is often referred to as \\\"sea sparkle\\\" and can be seen in coastal areas around the world.\\n\\nBioluminescence not only captivates our imagination but also has practical applications in science and medicine, including the development of biosensors and imaging techniques. It's a remarkable example of nature's creativity and complexity!\",\n          \"sender\": \"Machine\",\n          \"sender_name\": \"AI\",\n          \"session_id\": \"chat-123\",\n          \"timestamp\": \"2025-03-03T17:17:37+00:00\",\n          \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\",\n          \"properties\": {\n            \"source\": {\n              \"id\": \"OpenAIModel-d1wOZ\",\n              \"display_name\": \"OpenAI\",\n              \"source\": \"gpt-4o-mini\"\n            },\n            \"icon\": \"OpenAI\"\n          },\n          \"component_id\": \"ChatOutput-ylMzN\"\n        }\n      }\n    }]\n  }]\n}\n```\n</details>\n\nIf you are parsing the response in an application, you most likely need to extract the relevant content from the response, rather than pass the entire response back to the user.\nFor an example of a script that extracts data from a Langflow API response, see the [Quickstart](/get-started-quickstart).\n\n### Stream LLM token responses\n\nWith `/v1/run/$FLOW_ID`, the flow is executed as a batch with optional LLM token response streaming.\n\nTo stream LLM token responses, append the `?stream=true` query parameter to the request:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"message\": \"Tell me something interesting!\",\n    \"session_id\": \"chat-123\"\n  }'\n```\n\nLLM chat responses are streamed back as `token` events, culminating in a final `end` event that closes the connection.\n\n<details>\n<summary>Result</summary>\n\nThe following example is truncated to illustrate a series of `token` events as well as the final `end` event that closes the LLM's token streaming response:\n\n```text\n{\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:20:18\", \"sender\": \"User\", \"sender_name\": \"User\", \"session_id\": \"chat-123\", \"text\": \"Tell me about something interesting!\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": null, \"display_name\": null, \"source\": null}, \"icon\": \"\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"0103a21b-ebf7-4c02-9d72-017fb297f812\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n{\"event\": \"add_message\", \"data\": {\"timestamp\": \"2025-03-03T17:20:18\", \"sender\": \"Machine\", \"sender_name\": \"AI\", \"session_id\": \"chat-123\", \"text\": \"\", \"files\": [], \"error\": false, \"edit\": false, \"properties\": {\"text_color\": \"\", \"background_color\": \"\", \"edited\": false, \"source\": {\"id\": \"OpenAIModel-d1wOZ\", \"display_name\": \"OpenAI\", \"source\": \"gpt-4o-mini\"}, \"icon\": \"OpenAI\", \"allow_markdown\": false, \"positive_feedback\": null, \"state\": \"complete\", \"targets\": []}, \"category\": \"message\", \"content_blocks\": [], \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"flow_id\": \"d2bbd92b-187e-4c84-b2d4-5df365704201\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" Have\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" you\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" ever\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" heard\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" of\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" the\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"token\", \"data\": {\"chunk\": \" phenomenon\", \"id\": \"27b66789-e673-4c65-9e81-021752925161\", \"timestamp\": \"2025-03-03 17:20:18 UTC\"}}\n\n{\"event\": \"end\", \"data\": {\"result\": {\"session_id\": \"chat-123\", \"message\": \"Sure! Have you ever heard of the phenomenon known as \\\"bioluminescence\\\"?...\"}}}\n```\n\n</details>\n\n### Run endpoint headers\n\n| Header | Info | Example |\n|--------|------|---------|\n| Content-Type | Required. Specifies the JSON format. | \"application/json\" |\n| accept | Optional. Specifies the response format. | \"application/json\" |\n| x-api-key | Optional. Required only if authentication is enabled. | \"sk-...\" |\n\n### Run endpoint parameters\n\n| Parameter | Type | Info |\n|-----------|------|------|\n| flow_id | UUID/string | Required. Part of URL: `/run/$FLOW_ID` |\n| stream | Boolean | Optional. Query parameter: `/run/$FLOW_ID?stream=true` |\n| input_value | string | Optional. JSON body field. Main input text/prompt. Default: `null` |\n| input_type | string | Optional. JSON body field. Input type (\"chat\" or \"text\"). Default: `\"chat\"` |\n| output_type | string | Optional. JSON body field. Output type (\"chat\", \"any\", \"debug\"). Default: `\"chat\"` |\n| output_component | string | Optional. JSON body field. Target component for output. Default: `\"\"` |\n| tweaks | object | Optional. JSON body field. Component adjustments. Default: `null` |\n| session_id | string | Optional. JSON body field. Conversation context ID. See [Session ID](/session-id). Default: `null` |\n\n### Request example with all headers and parameters\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"input_value\": \"Tell me a story\",\n    \"input_type\": \"chat\",\n    \"output_type\": \"chat\",\n    \"output_component\": \"chat_output\",\n    \"session_id\": \"chat-123\",\n    \"tweaks\": {\n      \"component_id\": {\n        \"parameter_name\": \"value\"\n      }\n    }\n  }'\n```\n\n## Webhook run flow\n\nUse the `/webhook` endpoint to start a flow by sending an HTTP `POST` request.\n\n**Note:** After you add a [**Webhook** component](/components-data#webhook) to a flow, open the [**API access** pane](/concepts-publish), and then click the **Webhook curl** tab to get an automatically generated `POST /webhook` request for your flow.\nFor more information, see [Trigger flows with webhooks](/webhook).\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\"data\": \"example-data\"}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"Task started in the background\",\n  \"status\": \"in progress\"\n}\n```\n\n</details>\n\n## Deprecated flow trigger endpoints\n\nThe following endpoints are deprecated and replaced by the `/run` endpoint:\n\n* `/process`\n* `/predict`",
      "summary": "Use the `/run` and `/webhook` endpoints to run flows.",
      "keywords": [
        "access",
        "all",
        "api",
        "basic",
        "curl",
        "deprecated",
        "endpoint",
        "endpoints",
        "flow",
        "headers",
        "llm",
        "note",
        "parameters",
        "prompting",
        "request",
        "responses",
        "run",
        "stream",
        "token",
        "trigger"
      ]
    },
    "/api-flows": {
      "title": "Flow management endpoints",
      "slug": "/api-flows",
      "category": "api_reference",
      "content": "Use the `/flows` endpoint to create, read, update, and delete flows.\n\nIf you want to use the Langflow API to run a flow, see [Flow trigger endpoints](/api-flows-run).\n\n## Create flow\n\nCreates a new flow.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string2\",\n  \"description\": \"string\",\n  \"icon\": \"string\",\n  \"icon_bg_color\": \"#FF0000\",\n  \"gradient\": \"string\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2024-12-30T15:48:01.519Z\",\n  \"webhook\": false,\n  \"endpoint_name\": \"string\",\n  \"tags\": [\n    \"string\"\n  ]\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string2\",\n  \"description\": \"string\",\n  \"icon\": \"string\",\n  \"icon_bg_color\": \"#FF0000\",\n  \"gradient\": \"string\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2025-02-04T21:07:36+00:00\",\n  \"webhook\": false,\n  \"endpoint_name\": \"string\",\n  \"tags\": [\"string\"],\n  \"locked\": false,\n  \"id\": \"e8d81c37-714b-49ae-ba82-e61141f020ee\",\n  \"user_id\": \"f58396d4-a387-4bb8-b749-f40825c3d9f3\",\n  \"project_id\": \"1415de42-8f01-4f36-bf34-539f23e47466\"\n}\n```\n\n</details>\n\n## Create flows\n\nCreates multiple new flows, returning an array of flow objects.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/batch/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"flows\": [\n    {\n      \"name\": \"string\",\n      \"description\": \"string\",\n      \"icon\": \"string\",\n      \"icon_bg_color\": \"string\",\n      \"gradient\": \"string\",\n      \"data\": {},\n      \"is_component\": false,\n      \"updated_at\": \"2024-12-30T18:36:02.737Z\",\n      \"webhook\": false,\n      \"endpoint_name\": \"string\",\n      \"tags\": [\n        \"string\"\n      ],\n      \"locked\": false,\n      \"user_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    },\n    {\n      \"name\": \"string\",\n      \"description\": \"string\",\n      \"icon\": \"string\",\n      \"icon_bg_color\": \"string\",\n      \"gradient\": \"string\",\n      \"data\": {},\n      \"is_component\": false,\n      \"updated_at\": \"2024-12-30T18:36:02.737Z\",\n      \"webhook\": false,\n      \"endpoint_name\": \"string\",\n      \"tags\": [\n        \"string\"\n      ],\n      \"locked\": false,\n      \"user_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    }\n  ]\n}'\n```\n\n## Read flow\n\nRetrieves a specific flow by its ID.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"Basic Prompting\",\n  \"description\": \"Perform basic prompting with an OpenAI model.\",\n  \"icon\": \"Braces\",\n  \"icon_bg_color\": null,\n  \"gradient\": \"2\",\n  \"data\": {\n    \"nodes\": [\n     ...\n    ]\n  }\n}\n```\n\n</details>\n\n## Read flows\n\nReturns a JSON object containing a list of flows.\n\nRetrieve all flows with pagination:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/?remove_example_flows=false&components_only=false&get_all=true&header_flows=false&page=1&size=50\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\nTo retrieve flows from a specific project, use the `project_id` query parameter:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/?remove_example_flows=true&components_only=false&get_all=false&project_id=$PROJECT_ID&header_flows=false&page=1&size=1\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Read sample flows\n\nRetrieves a list of sample flows:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/basic_examples/\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Update flow\n\nUpdates an existing flow by its ID.\n\nThis example changes the value for `endpoint_name` from a random UUID to `my_new_endpoint_name`.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"data\": {},\n  \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n  \"endpoint_name\": \"my_new_endpoint_name\",\n  \"locked\": true\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"icon\": \"Braces\",\n  \"icon_bg_color\": null,\n  \"gradient\": \"2\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2024-12-30T18:30:22+00:00\",\n  \"webhook\": false,\n  \"endpoint_name\": \"my_new_endpoint_name\",\n  \"tags\": null,\n  \"locked\": true,\n  \"id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n  \"user_id\": \"f58396d4-a387-4bb8-b749-f40825c3d9f3\",\n  \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n}\n```\n\n</details>\n\n## Delete flow\n\nDeletes a specific flow by its ID.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"Flow deleted successfully\"\n}\n```\n\n</details>\n\n## Export flows\n\nExports specified flows to a ZIP file.\n\nThis endpoint downloads a ZIP file containing [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) for each flow ID listed in the request body.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/download/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '[\n  \"e1e40c77-0541-41a9-88ab-ddb3419398b5\",\n  \"92f9a4c5-cfc8-4656-ae63-1f0881163c28\"\n]' \\\n  --output langflow-flows.zip\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 76437    0 76353  100    84  4516k   5088 --:--:-- --:--:-- --:--:-- 4665k\n```\n\n</details>\n\n## Import flows\n\nImports flows by uploading a [Langflow-compatible JSON file](/concepts-flows-import#langflow-json-file-contents).\n\nTo specify a target project for the flow, include the query parameter `folder_id`.\nThe target `folder_id` must already exist before uploading a flow. Call the [/api/v1/projects/](/api-projects#read-projects) endpoint for a list of available folders and projects.\n\nThis example uploads a local file named `agent-with-astra-db-tool.json` to a folder specified by a `FOLDER_ID` variable:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/upload/?folder_id=$FOLDER_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@agent-with-astra-db-tool.json;type=application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"agent-with-astra-db-tool\",\n    \"description\": \"\",\n    \"icon\": null,\n    \"icon_bg_color\": null,\n    \"gradient\": null,\n    \"data\": {}\n  ...\n  }\n]\n```\n</details>",
      "summary": "Use the `/flows` endpoint to create, read, update, and delete flows.",
      "keywords": [
        "create",
        "delete",
        "endpoints",
        "export",
        "flow",
        "flows",
        "import",
        "management",
        "read",
        "sample",
        "update"
      ]
    },
    "api-flows": {
      "title": "Flow management endpoints",
      "slug": "/api-flows",
      "category": "api_reference",
      "content": "Use the `/flows` endpoint to create, read, update, and delete flows.\n\nIf you want to use the Langflow API to run a flow, see [Flow trigger endpoints](/api-flows-run).\n\n## Create flow\n\nCreates a new flow.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string2\",\n  \"description\": \"string\",\n  \"icon\": \"string\",\n  \"icon_bg_color\": \"#FF0000\",\n  \"gradient\": \"string\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2024-12-30T15:48:01.519Z\",\n  \"webhook\": false,\n  \"endpoint_name\": \"string\",\n  \"tags\": [\n    \"string\"\n  ]\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string2\",\n  \"description\": \"string\",\n  \"icon\": \"string\",\n  \"icon_bg_color\": \"#FF0000\",\n  \"gradient\": \"string\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2025-02-04T21:07:36+00:00\",\n  \"webhook\": false,\n  \"endpoint_name\": \"string\",\n  \"tags\": [\"string\"],\n  \"locked\": false,\n  \"id\": \"e8d81c37-714b-49ae-ba82-e61141f020ee\",\n  \"user_id\": \"f58396d4-a387-4bb8-b749-f40825c3d9f3\",\n  \"project_id\": \"1415de42-8f01-4f36-bf34-539f23e47466\"\n}\n```\n\n</details>\n\n## Create flows\n\nCreates multiple new flows, returning an array of flow objects.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/batch/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"flows\": [\n    {\n      \"name\": \"string\",\n      \"description\": \"string\",\n      \"icon\": \"string\",\n      \"icon_bg_color\": \"string\",\n      \"gradient\": \"string\",\n      \"data\": {},\n      \"is_component\": false,\n      \"updated_at\": \"2024-12-30T18:36:02.737Z\",\n      \"webhook\": false,\n      \"endpoint_name\": \"string\",\n      \"tags\": [\n        \"string\"\n      ],\n      \"locked\": false,\n      \"user_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    },\n    {\n      \"name\": \"string\",\n      \"description\": \"string\",\n      \"icon\": \"string\",\n      \"icon_bg_color\": \"string\",\n      \"gradient\": \"string\",\n      \"data\": {},\n      \"is_component\": false,\n      \"updated_at\": \"2024-12-30T18:36:02.737Z\",\n      \"webhook\": false,\n      \"endpoint_name\": \"string\",\n      \"tags\": [\n        \"string\"\n      ],\n      \"locked\": false,\n      \"user_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    }\n  ]\n}'\n```\n\n## Read flow\n\nRetrieves a specific flow by its ID.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"Basic Prompting\",\n  \"description\": \"Perform basic prompting with an OpenAI model.\",\n  \"icon\": \"Braces\",\n  \"icon_bg_color\": null,\n  \"gradient\": \"2\",\n  \"data\": {\n    \"nodes\": [\n     ...\n    ]\n  }\n}\n```\n\n</details>\n\n## Read flows\n\nReturns a JSON object containing a list of flows.\n\nRetrieve all flows with pagination:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/?remove_example_flows=false&components_only=false&get_all=true&header_flows=false&page=1&size=50\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\nTo retrieve flows from a specific project, use the `project_id` query parameter:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/?remove_example_flows=true&components_only=false&get_all=false&project_id=$PROJECT_ID&header_flows=false&page=1&size=1\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Read sample flows\n\nRetrieves a list of sample flows:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/flows/basic_examples/\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Update flow\n\nUpdates an existing flow by its ID.\n\nThis example changes the value for `endpoint_name` from a random UUID to `my_new_endpoint_name`.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"data\": {},\n  \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n  \"endpoint_name\": \"my_new_endpoint_name\",\n  \"locked\": true\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"icon\": \"Braces\",\n  \"icon_bg_color\": null,\n  \"gradient\": \"2\",\n  \"data\": {},\n  \"is_component\": false,\n  \"updated_at\": \"2024-12-30T18:30:22+00:00\",\n  \"webhook\": false,\n  \"endpoint_name\": \"my_new_endpoint_name\",\n  \"tags\": null,\n  \"locked\": true,\n  \"id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n  \"user_id\": \"f58396d4-a387-4bb8-b749-f40825c3d9f3\",\n  \"project_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n}\n```\n\n</details>\n\n## Delete flow\n\nDeletes a specific flow by its ID.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/flows/$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"message\": \"Flow deleted successfully\"\n}\n```\n\n</details>\n\n## Export flows\n\nExports specified flows to a ZIP file.\n\nThis endpoint downloads a ZIP file containing [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) for each flow ID listed in the request body.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/download/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '[\n  \"e1e40c77-0541-41a9-88ab-ddb3419398b5\",\n  \"92f9a4c5-cfc8-4656-ae63-1f0881163c28\"\n]' \\\n  --output langflow-flows.zip\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 76437    0 76353  100    84  4516k   5088 --:--:-- --:--:-- --:--:-- 4665k\n```\n\n</details>\n\n## Import flows\n\nImports flows by uploading a [Langflow-compatible JSON file](/concepts-flows-import#langflow-json-file-contents).\n\nTo specify a target project for the flow, include the query parameter `folder_id`.\nThe target `folder_id` must already exist before uploading a flow. Call the [/api/v1/projects/](/api-projects#read-projects) endpoint for a list of available folders and projects.\n\nThis example uploads a local file named `agent-with-astra-db-tool.json` to a folder specified by a `FOLDER_ID` variable:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/flows/upload/?folder_id=$FOLDER_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@agent-with-astra-db-tool.json;type=application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"agent-with-astra-db-tool\",\n    \"description\": \"\",\n    \"icon\": null,\n    \"icon_bg_color\": null,\n    \"gradient\": null,\n    \"data\": {}\n  ...\n  }\n]\n```\n</details>",
      "summary": "Use the `/flows` endpoint to create, read, update, and delete flows.",
      "keywords": [
        "create",
        "delete",
        "endpoints",
        "export",
        "flow",
        "flows",
        "import",
        "management",
        "read",
        "sample",
        "update"
      ]
    },
    "/api-logs": {
      "title": "Logs endpoints",
      "slug": "/api-logs",
      "category": "api_reference",
      "content": "Retrieve logs for your Langflow flows and server.\n\n## Enable log retrieval\n\nThe `/logs` endpoint requires log retrieval to be enabled in your Langflow instance.\n\nTo enable log retrieval, include set the following [environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:\n\n```text\nLANGFLOW_ENABLE_LOG_RETRIEVAL=True\nLANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000. // Must be greater than 0\nLANGFLOW_LOG_LEVEL=DEBUG. // Can be DEBUG, ERROR, INFO, WARNING, or CRITICAL\n```\n\n## Stream logs\n\nStream logs in real-time using Server Sent Events (SSE):\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/logs-stream\" \\\n  -H \"accept: text/event-stream\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nkeepalive\n\n{\"1736355791151\": \"2025-01-08T12:03:11.151218-0500 DEBUG Building Chat Input\\n\"}\n\n{\"1736355791485\": \"2025-01-08T12:03:11.485380-0500 DEBUG consumed event add_message-153bcd5d-ef4d-4ece-8cc0-47c6b6a9ef92 (time in queue, 0.0000, client 0.0001)\\n\"}\n\n{\"1736355791499\": \"2025-01-08T12:03:11.499704-0500 DEBUG consumed event end_vertex-3d7125cd-7b8a-44eb-9113-ed5b785e3cf3 (time in queue, 0.0056, client 0.0047)\\n\"}\n\n{\"1736355791502\": \"2025-01-08T12:03:11.502510-0500 DEBUG consumed event end-40d0b363-5618-4a23-bbae-487cd0b9594d (time in queue, 0.0001, client 0.0004)\\n\"}\n\n{\"1736355791513\": \"2025-01-08T12:03:11.513097-0500 DEBUG Logged vertex build: 729ff2f8-6b01-48c8-9ad0-3743c2af9e8a\\n\"}\n\n{\"1736355791834\": \"2025-01-08T12:03:11.834982-0500 DEBUG Telemetry data sent successfully.\\n\"}\n\n{\"1736355791941\": \"2025-01-08T12:03:11.941840-0500 DEBUG Telemetry data sent successfully.\\n\"}\n\nkeepalive\n```\n\n</details>\n\n## Retrieve logs with optional parameters\n\nRetrieve logs with optional query parameters:\n\n- `lines_before`: The number of logs before the timestamp or the last log.\n- `lines_after`: The number of logs after the timestamp.\n- `timestamp`: The timestamp to start getting logs from.\n\nThe default values for all three parameters is `0`.\nWith default values, the endpoint returns the last 10 lines of logs.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/logs?lines_before=0&lines_after=0&timestamp=0\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n{\n  \"1736354770500\": \"2025-01-08T11:46:10.500363-0500 DEBUG Creating starter project Document Q&A\\n\",\n  \"1736354770511\": \"2025-01-08T11:46:10.511146-0500 DEBUG Creating starter project Image Sentiment Analysis\\n\",\n  \"1736354770521\": \"2025-01-08T11:46:10.521018-0500 DEBUG Creating starter project SEO Keyword Generator\\n\",\n  \"1736354770532\": \"2025-01-08T11:46:10.532677-0500 DEBUG Creating starter project Sequential Tasks Agents\\n\",\n  \"1736354770544\": \"2025-01-08T11:46:10.544010-0500 DEBUG Creating starter project Custom Component Generator\\n\",\n  \"1736354770555\": \"2025-01-08T11:46:10.555513-0500 DEBUG Creating starter project Prompt Chaining\\n\",\n  \"1736354770588\": \"2025-01-08T11:46:10.588105-0500 DEBUG Create service ServiceType.CHAT_SERVICE\\n\",\n  \"1736354771021\": \"2025-01-08T11:46:11.021817-0500 DEBUG Telemetry data sent successfully.\\n\",\n  \"1736354775619\": \"2025-01-08T11:46:15.619545-0500 DEBUG Create service ServiceType.STORE_SERVICE\\n\",\n  \"1736354775699\": \"2025-01-08T11:46:15.699661-0500 DEBUG File 046-rocket.svg retrieved successfully from flow /Users/USER/Library/Caches/langflow/profile_pictures/Space.\\n\"\n}\n```\n\n</details>",
      "summary": "Retrieve logs for your Langflow flows and server.",
      "keywords": [
        "enable",
        "endpoints",
        "log",
        "logs",
        "optional",
        "parameters",
        "retrieval",
        "retrieve",
        "stream"
      ]
    },
    "api-logs": {
      "title": "Logs endpoints",
      "slug": "/api-logs",
      "category": "api_reference",
      "content": "Retrieve logs for your Langflow flows and server.\n\n## Enable log retrieval\n\nThe `/logs` endpoint requires log retrieval to be enabled in your Langflow instance.\n\nTo enable log retrieval, include set the following [environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:\n\n```text\nLANGFLOW_ENABLE_LOG_RETRIEVAL=True\nLANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000. // Must be greater than 0\nLANGFLOW_LOG_LEVEL=DEBUG. // Can be DEBUG, ERROR, INFO, WARNING, or CRITICAL\n```\n\n## Stream logs\n\nStream logs in real-time using Server Sent Events (SSE):\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/logs-stream\" \\\n  -H \"accept: text/event-stream\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nkeepalive\n\n{\"1736355791151\": \"2025-01-08T12:03:11.151218-0500 DEBUG Building Chat Input\\n\"}\n\n{\"1736355791485\": \"2025-01-08T12:03:11.485380-0500 DEBUG consumed event add_message-153bcd5d-ef4d-4ece-8cc0-47c6b6a9ef92 (time in queue, 0.0000, client 0.0001)\\n\"}\n\n{\"1736355791499\": \"2025-01-08T12:03:11.499704-0500 DEBUG consumed event end_vertex-3d7125cd-7b8a-44eb-9113-ed5b785e3cf3 (time in queue, 0.0056, client 0.0047)\\n\"}\n\n{\"1736355791502\": \"2025-01-08T12:03:11.502510-0500 DEBUG consumed event end-40d0b363-5618-4a23-bbae-487cd0b9594d (time in queue, 0.0001, client 0.0004)\\n\"}\n\n{\"1736355791513\": \"2025-01-08T12:03:11.513097-0500 DEBUG Logged vertex build: 729ff2f8-6b01-48c8-9ad0-3743c2af9e8a\\n\"}\n\n{\"1736355791834\": \"2025-01-08T12:03:11.834982-0500 DEBUG Telemetry data sent successfully.\\n\"}\n\n{\"1736355791941\": \"2025-01-08T12:03:11.941840-0500 DEBUG Telemetry data sent successfully.\\n\"}\n\nkeepalive\n```\n\n</details>\n\n## Retrieve logs with optional parameters\n\nRetrieve logs with optional query parameters:\n\n- `lines_before`: The number of logs before the timestamp or the last log.\n- `lines_after`: The number of logs after the timestamp.\n- `timestamp`: The timestamp to start getting logs from.\n\nThe default values for all three parameters is `0`.\nWith default values, the endpoint returns the last 10 lines of logs.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/logs?lines_before=0&lines_after=0&timestamp=0\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n{\n  \"1736354770500\": \"2025-01-08T11:46:10.500363-0500 DEBUG Creating starter project Document Q&A\\n\",\n  \"1736354770511\": \"2025-01-08T11:46:10.511146-0500 DEBUG Creating starter project Image Sentiment Analysis\\n\",\n  \"1736354770521\": \"2025-01-08T11:46:10.521018-0500 DEBUG Creating starter project SEO Keyword Generator\\n\",\n  \"1736354770532\": \"2025-01-08T11:46:10.532677-0500 DEBUG Creating starter project Sequential Tasks Agents\\n\",\n  \"1736354770544\": \"2025-01-08T11:46:10.544010-0500 DEBUG Creating starter project Custom Component Generator\\n\",\n  \"1736354770555\": \"2025-01-08T11:46:10.555513-0500 DEBUG Creating starter project Prompt Chaining\\n\",\n  \"1736354770588\": \"2025-01-08T11:46:10.588105-0500 DEBUG Create service ServiceType.CHAT_SERVICE\\n\",\n  \"1736354771021\": \"2025-01-08T11:46:11.021817-0500 DEBUG Telemetry data sent successfully.\\n\",\n  \"1736354775619\": \"2025-01-08T11:46:15.619545-0500 DEBUG Create service ServiceType.STORE_SERVICE\\n\",\n  \"1736354775699\": \"2025-01-08T11:46:15.699661-0500 DEBUG File 046-rocket.svg retrieved successfully from flow /Users/USER/Library/Caches/langflow/profile_pictures/Space.\\n\"\n}\n```\n\n</details>",
      "summary": "Retrieve logs for your Langflow flows and server.",
      "keywords": [
        "enable",
        "endpoints",
        "log",
        "logs",
        "optional",
        "parameters",
        "retrieval",
        "retrieve",
        "stream"
      ]
    },
    "/api-monitor": {
      "title": "Monitor endpoints",
      "slug": "/api-monitor",
      "category": "api_reference",
      "content": "The `/monitor` endpoints are for internal Langflow functionality, primarily related to running flows in the **Playground**, storing chat history, and generating flow logs.\n\nThis information is primarily for those who are building custom components or contributing to the Langflow codebase in a way that requires calling or understanding these endpoints.\n\nFor typical application development with Langflow, there are more appropriate options for monitoring, debugging, and memory management.\nFor more information, see the following:\n\n* [Logs](/logging): Langflow log storage locations, customization options, and where to view logs in the visual editor\n* [Test flows in the Playground](/concepts-playground): Run flows and inspect message history\n* [Memory management options](/memory): Langflow storage locations and options, including the database, cache, and chat history\n\n## Vertex builds\n\nThe Vertex build endpoints (`/monitor/builds`) are exclusively for **Playground** functionality.\n\nWhen you run a flow in the **Playground**, Langflow calls the `/build/$FLOW_ID/flow` endpoint in [chat.py](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/api/v1/chat.py#L143). This call retrieves the flow data, builds a graph, and executes the graph. As each component (or node) is executed, the `build_vertex` function calls `build_and_run`, which may call the individual components' `def_build` method, if it exists. If a component doesn't have a `def_build` function, the build still returns a component.\n\nThe `build` function allows components to execute logic at runtime. For example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the `LCTextSplitterComponent` class. When text needs to be processed, the parent class's `build` method is called, which creates a `RecursiveCharacterTextSplitter` object and uses it to split the text according to the defined parameters. The split text is then passed on to the next component. This all occurs when the component is built.\n\n### Get Vertex builds\n\nRetrieve Vertex builds for a specific flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"vertex_builds\": {\n    \"ChatInput-NCmix\": [\n      {\n        \"data\": {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2024-12-23 19:10:57\",\n                \"sender\": \"User\",\n                \"sender_name\": \"User\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello\",\n                \"files\": [],\n                \"error\": \"False\",\n                \"edit\": \"False\",\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": \"False\",\n                  \"source\": {\n                    \"id\": \"None\",\n                    \"display_name\": \"None\",\n                    \"source\": \"None\"\n                  },\n                  \"icon\": \"\",\n                  \"allow_markdown\": \"False\",\n                  \"positive_feedback\": \"None\",\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"c95bed34-f906-4aa6-84e4-68553f6db772\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello\",\n              \"sender\": \"User\",\n              \"sender_name\": \"User\",\n              \"files\": [],\n              \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"timestamp\": \"2024-12-23 19:10:57+00:00\",\n              \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"error\": \"False\",\n              \"edit\": \"False\",\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": \"False\",\n                \"source\": {\n                  \"id\": \"None\",\n                  \"display_name\": \"None\",\n                  \"source\": \"None\"\n                },\n                \"icon\": \"\",\n                \"allow_markdown\": \"False\",\n                \"positive_feedback\": \"None\",\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": []\n            }\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": {\n                \"timestamp\": \"2024-12-23T19:10:57\",\n                \"sender\": \"User\",\n                \"sender_name\": \"User\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": null,\n                    \"display_name\": null,\n                    \"source\": null\n                  },\n                  \"icon\": \"\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"c95bed34-f906-4aa6-84e4-68553f6db772\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"type\": \"object\"\n            }\n          },\n          \"logs\": { \"message\": [] },\n          \"message\": {\n            \"message\": \"Hello\",\n            \"sender\": \"User\",\n            \"sender_name\": \"User\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"artifacts\": {\n            \"message\": \"Hello\",\n            \"sender\": \"User\",\n            \"sender_name\": \"User\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"timedelta\": 0.015060124918818474,\n          \"duration\": \"15 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"message\": \"Hello\",\n          \"sender\": \"User\",\n          \"sender_name\": \"User\",\n          \"files\": [],\n          \"type\": \"object\"\n        },\n        \"params\": \"- Files: []\\n  Message: Hello\\n  Sender: User\\n  Sender Name: User\\n  Type: object\\n\",\n        \"valid\": true,\n        \"build_id\": \"40aa200e-74db-4651-b698-f80301d2b26b\",\n        \"id\": \"ChatInput-NCmix\",\n        \"timestamp\": \"2024-12-23T19:10:58.772766Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"Prompt-BEn9c\": [\n      {\n        \"data\": {\n          \"results\": {},\n          \"outputs\": {\n            \"prompt\": {\n              \"message\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"logs\": { \"prompt\": [] },\n          \"message\": {\n            \"prompt\": {\n              \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"artifacts\": {\n            \"prompt\": {\n              \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"timedelta\": 0.0057758750626817346,\n          \"duration\": \"6 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"prompt\": {\n            \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n            \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n            \"type\": \"text\"\n          }\n        },\n        \"params\": \"None\",\n        \"valid\": true,\n        \"build_id\": \"39bbbfde-97fd-42a5-a9ed-d42a5c5d532b\",\n        \"id\": \"Prompt-BEn9c\",\n        \"timestamp\": \"2024-12-23T19:10:58.781019Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"OpenAIModel-7AjrN\": [\n      {\n        \"data\": {\n          \"results\": {},\n          \"outputs\": {\n            \"text_output\": {\n              \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            },\n            \"model_output\": { \"message\": \"\", \"type\": \"unknown\" }\n          },\n          \"logs\": { \"text_output\": [] },\n          \"message\": {\n            \"text_output\": {\n              \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            }\n          },\n          \"artifacts\": {\n            \"text_output\": {\n              \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            }\n          },\n          \"timedelta\": 1.034765167045407,\n          \"duration\": \"1.03 seconds\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"text_output\": {\n            \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"type\": \"text\"\n          }\n        },\n        \"params\": \"None\",\n        \"valid\": true,\n        \"build_id\": \"4f0ae730-a266-4d35-b89f-7b825c620a0f\",\n        \"id\": \"OpenAIModel-7AjrN\",\n        \"timestamp\": \"2024-12-23T19:10:58.790484Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"ChatOutput-sfUhT\": [\n      {\n        \"data\": {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2024-12-23 19:10:58\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n                \"files\": [],\n                \"error\": \"False\",\n                \"edit\": \"False\",\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": \"False\",\n                  \"source\": {\n                    \"id\": \"OpenAIModel-7AjrN\",\n                    \"display_name\": \"OpenAI\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"OpenAI\",\n                  \"allow_markdown\": \"False\",\n                  \"positive_feedback\": \"None\",\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"5688356d-9f30-40ca-9907-79a7a2fc16fd\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"files\": [],\n              \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"timestamp\": \"2024-12-23 19:10:58+00:00\",\n              \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"error\": \"False\",\n              \"edit\": \"False\",\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": \"False\",\n                \"source\": {\n                  \"id\": \"OpenAIModel-7AjrN\",\n                  \"display_name\": \"OpenAI\",\n                  \"source\": \"gpt-4o-mini\"\n                },\n                \"icon\": \"OpenAI\",\n                \"allow_markdown\": \"False\",\n                \"positive_feedback\": \"None\",\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": []\n            }\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": {\n                \"timestamp\": \"2024-12-23T19:10:58\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": \"OpenAIModel-7AjrN\",\n                    \"display_name\": \"OpenAI\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"OpenAI\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"5688356d-9f30-40ca-9907-79a7a2fc16fd\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"type\": \"object\"\n            }\n          },\n          \"logs\": { \"message\": [] },\n          \"message\": {\n            \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"artifacts\": {\n            \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"timedelta\": 0.017838125000707805,\n          \"duration\": \"18 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n          \"sender\": \"Machine\",\n          \"sender_name\": \"AI\",\n          \"files\": [],\n          \"type\": \"object\"\n        },\n        \"params\": \"- Files: []\\n  Message: Hello!  I'm excited to help you get started on your journey to building\\n    something fresh! What do you have in mind? Whether it's a project, an idea, or\\n    a concept, let's dive in and make it happen!\\n  Sender: Machine\\n  Sender Name: AI\\n  Type: object\\n\",\n        \"valid\": true,\n        \"build_id\": \"1e8b908b-aba7-403b-9e9b-eca92bb78668\",\n        \"id\": \"ChatOutput-sfUhT\",\n        \"timestamp\": \"2024-12-23T19:10:58.813268Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ]\n  }\n}\n```\n\n</details>\n\n### Delete Vertex builds\n\nDelete Vertex builds for a specific flow.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n## Messages endpoints\n\nThe `/monitor/messages` endpoints store, retrieve, edit, and delete records in the message table in [`langflow.db`](/memory)\nTypically, these are called implicitly when running flows that produce message history, or when inspecting and modifying **Playground** memories.\n\n### Get messages\n\nRetrieve a list of all messages:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\nTo filter messages, use the `flow_id`, `session_id`, `sender`, and `sender_name` query parameters.\n\nTo sort the results, use the `order_by` query parameter.\n\nThis example retrieves messages sent by `Machine` and `AI` in a given chat session (`session_id`) and orders the messages by timestamp.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages?flow_id=$FLOW_ID&session_id=01ce083d-748b-4b8d-97b6-33adbb6a528a&sender=Machine&sender_name=AI&order_by=timestamp\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"1c1d6134-9b8b-4079-931c-84dcaddf19ba\",\n    \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"timestamp\": \"2024-12-23 19:20:11 UTC\",\n    \"sender\": \"Machine\",\n    \"sender_name\": \"AI\",\n    \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"text\": \"Hello! It's great to see you here! What exciting project or idea are you thinking about diving into today? Whether it's something fresh and innovative or a classic concept with a twist, I'm here to help you get started! Let's brainstorm together!\",\n    \"files\": \"[]\",\n    \"edit\": false,\n    \"properties\": {\n      \"text_color\": \"\",\n      \"background_color\": \"\",\n      \"edited\": false,\n      \"source\": {\n        \"id\": \"OpenAIModel-7AjrN\",\n        \"display_name\": \"OpenAI\",\n        \"source\": \"gpt-4o-mini\"\n      },\n      \"icon\": \"OpenAI\",\n      \"allow_markdown\": false,\n      \"positive_feedback\": null,\n      \"state\": \"complete\",\n      \"targets\": []\n    },\n    \"category\": \"message\",\n    \"content_blocks\": []\n  }\n]\n```\n\n</details>\n\n### Delete messages\n\nDelete specific messages by their IDs.\n\nThis example deletes the message retrieved in the previous `GET /messages` example.\n\n```bash\ncurl -v -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages\" \\\n  -H \"accept: */*\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '[\"MESSAGE_ID_1\", \"MESSAGE_ID_2\"]'\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n### Update message\n\nUpdate a specific message by its ID.\n\nThis example updates the `text` value of message `3ab66cc6-c048-48f8-ab07-570f5af7b160`.\n\n```bash\ncurl -X PUT \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/3ab66cc6-c048-48f8-ab07-570f5af7b160\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"text\": \"testing 1234\"\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"timestamp\": \"2024-12-23T18:49:06\",\n  \"sender\": \"string\",\n  \"sender_name\": \"string\",\n  \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n  \"text\": \"testing 1234\",\n  \"files\": [\"string\"],\n  \"error\": true,\n  \"edit\": true,\n  \"properties\": {\n    \"text_color\": \"string\",\n    \"background_color\": \"string\",\n    \"edited\": false,\n    \"source\": { \"id\": \"string\", \"display_name\": \"string\", \"source\": \"string\" },\n    \"icon\": \"string\",\n    \"allow_markdown\": false,\n    \"positive_feedback\": true,\n    \"state\": \"complete\",\n    \"targets\": []\n  },\n  \"category\": \"message\",\n  \"content_blocks\": [],\n  \"id\": \"3ab66cc6-c048-48f8-ab07-570f5af7b160\",\n  \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n}\n```\n\n</details>\n\n### Update session ID\n\nUpdate the session ID for messages.\n\nThis example updates the `session_ID` value `01ce083d-748b-4b8d-97b6-33adbb6a528a` to `different_session_id`.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/session/01ce083d-748b-4b8d-97b6-33adbb6a528a?new_session_id=different_session_id\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"8dd7f064-e63a-4773-b472-ca0475249dfd\",\n    \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"timestamp\": \"2024-12-23 18:49:55 UTC\",\n    \"sender\": \"User\",\n    \"sender_name\": \"User\",\n    \"session_id\": \"different_session_id\",\n    \"text\": \"message\",\n    \"files\": \"[]\",\n    \"edit\": false,\n    \"properties\": {\n      \"text_color\": \"\",\n      \"background_color\": \"\",\n      \"edited\": false,\n      \"source\": {\n        \"id\": null,\n        \"display_name\": null,\n        \"source\": null\n      },\n      \"icon\": \"\",\n      \"allow_markdown\": false,\n      \"positive_feedback\": null,\n      \"state\": \"complete\",\n      \"targets\": []\n    },\n    \"category\": \"message\",\n    \"content_blocks\": []\n  }\n]\n```\n\n</details>\n\n### Delete messages by session\n\nDelete all messages for a specific session.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/session/different_session_id_2\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nHTTP/1.1 204 No Content\n```\n\n</details>\n\n## Get transactions\n\nRetrieve all transactions, which are interactions between components, for a specific flow.\nThis information is also available in [flow logs](/logging).\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/transactions?flow_id=$FLOW_ID&page=1&size=50\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"items\": [\n    {\n      \"timestamp\": \"2024-12-23T20:05:01.061Z\",\n      \"vertex_id\": \"string\",\n      \"target_id\": \"string\",\n      \"inputs\": {},\n      \"outputs\": {},\n      \"status\": \"string\",\n      \"error\": \"string\",\n      \"flow_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    }\n  ],\n  \"total\": 0,\n  \"page\": 1,\n  \"size\": 1,\n  \"pages\": 0\n}\n```\n\n</details>\n\n## See also\n\n- [Use voice mode](/concepts-voice-mode)\n- [Session ID](/session-id)",
      "summary": "The `/monitor` endpoints are for internal Langflow functionality, primarily related to running flows in the **Playground**, storing chat history, and generating flow logs.",
      "keywords": [
        "builds",
        "character",
        "delete",
        "endpoints",
        "get",
        "message",
        "messages",
        "monitor",
        "playground",
        "recursive",
        "session",
        "splitter",
        "text",
        "transactions",
        "update",
        "vertex"
      ]
    },
    "api-monitor": {
      "title": "Monitor endpoints",
      "slug": "/api-monitor",
      "category": "api_reference",
      "content": "The `/monitor` endpoints are for internal Langflow functionality, primarily related to running flows in the **Playground**, storing chat history, and generating flow logs.\n\nThis information is primarily for those who are building custom components or contributing to the Langflow codebase in a way that requires calling or understanding these endpoints.\n\nFor typical application development with Langflow, there are more appropriate options for monitoring, debugging, and memory management.\nFor more information, see the following:\n\n* [Logs](/logging): Langflow log storage locations, customization options, and where to view logs in the visual editor\n* [Test flows in the Playground](/concepts-playground): Run flows and inspect message history\n* [Memory management options](/memory): Langflow storage locations and options, including the database, cache, and chat history\n\n## Vertex builds\n\nThe Vertex build endpoints (`/monitor/builds`) are exclusively for **Playground** functionality.\n\nWhen you run a flow in the **Playground**, Langflow calls the `/build/$FLOW_ID/flow` endpoint in [chat.py](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/api/v1/chat.py#L143). This call retrieves the flow data, builds a graph, and executes the graph. As each component (or node) is executed, the `build_vertex` function calls `build_and_run`, which may call the individual components' `def_build` method, if it exists. If a component doesn't have a `def_build` function, the build still returns a component.\n\nThe `build` function allows components to execute logic at runtime. For example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the `LCTextSplitterComponent` class. When text needs to be processed, the parent class's `build` method is called, which creates a `RecursiveCharacterTextSplitter` object and uses it to split the text according to the defined parameters. The split text is then passed on to the next component. This all occurs when the component is built.\n\n### Get Vertex builds\n\nRetrieve Vertex builds for a specific flow.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"vertex_builds\": {\n    \"ChatInput-NCmix\": [\n      {\n        \"data\": {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2024-12-23 19:10:57\",\n                \"sender\": \"User\",\n                \"sender_name\": \"User\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello\",\n                \"files\": [],\n                \"error\": \"False\",\n                \"edit\": \"False\",\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": \"False\",\n                  \"source\": {\n                    \"id\": \"None\",\n                    \"display_name\": \"None\",\n                    \"source\": \"None\"\n                  },\n                  \"icon\": \"\",\n                  \"allow_markdown\": \"False\",\n                  \"positive_feedback\": \"None\",\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"c95bed34-f906-4aa6-84e4-68553f6db772\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello\",\n              \"sender\": \"User\",\n              \"sender_name\": \"User\",\n              \"files\": [],\n              \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"timestamp\": \"2024-12-23 19:10:57+00:00\",\n              \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"error\": \"False\",\n              \"edit\": \"False\",\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": \"False\",\n                \"source\": {\n                  \"id\": \"None\",\n                  \"display_name\": \"None\",\n                  \"source\": \"None\"\n                },\n                \"icon\": \"\",\n                \"allow_markdown\": \"False\",\n                \"positive_feedback\": \"None\",\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": []\n            }\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": {\n                \"timestamp\": \"2024-12-23T19:10:57\",\n                \"sender\": \"User\",\n                \"sender_name\": \"User\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": null,\n                    \"display_name\": null,\n                    \"source\": null\n                  },\n                  \"icon\": \"\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"c95bed34-f906-4aa6-84e4-68553f6db772\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"type\": \"object\"\n            }\n          },\n          \"logs\": { \"message\": [] },\n          \"message\": {\n            \"message\": \"Hello\",\n            \"sender\": \"User\",\n            \"sender_name\": \"User\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"artifacts\": {\n            \"message\": \"Hello\",\n            \"sender\": \"User\",\n            \"sender_name\": \"User\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"timedelta\": 0.015060124918818474,\n          \"duration\": \"15 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"message\": \"Hello\",\n          \"sender\": \"User\",\n          \"sender_name\": \"User\",\n          \"files\": [],\n          \"type\": \"object\"\n        },\n        \"params\": \"- Files: []\\n  Message: Hello\\n  Sender: User\\n  Sender Name: User\\n  Type: object\\n\",\n        \"valid\": true,\n        \"build_id\": \"40aa200e-74db-4651-b698-f80301d2b26b\",\n        \"id\": \"ChatInput-NCmix\",\n        \"timestamp\": \"2024-12-23T19:10:58.772766Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"Prompt-BEn9c\": [\n      {\n        \"data\": {\n          \"results\": {},\n          \"outputs\": {\n            \"prompt\": {\n              \"message\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"logs\": { \"prompt\": [] },\n          \"message\": {\n            \"prompt\": {\n              \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"artifacts\": {\n            \"prompt\": {\n              \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n              \"type\": \"text\"\n            }\n          },\n          \"timedelta\": 0.0057758750626817346,\n          \"duration\": \"6 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"prompt\": {\n            \"repr\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n            \"raw\": \"Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.\",\n            \"type\": \"text\"\n          }\n        },\n        \"params\": \"None\",\n        \"valid\": true,\n        \"build_id\": \"39bbbfde-97fd-42a5-a9ed-d42a5c5d532b\",\n        \"id\": \"Prompt-BEn9c\",\n        \"timestamp\": \"2024-12-23T19:10:58.781019Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"OpenAIModel-7AjrN\": [\n      {\n        \"data\": {\n          \"results\": {},\n          \"outputs\": {\n            \"text_output\": {\n              \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            },\n            \"model_output\": { \"message\": \"\", \"type\": \"unknown\" }\n          },\n          \"logs\": { \"text_output\": [] },\n          \"message\": {\n            \"text_output\": {\n              \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            }\n          },\n          \"artifacts\": {\n            \"text_output\": {\n              \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"type\": \"text\"\n            }\n          },\n          \"timedelta\": 1.034765167045407,\n          \"duration\": \"1.03 seconds\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"text_output\": {\n            \"repr\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"raw\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"type\": \"text\"\n          }\n        },\n        \"params\": \"None\",\n        \"valid\": true,\n        \"build_id\": \"4f0ae730-a266-4d35-b89f-7b825c620a0f\",\n        \"id\": \"OpenAIModel-7AjrN\",\n        \"timestamp\": \"2024-12-23T19:10:58.790484Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ],\n    \"ChatOutput-sfUhT\": [\n      {\n        \"data\": {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2024-12-23 19:10:58\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n                \"files\": [],\n                \"error\": \"False\",\n                \"edit\": \"False\",\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": \"False\",\n                  \"source\": {\n                    \"id\": \"OpenAIModel-7AjrN\",\n                    \"display_name\": \"OpenAI\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"OpenAI\",\n                  \"allow_markdown\": \"False\",\n                  \"positive_feedback\": \"None\",\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"5688356d-9f30-40ca-9907-79a7a2fc16fd\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"files\": [],\n              \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"timestamp\": \"2024-12-23 19:10:58+00:00\",\n              \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n              \"error\": \"False\",\n              \"edit\": \"False\",\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": \"False\",\n                \"source\": {\n                  \"id\": \"OpenAIModel-7AjrN\",\n                  \"display_name\": \"OpenAI\",\n                  \"source\": \"gpt-4o-mini\"\n                },\n                \"icon\": \"OpenAI\",\n                \"allow_markdown\": \"False\",\n                \"positive_feedback\": \"None\",\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": []\n            }\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": {\n                \"timestamp\": \"2024-12-23T19:10:58\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n                \"text\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": \"OpenAIModel-7AjrN\",\n                    \"display_name\": \"OpenAI\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"OpenAI\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [],\n                \"id\": \"5688356d-9f30-40ca-9907-79a7a2fc16fd\",\n                \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n              },\n              \"type\": \"object\"\n            }\n          },\n          \"logs\": { \"message\": [] },\n          \"message\": {\n            \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"artifacts\": {\n            \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"timedelta\": 0.017838125000707805,\n          \"duration\": \"18 ms\",\n          \"used_frozen_result\": false\n        },\n        \"artifacts\": {\n          \"message\": \"Hello!  I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!\",\n          \"sender\": \"Machine\",\n          \"sender_name\": \"AI\",\n          \"files\": [],\n          \"type\": \"object\"\n        },\n        \"params\": \"- Files: []\\n  Message: Hello!  I'm excited to help you get started on your journey to building\\n    something fresh! What do you have in mind? Whether it's a project, an idea, or\\n    a concept, let's dive in and make it happen!\\n  Sender: Machine\\n  Sender Name: AI\\n  Type: object\\n\",\n        \"valid\": true,\n        \"build_id\": \"1e8b908b-aba7-403b-9e9b-eca92bb78668\",\n        \"id\": \"ChatOutput-sfUhT\",\n        \"timestamp\": \"2024-12-23T19:10:58.813268Z\",\n        \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n      }\n    ]\n  }\n}\n```\n\n</details>\n\n### Delete Vertex builds\n\nDelete Vertex builds for a specific flow.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n## Messages endpoints\n\nThe `/monitor/messages` endpoints store, retrieve, edit, and delete records in the message table in [`langflow.db`](/memory)\nTypically, these are called implicitly when running flows that produce message history, or when inspecting and modifying **Playground** memories.\n\n### Get messages\n\nRetrieve a list of all messages:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\nTo filter messages, use the `flow_id`, `session_id`, `sender`, and `sender_name` query parameters.\n\nTo sort the results, use the `order_by` query parameter.\n\nThis example retrieves messages sent by `Machine` and `AI` in a given chat session (`session_id`) and orders the messages by timestamp.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages?flow_id=$FLOW_ID&session_id=01ce083d-748b-4b8d-97b6-33adbb6a528a&sender=Machine&sender_name=AI&order_by=timestamp\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"1c1d6134-9b8b-4079-931c-84dcaddf19ba\",\n    \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"timestamp\": \"2024-12-23 19:20:11 UTC\",\n    \"sender\": \"Machine\",\n    \"sender_name\": \"AI\",\n    \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"text\": \"Hello! It's great to see you here! What exciting project or idea are you thinking about diving into today? Whether it's something fresh and innovative or a classic concept with a twist, I'm here to help you get started! Let's brainstorm together!\",\n    \"files\": \"[]\",\n    \"edit\": false,\n    \"properties\": {\n      \"text_color\": \"\",\n      \"background_color\": \"\",\n      \"edited\": false,\n      \"source\": {\n        \"id\": \"OpenAIModel-7AjrN\",\n        \"display_name\": \"OpenAI\",\n        \"source\": \"gpt-4o-mini\"\n      },\n      \"icon\": \"OpenAI\",\n      \"allow_markdown\": false,\n      \"positive_feedback\": null,\n      \"state\": \"complete\",\n      \"targets\": []\n    },\n    \"category\": \"message\",\n    \"content_blocks\": []\n  }\n]\n```\n\n</details>\n\n### Delete messages\n\nDelete specific messages by their IDs.\n\nThis example deletes the message retrieved in the previous `GET /messages` example.\n\n```bash\ncurl -v -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages\" \\\n  -H \"accept: */*\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '[\"MESSAGE_ID_1\", \"MESSAGE_ID_2\"]'\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n### Update message\n\nUpdate a specific message by its ID.\n\nThis example updates the `text` value of message `3ab66cc6-c048-48f8-ab07-570f5af7b160`.\n\n```bash\ncurl -X PUT \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/3ab66cc6-c048-48f8-ab07-570f5af7b160\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"text\": \"testing 1234\"\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"timestamp\": \"2024-12-23T18:49:06\",\n  \"sender\": \"string\",\n  \"sender_name\": \"string\",\n  \"session_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n  \"text\": \"testing 1234\",\n  \"files\": [\"string\"],\n  \"error\": true,\n  \"edit\": true,\n  \"properties\": {\n    \"text_color\": \"string\",\n    \"background_color\": \"string\",\n    \"edited\": false,\n    \"source\": { \"id\": \"string\", \"display_name\": \"string\", \"source\": \"string\" },\n    \"icon\": \"string\",\n    \"allow_markdown\": false,\n    \"positive_feedback\": true,\n    \"state\": \"complete\",\n    \"targets\": []\n  },\n  \"category\": \"message\",\n  \"content_blocks\": [],\n  \"id\": \"3ab66cc6-c048-48f8-ab07-570f5af7b160\",\n  \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\"\n}\n```\n\n</details>\n\n### Update session ID\n\nUpdate the session ID for messages.\n\nThis example updates the `session_ID` value `01ce083d-748b-4b8d-97b6-33adbb6a528a` to `different_session_id`.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/session/01ce083d-748b-4b8d-97b6-33adbb6a528a?new_session_id=different_session_id\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"id\": \"8dd7f064-e63a-4773-b472-ca0475249dfd\",\n    \"flow_id\": \"01ce083d-748b-4b8d-97b6-33adbb6a528a\",\n    \"timestamp\": \"2024-12-23 18:49:55 UTC\",\n    \"sender\": \"User\",\n    \"sender_name\": \"User\",\n    \"session_id\": \"different_session_id\",\n    \"text\": \"message\",\n    \"files\": \"[]\",\n    \"edit\": false,\n    \"properties\": {\n      \"text_color\": \"\",\n      \"background_color\": \"\",\n      \"edited\": false,\n      \"source\": {\n        \"id\": null,\n        \"display_name\": null,\n        \"source\": null\n      },\n      \"icon\": \"\",\n      \"allow_markdown\": false,\n      \"positive_feedback\": null,\n      \"state\": \"complete\",\n      \"targets\": []\n    },\n    \"category\": \"message\",\n    \"content_blocks\": []\n  }\n]\n```\n\n</details>\n\n### Delete messages by session\n\nDelete all messages for a specific session.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/monitor/messages/session/different_session_id_2\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\nHTTP/1.1 204 No Content\n```\n\n</details>\n\n## Get transactions\n\nRetrieve all transactions, which are interactions between components, for a specific flow.\nThis information is also available in [flow logs](/logging).\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/monitor/transactions?flow_id=$FLOW_ID&page=1&size=50\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"items\": [\n    {\n      \"timestamp\": \"2024-12-23T20:05:01.061Z\",\n      \"vertex_id\": \"string\",\n      \"target_id\": \"string\",\n      \"inputs\": {},\n      \"outputs\": {},\n      \"status\": \"string\",\n      \"error\": \"string\",\n      \"flow_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n      \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n    }\n  ],\n  \"total\": 0,\n  \"page\": 1,\n  \"size\": 1,\n  \"pages\": 0\n}\n```\n\n</details>\n\n## See also\n\n- [Use voice mode](/concepts-voice-mode)\n- [Session ID](/session-id)",
      "summary": "The `/monitor` endpoints are for internal Langflow functionality, primarily related to running flows in the **Playground**, storing chat history, and generating flow logs.",
      "keywords": [
        "builds",
        "character",
        "delete",
        "endpoints",
        "get",
        "message",
        "messages",
        "monitor",
        "playground",
        "recursive",
        "session",
        "splitter",
        "text",
        "transactions",
        "update",
        "vertex"
      ]
    },
    "/api-openai-responses": {
      "title": "OpenAI Responses API",
      "slug": "/api-openai-responses",
      "category": "api_reference",
      "content": "Langflow includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\nIt is available at `POST /api/v1/responses`.\n\nThis endpoint allows you to use existing OpenAI client libraries with minimal code changes.\nYou only need to replace the `model` name, such as `gpt-4`, with your `flow_id`.\nYou can find Flow IDs in the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.\n\n## Prerequisites\n\nTo be compatible with Langflow's OpenAI Responses API endpoint, your flow and request must adhere to the following requirements:\n\n- **Chat Input**: Your flow must contain a **Chat Input** component.\nFlows without this component return an error when passed to this endpoint.\nThe component types `ChatInput` and `Chat Input` are recognized as chat inputs.\n- **Tools**: The `tools` parameter isn't supported, and returns an error if provided.\n- **Model Names**: In your request, the `model` field must contain a valid flow ID or endpoint name.\n- **Authentication**: All requests require an API key passed in the `x-api-key` header.\nFor more information, see [API keys and authentication](/api-keys-and-authentication).\n\n### Additional configuration for OpenAI client libraries\n\nThis endpoint is compatible with OpenAI's API, but requires special configuration when using OpenAI client libraries.\nLangflow uses `x-api-key` headers for authentication, while OpenAI uses `Authorization: Bearer` headers.\nWhen sending requests to Langflow with OpenAI client libraries, you must configure custom headers and include an `api_key` configuration.\nThe `api_key` parameter can have any value, such as `\"dummy-api-key\"` in the client examples, as the actual authentication is handled through the `default_headers` configuration.\n\nIn the following examples, replace the values for `LANGFLOW_SERVER_URL`, `LANGFLOW_API_KEY`, and `FLOW_ID` with values from your deployment.\n\n</Tabs>\n\n<details closed>\n<summary>Response</summary>\n```text\nHere are the event dates for the second Wednesday of each month in 2026:\n- January 14, 2026\n- February 11, 2026\n- March 11, 2026\n- April 8, 2026\n- May 13, 2026\n- June 10, 2026\n- July 8, 2026\n- August 12, 2026\n- September 9, 2026\n- October 14, 2026\n- November 11, 2026\n- December 9, 2026\nIf you need these in a different format or want a downloadable calendar, let me know!\n```\n</details>\n\n## Example request\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$YOUR_FLOW_ID\",\n    \"input\": \"Hello, how are you?\",\n    \"stream\": false\n  }'\n```\n\n### Headers\n\n| Header | Required | Description | Example |\n|--------|----------|-------------|---------|\n| `x-api-key` | Yes | Your Langflow API key for authentication | `\"sk-...\"` |\n| `Content-Type` | Yes | Specifies the JSON format | `\"application/json\"` |\n| `X-LANGFLOW-GLOBAL-VAR-*` | No | Global variables for the flow | `\"X-LANGFLOW-GLOBAL-VAR-API_KEY: sk-...\"` For more, see [Pass global variables to your flows in headers](#global-var). |\n\n### Request body\n\n| Field | Type | Required | Default | Description |\n|-------|------|----------|---------|-------------|\n| `model` | `string` | Yes | - | The flow ID or endpoint name to execute. |\n| `input` | `string` | Yes | - | The input text to process. |\n| `stream` | `boolean` | No | `false` | Whether to stream the response. |\n| `background` | `boolean` | No | `false` | Whether to process in background. |\n| `tools` | `list[Any]` | No | `null` | Tools are not supported yet. |\n| `previous_response_id` | `string` | No | `null` | ID of previous response to continue conversation. For more, see [Continue conversations with response and session IDs](#response-id). |\n| `include` | `list[string]` | No | `null` | Additional response data to include, such as `['tool_call.results']`. For more, see [Retrieve tool call results](#tool-call-results). |\n\n## Example response\n\n```json\n{\n  \"id\": \"e5e8ef8a-7efd-4090-a110-6aca082bceb7\",\n  \"object\": \"response\",\n  \"created_at\": 1756837941,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_e5e8ef8a-7efd-4090-a110-6aca082bceb7\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello! I'm here and ready to help. How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"parallel_tool_calls\": true,\n  \"previous_response_id\": null,\n  \"reasoning\": {\"effort\": null, \"summary\": null},\n  \"store\": true,\n  \"temperature\": 1.0,\n  \"text\": {\"format\": {\"type\": \"text\"}},\n  \"tool_choice\": \"auto\",\n  \"tools\": [],\n  \"top_p\": 1.0,\n  \"truncation\": \"disabled\",\n  \"usage\": null,\n  \"user\": null,\n  \"metadata\": {}\n}\n```\n\n### Response body\n\nThe response contains fields that Langflow sets dynamically and fields that use OpenAI-compatible defaults.\n\nThe OpenAI-compatible default values shown above are currently fixed and cannot be modified via the request.\nThey are included to maintain API compatibility and provide a consistent response format.\n\nFor your requests, you will only be setting the dynamic fields.\nThe default values are documented here for completeness and to show the full response structure.\n\nFields set dynamically by Langflow:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | `string` | Unique response identifier. |\n| `created_at` | `int` | Unix timestamp of response creation. |\n| `model` | `string` | The flow ID that was executed. |\n| `output` | `list[dict]` | Array of output items (messages, tool calls, etc.). |\n| `previous_response_id` | `string` | ID of previous response if continuing conversation. |\n\n<details>\n<summary>Fields with OpenAI-compatible default values</summary>\n\n| Field | Type | Default Value | Description |\n|-------|------|---------------|-------------|\n| `object` | `string` | `\"response\"` | Always `\"response\"`. |\n| `status` | `string` | `\"completed\"` | Response status: `\"completed\"`, `\"in_progress\"`, or `\"failed\"`. |\n| `error` | `dict` | `null` | Error details (if any). |\n| `incomplete_details` | `dict` | `null` | Incomplete response details (if any). |\n| `instructions` | `string` | `null` | Response instructions (if any). |\n| `max_output_tokens` | `int` | `null` | Maximum output tokens (if any). |\n| `parallel_tool_calls` | `boolean` | `true` | Whether parallel tool calls are enabled. |\n| `reasoning` | `dict` | `{\"effort\": null, \"summary\": null}` | Reasoning information with effort and summary. |\n| `store` | `boolean` | `true` | Whether response is stored. |\n| `temperature` | `float` | `1.0` | Temperature setting. |\n| `text` | `dict` | `{\"format\": {\"type\": \"text\"}}` | Text format configuration. |\n| `tool_choice` | `string` | `\"auto\"` | Tool choice setting. |\n| `tools` | `list[dict]` | `[]` | Available tools. |\n| `top_p` | `float` | `1.0` | Top-p setting. |\n| `truncation` | `string` | `\"disabled\"` | Truncation setting. |\n| `usage` | `dict` | `null` | Usage statistics (if any). |\n| `user` | `string` | `null` | User identifier (if any). |\n| `metadata` | `dict` | `{}` | Additional metadata. |\n\n</details>\n\n## Example streaming request\n\nWhen you set `\"stream\": true` with your request, the API returns a stream where each chunk contains a small piece of the response as it's generated. This provides a real-time experience where users can see the AI's output appear word by word, similar to ChatGPT's typing effect.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$FLOW_ID\",\n    \"input\": \"Tell me a story about a robot\",\n    \"stream\": true\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"f7fcea36-f128-41c4-9ac1-e683137375d5\",\n  \"object\": \"response.chunk\",\n  \"created\": 1756838094,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"delta\": {\n    \"content\": \"Once\"\n  },\n  \"status\": null\n}\n```\n\n</details>\n\n### Streaming response body\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | `string` | Unique response identifier. |\n| `object` | `string` | Always `\"response.chunk\"`. |\n| `created` | `int` | Unix timestamp of chunk creation. |\n| `model` | `string` | The flow ID that was executed. |\n| `delta` | `dict` | The new content chunk. |\n| `status` | `string` | Response status: `\"completed\"`, `\"in_progress\"`, or `\"failed\"` (optional). |\n\nThe stream continues until a final chunk with `\"status\": \"completed\"` indicates the response is finished.\n\n<details>\n<summary>Final completion chunk</summary>\n\n```\n{\n  \"id\": \"f7fcea36-f128-41c4-9ac1-e683137375d5\",\n  \"object\": \"response.chunk\",\n  \"created\": 1756838094,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"delta\": {},\n  \"status\": \"completed\"\n}\n```\n</details>\n\n## Continue conversations with response and session IDs {#response-id}\n\nConversation continuity allows you to maintain context across multiple API calls, enabling multi-turn conversations with your flows. This is essential for building chat applications where users can have ongoing conversations.\n\nWhen you make a request, the API returns a response with an `id` field. You can use this `id` as the `previous_response_id` in your next request to continue the conversation from where it left off.\n\nFirst Message:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$FLOW_ID\",\n    \"input\": \"Hello, my name is Alice\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\",\n  \"object\": \"response\",\n  \"created_at\": 1756839042,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_c45f4ac8-772b-4675-8551-c560b1afd590\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello, Alice! How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": null\n}\n```\n\n</details>\n\nFollow-up message:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n    \"input\": \"What's my name?\",\n    \"previous_response_id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\",\n  \"object\": \"response\",\n  \"created_at\": 1756839043,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_c45f4ac8-772b-4675-8551-c560b1afd590\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Your name is Alice. How can I help you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\"\n}\n```\n\n</details>\n\nOptionally, you can use your own session ID values for the `previous_response_id`:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n    \"input\": \"What's my name?\",\n    \"previous_response_id\": \"session-alice-1756839048\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\nThis example uses the same flow as the other `previous_response_id` examples, but the LLM had not yet been introduced to Alice in the specified session:\n\n```json\n{\n  \"id\": \"session-alice-1756839048\",\n  \"object\": \"response\",\n  \"created_at\": 1756839048,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_session-alice-1756839048\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"I don't have access to your name unless you tell me. If you'd like, you can share your name, and I'll remember it for this conversation!\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": \"session-alice-1756839048\"\n}\n```\n\n</details>\n\n## Retrieve tool call results {#tool-call-results}\n\nWhen you send a request to the `/api/v1/responses` endpoint to run a flow that includes tools or function calls, you can retrieve the raw tool execution details by adding `\"include\": [\"tool_call.results\"]` to the request payload.\n\nWithout the `include` parameter, tool calls return basic function call information, but not the raw tool results.\nFor example:\n\n```json\n{\n  \"id\": \"fc_1\",\n  \"type\": \"function_call\",\n  \"status\": \"completed\",\n  \"name\": \"evaluate_expression\",\n  \"arguments\": \"{\\\"expression\\\": \\\"15*23\\\"}\"\n},\n```\n\nTo get the raw `results` of each tool execution, add  `include: [\"tool_call.results\"]` to the request payload:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"model\": \"FLOW_ID\",\n    \"input\": \"Calculate 23 * 15 and show me the result\",\n    \"stream\": false,\n    \"include\": [\"tool_call.results\"]\n  }'\n```\n\nThe response now includes the tool call's results.\nFor example:\n\n```json\n{\n  \"id\": \"evaluate_expression_1\",\n  \"type\": \"tool_call\",\n  \"tool_name\": \"evaluate_expression\",\n  \"queries\": [\"15*23\"],\n  \"results\": {\"result\": \"345\"}\n}\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"a6e5511e-71f8-457a-88d2-7d8c6ea34e36\",\n  \"object\": \"response\",\n  \"created_at\": 1756835379,\n  \"status\": \"completed\",\n  \"error\": null,\n  \"incomplete_details\": null,\n  \"instructions\": null,\n  \"max_output_tokens\": null,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"id\": \"evaluate_expression_1\",\n      \"queries\": [\n        \"15*23\"\n      ],\n      \"status\": \"completed\",\n      \"tool_name\": \"evaluate_expression\",\n      \"type\": \"tool_call\",\n      \"results\": {\n        \"result\": \"345\"\n      }\n    },\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_a6e5511e-71f8-457a-88d2-7d8c6ea34e36\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"The result of 23 * 15 is 345.\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"parallel_tool_calls\": true,\n  \"previous_response_id\": null,\n  \"reasoning\": {\n    \"effort\": null,\n    \"summary\": null\n  },\n  \"store\": true,\n  \"temperature\": 1.0,\n  \"text\": {\n    \"format\": {\n      \"type\": \"text\"\n    }\n  },\n  \"tool_choice\": \"auto\",\n  \"tools\": [],\n  \"top_p\": 1.0,\n  \"truncation\": \"disabled\",\n  \"usage\": null,\n  \"user\": null,\n  \"metadata\": {}\n}\n```\n</details>\n\n## Pass global variables to your flows in headers {#global-var}\n\nGlobal variables allow you to pass dynamic values to your flows that can be used by components within that flow run.\nThis is useful for passing API keys, user IDs, or any other configuration that might change between requests.\n\nThe `/responses` endpoint accepts global variables as custom HTTP headers with the format `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}`.\nVariables are only available during this specific request execution and aren't persisted.\nVariable names are automatically converted to uppercase.\n\nThis example demonstrates passing an `OPENAI_API_KEY` variable, which is a variable Langflow automatically detects from environment variables, with two custom variables for `USER_ID` and `ENVIRONMENT`. The variables don't have to be created in Langflow's Global Variables section - you can pass any variable name in the `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` header format.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-OPENAI_API_KEY: sk-...\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-USER_ID: user123\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-ENVIRONMENT: production\" \\\n  -d '{\n    \"model\": \"your-flow-id\",\n    \"input\": \"Hello\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"4a4d2f24-bb45-4a55-a499-0191305264be\",\n  \"object\": \"response\",\n  \"created_at\": 1756839935,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_4a4d2f24-bb45-4a55-a499-0191305264be\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello! How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": null\n}\n```\n\n</details>\n\nVariables passed with `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` are always available to your flow, regardless of whether they exist in the database.\n\nIf your flow components reference variables that aren't provided in headers or your Langflow database, the flow fails by default.\nTo avoid this, you can set the `FALLBACK_TO_ENV_VARS` environment variable is `true`, which allows the flow to use values from the `.env` file if they aren't otherwise specified.\n\nIn the above example, `OPENAI_API_KEY` will fall back to the database variable if not provided in the header.\n`USER_ID` and `ENVIRONMENT` will fall back to environment variables if `FALLBACK_TO_ENV_VARS` is enabled.\nOtherwise, the flow fails.",
      "summary": "Langflow includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\nIt is available at `POST /api/v1/responses`.",
      "keywords": [
        "access",
        "additional",
        "api",
        "authentication",
        "body",
        "call",
        "chat",
        "client",
        "configuration",
        "continue",
        "conversations",
        "flows",
        "global",
        "headers",
        "ids",
        "input",
        "libraries",
        "model",
        "names",
        "openai"
      ]
    },
    "api-openai-responses": {
      "title": "OpenAI Responses API",
      "slug": "/api-openai-responses",
      "category": "api_reference",
      "content": "Langflow includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\nIt is available at `POST /api/v1/responses`.\n\nThis endpoint allows you to use existing OpenAI client libraries with minimal code changes.\nYou only need to replace the `model` name, such as `gpt-4`, with your `flow_id`.\nYou can find Flow IDs in the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.\n\n## Prerequisites\n\nTo be compatible with Langflow's OpenAI Responses API endpoint, your flow and request must adhere to the following requirements:\n\n- **Chat Input**: Your flow must contain a **Chat Input** component.\nFlows without this component return an error when passed to this endpoint.\nThe component types `ChatInput` and `Chat Input` are recognized as chat inputs.\n- **Tools**: The `tools` parameter isn't supported, and returns an error if provided.\n- **Model Names**: In your request, the `model` field must contain a valid flow ID or endpoint name.\n- **Authentication**: All requests require an API key passed in the `x-api-key` header.\nFor more information, see [API keys and authentication](/api-keys-and-authentication).\n\n### Additional configuration for OpenAI client libraries\n\nThis endpoint is compatible with OpenAI's API, but requires special configuration when using OpenAI client libraries.\nLangflow uses `x-api-key` headers for authentication, while OpenAI uses `Authorization: Bearer` headers.\nWhen sending requests to Langflow with OpenAI client libraries, you must configure custom headers and include an `api_key` configuration.\nThe `api_key` parameter can have any value, such as `\"dummy-api-key\"` in the client examples, as the actual authentication is handled through the `default_headers` configuration.\n\nIn the following examples, replace the values for `LANGFLOW_SERVER_URL`, `LANGFLOW_API_KEY`, and `FLOW_ID` with values from your deployment.\n\n</Tabs>\n\n<details closed>\n<summary>Response</summary>\n```text\nHere are the event dates for the second Wednesday of each month in 2026:\n- January 14, 2026\n- February 11, 2026\n- March 11, 2026\n- April 8, 2026\n- May 13, 2026\n- June 10, 2026\n- July 8, 2026\n- August 12, 2026\n- September 9, 2026\n- October 14, 2026\n- November 11, 2026\n- December 9, 2026\nIf you need these in a different format or want a downloadable calendar, let me know!\n```\n</details>\n\n## Example request\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$YOUR_FLOW_ID\",\n    \"input\": \"Hello, how are you?\",\n    \"stream\": false\n  }'\n```\n\n### Headers\n\n| Header | Required | Description | Example |\n|--------|----------|-------------|---------|\n| `x-api-key` | Yes | Your Langflow API key for authentication | `\"sk-...\"` |\n| `Content-Type` | Yes | Specifies the JSON format | `\"application/json\"` |\n| `X-LANGFLOW-GLOBAL-VAR-*` | No | Global variables for the flow | `\"X-LANGFLOW-GLOBAL-VAR-API_KEY: sk-...\"` For more, see [Pass global variables to your flows in headers](#global-var). |\n\n### Request body\n\n| Field | Type | Required | Default | Description |\n|-------|------|----------|---------|-------------|\n| `model` | `string` | Yes | - | The flow ID or endpoint name to execute. |\n| `input` | `string` | Yes | - | The input text to process. |\n| `stream` | `boolean` | No | `false` | Whether to stream the response. |\n| `background` | `boolean` | No | `false` | Whether to process in background. |\n| `tools` | `list[Any]` | No | `null` | Tools are not supported yet. |\n| `previous_response_id` | `string` | No | `null` | ID of previous response to continue conversation. For more, see [Continue conversations with response and session IDs](#response-id). |\n| `include` | `list[string]` | No | `null` | Additional response data to include, such as `['tool_call.results']`. For more, see [Retrieve tool call results](#tool-call-results). |\n\n## Example response\n\n```json\n{\n  \"id\": \"e5e8ef8a-7efd-4090-a110-6aca082bceb7\",\n  \"object\": \"response\",\n  \"created_at\": 1756837941,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_e5e8ef8a-7efd-4090-a110-6aca082bceb7\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello! I'm here and ready to help. How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"parallel_tool_calls\": true,\n  \"previous_response_id\": null,\n  \"reasoning\": {\"effort\": null, \"summary\": null},\n  \"store\": true,\n  \"temperature\": 1.0,\n  \"text\": {\"format\": {\"type\": \"text\"}},\n  \"tool_choice\": \"auto\",\n  \"tools\": [],\n  \"top_p\": 1.0,\n  \"truncation\": \"disabled\",\n  \"usage\": null,\n  \"user\": null,\n  \"metadata\": {}\n}\n```\n\n### Response body\n\nThe response contains fields that Langflow sets dynamically and fields that use OpenAI-compatible defaults.\n\nThe OpenAI-compatible default values shown above are currently fixed and cannot be modified via the request.\nThey are included to maintain API compatibility and provide a consistent response format.\n\nFor your requests, you will only be setting the dynamic fields.\nThe default values are documented here for completeness and to show the full response structure.\n\nFields set dynamically by Langflow:\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | `string` | Unique response identifier. |\n| `created_at` | `int` | Unix timestamp of response creation. |\n| `model` | `string` | The flow ID that was executed. |\n| `output` | `list[dict]` | Array of output items (messages, tool calls, etc.). |\n| `previous_response_id` | `string` | ID of previous response if continuing conversation. |\n\n<details>\n<summary>Fields with OpenAI-compatible default values</summary>\n\n| Field | Type | Default Value | Description |\n|-------|------|---------------|-------------|\n| `object` | `string` | `\"response\"` | Always `\"response\"`. |\n| `status` | `string` | `\"completed\"` | Response status: `\"completed\"`, `\"in_progress\"`, or `\"failed\"`. |\n| `error` | `dict` | `null` | Error details (if any). |\n| `incomplete_details` | `dict` | `null` | Incomplete response details (if any). |\n| `instructions` | `string` | `null` | Response instructions (if any). |\n| `max_output_tokens` | `int` | `null` | Maximum output tokens (if any). |\n| `parallel_tool_calls` | `boolean` | `true` | Whether parallel tool calls are enabled. |\n| `reasoning` | `dict` | `{\"effort\": null, \"summary\": null}` | Reasoning information with effort and summary. |\n| `store` | `boolean` | `true` | Whether response is stored. |\n| `temperature` | `float` | `1.0` | Temperature setting. |\n| `text` | `dict` | `{\"format\": {\"type\": \"text\"}}` | Text format configuration. |\n| `tool_choice` | `string` | `\"auto\"` | Tool choice setting. |\n| `tools` | `list[dict]` | `[]` | Available tools. |\n| `top_p` | `float` | `1.0` | Top-p setting. |\n| `truncation` | `string` | `\"disabled\"` | Truncation setting. |\n| `usage` | `dict` | `null` | Usage statistics (if any). |\n| `user` | `string` | `null` | User identifier (if any). |\n| `metadata` | `dict` | `{}` | Additional metadata. |\n\n</details>\n\n## Example streaming request\n\nWhen you set `\"stream\": true` with your request, the API returns a stream where each chunk contains a small piece of the response as it's generated. This provides a real-time experience where users can see the AI's output appear word by word, similar to ChatGPT's typing effect.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$FLOW_ID\",\n    \"input\": \"Tell me a story about a robot\",\n    \"stream\": true\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"f7fcea36-f128-41c4-9ac1-e683137375d5\",\n  \"object\": \"response.chunk\",\n  \"created\": 1756838094,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"delta\": {\n    \"content\": \"Once\"\n  },\n  \"status\": null\n}\n```\n\n</details>\n\n### Streaming response body\n\n| Field | Type | Description |\n|-------|------|-------------|\n| `id` | `string` | Unique response identifier. |\n| `object` | `string` | Always `\"response.chunk\"`. |\n| `created` | `int` | Unix timestamp of chunk creation. |\n| `model` | `string` | The flow ID that was executed. |\n| `delta` | `dict` | The new content chunk. |\n| `status` | `string` | Response status: `\"completed\"`, `\"in_progress\"`, or `\"failed\"` (optional). |\n\nThe stream continues until a final chunk with `\"status\": \"completed\"` indicates the response is finished.\n\n<details>\n<summary>Final completion chunk</summary>\n\n```\n{\n  \"id\": \"f7fcea36-f128-41c4-9ac1-e683137375d5\",\n  \"object\": \"response.chunk\",\n  \"created\": 1756838094,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"delta\": {},\n  \"status\": \"completed\"\n}\n```\n</details>\n\n## Continue conversations with response and session IDs {#response-id}\n\nConversation continuity allows you to maintain context across multiple API calls, enabling multi-turn conversations with your flows. This is essential for building chat applications where users can have ongoing conversations.\n\nWhen you make a request, the API returns a response with an `id` field. You can use this `id` as the `previous_response_id` in your next request to continue the conversation from where it left off.\n\nFirst Message:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"$FLOW_ID\",\n    \"input\": \"Hello, my name is Alice\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\",\n  \"object\": \"response\",\n  \"created_at\": 1756839042,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_c45f4ac8-772b-4675-8551-c560b1afd590\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello, Alice! How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": null\n}\n```\n\n</details>\n\nFollow-up message:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n    \"input\": \"What's my name?\",\n    \"previous_response_id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\",\n  \"object\": \"response\",\n  \"created_at\": 1756839043,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_c45f4ac8-772b-4675-8551-c560b1afd590\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Your name is Alice. How can I help you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": \"c45f4ac8-772b-4675-8551-c560b1afd590\"\n}\n```\n\n</details>\n\nOptionally, you can use your own session ID values for the `previous_response_id`:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n    \"input\": \"What's my name?\",\n    \"previous_response_id\": \"session-alice-1756839048\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\nThis example uses the same flow as the other `previous_response_id` examples, but the LLM had not yet been introduced to Alice in the specified session:\n\n```json\n{\n  \"id\": \"session-alice-1756839048\",\n  \"object\": \"response\",\n  \"created_at\": 1756839048,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_session-alice-1756839048\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"I don't have access to your name unless you tell me. If you'd like, you can share your name, and I'll remember it for this conversation!\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": \"session-alice-1756839048\"\n}\n```\n\n</details>\n\n## Retrieve tool call results {#tool-call-results}\n\nWhen you send a request to the `/api/v1/responses` endpoint to run a flow that includes tools or function calls, you can retrieve the raw tool execution details by adding `\"include\": [\"tool_call.results\"]` to the request payload.\n\nWithout the `include` parameter, tool calls return basic function call information, but not the raw tool results.\nFor example:\n\n```json\n{\n  \"id\": \"fc_1\",\n  \"type\": \"function_call\",\n  \"status\": \"completed\",\n  \"name\": \"evaluate_expression\",\n  \"arguments\": \"{\\\"expression\\\": \\\"15*23\\\"}\"\n},\n```\n\nTo get the raw `results` of each tool execution, add  `include: [\"tool_call.results\"]` to the request payload:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"model\": \"FLOW_ID\",\n    \"input\": \"Calculate 23 * 15 and show me the result\",\n    \"stream\": false,\n    \"include\": [\"tool_call.results\"]\n  }'\n```\n\nThe response now includes the tool call's results.\nFor example:\n\n```json\n{\n  \"id\": \"evaluate_expression_1\",\n  \"type\": \"tool_call\",\n  \"tool_name\": \"evaluate_expression\",\n  \"queries\": [\"15*23\"],\n  \"results\": {\"result\": \"345\"}\n}\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"a6e5511e-71f8-457a-88d2-7d8c6ea34e36\",\n  \"object\": \"response\",\n  \"created_at\": 1756835379,\n  \"status\": \"completed\",\n  \"error\": null,\n  \"incomplete_details\": null,\n  \"instructions\": null,\n  \"max_output_tokens\": null,\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"id\": \"evaluate_expression_1\",\n      \"queries\": [\n        \"15*23\"\n      ],\n      \"status\": \"completed\",\n      \"tool_name\": \"evaluate_expression\",\n      \"type\": \"tool_call\",\n      \"results\": {\n        \"result\": \"345\"\n      }\n    },\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_a6e5511e-71f8-457a-88d2-7d8c6ea34e36\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"The result of 23 * 15 is 345.\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"parallel_tool_calls\": true,\n  \"previous_response_id\": null,\n  \"reasoning\": {\n    \"effort\": null,\n    \"summary\": null\n  },\n  \"store\": true,\n  \"temperature\": 1.0,\n  \"text\": {\n    \"format\": {\n      \"type\": \"text\"\n    }\n  },\n  \"tool_choice\": \"auto\",\n  \"tools\": [],\n  \"top_p\": 1.0,\n  \"truncation\": \"disabled\",\n  \"usage\": null,\n  \"user\": null,\n  \"metadata\": {}\n}\n```\n</details>\n\n## Pass global variables to your flows in headers {#global-var}\n\nGlobal variables allow you to pass dynamic values to your flows that can be used by components within that flow run.\nThis is useful for passing API keys, user IDs, or any other configuration that might change between requests.\n\nThe `/responses` endpoint accepts global variables as custom HTTP headers with the format `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}`.\nVariables are only available during this specific request execution and aren't persisted.\nVariable names are automatically converted to uppercase.\n\nThis example demonstrates passing an `OPENAI_API_KEY` variable, which is a variable Langflow automatically detects from environment variables, with two custom variables for `USER_ID` and `ENVIRONMENT`. The variables don't have to be created in Langflow's Global Variables section - you can pass any variable name in the `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` header format.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_SERVER_URL/api/v1/responses\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-OPENAI_API_KEY: sk-...\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-USER_ID: user123\" \\\n  -H \"X-LANGFLOW-GLOBAL-VAR-ENVIRONMENT: production\" \\\n  -d '{\n    \"model\": \"your-flow-id\",\n    \"input\": \"Hello\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"4a4d2f24-bb45-4a55-a499-0191305264be\",\n  \"object\": \"response\",\n  \"created_at\": 1756839935,\n  \"status\": \"completed\",\n  \"model\": \"ced2ec91-f325-4bf0-8754-f3198c2b1563\",\n  \"output\": [\n    {\n      \"type\": \"message\",\n      \"id\": \"msg_4a4d2f24-bb45-4a55-a499-0191305264be\",\n      \"status\": \"completed\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"output_text\",\n          \"text\": \"Hello! How can I assist you today?\",\n          \"annotations\": []\n        }\n      ]\n    }\n  ],\n  \"previous_response_id\": null\n}\n```\n\n</details>\n\nVariables passed with `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` are always available to your flow, regardless of whether they exist in the database.\n\nIf your flow components reference variables that aren't provided in headers or your Langflow database, the flow fails by default.\nTo avoid this, you can set the `FALLBACK_TO_ENV_VARS` environment variable is `true`, which allows the flow to use values from the `.env` file if they aren't otherwise specified.\n\nIn the above example, `OPENAI_API_KEY` will fall back to the database variable if not provided in the header.\n`USER_ID` and `ENVIRONMENT` will fall back to environment variables if `FALLBACK_TO_ENV_VARS` is enabled.\nOtherwise, the flow fails.",
      "summary": "Langflow includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).\nIt is available at `POST /api/v1/responses`.",
      "keywords": [
        "access",
        "additional",
        "api",
        "authentication",
        "body",
        "call",
        "chat",
        "client",
        "configuration",
        "continue",
        "conversations",
        "flows",
        "global",
        "headers",
        "ids",
        "input",
        "libraries",
        "model",
        "names",
        "openai"
      ]
    },
    "/api-projects": {
      "title": "Projects endpoints",
      "slug": "/api-projects",
      "category": "api_reference",
      "content": "Use the `/projects` endpoint to create, read, update, and delete [Langflow projects](/concepts-flows#projects).\n\n## Read projects\n\nGet a list of Langflow projects, including project IDs, names, and descriptions.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"Starter Project\",\n    \"description\": \"Manage your own projects. Download and upload projects.\",\n    \"id\": \"1415de42-8f01-4f36-bf34-539f23e47466\",\n    \"parent_id\": null\n  }\n]\n```\n\n</details>\n\n## Create project\n\nCreate a new project.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"components_list\": [],\n  \"flows_list\": []\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"id\": \"b408ddb9-6266-4431-9be8-e04a62758331\",\n  \"parent_id\": null\n}\n```\n\n</details>\n\nTo add flows and components at project creation, retrieve the `components_list` and `flows_list` values from the [`/all`](/api-reference-api-examples#get-all-components) and [/flows/read](/api-flows#read-flows) endpoints and add them to the request body.\n\nAdding a flow to a project moves the flow from its previous location. The flow isn't copied.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"components_list\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ],\n  \"flows_list\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ]\n}'\n```\n\n## Read project\n\nRetrieve details of a specific project.\n\nTo find the UUID of your project, call the [read projects](#read-projects) endpoint.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/$PROJECT_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"Starter Project\",\n    \"description\": \"Manage your own projects. Download and upload projects.\",\n    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n    \"parent_id\": null\n  }\n]\n```\n\n</details>\n\n## Update project\n\nUpdate the information of a specific project with a `PATCH` request.\n\nEach PATCH request updates the project with the values you send.\nOnly the fields you include in your request are updated.\nIf you send the same values multiple times, the update is still processed, even if the values are unchanged.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/projects/b408ddb9-6266-4431-9be8-e04a62758331\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"parent_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n  \"components\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ],\n  \"flows\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ]\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"id\": \"b408ddb9-6266-4431-9be8-e04a62758331\",\n  \"parent_id\": null\n}\n```\n\n</details>\n\n## Delete project\n\nDelete a specific project.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/projects/$PROJECT_ID\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n## Export a project\n\nDownload all flows from a project as a zip file.\n\nThe `--output` flag is optional.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/download/$PROJECT_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output langflow-project.zip\n```\n\n## Import a project\n\nImport a project and its flows by uploading a Langflow project zip file:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/upload/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@20241230_135006_langflow_flows.zip;type=application/zip\"\n```",
      "summary": "Use the `/projects` endpoint to create, read, update, and delete [Langflow projects](/concepts-flows#projects).",
      "keywords": [
        "create",
        "delete",
        "endpoints",
        "export",
        "import",
        "project",
        "projects",
        "read",
        "update"
      ]
    },
    "api-projects": {
      "title": "Projects endpoints",
      "slug": "/api-projects",
      "category": "api_reference",
      "content": "Use the `/projects` endpoint to create, read, update, and delete [Langflow projects](/concepts-flows#projects).\n\n## Read projects\n\nGet a list of Langflow projects, including project IDs, names, and descriptions.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"Starter Project\",\n    \"description\": \"Manage your own projects. Download and upload projects.\",\n    \"id\": \"1415de42-8f01-4f36-bf34-539f23e47466\",\n    \"parent_id\": null\n  }\n]\n```\n\n</details>\n\n## Create project\n\nCreate a new project.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"components_list\": [],\n  \"flows_list\": []\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"id\": \"b408ddb9-6266-4431-9be8-e04a62758331\",\n  \"parent_id\": null\n}\n```\n\n</details>\n\nTo add flows and components at project creation, retrieve the `components_list` and `flows_list` values from the [`/all`](/api-reference-api-examples#get-all-components) and [/flows/read](/api-flows#read-flows) endpoints and add them to the request body.\n\nAdding a flow to a project moves the flow from its previous location. The flow isn't copied.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"new_project_name\",\n  \"description\": \"string\",\n  \"components_list\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ],\n  \"flows_list\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ]\n}'\n```\n\n## Read project\n\nRetrieve details of a specific project.\n\nTo find the UUID of your project, call the [read projects](#read-projects) endpoint.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/$PROJECT_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n[\n  {\n    \"name\": \"Starter Project\",\n    \"description\": \"Manage your own projects. Download and upload projects.\",\n    \"id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n    \"parent_id\": null\n  }\n]\n```\n\n</details>\n\n## Update project\n\nUpdate the information of a specific project with a `PATCH` request.\n\nEach PATCH request updates the project with the values you send.\nOnly the fields you include in your request are updated.\nIf you send the same values multiple times, the update is still processed, even if the values are unchanged.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/projects/b408ddb9-6266-4431-9be8-e04a62758331\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"parent_id\": \"3fa85f64-5717-4562-b3fc-2c963f66afa6\",\n  \"components\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ],\n  \"flows\": [\n    \"3fa85f64-5717-4562-b3fc-2c963f66afa6\"\n  ]\n}'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"name\": \"string\",\n  \"description\": \"string\",\n  \"id\": \"b408ddb9-6266-4431-9be8-e04a62758331\",\n  \"parent_id\": null\n}\n```\n\n</details>\n\n## Delete project\n\nDelete a specific project.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/projects/$PROJECT_ID\" \\\n  -H \"accept: */*\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n204 No Content\n```\n\n</details>\n\n## Export a project\n\nDownload all flows from a project as a zip file.\n\nThe `--output` flag is optional.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/projects/download/$PROJECT_ID\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --output langflow-project.zip\n```\n\n## Import a project\n\nImport a project and its flows by uploading a Langflow project zip file:\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/projects/upload/\" \\\n  -H \"accept: application/json\" \\\n  -H \"Content-Type: multipart/form-data\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -F \"file=@20241230_135006_langflow_flows.zip;type=application/zip\"\n```",
      "summary": "Use the `/projects` endpoint to create, read, update, and delete [Langflow projects](/concepts-flows#projects).",
      "keywords": [
        "create",
        "delete",
        "endpoints",
        "export",
        "import",
        "project",
        "projects",
        "read",
        "update"
      ]
    },
    "/api-reference-api-examples": {
      "title": "Get started with the Langflow API",
      "slug": "/api-reference-api-examples",
      "category": "api_reference",
      "content": "You can use the Langflow API for programmatic interactions with Langflow, such as the following:\n\n* Create and edit flows, including file management for flows.\n* Develop applications that use your flows.\n* Develop custom components.\n* Build Langflow as a dependency of a larger application, codebase, or service.\n* Contribute to the overall Langflow codebase.\n\nTo view and test all available endpoints, you can access the Langflow API's OpenAPI specification at your Langflow deployment's `/docs` endpoint, such as `http://localhost:7860/docs`.\n\n**Note:** Try it\nFor an example of the Langflow API in a script, see the [Langflow quickstart](/get-started-quickstart).\n\nThe quickstart demonstrates how to get automatically generated code snippets for your flows, use a script to run a flow, and extract data from the Langfow API response.\n\n## Form Langflow API requests\n\nWhile individual options vary by endpoint, all Langflow API requests share some commonalities, like a URL, method, parameters, and authentication.\n\nAs an example of a Langflow API request, the following curl command calls the `/v1/run` endpoint, and it passes a runtime override (`tweaks`) to the flow's **Chat Output** component:\n\n```bash\ncurl --request POST \\\n  --url \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=false\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"hello world!\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatOutput-6zcZt\": {\n      \"should_store_message\": true\n    }\n  }\n}'\n```\n\n### Base URL\n\nBy default, local deployments serve the Langflow API at `http://localhost:7860/api`.\n\nRemotely hosted Langflow deployments are available at the domain set by the hosting service, such as `http://IP_OR_DNS/api` or `http://IP_OR_DNS:LANGFLOW_PORT/api`.\n\nYou can configure the Langflow port number in the `LANGFLOW_PORT` [environment variable](/environment-variables).\n\n* `https://UUID.ngrok.app/api`\n* `http://IP_OR_DNS/api`\n* `http://IP_OR_DNS:LANGFLOW_PORT/api`\n\n### Authentication\n\nIn Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key in either an `x-api-key` header or query parameter.\nFor more information, see [API keys and authentication](/api-keys-and-authentication).\n\nAs with any API, follow industry best practices for storing and referencing sensitive credentials.\nFor example, you can [set environment variables](#set-environment-variables) for your API keys, and then reference those environment variables in your API requests.\n\n### Methods, paths, and parameters\n\nLangflow API requests use various methods, paths, path parameters, query parameters, and body parameters.\nThe specific requirements and options depend on the endpoint that you want to call.\n\nFor example, to create a flow, you pass a JSON-formatted flow definition to `POST /v1/flows`.\nThen, to run your flow, you call `POST /v1/run/$FLOW_ID` with optional run parameters in the request body.\n\n### API versions\n\nThe Langflow API serves `/v1` and `/v2` endpoints.\n\nSome endpoints only exist under a single version and some exist under both the `/v1` and `/v2` versions.\n\nIf a request fails or has an unexpected result, make sure your endpoint path has the correct version.\n\n## Set environment variables\n\nYou can store commonly used values in environment variables to facilitate reuse, simplify token rotation, and securely reference sensitive values.\n\nYou can use any method you prefer to set environment variables, such as `export`, `.env`, `zshrc`, or `.curlrc`.\nThen, reference those environment variables in your API requests.\nFor example:\n\n```bash\n# Set environment variables\nexport LANGFLOW_API_KEY=\"sk...\"\nexport LANGFLOW_SERVER_URL=\"https://localhost:7860\"\nexport FLOW_ID=\"359cd752-07ea-46f2-9d3b-a4407ef618da\"\nexport PROJECT_ID=\"1415de42-8f01-4f36-bf34-539f23e47466\"\nexport LANGFLOW_API_KEY=\"sk-...\"\n\n# Use environment variables in API requests\ncurl --request POST \\\n  --url \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID$?stream=false\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"hello world!\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatOutput-6zcZt\": {\n      \"should_store_message\": true\n    }\n  }\n}'\n```\n\nCommonly used values in Langflow API requests include your [Langflow server URL](#base-url), [Langflow API keys](#authentication), flow IDs, and [project IDs](/api-projects#read-projects).\n\nYou can retrieve flow IDs from the [**API access** pane](/concepts-publish#api-access), in a flow's URL, and with [`GET /flows`](/api-flows#read-flows).\n\n## Try some Langflow API requests\n\nOnce you have your Langflow server URL, try calling these endpoints that return Langflow metadata.\n\n### Health check\n\nReturns the health status of the Langflow database and chat services:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/health_check\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"status\": \"ok\",\n  \"chat\": \"ok\",\n  \"db\": \"ok\"\n}\n```\n\n</details>\n\nLangflow provides an additional `GET /health` endpoint.\nThis endpoint is served by uvicorn before Langflow is fully initialized, so it's not reliable for checking Langflow service health.\n\n### Get version\n\nReturns the current Langflow API version:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/version\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n{\n    \"version\": \"1.6.0\",\n    \"main_version\": \"1.6.0\",\n    \"package\": \"Langflow\"\n}\n```\n\n</details>\n\n### Get configuration\n\nReturns configuration details for your Langflow deployment:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/config\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"feature_flags\": {\n    \"mvp_components\": false\n  },\n  \"serialization_max_items_length\": 1000,\n  \"serialization_max_text_length\": 6000,\n  \"frontend_timeout\": 0,\n  \"auto_saving\": true,\n  \"auto_saving_interval\": 1000,\n  \"health_check_max_retries\": 5,\n  \"max_file_size_upload\": 1024,\n  \"webhook_polling_interval\": 5000,\n  \"public_flow_cleanup_interval\": 3600,\n  \"public_flow_expiration\": 86400,\n  \"event_delivery\": \"streaming\",\n  \"webhook_auth_enable\": false,\n  \"voice_mode_available\": false,\n  \"default_folder_name\": \"Starter Project\",\n  \"hide_getting_started_progress\": false\n}\n```\n\n</details>\n\n### Get all components\n\nReturns a dictionary of all Langflow components.\nRequires a [Langflow API key](/api-keys-and-authentication).\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/all\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Available endpoints\n\nBecause you can run Langflow as either an IDE (frontend and backend) or a runtime (headless, backend-only), it serves endpoints that support frontend and backend operations.\nMany endpoints are for orchestration between the frontend and backend, reading and writing to the Langflow database, or enabling frontend functionality, like the **Playground**.\nUnless you are contributing to the Langflow codebase, you won't directly call most of the Langflow endpoints.\n\nFor application development, the most commonly used endpoints are the `/run` and `/webhook` [flow trigger endpoints](/api-flows-run).\nFor some use cases, you might use some other endpoints, such as the `/files` endpoints to use files in flows.\n\nTo help you explore the available endpoints, the following lists are sorted by primary use case, although some endpoints might support multiple use cases.\n\n</Tabs>\n\n## Next steps\n\n* Use the Langflow API to [run a flow](/api-flows-run).\n* Use the Langflow API to [upload files](/api-files).\n* Use the Langflow API to [get flow logs](/api-logs).\n* Explore all endpoints in the [Langflow API specification](/api).",
      "summary": "You can use the Langflow API for programmatic interactions with Langflow, such as the following:",
      "keywords": [
        "access",
        "all",
        "api",
        "authentication",
        "available",
        "base",
        "chat",
        "check",
        "components",
        "configuration",
        "endpoints",
        "environment",
        "form",
        "get",
        "health",
        "langflow",
        "methods",
        "next",
        "note",
        "output"
      ]
    },
    "api-reference-api-examples": {
      "title": "Get started with the Langflow API",
      "slug": "/api-reference-api-examples",
      "category": "api_reference",
      "content": "You can use the Langflow API for programmatic interactions with Langflow, such as the following:\n\n* Create and edit flows, including file management for flows.\n* Develop applications that use your flows.\n* Develop custom components.\n* Build Langflow as a dependency of a larger application, codebase, or service.\n* Contribute to the overall Langflow codebase.\n\nTo view and test all available endpoints, you can access the Langflow API's OpenAPI specification at your Langflow deployment's `/docs` endpoint, such as `http://localhost:7860/docs`.\n\n**Note:** Try it\nFor an example of the Langflow API in a script, see the [Langflow quickstart](/get-started-quickstart).\n\nThe quickstart demonstrates how to get automatically generated code snippets for your flows, use a script to run a flow, and extract data from the Langfow API response.\n\n## Form Langflow API requests\n\nWhile individual options vary by endpoint, all Langflow API requests share some commonalities, like a URL, method, parameters, and authentication.\n\nAs an example of a Langflow API request, the following curl command calls the `/v1/run` endpoint, and it passes a runtime override (`tweaks`) to the flow's **Chat Output** component:\n\n```bash\ncurl --request POST \\\n  --url \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=false\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"hello world!\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatOutput-6zcZt\": {\n      \"should_store_message\": true\n    }\n  }\n}'\n```\n\n### Base URL\n\nBy default, local deployments serve the Langflow API at `http://localhost:7860/api`.\n\nRemotely hosted Langflow deployments are available at the domain set by the hosting service, such as `http://IP_OR_DNS/api` or `http://IP_OR_DNS:LANGFLOW_PORT/api`.\n\nYou can configure the Langflow port number in the `LANGFLOW_PORT` [environment variable](/environment-variables).\n\n* `https://UUID.ngrok.app/api`\n* `http://IP_OR_DNS/api`\n* `http://IP_OR_DNS:LANGFLOW_PORT/api`\n\n### Authentication\n\nIn Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key in either an `x-api-key` header or query parameter.\nFor more information, see [API keys and authentication](/api-keys-and-authentication).\n\nAs with any API, follow industry best practices for storing and referencing sensitive credentials.\nFor example, you can [set environment variables](#set-environment-variables) for your API keys, and then reference those environment variables in your API requests.\n\n### Methods, paths, and parameters\n\nLangflow API requests use various methods, paths, path parameters, query parameters, and body parameters.\nThe specific requirements and options depend on the endpoint that you want to call.\n\nFor example, to create a flow, you pass a JSON-formatted flow definition to `POST /v1/flows`.\nThen, to run your flow, you call `POST /v1/run/$FLOW_ID` with optional run parameters in the request body.\n\n### API versions\n\nThe Langflow API serves `/v1` and `/v2` endpoints.\n\nSome endpoints only exist under a single version and some exist under both the `/v1` and `/v2` versions.\n\nIf a request fails or has an unexpected result, make sure your endpoint path has the correct version.\n\n## Set environment variables\n\nYou can store commonly used values in environment variables to facilitate reuse, simplify token rotation, and securely reference sensitive values.\n\nYou can use any method you prefer to set environment variables, such as `export`, `.env`, `zshrc`, or `.curlrc`.\nThen, reference those environment variables in your API requests.\nFor example:\n\n```bash\n# Set environment variables\nexport LANGFLOW_API_KEY=\"sk...\"\nexport LANGFLOW_SERVER_URL=\"https://localhost:7860\"\nexport FLOW_ID=\"359cd752-07ea-46f2-9d3b-a4407ef618da\"\nexport PROJECT_ID=\"1415de42-8f01-4f36-bf34-539f23e47466\"\nexport LANGFLOW_API_KEY=\"sk-...\"\n\n# Use environment variables in API requests\ncurl --request POST \\\n  --url \"$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID$?stream=false\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"hello world!\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatOutput-6zcZt\": {\n      \"should_store_message\": true\n    }\n  }\n}'\n```\n\nCommonly used values in Langflow API requests include your [Langflow server URL](#base-url), [Langflow API keys](#authentication), flow IDs, and [project IDs](/api-projects#read-projects).\n\nYou can retrieve flow IDs from the [**API access** pane](/concepts-publish#api-access), in a flow's URL, and with [`GET /flows`](/api-flows#read-flows).\n\n## Try some Langflow API requests\n\nOnce you have your Langflow server URL, try calling these endpoints that return Langflow metadata.\n\n### Health check\n\nReturns the health status of the Langflow database and chat services:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/health_check\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"status\": \"ok\",\n  \"chat\": \"ok\",\n  \"db\": \"ok\"\n}\n```\n\n</details>\n\nLangflow provides an additional `GET /health` endpoint.\nThis endpoint is served by uvicorn before Langflow is fully initialized, so it's not reliable for checking Langflow service health.\n\n### Get version\n\nReturns the current Langflow API version:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/version\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```text\n{\n    \"version\": \"1.6.0\",\n    \"main_version\": \"1.6.0\",\n    \"package\": \"Langflow\"\n}\n```\n\n</details>\n\n### Get configuration\n\nReturns configuration details for your Langflow deployment:\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/config\" \\\n  -H \"accept: application/json\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"feature_flags\": {\n    \"mvp_components\": false\n  },\n  \"serialization_max_items_length\": 1000,\n  \"serialization_max_text_length\": 6000,\n  \"frontend_timeout\": 0,\n  \"auto_saving\": true,\n  \"auto_saving_interval\": 1000,\n  \"health_check_max_retries\": 5,\n  \"max_file_size_upload\": 1024,\n  \"webhook_polling_interval\": 5000,\n  \"public_flow_cleanup_interval\": 3600,\n  \"public_flow_expiration\": 86400,\n  \"event_delivery\": \"streaming\",\n  \"webhook_auth_enable\": false,\n  \"voice_mode_available\": false,\n  \"default_folder_name\": \"Starter Project\",\n  \"hide_getting_started_progress\": false\n}\n```\n\n</details>\n\n### Get all components\n\nReturns a dictionary of all Langflow components.\nRequires a [Langflow API key](/api-keys-and-authentication).\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_SERVER_URL/api/v1/all\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n## Available endpoints\n\nBecause you can run Langflow as either an IDE (frontend and backend) or a runtime (headless, backend-only), it serves endpoints that support frontend and backend operations.\nMany endpoints are for orchestration between the frontend and backend, reading and writing to the Langflow database, or enabling frontend functionality, like the **Playground**.\nUnless you are contributing to the Langflow codebase, you won't directly call most of the Langflow endpoints.\n\nFor application development, the most commonly used endpoints are the `/run` and `/webhook` [flow trigger endpoints](/api-flows-run).\nFor some use cases, you might use some other endpoints, such as the `/files` endpoints to use files in flows.\n\nTo help you explore the available endpoints, the following lists are sorted by primary use case, although some endpoints might support multiple use cases.\n\n</Tabs>\n\n## Next steps\n\n* Use the Langflow API to [run a flow](/api-flows-run).\n* Use the Langflow API to [upload files](/api-files).\n* Use the Langflow API to [get flow logs](/api-logs).\n* Explore all endpoints in the [Langflow API specification](/api).",
      "summary": "You can use the Langflow API for programmatic interactions with Langflow, such as the following:",
      "keywords": [
        "access",
        "all",
        "api",
        "authentication",
        "available",
        "base",
        "chat",
        "check",
        "components",
        "configuration",
        "endpoints",
        "environment",
        "form",
        "get",
        "health",
        "langflow",
        "methods",
        "next",
        "note",
        "output"
      ]
    },
    "/api-users": {
      "title": "Users endpoints",
      "slug": "/api-users",
      "category": "api_reference",
      "content": "Use the `/users` endpoint to manage user accounts in Langflow.\n\n## Add user\n\nCreate a new user account with a given username and password.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/users/\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"username\": \"newuser2\",\n    \"password\": \"securepassword123\"\n  }'\n```\n\nThe request returns an object describing the new user.\nThe user's UUID is stored in `user_id` in the Langflow database, and returned as `id` in the `/users` API response.\nThis `user_id` key is specifically for Langflow user management.\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"newuser2\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": false,\n  \"is_superuser\": false,\n  \"create_at\": \"2025-05-29T16:02:20.132436\",\n  \"updated_at\": \"2025-05-29T16:02:20.132442\",\n  \"last_login_at\": null,\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": false,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Get current user\n\nRetrieve information about the authenticated user.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/users/whoami\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"07e5b864-e367-4f52-b647-a48035ae7e5e\",\n  \"username\": \"langflow\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-08T17:59:07.855965\",\n  \"updated_at\": \"2025-05-29T15:06:56.157860\",\n  \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n}\n```\n\n</details>\n\n## List all users\n\nGet a paginated list of all users in the system.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/users/?skip=0&limit=10\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"total_count\": 3,\n  \"users\": [\n    {\n      \"id\": \"07e5b864-e367-4f52-b647-a48035ae7e5e\",\n      \"username\": \"langflow\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": true,\n      \"is_superuser\": true,\n      \"create_at\": \"2025-05-08T17:59:07.855965\",\n      \"updated_at\": \"2025-05-29T15:06:56.157860\",\n      \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": true,\n        \"discord_clicked\": false,\n        \"mcp_dialog_dismissed\": true\n      }\n    },\n    {\n      \"id\": \"c48a1f68-cc7e-491a-a507-a1a627708470\",\n      \"username\": \"newuser\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": false,\n      \"is_superuser\": false,\n      \"create_at\": \"2025-05-29T16:00:33.483386\",\n      \"updated_at\": \"2025-05-29T16:00:33.483392\",\n      \"last_login_at\": null,\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": false,\n        \"discord_clicked\": false\n      }\n    },\n    {\n      \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n      \"username\": \"newuser2\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": false,\n      \"is_superuser\": false,\n      \"create_at\": \"2025-05-29T16:02:20.132436\",\n      \"updated_at\": \"2025-05-29T16:02:20.132442\",\n      \"last_login_at\": null,\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": false,\n        \"discord_clicked\": false\n      }\n    }\n  ]\n}\n```\n\n</details>\n\n## Update user\n\nModify an existing user's information with a PATCH request.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\nThis example activates the specified user's account and makes them a superuser:\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"is_active\": true,\n    \"is_superuser\": true\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"newuser2\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-29T16:02:20.132436\",\n  \"updated_at\": \"2025-05-29T16:19:03.514527Z\",\n  \"last_login_at\": null,\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": false,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Reset password\n\nChange the specified user's password to a new secure value.\n\nRequires authentication as the target user.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8/reset-password\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"password\": \"newsecurepassword123\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"langflow\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-08T17:59:07.855965\",\n  \"updated_at\": \"2025-05-29T15:06:56.157860\",\n  \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": true,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Delete user\n\nRemove a user account from the system.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"detail\": \"User deleted\"\n}\n```\n\n</details>",
      "summary": "Use the `/users` endpoint to manage user accounts in Langflow.",
      "keywords": [
        "add",
        "all",
        "current",
        "delete",
        "endpoints",
        "get",
        "list",
        "password",
        "reset",
        "update",
        "user",
        "users"
      ]
    },
    "api-users": {
      "title": "Users endpoints",
      "slug": "/api-users",
      "category": "api_reference",
      "content": "Use the `/users` endpoint to manage user accounts in Langflow.\n\n## Add user\n\nCreate a new user account with a given username and password.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X POST \\\n  \"$LANGFLOW_URL/api/v1/users/\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"username\": \"newuser2\",\n    \"password\": \"securepassword123\"\n  }'\n```\n\nThe request returns an object describing the new user.\nThe user's UUID is stored in `user_id` in the Langflow database, and returned as `id` in the `/users` API response.\nThis `user_id` key is specifically for Langflow user management.\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"newuser2\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": false,\n  \"is_superuser\": false,\n  \"create_at\": \"2025-05-29T16:02:20.132436\",\n  \"updated_at\": \"2025-05-29T16:02:20.132442\",\n  \"last_login_at\": null,\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": false,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Get current user\n\nRetrieve information about the authenticated user.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/users/whoami\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"07e5b864-e367-4f52-b647-a48035ae7e5e\",\n  \"username\": \"langflow\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-08T17:59:07.855965\",\n  \"updated_at\": \"2025-05-29T15:06:56.157860\",\n  \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n}\n```\n\n</details>\n\n## List all users\n\nGet a paginated list of all users in the system.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X GET \\\n  \"$LANGFLOW_URL/api/v1/users/?skip=0&limit=10\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"total_count\": 3,\n  \"users\": [\n    {\n      \"id\": \"07e5b864-e367-4f52-b647-a48035ae7e5e\",\n      \"username\": \"langflow\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": true,\n      \"is_superuser\": true,\n      \"create_at\": \"2025-05-08T17:59:07.855965\",\n      \"updated_at\": \"2025-05-29T15:06:56.157860\",\n      \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": true,\n        \"discord_clicked\": false,\n        \"mcp_dialog_dismissed\": true\n      }\n    },\n    {\n      \"id\": \"c48a1f68-cc7e-491a-a507-a1a627708470\",\n      \"username\": \"newuser\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": false,\n      \"is_superuser\": false,\n      \"create_at\": \"2025-05-29T16:00:33.483386\",\n      \"updated_at\": \"2025-05-29T16:00:33.483392\",\n      \"last_login_at\": null,\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": false,\n        \"discord_clicked\": false\n      }\n    },\n    {\n      \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n      \"username\": \"newuser2\",\n      \"profile_image\": null,\n      \"store_api_key\": null,\n      \"is_active\": false,\n      \"is_superuser\": false,\n      \"create_at\": \"2025-05-29T16:02:20.132436\",\n      \"updated_at\": \"2025-05-29T16:02:20.132442\",\n      \"last_login_at\": null,\n      \"optins\": {\n        \"github_starred\": false,\n        \"dialog_dismissed\": false,\n        \"discord_clicked\": false\n      }\n    }\n  ]\n}\n```\n\n</details>\n\n## Update user\n\nModify an existing user's information with a PATCH request.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\nThis example activates the specified user's account and makes them a superuser:\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"is_active\": true,\n    \"is_superuser\": true\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"newuser2\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-29T16:02:20.132436\",\n  \"updated_at\": \"2025-05-29T16:19:03.514527Z\",\n  \"last_login_at\": null,\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": false,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Reset password\n\nChange the specified user's password to a new secure value.\n\nRequires authentication as the target user.\n\n```bash\ncurl -X PATCH \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8/reset-password\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n  -d '{\n    \"password\": \"newsecurepassword123\"\n  }'\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"id\": \"10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\",\n  \"username\": \"langflow\",\n  \"profile_image\": null,\n  \"store_api_key\": null,\n  \"is_active\": true,\n  \"is_superuser\": true,\n  \"create_at\": \"2025-05-08T17:59:07.855965\",\n  \"updated_at\": \"2025-05-29T15:06:56.157860\",\n  \"last_login_at\": \"2025-05-29T15:06:56.157016\",\n  \"optins\": {\n    \"github_starred\": false,\n    \"dialog_dismissed\": true,\n    \"discord_clicked\": false\n  }\n}\n```\n\n</details>\n\n## Delete user\n\nRemove a user account from the system.\n\nRequires authentication as a superuser if the Langflow server has authentication enabled.\n\n```bash\ncurl -X DELETE \\\n  \"$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8\" \\\n  -H \"accept: application/json\" \\\n  -H \"x-api-key: $LANGFLOW_API_KEY\"\n```\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"detail\": \"User deleted\"\n}\n```\n\n</details>",
      "summary": "Use the `/users` endpoint to manage user accounts in Langflow.",
      "keywords": [
        "add",
        "all",
        "current",
        "delete",
        "endpoints",
        "get",
        "list",
        "password",
        "reset",
        "update",
        "user",
        "users"
      ]
    },
    "/typescript-client": {
      "title": "Langflow TypeScript client",
      "slug": "/typescript-client",
      "category": "api_reference",
      "content": "The Langflow TypeScript client allows your TypeScript applications to programmatically interact with the Langflow API.\n\nFor the client code repository, see [langflow-client-ts](https://github.com/datastax/langflow-client-ts/).\n\nFor the npm package, see [@datastax/langflow-client](https://www.npmjs.com/package/@datastax/langflow-client).\n\n## Install the Langflow TypeScript package\n\nTo install the Langflow typescript client package, use one of the following commands:\n\n</Tabs>\n\n## Initialize the Langflow TypeScript client\n\n1. Import the client into your code.\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n    ```\n\n2. Initialize a `LangflowClient` object to interact with your server:\n\n    ```tsx\n    const baseUrl = \"BASE_URL\";\n    const apiKey = \"API_KEY\";\n    const client = new LangflowClient({ baseUrl, apiKey });\n    ```\n\n    Replace `BASE_URL` and `API_KEY` with values from your deployment.\n    The default Langflow base URL is `http://localhost:7860`.\n    To create an API key, see [API keys and authentication](/api-keys-and-authentication).\n\n## Connect to your server and get responses\n\n1. With your Langflow client initialized, test the connection by calling your Langflow server.\n\n    The following example runs a flow (`runFlow`) by sending the flow ID and a chat input string:\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const baseUrl = \"http://localhost:7860\";\n    const client = new LangflowClient({ baseUrl });\n\n    async function runFlow() {\n        const flowId = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n        const flow = client.flow(flowId);\n        const input = \"Is anyone there?\";\n\n        const response = await flow.run(input);\n        console.log(response);\n    }\n\n    runFlow().catch(console.error);\n    ```\n\n    Replace the following:\n\n    * `baseUrl`: The URL of your Langflow server.\n    * `flowId`: The ID of the flow you want to run.\n    * `input`: The chat input message you want to send to trigger the flow.\n    This is only valid for flows with a **Chat Input** component.\n\n2. Review the result to confirm that the client connected to your Langflow server.\n\n    The following example shows the response from a well-formed `runFlow` request that reached the Langflow server and successfully started the flow:\n\n    ```\n    FlowResponse {\n      sessionId: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',\n      outputs: [ { inputs: [Object], outputs: [Array] } ]\n    }\n    ```\n\n    In this case, the response includes a [`sessionID`](/session-id) that is a unique identifier for the client-server session and an `outputs` array that contains information about the flow run.\n\n3. Optional: If you want to get full response objects from the server, change `console.log` to stringify the returned JSON object:\n\n    ```tsx\n    console.log(JSON.stringify(response, null, 2));\n    ```\n\n    The exact structure of the returned `inputs` and `outputs` objects depends on the components and configuration of your flow.\n\n4. Optional: If you want the response to include only the chat message from the **Chat Output** component, change `console.log` to use the `chatOutputText` convenience function:\n\n    ```tsx\n    console.log(response.chatOutputText());\n    ```\n\n## Use advanced TypeScript client features\n\nThe TypeScript client can do more than just connect to your server and run a flow.\n\nThis example builds on the quickstart with additional features for interacting with Langflow:\n\n1. Pass [tweaks](/concepts-publish#input-schema) as an object with the request.\nTweaks are programmatic run-time overrides for component settings.\n\n    This example changes the LLM used by a language model component in a flow::\n\n    ```tsx\n    const tweaks = { model_name: \"gpt-4o-mini\" };\n    ```\n\n2. Pass a [session ID](/session-id) with the request to separate the conversation from other flow runs, and to be able to continue this conversation by calling the same session ID in the future:\n\n    ```tsx\n    const session_id = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n    ```\n\n3. Instead of calling `run` on the Flow object, call `stream` with the same arguments to get a streaming response:\n\n    ```tsx\n    const response = await client.flow(flowId).stream(input);\n\n    for await (const event of response) {\n      console.log(event);\n    }\n    ```\n\n    The response is a [`ReadableStream`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) of objects.\n    For more information on streaming Langflow responses, see the [`/run` endpoint](/api-flows-run#run-flow).\n\n4. Run the modified TypeScript application to run the flow with `tweaks`, `session_id`, and streaming:\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const baseUrl = \"http://localhost:7860\";\n    const client = new LangflowClient({ baseUrl });\n\n    async function runFlow() {\n        const flowId = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n        const input = \"Is anyone there?\";\n        const tweaks = { model_name: \"gpt-4o-mini\" };\n        const session_id = \"test-session\";\n\n        const response = await client.flow(flowId).stream(input, {\n            session_id,\n            tweaks,\n          });\n\n        for await (const event of response) {\n            console.log(event);\n        }\n\n    }\n    runFlow().catch(console.error);\n    ```\n\n    Replace the following:\n\n    * `baseUrl`: The URL of your Langflow server.\n    * `flowId`: The ID of the flow you want to run.\n    * `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.\n    * `tweaks`: Any tweak modifiers to apply to the flow run.\n    This example changes the LLM used by a component in the flow.\n    * `session_id`: Pass a custom session ID.\n    If omitted or empty, the flow ID is the default session ID.\n\n    <details>\n    <summary>Result</summary>\n\n    With streaming enabled, the response includes the flow metatadata and timestamped events for flow activity.\n    For example:\n\n    ```text\n    {\n      event: 'add_message',\n      data: {\n        timestamp: '2025-05-23 15:52:48 UTC',\n        sender: 'User',\n        sender_name: 'User',\n        session_id: 'test-session',\n        text: 'Is anyone there?',\n        files: [],\n        error: false,\n        edit: false,\n        properties: {\n          text_color: '',\n          background_color: '',\n          edited: false,\n          source: [Object],\n          icon: '',\n          allow_markdown: false,\n          positive_feedback: null,\n          state: 'complete',\n          targets: []\n        },\n        category: 'message',\n        content_blocks: [],\n        id: '7f096715-3f2d-4d84-88d6-5e2f76bf3fbe',\n        flow_id: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',\n        duration: null\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: 'Absolutely',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: ',',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: \" I'm\",\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: ' here',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n\n    // this response is abbreviated\n\n    {\n      event: 'end',\n      data: { result: { session_id: 'test-session', outputs: [Array] } }\n    }\n    ```\n\n    </details>\n\n## Retrieve Langflow logs with the TypeScript client\n\nTo retrieve [Langflow logs](/logging), you must enable log retrieval on your Langflow server by including the following values in your Langflow `.env` file:\n\n```text\nLANGFLOW_ENABLE_LOG_RETRIEVAL=True\nLANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000\nLANGFLOW_LOG_LEVEL=DEBUG\n```\n\nThe following example script starts streaming logs in the background, and then runs a flow so you can monitor the flow run:\n\n```tsx\n\nconst baseUrl = \"http://localhost:7863\";\nconst flowId = \"86f0bf45-0544-4e88-b0b1-8e622da7a7f0\";\n\nasync function runFlow(client: LangflowClient) {\n    const input = \"Is anyone there?\";\n    const response = await client.flow(flowId).run(input);\n    console.log('Flow response:', response);\n}\n\nasync function main() {\n    const client = new LangflowClient({ baseUrl: baseUrl });\n\n    // Start streaming logs\n    console.log('Starting log stream...');\n    for await (const log of await client.logs.stream()) {\n        console.log('Log:', log);\n    }\n\n    // Run the flow\n    await runFlow(client);\n\n}\n\nmain().catch(console.error);\n```\n\nReplace the following:\n\n* `baseUrl`: The URL of your Langflow server.\n* `flowId`: The ID of the flow you want to run.\n* `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.\n\nLogs begin streaming indefinitely, and the flow runs once.\n\n<details>\n<summary>Result</summary>\n\nThe following example result is truncated for readability, but you can follow the messages to see how the flow instantiates its components, configures its model, and processes the outputs.\n\nThe `FlowResponse` object, at the end of the stream, is returned to the client with the flow result in the `outputs` array.\n\n```text\nStarting log stream...\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.006Z,\n  message: '2025-05-30T07:49:16.006127-0400 DEBUG Instantiating ChatInput of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.029Z,\n  message: '2025-05-30T07:49:16.029957-0400 DEBUG Instantiating Prompt of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.049Z,\n  message: '2025-05-30T07:49:16.049520-0400 DEBUG Instantiating ChatOutput of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.069Z,\n  message: '2025-05-30T07:49:16.069359-0400 DEBUG Instantiating OpenAIModel of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.086Z,\n  message: \"2025-05-30T07:49:16.086426-0400 DEBUG Running layer 0 with 2 tasks, ['ChatInput-xjucM', 'Prompt-I3pxU']\\n\"\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.101Z,\n  message: '2025-05-30T07:49:16.101766-0400 DEBUG Building Chat Input\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.113Z,\n  message: '2025-05-30T07:49:16.113343-0400 DEBUG Building Prompt\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.131Z,\n  message: '2025-05-30T07:49:16.131423-0400 DEBUG Logged vertex build: 6bd9fe9c-5eea-4f05-a96d-f6de9dc77e3c\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.143Z,\n  message: '2025-05-30T07:49:16.143295-0400 DEBUG Logged vertex build: 39c68ec9-3859-4fff-9b14-80b3271f8fbf\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.188Z,\n  message: \"2025-05-30T07:49:16.188730-0400 DEBUG Running layer 1 with 1 tasks, ['OpenAIModel-RtlZm']\\n\"\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.201Z,\n  message: '2025-05-30T07:49:16.201946-0400 DEBUG Building OpenAI\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.216Z,\n  message: '2025-05-30T07:49:16.216622-0400 INFO Model name: gpt-4.1-mini\\n'\n}\nFlow response: FlowResponse {\n  sessionId: '86f0bf45-0544-4e88-b0b1-8e622da7a7f0',\n  outputs: [ { inputs: [Object], outputs: [Array] } ]\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:18.094Z,\n  message: `2025-05-30T07:49:18.094364-0400 DEBUG Vertex OpenAIModel-RtlZm, result: <langflow.graph.utils.UnbuiltResult object at 0x364d24dd0>, object: {'text_output': \"Hey there! I'm here and ready to help you build something awesome with AI. What are you thinking about creating today?\"}\\n`\n}\n```\n\n</details>\n\nFor more information, see [Logs endpoints](/api-logs).",
      "summary": "The Langflow TypeScript client allows your TypeScript applications to programmatically interact with the Langflow API.",
      "keywords": [
        "advanced",
        "chat",
        "client",
        "connect",
        "features",
        "get",
        "initialize",
        "input",
        "install",
        "langflow",
        "logs",
        "output",
        "package",
        "responses",
        "retrieve",
        "server",
        "typescript"
      ]
    },
    "typescript-client": {
      "title": "Langflow TypeScript client",
      "slug": "/typescript-client",
      "category": "api_reference",
      "content": "The Langflow TypeScript client allows your TypeScript applications to programmatically interact with the Langflow API.\n\nFor the client code repository, see [langflow-client-ts](https://github.com/datastax/langflow-client-ts/).\n\nFor the npm package, see [@datastax/langflow-client](https://www.npmjs.com/package/@datastax/langflow-client).\n\n## Install the Langflow TypeScript package\n\nTo install the Langflow typescript client package, use one of the following commands:\n\n</Tabs>\n\n## Initialize the Langflow TypeScript client\n\n1. Import the client into your code.\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n    ```\n\n2. Initialize a `LangflowClient` object to interact with your server:\n\n    ```tsx\n    const baseUrl = \"BASE_URL\";\n    const apiKey = \"API_KEY\";\n    const client = new LangflowClient({ baseUrl, apiKey });\n    ```\n\n    Replace `BASE_URL` and `API_KEY` with values from your deployment.\n    The default Langflow base URL is `http://localhost:7860`.\n    To create an API key, see [API keys and authentication](/api-keys-and-authentication).\n\n## Connect to your server and get responses\n\n1. With your Langflow client initialized, test the connection by calling your Langflow server.\n\n    The following example runs a flow (`runFlow`) by sending the flow ID and a chat input string:\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const baseUrl = \"http://localhost:7860\";\n    const client = new LangflowClient({ baseUrl });\n\n    async function runFlow() {\n        const flowId = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n        const flow = client.flow(flowId);\n        const input = \"Is anyone there?\";\n\n        const response = await flow.run(input);\n        console.log(response);\n    }\n\n    runFlow().catch(console.error);\n    ```\n\n    Replace the following:\n\n    * `baseUrl`: The URL of your Langflow server.\n    * `flowId`: The ID of the flow you want to run.\n    * `input`: The chat input message you want to send to trigger the flow.\n    This is only valid for flows with a **Chat Input** component.\n\n2. Review the result to confirm that the client connected to your Langflow server.\n\n    The following example shows the response from a well-formed `runFlow` request that reached the Langflow server and successfully started the flow:\n\n    ```\n    FlowResponse {\n      sessionId: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',\n      outputs: [ { inputs: [Object], outputs: [Array] } ]\n    }\n    ```\n\n    In this case, the response includes a [`sessionID`](/session-id) that is a unique identifier for the client-server session and an `outputs` array that contains information about the flow run.\n\n3. Optional: If you want to get full response objects from the server, change `console.log` to stringify the returned JSON object:\n\n    ```tsx\n    console.log(JSON.stringify(response, null, 2));\n    ```\n\n    The exact structure of the returned `inputs` and `outputs` objects depends on the components and configuration of your flow.\n\n4. Optional: If you want the response to include only the chat message from the **Chat Output** component, change `console.log` to use the `chatOutputText` convenience function:\n\n    ```tsx\n    console.log(response.chatOutputText());\n    ```\n\n## Use advanced TypeScript client features\n\nThe TypeScript client can do more than just connect to your server and run a flow.\n\nThis example builds on the quickstart with additional features for interacting with Langflow:\n\n1. Pass [tweaks](/concepts-publish#input-schema) as an object with the request.\nTweaks are programmatic run-time overrides for component settings.\n\n    This example changes the LLM used by a language model component in a flow::\n\n    ```tsx\n    const tweaks = { model_name: \"gpt-4o-mini\" };\n    ```\n\n2. Pass a [session ID](/session-id) with the request to separate the conversation from other flow runs, and to be able to continue this conversation by calling the same session ID in the future:\n\n    ```tsx\n    const session_id = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n    ```\n\n3. Instead of calling `run` on the Flow object, call `stream` with the same arguments to get a streaming response:\n\n    ```tsx\n    const response = await client.flow(flowId).stream(input);\n\n    for await (const event of response) {\n      console.log(event);\n    }\n    ```\n\n    The response is a [`ReadableStream`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) of objects.\n    For more information on streaming Langflow responses, see the [`/run` endpoint](/api-flows-run#run-flow).\n\n4. Run the modified TypeScript application to run the flow with `tweaks`, `session_id`, and streaming:\n\n    ```tsx\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const baseUrl = \"http://localhost:7860\";\n    const client = new LangflowClient({ baseUrl });\n\n    async function runFlow() {\n        const flowId = \"aa5a238b-02c0-4f03-bc5c-cc3a83335cdf\";\n        const input = \"Is anyone there?\";\n        const tweaks = { model_name: \"gpt-4o-mini\" };\n        const session_id = \"test-session\";\n\n        const response = await client.flow(flowId).stream(input, {\n            session_id,\n            tweaks,\n          });\n\n        for await (const event of response) {\n            console.log(event);\n        }\n\n    }\n    runFlow().catch(console.error);\n    ```\n\n    Replace the following:\n\n    * `baseUrl`: The URL of your Langflow server.\n    * `flowId`: The ID of the flow you want to run.\n    * `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.\n    * `tweaks`: Any tweak modifiers to apply to the flow run.\n    This example changes the LLM used by a component in the flow.\n    * `session_id`: Pass a custom session ID.\n    If omitted or empty, the flow ID is the default session ID.\n\n    <details>\n    <summary>Result</summary>\n\n    With streaming enabled, the response includes the flow metatadata and timestamped events for flow activity.\n    For example:\n\n    ```text\n    {\n      event: 'add_message',\n      data: {\n        timestamp: '2025-05-23 15:52:48 UTC',\n        sender: 'User',\n        sender_name: 'User',\n        session_id: 'test-session',\n        text: 'Is anyone there?',\n        files: [],\n        error: false,\n        edit: false,\n        properties: {\n          text_color: '',\n          background_color: '',\n          edited: false,\n          source: [Object],\n          icon: '',\n          allow_markdown: false,\n          positive_feedback: null,\n          state: 'complete',\n          targets: []\n        },\n        category: 'message',\n        content_blocks: [],\n        id: '7f096715-3f2d-4d84-88d6-5e2f76bf3fbe',\n        flow_id: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',\n        duration: null\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: 'Absolutely',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: ',',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: \" I'm\",\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n    {\n      event: 'token',\n      data: {\n        chunk: ' here',\n        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',\n        timestamp: '2025-05-23 15:52:48 UTC'\n      }\n    }\n\n    // this response is abbreviated\n\n    {\n      event: 'end',\n      data: { result: { session_id: 'test-session', outputs: [Array] } }\n    }\n    ```\n\n    </details>\n\n## Retrieve Langflow logs with the TypeScript client\n\nTo retrieve [Langflow logs](/logging), you must enable log retrieval on your Langflow server by including the following values in your Langflow `.env` file:\n\n```text\nLANGFLOW_ENABLE_LOG_RETRIEVAL=True\nLANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000\nLANGFLOW_LOG_LEVEL=DEBUG\n```\n\nThe following example script starts streaming logs in the background, and then runs a flow so you can monitor the flow run:\n\n```tsx\n\nconst baseUrl = \"http://localhost:7863\";\nconst flowId = \"86f0bf45-0544-4e88-b0b1-8e622da7a7f0\";\n\nasync function runFlow(client: LangflowClient) {\n    const input = \"Is anyone there?\";\n    const response = await client.flow(flowId).run(input);\n    console.log('Flow response:', response);\n}\n\nasync function main() {\n    const client = new LangflowClient({ baseUrl: baseUrl });\n\n    // Start streaming logs\n    console.log('Starting log stream...');\n    for await (const log of await client.logs.stream()) {\n        console.log('Log:', log);\n    }\n\n    // Run the flow\n    await runFlow(client);\n\n}\n\nmain().catch(console.error);\n```\n\nReplace the following:\n\n* `baseUrl`: The URL of your Langflow server.\n* `flowId`: The ID of the flow you want to run.\n* `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.\n\nLogs begin streaming indefinitely, and the flow runs once.\n\n<details>\n<summary>Result</summary>\n\nThe following example result is truncated for readability, but you can follow the messages to see how the flow instantiates its components, configures its model, and processes the outputs.\n\nThe `FlowResponse` object, at the end of the stream, is returned to the client with the flow result in the `outputs` array.\n\n```text\nStarting log stream...\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.006Z,\n  message: '2025-05-30T07:49:16.006127-0400 DEBUG Instantiating ChatInput of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.029Z,\n  message: '2025-05-30T07:49:16.029957-0400 DEBUG Instantiating Prompt of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.049Z,\n  message: '2025-05-30T07:49:16.049520-0400 DEBUG Instantiating ChatOutput of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.069Z,\n  message: '2025-05-30T07:49:16.069359-0400 DEBUG Instantiating OpenAIModel of type component\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.086Z,\n  message: \"2025-05-30T07:49:16.086426-0400 DEBUG Running layer 0 with 2 tasks, ['ChatInput-xjucM', 'Prompt-I3pxU']\\n\"\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.101Z,\n  message: '2025-05-30T07:49:16.101766-0400 DEBUG Building Chat Input\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.113Z,\n  message: '2025-05-30T07:49:16.113343-0400 DEBUG Building Prompt\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.131Z,\n  message: '2025-05-30T07:49:16.131423-0400 DEBUG Logged vertex build: 6bd9fe9c-5eea-4f05-a96d-f6de9dc77e3c\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.143Z,\n  message: '2025-05-30T07:49:16.143295-0400 DEBUG Logged vertex build: 39c68ec9-3859-4fff-9b14-80b3271f8fbf\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.188Z,\n  message: \"2025-05-30T07:49:16.188730-0400 DEBUG Running layer 1 with 1 tasks, ['OpenAIModel-RtlZm']\\n\"\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.201Z,\n  message: '2025-05-30T07:49:16.201946-0400 DEBUG Building OpenAI\\n'\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:16.216Z,\n  message: '2025-05-30T07:49:16.216622-0400 INFO Model name: gpt-4.1-mini\\n'\n}\nFlow response: FlowResponse {\n  sessionId: '86f0bf45-0544-4e88-b0b1-8e622da7a7f0',\n  outputs: [ { inputs: [Object], outputs: [Array] } ]\n}\nLog: Log {\n  timestamp: 2025-05-30T11:49:18.094Z,\n  message: `2025-05-30T07:49:18.094364-0400 DEBUG Vertex OpenAIModel-RtlZm, result: <langflow.graph.utils.UnbuiltResult object at 0x364d24dd0>, object: {'text_output': \"Hey there! I'm here and ready to help you build something awesome with AI. What are you thinking about creating today?\"}\\n`\n}\n```\n\n</details>\n\nFor more information, see [Logs endpoints](/api-logs).",
      "summary": "The Langflow TypeScript client allows your TypeScript applications to programmatically interact with the Langflow API.",
      "keywords": [
        "advanced",
        "chat",
        "client",
        "connect",
        "features",
        "get",
        "initialize",
        "input",
        "install",
        "langflow",
        "logs",
        "output",
        "package",
        "responses",
        "retrieve",
        "server",
        "typescript"
      ]
    },
    "/agents-tools": {
      "title": "Configure tools for agents",
      "slug": "/agents-tools",
      "category": "agents",
      "content": "By default, [Langflow agents](/agents) only include the functionality built-in to their base LLM.\n\nYou can attach tools to agents to provide access to additional, targeted functionality.\nFor example, tools can be used to create domain-specific agents, such as a customer support agent that can access a company's knowledge base, a financial agent that can retrieve stock prices, or a math tutor agent that can use advanced math functions to solve complex equations.\n\n## Attach tools\n\nTo attach a tool to an agent, you connect any component's **Tool** output to the **Agent** component's **Tools** input.\n\nSome components emit **Tool** output by default.\nFor all other components, you must enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus).\nThen, you can connect the tool to the agent.\n\nYou can connect multiple tools to one agent, and each tool can have multiple actions (functions) that the agent can call.\n\nWhen you run your flow, the agent decides when to call on certain tools, if it determines that a tool can help it respond to the user's prompt.\n\n### Edit a tool's actions {#edit-a-tools-actions}\n\nWhen you attach components to an agent as tools, each tool can have multiple actions (functions) that the agent can call.\nAvailable actions are listed in each tool component's **Actions** list.\n\nYou can change each action's labels, descriptions, and availability to help the agent understand how to use the tool and prevent it from using irrelevant or undesired actions.\n\n**Note:** If an agent seems to be using a tool incorrectly, try editing the actions metadata to clarify the tool's purpose and disable unnecessary actions.\n\nYou can also try using a **Prompt Template** component to pass additional instructions or examples to the agent.\n\nTo view and edit a tool's actions, click  **Edit Tool Actions** on the tool component.\n\nThe following information is provided for each action:\n\n* **Enabled**: A checkbox that determines whether the action is available to the agent.\nIf selected, the action is enabled.\nIf not selected, the action is disabled.\n\n* **Name**: A human-readable string name for the action, such as `Fetch Content`. This cannot be changed.\n\n* **Description**: A human-readable description of the action's purpose, such as `Fetch content from web pages recursively`.\nTo edit this value, double-click the action's row to open the edit pane.\nChanges are saved automatically when you click out of the field or close the dialog.\n\n* **Slug**: An encoded name for the action, usually the same as the name but in snake case, such as `fetch_content`.\nTo edit this value, double-click the action's row to open the edit pane.\nChanges are saved automatically when you click out of the field or close the dialog.\n\nSome actions allow you to provide fixed values for their inputs.\nTypically, you want to leave these blank so the agent can provide its own values.\nHowever, you might use a fixed value if you're trying to debug an agent's behavior or your use case requires a fixed input for an action.\n\n## Use an agent as a tool\n\nTo create multi-agent flows, you can set another **Agent** component to **Tool Mode**, and then attach that agent as a tool for your primary **Agent** component.\n\nTo try this for yourself, add an additional agent to the **Simple Agent** template:\n\n1. Create a flow based on the **Simple Agent** template.\n2. Add a second **Agent** component to the flow.\n3. Add your **OpenAI API Key** to both **Agent** components.\n4. In the second **Agent** component, change the model to `gpt-4.1`, and then enable **Tool Mode**.\n5. Click  **Edit Tool Actions** to [edit the tool's actions](#edit-a-tools-actions).\n\n    For this example, change the action's slug to `Agent-gpt-41`, and set the description to `Use the gpt-4.1 model for complex problem solving`.\n    This lets the primary agent know that this tool uses the `gpt-4.1` model, which could be helpful for tasks requiring a larger context window, such as large scrape and search tasks.\n\n    As another example, you could attach several specialized models to a primary agent, such as agents that are trained on certain tasks or domains, and then the primary agent would call each specialized agent as needed to respond to queries.\n\n    You can also enable and disable tools if you want to limit the available toolset.\n\n6. Connect the new agent's **Toolset** port to the existing agent's **Tools** port.\n\n    ![Agent as a tool](/img/agent-example-agent-as-tool.png)\n\n## Add custom components as tools {#components-as-tools}\n\nAn agent can use [custom components](/components-custom-components) as tools.\n\n1. To add a custom component to an agent flow, click **New Custom Component** in the  **Core components** or  **Bundles** menus.\n\n2. Enter Python code into the **Code** pane to create the custom component.\n\n    If you don't already have code for a custom component, you can use the following code snippet as an example before creating your own.\n\n    <details>\n    <summary>Text Analyzer custom component</summary>\n\n    This code creates a text analyzer component.\n\n    ```python\n    from langflow.custom import Component\n    from langflow.io import MessageTextInput, Output\n    from langflow.schema import Data\n    import re\n\n    class TextAnalyzerComponent(Component):\n        display_name = \"Text Analyzer\"\n        description = \"Analyzes and transforms input text.\"\n        documentation: str = \"http://docs.langflow.org/components/custom\"\n        icon = \"chart-bar\"\n        name = \"TextAnalyzerComponent\"\n\n        inputs = [\n            MessageTextInput(\n                name=\"input_text\",\n                display_name=\"Input Text\",\n                info=\"Enter text to analyze\",\n                value=\"Hello, World!\",\n                tool_mode=True,\n            ),\n        ]\n\n        outputs = [\n            Output(display_name=\"Analysis Result\", name=\"output\", method=\"analyze_text\"),\n        ]\n\n        def analyze_text(self) -> Data:\n            text = self.input_text\n\n            # Perform text analysis\n            word_count = len(text.split())\n            char_count = len(text)\n            sentence_count = len(re.findall(r'\\w+[.!?]', text))\n\n            # Transform text\n            reversed_text = text[::-1]\n            uppercase_text = text.upper()\n\n            analysis_result = {\n                \"original_text\": text,\n                \"word_count\": word_count,\n                \"character_count\": char_count,\n                \"sentence_count\": sentence_count,\n                \"reversed_text\": reversed_text,\n                \"uppercase_text\": uppercase_text\n            }\n\n            data = Data(value=analysis_result)\n            self.status = data\n            return data\n    ```\n    </details>\n\n3. Enable **Tool Mode** in the custom component.\n4. Connect the custom component's tool output to the **Agent** component's **Tools** input.\n5. Open the  **Playground** and instruct the agent, `Use the text analyzer on this text: \"Agents really are thinking machines!\"`\n\n    Based on your instruction, the agent should call the `analyze_text` action and return the result.\n    For example:\n\n    ```\n    gpt-4o\n    Finished\n    0.6s\n    Here is the analysis of the text \"Agents really are thinking machines!\":\n    Original Text: Agents really are thinking machines!\n    Word Count: 5\n    Character Count: 36\n    Sentence Count: 1\n    Reversed Text: !senihcam gnikniht era yllaer stnegA\n    Uppercase Text: AGENTS REALLY ARE THINKING MACHINES!\n    ```\n\n## Make any component a tool\n\nIf the component you want to use as a tool doesn't have a **Tool Mode** button, add `tool_mode=True` to one of the component's inputs, and connect the new **Toolset** output to the agent's **Tools** input.\n\nLangflow supports **Tool Mode** for the following data types:\n\n* `DataInput`\n* `DataFrameInput`\n* `PromptInput`\n* `MessageTextInput`\n* `MultilineInput`\n* `DropdownInput`\n\nFor example, the example code in [Use custom components as tools](#components-as-tools) included `tool_mode=True` to the `MessageTextInput` input so the custom component could be used as a tool:\n\n```python\ninputs = [\n    MessageTextInput(\n        name=\"input_text\",\n        display_name=\"Input Text\",\n        info=\"Enter text to analyze\",\n        value=\"Hello, World!\",\n        tool_mode=True,\n    ),\n]\n```\n\n## Use flows as tools\n\nAn agent can use your other flows as tools with the [**Run Flow** component](/components-logic#run-flow).\n\n1. Add a **Run Flow** component to your flow.\n2. Select the flow you want the agent to use as a tool.\n3. Enable **Tool Mode**.\nThe selected flow becomes an [action](#edit-a-tools-actions) in the **Run Flow** component.\n4. Connect the **Run Flow** component's **Tool** output to the **Agent** component's **Tools** input.\n5. Open the **Playground**, and then ask the agent, `What tools are you using to answer my questions?`\nYour flow should be visible in the response as an available tool.\n6. Ask the agent a question that specifically uses the connected flow as a tool.\nThe connected flow returns an answer based on your question.\n\n![Run Flow component connected to an Agent component as a tool](/img/agent-example-run-flow-as-tool.png)\n\n## See also\n\n* [Agent components](/components-agents)\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use Langflow as an MCP server](/mcp-server)",
      "summary": "By default, [Langflow agents](/agents) only include the functionality built-in to their base LLM.",
      "keywords": [
        "actions",
        "add",
        "agent",
        "agents",
        "any",
        "api",
        "attach",
        "bundles",
        "code",
        "component",
        "components",
        "configure",
        "core",
        "custom",
        "description",
        "edit",
        "enabled",
        "flow",
        "flows",
        "key"
      ]
    },
    "agents-tools": {
      "title": "Configure tools for agents",
      "slug": "/agents-tools",
      "category": "agents",
      "content": "By default, [Langflow agents](/agents) only include the functionality built-in to their base LLM.\n\nYou can attach tools to agents to provide access to additional, targeted functionality.\nFor example, tools can be used to create domain-specific agents, such as a customer support agent that can access a company's knowledge base, a financial agent that can retrieve stock prices, or a math tutor agent that can use advanced math functions to solve complex equations.\n\n## Attach tools\n\nTo attach a tool to an agent, you connect any component's **Tool** output to the **Agent** component's **Tools** input.\n\nSome components emit **Tool** output by default.\nFor all other components, you must enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus).\nThen, you can connect the tool to the agent.\n\nYou can connect multiple tools to one agent, and each tool can have multiple actions (functions) that the agent can call.\n\nWhen you run your flow, the agent decides when to call on certain tools, if it determines that a tool can help it respond to the user's prompt.\n\n### Edit a tool's actions {#edit-a-tools-actions}\n\nWhen you attach components to an agent as tools, each tool can have multiple actions (functions) that the agent can call.\nAvailable actions are listed in each tool component's **Actions** list.\n\nYou can change each action's labels, descriptions, and availability to help the agent understand how to use the tool and prevent it from using irrelevant or undesired actions.\n\n**Note:** If an agent seems to be using a tool incorrectly, try editing the actions metadata to clarify the tool's purpose and disable unnecessary actions.\n\nYou can also try using a **Prompt Template** component to pass additional instructions or examples to the agent.\n\nTo view and edit a tool's actions, click  **Edit Tool Actions** on the tool component.\n\nThe following information is provided for each action:\n\n* **Enabled**: A checkbox that determines whether the action is available to the agent.\nIf selected, the action is enabled.\nIf not selected, the action is disabled.\n\n* **Name**: A human-readable string name for the action, such as `Fetch Content`. This cannot be changed.\n\n* **Description**: A human-readable description of the action's purpose, such as `Fetch content from web pages recursively`.\nTo edit this value, double-click the action's row to open the edit pane.\nChanges are saved automatically when you click out of the field or close the dialog.\n\n* **Slug**: An encoded name for the action, usually the same as the name but in snake case, such as `fetch_content`.\nTo edit this value, double-click the action's row to open the edit pane.\nChanges are saved automatically when you click out of the field or close the dialog.\n\nSome actions allow you to provide fixed values for their inputs.\nTypically, you want to leave these blank so the agent can provide its own values.\nHowever, you might use a fixed value if you're trying to debug an agent's behavior or your use case requires a fixed input for an action.\n\n## Use an agent as a tool\n\nTo create multi-agent flows, you can set another **Agent** component to **Tool Mode**, and then attach that agent as a tool for your primary **Agent** component.\n\nTo try this for yourself, add an additional agent to the **Simple Agent** template:\n\n1. Create a flow based on the **Simple Agent** template.\n2. Add a second **Agent** component to the flow.\n3. Add your **OpenAI API Key** to both **Agent** components.\n4. In the second **Agent** component, change the model to `gpt-4.1`, and then enable **Tool Mode**.\n5. Click  **Edit Tool Actions** to [edit the tool's actions](#edit-a-tools-actions).\n\n    For this example, change the action's slug to `Agent-gpt-41`, and set the description to `Use the gpt-4.1 model for complex problem solving`.\n    This lets the primary agent know that this tool uses the `gpt-4.1` model, which could be helpful for tasks requiring a larger context window, such as large scrape and search tasks.\n\n    As another example, you could attach several specialized models to a primary agent, such as agents that are trained on certain tasks or domains, and then the primary agent would call each specialized agent as needed to respond to queries.\n\n    You can also enable and disable tools if you want to limit the available toolset.\n\n6. Connect the new agent's **Toolset** port to the existing agent's **Tools** port.\n\n    ![Agent as a tool](/img/agent-example-agent-as-tool.png)\n\n## Add custom components as tools {#components-as-tools}\n\nAn agent can use [custom components](/components-custom-components) as tools.\n\n1. To add a custom component to an agent flow, click **New Custom Component** in the  **Core components** or  **Bundles** menus.\n\n2. Enter Python code into the **Code** pane to create the custom component.\n\n    If you don't already have code for a custom component, you can use the following code snippet as an example before creating your own.\n\n    <details>\n    <summary>Text Analyzer custom component</summary>\n\n    This code creates a text analyzer component.\n\n    ```python\n    from langflow.custom import Component\n    from langflow.io import MessageTextInput, Output\n    from langflow.schema import Data\n    import re\n\n    class TextAnalyzerComponent(Component):\n        display_name = \"Text Analyzer\"\n        description = \"Analyzes and transforms input text.\"\n        documentation: str = \"http://docs.langflow.org/components/custom\"\n        icon = \"chart-bar\"\n        name = \"TextAnalyzerComponent\"\n\n        inputs = [\n            MessageTextInput(\n                name=\"input_text\",\n                display_name=\"Input Text\",\n                info=\"Enter text to analyze\",\n                value=\"Hello, World!\",\n                tool_mode=True,\n            ),\n        ]\n\n        outputs = [\n            Output(display_name=\"Analysis Result\", name=\"output\", method=\"analyze_text\"),\n        ]\n\n        def analyze_text(self) -> Data:\n            text = self.input_text\n\n            # Perform text analysis\n            word_count = len(text.split())\n            char_count = len(text)\n            sentence_count = len(re.findall(r'\\w+[.!?]', text))\n\n            # Transform text\n            reversed_text = text[::-1]\n            uppercase_text = text.upper()\n\n            analysis_result = {\n                \"original_text\": text,\n                \"word_count\": word_count,\n                \"character_count\": char_count,\n                \"sentence_count\": sentence_count,\n                \"reversed_text\": reversed_text,\n                \"uppercase_text\": uppercase_text\n            }\n\n            data = Data(value=analysis_result)\n            self.status = data\n            return data\n    ```\n    </details>\n\n3. Enable **Tool Mode** in the custom component.\n4. Connect the custom component's tool output to the **Agent** component's **Tools** input.\n5. Open the  **Playground** and instruct the agent, `Use the text analyzer on this text: \"Agents really are thinking machines!\"`\n\n    Based on your instruction, the agent should call the `analyze_text` action and return the result.\n    For example:\n\n    ```\n    gpt-4o\n    Finished\n    0.6s\n    Here is the analysis of the text \"Agents really are thinking machines!\":\n    Original Text: Agents really are thinking machines!\n    Word Count: 5\n    Character Count: 36\n    Sentence Count: 1\n    Reversed Text: !senihcam gnikniht era yllaer stnegA\n    Uppercase Text: AGENTS REALLY ARE THINKING MACHINES!\n    ```\n\n## Make any component a tool\n\nIf the component you want to use as a tool doesn't have a **Tool Mode** button, add `tool_mode=True` to one of the component's inputs, and connect the new **Toolset** output to the agent's **Tools** input.\n\nLangflow supports **Tool Mode** for the following data types:\n\n* `DataInput`\n* `DataFrameInput`\n* `PromptInput`\n* `MessageTextInput`\n* `MultilineInput`\n* `DropdownInput`\n\nFor example, the example code in [Use custom components as tools](#components-as-tools) included `tool_mode=True` to the `MessageTextInput` input so the custom component could be used as a tool:\n\n```python\ninputs = [\n    MessageTextInput(\n        name=\"input_text\",\n        display_name=\"Input Text\",\n        info=\"Enter text to analyze\",\n        value=\"Hello, World!\",\n        tool_mode=True,\n    ),\n]\n```\n\n## Use flows as tools\n\nAn agent can use your other flows as tools with the [**Run Flow** component](/components-logic#run-flow).\n\n1. Add a **Run Flow** component to your flow.\n2. Select the flow you want the agent to use as a tool.\n3. Enable **Tool Mode**.\nThe selected flow becomes an [action](#edit-a-tools-actions) in the **Run Flow** component.\n4. Connect the **Run Flow** component's **Tool** output to the **Agent** component's **Tools** input.\n5. Open the **Playground**, and then ask the agent, `What tools are you using to answer my questions?`\nYour flow should be visible in the response as an available tool.\n6. Ask the agent a question that specifically uses the connected flow as a tool.\nThe connected flow returns an answer based on your question.\n\n![Run Flow component connected to an Agent component as a tool](/img/agent-example-run-flow-as-tool.png)\n\n## See also\n\n* [Agent components](/components-agents)\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use Langflow as an MCP server](/mcp-server)",
      "summary": "By default, [Langflow agents](/agents) only include the functionality built-in to their base LLM.",
      "keywords": [
        "actions",
        "add",
        "agent",
        "agents",
        "any",
        "api",
        "attach",
        "bundles",
        "code",
        "component",
        "components",
        "configure",
        "core",
        "custom",
        "description",
        "edit",
        "enabled",
        "flow",
        "flows",
        "key"
      ]
    },
    "/agents": {
      "title": "Use Langflow agents",
      "slug": "/agents",
      "category": "agents",
      "content": "Langflow's [**Agent** component](/components-agents) is critical for building agent flows.\nThis component provides everything you need to create an agent, including multiple Large Language Model (LLM) providers, tool calling, and custom instructions.\nIt simplifies agent configuration so you can focus on application development.\n\n## Use the Agent component in a flow\n\nThe following steps explain how to create an agent flow in Langflow from a blank flow.\nFor a prebuilt example, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).\n\n1. Click **New Flow**, and then click **Blank Flow**.\n\n2. Add an **Agent** component to your flow.\n\n3. Select the provider and model that you want to use.\nThe default model for the **Agent** component is an OpenAI model.\nIf you want to use a different provider, edit the **Model Provider** and **Model Name** fields accordingly.\nIf your preferred model isn't listed, type the complete model name into the **Model Name** field, and then select it from the **Model Name** menu.\nMake sure that the model is enabled/verified in your model provider account.\nFor more information, see [Agent component parameters](#agent-component-parameters).\n\n4. Enter a valid credential for your selected model provider.\nMake sure that the credential has permission to call the selected model.\n\n5. Add [**Chat Input** and **Chat Output** components](/components-io) to your flow, and then connect them to the **Agent** component.\n\n    At this point, you have created a basic LLM-based chat flow that you can test in the  **Playground**.\n    However, this flow only chats with the LLM.\n    To enhance this flow and make it truly agentic, add some tools, as explained in the next steps.\n\n    ![A basic agent chat flow with Chat Input, Agent, and Chat Output components.](/img/agent-example-add-chat.png)\n\n6. Add **News Search**, **URL**, and **Calculator** components to your flow.\n7. Enable **Tool Mode** in the **News Search**, **URL**, and **Calculator** components:\n\n    1. Click the **News Search** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.\n    2. Repeat for the **URL** and **Calculator** components.\n    3. Connect the **Toolset** port for each tool component to the **Tools** port on the **Agent** component.\n\n    **Tool Mode** makes a component into a tool by modifying the component's inputs.\n    With **Tool Mode** enabled, a component can accept requests from an **Agent** component to use the component's available actions as tools.\n\n    When in **Tool Mode**, a component has a **Toolset** port that you must connect to an **Agent** component's **Tools** port if you want to allow the agent to use that component's actions as tools.\n\n    For more information, see [Configure tools for agents](/agents-tools).\n\n    ![A more complex agent chat flow where three components are connected to the Agent component as tools](/img/agent-example-add-tools.png)\n\n8. Open the  **Playground**, and then ask the agent, `What tools are you using to answer my questions?`\n\n    The agent should respond with a list of the connected tools.\n    It may also include built-in tools.\n\n    ```text\n    I use a combination of my built-in knowledge (up to June 2024) and a set of external tools to answer your questions. Here are the main types of tools I can use:\n    Web Search & Content Fetching: I can fetch and summarize content from web pages, including crawling links recursively.\n    News Search: I can search for recent news articles using Google News via RSS feeds.\n    Calculator: I can perform arithmetic calculations and evaluate mathematical expressions.\n    Date & Time: I can provide the current date and time in various time zones.\n    These tools help me provide up-to-date information, perform calculations, and retrieve specific data from the internet when needed. If you have a specific question, let me know, and I'll use the most appropriate tool(s) to help!\n    ```\n\n9. To test a specific tool, ask the agent a question that uses one of the tools, such as `Summarize today's tech news`.\n\n    To help you debug and test your flows, the **Playground** displays the agent's tool calls, the provided input, and the raw output the agent received before generating the summary.\n    With the given example, the agent should call the **News Search** component's `search_news` action.\n\nYou've successfully created a basic agent flow that uses some generic tools.\n\nTo continue building on this tutorial, try connecting other tool components or [use Langflow as an MCP client](/mcp-client) to support more complex and specialized tasks.\n\nFor a multi-agent example, see [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool).\n\n## Agent component parameters\n\nYou can configure the **Agent** component to use your preferred provider and model, custom instructions, and tools.\n\n### Provider and model\n\nUse the **Model Provider** (`agent_llm`) and **Model Name** (`llm_model`) settings to select the model provider and LLM that you want the agent to use.\n\nThe **Agent** component includes many models from several popular model providers.\nTo access other providers or models, you can do either of the following:\n\n* Set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).\n* Select your preferred provider, type the complete model name into the **Model Name** field, and then select your custom option from the **Model Name** menu.\nMake sure that the model is enabled/verified in your model provider account.\n\nIf you need to generate embeddings in your flow, use an [embedding model component](/components-embedding-models).\n\n### Model provider API key\n\nIn the **API Key** field, enter a valid authentication key for your selected model provider, if you are using a built-in provider.\nFor example, to use the default OpenAI model, you must provide a valid OpenAI API key for an OpenAI account that has credits and permission to call OpenAI LLMs.\n\nYou can enter the key directly, but it is recommended that you follow industry best practices for storing and referencing API keys.\nFor example, you can use a  [global variable](/configuration-global-variables) or [environment variables](/environment-variables).\nFor more information, see [Add component API keys to Langflow](/api-keys-and-authentication#component-api-keys).\n\nIf you select **Connect other models** as the model provider, authentication is handled in the incoming language model component.\n\n### Agent instructions and input\n\nIn the **Agent Instructions** (`system_prompt`) field, you can provide custom instructions that you want the **Agent** component to use for every conversation.\n\nThese instructions are applied in addition to the **Input** (`input_value`), which can be entered directly or provided through another component, such as a **Chat Input** component.\n\n### Tools\n\nAgents are most useful when they have the appropriate tools available to complete requests.\n\nAn **Agent** component can use any Langflow component as a tool, including other agents and MCP servers.\n\nTo attach a component as a tool, you must enable **Tool Mode** on the component that you want to attach, and then attach it to the **Agent** component's **Tools** port.\nFor more information, see [Configure tools for agents](/agents-tools).\n\n**Note:** To allow agents to use tools from MCP servers, use the [**MCP Tools** component](/components-agents#mcp-connection).\n\n### Agent memory\n\nLangflow agents have built-in chat memory that is enabled by default.\nThis memory allows them to retrieve and reference messages from previous conversations, maintaining a rolling context window for each chat session ID.\n\nChat memories are grouped by [session ID (`session_id`)](/session-id).\nIt is recommended to use custom session IDs if you need to segregate chat memory for different users or applications that run the same flow.\n\nBy default, the **Agent** component uses your Langflow installation's storage, and it retrieves a limited number of chat messages, which you can configure with the **Number of Chat History Messages** parameter.\n\nThe **Message History** component isn't required for default chat memory, but it is required if you want to use external chat memory like Mem0.\nAdditionally, the **Message History** component provides more options for sorting, filtering, and limiting memories. Although, most of these options are built-in to the **Agent** component with default values.\n\nFor more information, see [Store chat memory](/memory#store-chat-memory) and [**Message History** component](/components-helpers#message-history).\n\n### Additional parameters\n\nWith the **Agent** component, the available parameters can change depending on the selected provider and model, including support for additional modes, arguments, or features like chat memory and temperature.\nFor example:\n\n* **Current Date** (`add_current_date_tool`): When enabled (`true`), this setting adds a tool to the agent that can retrieve the current date.\n* **Handle Parse Errors** (`handle_parsing_errors`): When enabled (`true`), this setting allows the agent to fix errors, like typos, when analyzing user input.\n* **Verbose** (`verbose`): When enabled (`true`), this setting records detailed logging output for debugging and analysis.\n\n## Agent component output\n\nThe **Agent** component outputs a **Response** (`response`) that is [`Message` data](/data-types#message) containing the agent's raw response to the query.\n\nTypically, this is passed to a **Chat Output** component to return the response in a human-readable format.\nIt can also be passed to other components if you need to process the response further before, or in addition to, returning it to the user.\n\n## See also\n\n* [**Agent** and **MCP Tools** components](/components-agents)\n* [Configure tools for agents](/agents-tools)",
      "summary": "Langflow's [**Agent** component](/components-agents) is critical for building agent flows.\nThis component provides everything you need to create an agent, including multiple Large Language Model (LLM) providers, tool calling, and custom instructions.\nIt simplifies agent configuration so you can...",
      "keywords": [
        "additional",
        "agent",
        "agents",
        "api",
        "blank",
        "calculator",
        "chat",
        "component",
        "connect",
        "current",
        "date",
        "errors",
        "flow",
        "handle",
        "history",
        "input",
        "instructions",
        "key",
        "langflow",
        "mcp"
      ]
    },
    "agents": {
      "title": "Use Langflow agents",
      "slug": "/agents",
      "category": "agents",
      "content": "Langflow's [**Agent** component](/components-agents) is critical for building agent flows.\nThis component provides everything you need to create an agent, including multiple Large Language Model (LLM) providers, tool calling, and custom instructions.\nIt simplifies agent configuration so you can focus on application development.\n\n## Use the Agent component in a flow\n\nThe following steps explain how to create an agent flow in Langflow from a blank flow.\nFor a prebuilt example, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).\n\n1. Click **New Flow**, and then click **Blank Flow**.\n\n2. Add an **Agent** component to your flow.\n\n3. Select the provider and model that you want to use.\nThe default model for the **Agent** component is an OpenAI model.\nIf you want to use a different provider, edit the **Model Provider** and **Model Name** fields accordingly.\nIf your preferred model isn't listed, type the complete model name into the **Model Name** field, and then select it from the **Model Name** menu.\nMake sure that the model is enabled/verified in your model provider account.\nFor more information, see [Agent component parameters](#agent-component-parameters).\n\n4. Enter a valid credential for your selected model provider.\nMake sure that the credential has permission to call the selected model.\n\n5. Add [**Chat Input** and **Chat Output** components](/components-io) to your flow, and then connect them to the **Agent** component.\n\n    At this point, you have created a basic LLM-based chat flow that you can test in the  **Playground**.\n    However, this flow only chats with the LLM.\n    To enhance this flow and make it truly agentic, add some tools, as explained in the next steps.\n\n    ![A basic agent chat flow with Chat Input, Agent, and Chat Output components.](/img/agent-example-add-chat.png)\n\n6. Add **News Search**, **URL**, and **Calculator** components to your flow.\n7. Enable **Tool Mode** in the **News Search**, **URL**, and **Calculator** components:\n\n    1. Click the **News Search** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.\n    2. Repeat for the **URL** and **Calculator** components.\n    3. Connect the **Toolset** port for each tool component to the **Tools** port on the **Agent** component.\n\n    **Tool Mode** makes a component into a tool by modifying the component's inputs.\n    With **Tool Mode** enabled, a component can accept requests from an **Agent** component to use the component's available actions as tools.\n\n    When in **Tool Mode**, a component has a **Toolset** port that you must connect to an **Agent** component's **Tools** port if you want to allow the agent to use that component's actions as tools.\n\n    For more information, see [Configure tools for agents](/agents-tools).\n\n    ![A more complex agent chat flow where three components are connected to the Agent component as tools](/img/agent-example-add-tools.png)\n\n8. Open the  **Playground**, and then ask the agent, `What tools are you using to answer my questions?`\n\n    The agent should respond with a list of the connected tools.\n    It may also include built-in tools.\n\n    ```text\n    I use a combination of my built-in knowledge (up to June 2024) and a set of external tools to answer your questions. Here are the main types of tools I can use:\n    Web Search & Content Fetching: I can fetch and summarize content from web pages, including crawling links recursively.\n    News Search: I can search for recent news articles using Google News via RSS feeds.\n    Calculator: I can perform arithmetic calculations and evaluate mathematical expressions.\n    Date & Time: I can provide the current date and time in various time zones.\n    These tools help me provide up-to-date information, perform calculations, and retrieve specific data from the internet when needed. If you have a specific question, let me know, and I'll use the most appropriate tool(s) to help!\n    ```\n\n9. To test a specific tool, ask the agent a question that uses one of the tools, such as `Summarize today's tech news`.\n\n    To help you debug and test your flows, the **Playground** displays the agent's tool calls, the provided input, and the raw output the agent received before generating the summary.\n    With the given example, the agent should call the **News Search** component's `search_news` action.\n\nYou've successfully created a basic agent flow that uses some generic tools.\n\nTo continue building on this tutorial, try connecting other tool components or [use Langflow as an MCP client](/mcp-client) to support more complex and specialized tasks.\n\nFor a multi-agent example, see [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool).\n\n## Agent component parameters\n\nYou can configure the **Agent** component to use your preferred provider and model, custom instructions, and tools.\n\n### Provider and model\n\nUse the **Model Provider** (`agent_llm`) and **Model Name** (`llm_model`) settings to select the model provider and LLM that you want the agent to use.\n\nThe **Agent** component includes many models from several popular model providers.\nTo access other providers or models, you can do either of the following:\n\n* Set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).\n* Select your preferred provider, type the complete model name into the **Model Name** field, and then select your custom option from the **Model Name** menu.\nMake sure that the model is enabled/verified in your model provider account.\n\nIf you need to generate embeddings in your flow, use an [embedding model component](/components-embedding-models).\n\n### Model provider API key\n\nIn the **API Key** field, enter a valid authentication key for your selected model provider, if you are using a built-in provider.\nFor example, to use the default OpenAI model, you must provide a valid OpenAI API key for an OpenAI account that has credits and permission to call OpenAI LLMs.\n\nYou can enter the key directly, but it is recommended that you follow industry best practices for storing and referencing API keys.\nFor example, you can use a  [global variable](/configuration-global-variables) or [environment variables](/environment-variables).\nFor more information, see [Add component API keys to Langflow](/api-keys-and-authentication#component-api-keys).\n\nIf you select **Connect other models** as the model provider, authentication is handled in the incoming language model component.\n\n### Agent instructions and input\n\nIn the **Agent Instructions** (`system_prompt`) field, you can provide custom instructions that you want the **Agent** component to use for every conversation.\n\nThese instructions are applied in addition to the **Input** (`input_value`), which can be entered directly or provided through another component, such as a **Chat Input** component.\n\n### Tools\n\nAgents are most useful when they have the appropriate tools available to complete requests.\n\nAn **Agent** component can use any Langflow component as a tool, including other agents and MCP servers.\n\nTo attach a component as a tool, you must enable **Tool Mode** on the component that you want to attach, and then attach it to the **Agent** component's **Tools** port.\nFor more information, see [Configure tools for agents](/agents-tools).\n\n**Note:** To allow agents to use tools from MCP servers, use the [**MCP Tools** component](/components-agents#mcp-connection).\n\n### Agent memory\n\nLangflow agents have built-in chat memory that is enabled by default.\nThis memory allows them to retrieve and reference messages from previous conversations, maintaining a rolling context window for each chat session ID.\n\nChat memories are grouped by [session ID (`session_id`)](/session-id).\nIt is recommended to use custom session IDs if you need to segregate chat memory for different users or applications that run the same flow.\n\nBy default, the **Agent** component uses your Langflow installation's storage, and it retrieves a limited number of chat messages, which you can configure with the **Number of Chat History Messages** parameter.\n\nThe **Message History** component isn't required for default chat memory, but it is required if you want to use external chat memory like Mem0.\nAdditionally, the **Message History** component provides more options for sorting, filtering, and limiting memories. Although, most of these options are built-in to the **Agent** component with default values.\n\nFor more information, see [Store chat memory](/memory#store-chat-memory) and [**Message History** component](/components-helpers#message-history).\n\n### Additional parameters\n\nWith the **Agent** component, the available parameters can change depending on the selected provider and model, including support for additional modes, arguments, or features like chat memory and temperature.\nFor example:\n\n* **Current Date** (`add_current_date_tool`): When enabled (`true`), this setting adds a tool to the agent that can retrieve the current date.\n* **Handle Parse Errors** (`handle_parsing_errors`): When enabled (`true`), this setting allows the agent to fix errors, like typos, when analyzing user input.\n* **Verbose** (`verbose`): When enabled (`true`), this setting records detailed logging output for debugging and analysis.\n\n## Agent component output\n\nThe **Agent** component outputs a **Response** (`response`) that is [`Message` data](/data-types#message) containing the agent's raw response to the query.\n\nTypically, this is passed to a **Chat Output** component to return the response in a human-readable format.\nIt can also be passed to other components if you need to process the response further before, or in addition to, returning it to the user.\n\n## See also\n\n* [**Agent** and **MCP Tools** components](/components-agents)\n* [Configure tools for agents](/agents-tools)",
      "summary": "Langflow's [**Agent** component](/components-agents) is critical for building agent flows.\nThis component provides everything you need to create an agent, including multiple Large Language Model (LLM) providers, tool calling, and custom instructions.\nIt simplifies agent configuration so you can...",
      "keywords": [
        "additional",
        "agent",
        "agents",
        "api",
        "blank",
        "calculator",
        "chat",
        "component",
        "connect",
        "current",
        "date",
        "errors",
        "flow",
        "handle",
        "history",
        "input",
        "instructions",
        "key",
        "langflow",
        "mcp"
      ]
    },
    "/mcp-client": {
      "title": "Use Langflow as an MCP client",
      "slug": "/mcp-client",
      "category": "agents",
      "content": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.\n\nThis page describes how to use Langflow as an MCP client with the [**MCP Tools** component](#use-the-mcp-tools-component) and connected [MCP servers](#manage-connected-mcp-servers).\n\nFor information about using Langflow as an MCP server, see [Use Langflow as an MCP server](/mcp-server).\n\n## Use the MCP tools component\n\nThe **MCP Tools** component connects to an MCP server so that a [Langflow agent](/agents) can use the server's tools when responding to user queries.\n\nThis component has two modes, depending on the type of server you want to access:\n\n* [Connect to a non-Langflow MCP server](#mcp-stdio-mode) with a JSON configuration file, server start command, or SSE URL to access tools provided by external, non-Langflow MCP servers.\n* [Connect to a Langflow MCP server](#mcp-sse-mode) to use flows from your [Langflow projects](/concepts-flows#projects) as MCP tools.\n\n### Connect to a non-Langflow MCP server {#mcp-stdio-mode}\n\n1. Add an **MCP Tools** component to your flow.\n\n2. In the **MCP Server** field, select a previously connected server or click  **Add MCP Server**.\n\n    There are multiple ways to add a new server:\n\n    * **JSON**: Paste the MCP server's JSON configuration object into the field, including required and optional parameters that you want to use, and then click **Add Server**.\n    * **STDIO**: Enter the MCP server's **Name**, **Command**, and any **Arguments** and **Environment Variables** the server uses, and then click **Add Server**.\n    For example, to start a [Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) server, the **Command** is `uvx mcp-server-fetch`.\n    * **SSE**: Enter your Langflow MCP server's **Name**, **SSE URL**, and any **Headers** and **Environment Variables** the server uses, and then click **Add Server**.\n    The default **SSE URL** is `http://localhost:7860/api/v1/mcp/sse`. For more information, see [Use SSE mode](#mcp-sse-mode).\n\n    \n\n3. To use environment variables in your server command, enter each variable in the **Env** fields as key-value pairs.\n\n    \n**Note:** Langflow passes environment variables from the `.env` file to MCP, but it doesn't pass global variables declared in your Langflow **Settings**.\n    To define an MCP server environment variable as a global variable, add it to Langflow's `.env` file at startup.\n    For more information, see [global variables](/configuration-global-variables).\n    \n\n4. In the **Tool** field, select a tool that you want this component to use, or leave the field blank to allow access to all tools provided by the MCP server.\n\n    If you select a specific tool, you might need to configure additional tool-specific fields. For information about tool-specific fields, see your MCP server's documentation.\n\n    At this point, the **MCP Tools** component is serving a tool, but nothing is using the tool. The next steps explain how to make the tool available to an [**Agent** component](/components-agents) so that the agent can use the tool in its responses.\n\n5. In the [component's header menu](/concepts-components#component-menus), enable **Tool mode** so you can use the component with an agent.\n\n6. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.\n\n    If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.\n\n    ![MCP Tools component in STDIO mode](/img/component-mcp-stdio.png)\n\n7.  Test your flow to make sure the MCP server is connected and the selected tool is used by the agent. Open the **Playground**, and then enter a prompt that uses the tool you connected through the **MCP Tools** component.\n\n    For example, if you use `mcp-server-fetch` with the `fetch` tool, you could ask the agent to summarize recent tech news. The agent calls the MCP server function `fetch`, and then returns the response.\n\n8. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.\n\n### Connect a Langflow MCP server {#mcp-sse-mode}\n\nEvery Langflow project runs a separate MCP server that exposes the project's flows as MCP tools.\nFor more information about your projects' MCP servers, including exposing flows as MCP tools, see [Use Langflow as an MCP server](/mcp-server).\n\nTo leverage flows-as-tools, use the **MCP Tools** component in **Server-Sent Events (SSE)** mode to connect to a project's `/api/v1/mcp/sse` endpoint:\n\n1. Add an **MCP Tools** component to your flow, click  **Add MCP Server**, and then select **SSE** mode.\n2. In the **MCP SSE URL** field, modify the default address to point at your Langflow server's SSE endpoint. The default value for other Langflow installations is `http://localhost:7860/api/v1/mcp/sse`.\nIn SSE mode, all flows available from the targeted server are treated as tools.\n3. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode** so you can use the component with an agent.\n4. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.\n5. If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.\n\n    ![MCP component with SSE mode enabled](/img/component-mcp-sse-mode.png)\n\n6. Test your flow to make sure the agent uses your flows to respond to queries. Open the **Playground**, and then enter a prompt that uses a flow that you connected through the **MCP Tools** component.\n\n7. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.\n\n## MCP Tools parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| command | String | Input parameter. Stdio mode only. The MCP server startup command. Default: `uvx mcp-sse-shim@latest`. |\n| sse_url | String | Input parameter. SSE mode only. The SSE URL for a Langflow project's MCP server. Default for Langflow Desktop: `http://localhost:7868/`. Default for other installations: `http://localhost:7860/api/v1/mcp/sse` |\n| tools | List[Tool] | Output parameter. [`Tool`](/data-types#tool) object containing a list of tools exposed by the MCP server. |\n\n## Manage connected MCP servers\n\nTo manage all MCP server connections for your Langflow client, click  **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.\n\nTo add a new MCP server, click **Add MCP Server**, and then follow the steps in [Use the MCP Tools component](#use-the-mcp-tools-component) to configure the connection and use the server in a flow.\n\nClick  **More** to edit or delete an MCP server connection.\n\n## See also\n\n- [Use Langflow as an MCP server](/mcp-server)\n- [Use a DataStax Astra DB MCP server with the MCP Tools component](/mcp-component-astra)",
      "summary": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.",
      "keywords": [
        "add",
        "agent",
        "arguments",
        "chat",
        "client",
        "command",
        "component",
        "connect",
        "connected",
        "env",
        "environment",
        "events",
        "headers",
        "input",
        "json",
        "langflow",
        "manage",
        "mcp",
        "mode",
        "name"
      ]
    },
    "mcp-client": {
      "title": "Use Langflow as an MCP client",
      "slug": "/mcp-client",
      "category": "agents",
      "content": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.\n\nThis page describes how to use Langflow as an MCP client with the [**MCP Tools** component](#use-the-mcp-tools-component) and connected [MCP servers](#manage-connected-mcp-servers).\n\nFor information about using Langflow as an MCP server, see [Use Langflow as an MCP server](/mcp-server).\n\n## Use the MCP tools component\n\nThe **MCP Tools** component connects to an MCP server so that a [Langflow agent](/agents) can use the server's tools when responding to user queries.\n\nThis component has two modes, depending on the type of server you want to access:\n\n* [Connect to a non-Langflow MCP server](#mcp-stdio-mode) with a JSON configuration file, server start command, or SSE URL to access tools provided by external, non-Langflow MCP servers.\n* [Connect to a Langflow MCP server](#mcp-sse-mode) to use flows from your [Langflow projects](/concepts-flows#projects) as MCP tools.\n\n### Connect to a non-Langflow MCP server {#mcp-stdio-mode}\n\n1. Add an **MCP Tools** component to your flow.\n\n2. In the **MCP Server** field, select a previously connected server or click  **Add MCP Server**.\n\n    There are multiple ways to add a new server:\n\n    * **JSON**: Paste the MCP server's JSON configuration object into the field, including required and optional parameters that you want to use, and then click **Add Server**.\n    * **STDIO**: Enter the MCP server's **Name**, **Command**, and any **Arguments** and **Environment Variables** the server uses, and then click **Add Server**.\n    For example, to start a [Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) server, the **Command** is `uvx mcp-server-fetch`.\n    * **SSE**: Enter your Langflow MCP server's **Name**, **SSE URL**, and any **Headers** and **Environment Variables** the server uses, and then click **Add Server**.\n    The default **SSE URL** is `http://localhost:7860/api/v1/mcp/sse`. For more information, see [Use SSE mode](#mcp-sse-mode).\n\n    \n\n3. To use environment variables in your server command, enter each variable in the **Env** fields as key-value pairs.\n\n    \n**Note:** Langflow passes environment variables from the `.env` file to MCP, but it doesn't pass global variables declared in your Langflow **Settings**.\n    To define an MCP server environment variable as a global variable, add it to Langflow's `.env` file at startup.\n    For more information, see [global variables](/configuration-global-variables).\n    \n\n4. In the **Tool** field, select a tool that you want this component to use, or leave the field blank to allow access to all tools provided by the MCP server.\n\n    If you select a specific tool, you might need to configure additional tool-specific fields. For information about tool-specific fields, see your MCP server's documentation.\n\n    At this point, the **MCP Tools** component is serving a tool, but nothing is using the tool. The next steps explain how to make the tool available to an [**Agent** component](/components-agents) so that the agent can use the tool in its responses.\n\n5. In the [component's header menu](/concepts-components#component-menus), enable **Tool mode** so you can use the component with an agent.\n\n6. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.\n\n    If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.\n\n    ![MCP Tools component in STDIO mode](/img/component-mcp-stdio.png)\n\n7.  Test your flow to make sure the MCP server is connected and the selected tool is used by the agent. Open the **Playground**, and then enter a prompt that uses the tool you connected through the **MCP Tools** component.\n\n    For example, if you use `mcp-server-fetch` with the `fetch` tool, you could ask the agent to summarize recent tech news. The agent calls the MCP server function `fetch`, and then returns the response.\n\n8. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.\n\n### Connect a Langflow MCP server {#mcp-sse-mode}\n\nEvery Langflow project runs a separate MCP server that exposes the project's flows as MCP tools.\nFor more information about your projects' MCP servers, including exposing flows as MCP tools, see [Use Langflow as an MCP server](/mcp-server).\n\nTo leverage flows-as-tools, use the **MCP Tools** component in **Server-Sent Events (SSE)** mode to connect to a project's `/api/v1/mcp/sse` endpoint:\n\n1. Add an **MCP Tools** component to your flow, click  **Add MCP Server**, and then select **SSE** mode.\n2. In the **MCP SSE URL** field, modify the default address to point at your Langflow server's SSE endpoint. The default value for other Langflow installations is `http://localhost:7860/api/v1/mcp/sse`.\nIn SSE mode, all flows available from the targeted server are treated as tools.\n3. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode** so you can use the component with an agent.\n4. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.\n5. If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.\n\n    ![MCP component with SSE mode enabled](/img/component-mcp-sse-mode.png)\n\n6. Test your flow to make sure the agent uses your flows to respond to queries. Open the **Playground**, and then enter a prompt that uses a flow that you connected through the **MCP Tools** component.\n\n7. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.\n\n## MCP Tools parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| command | String | Input parameter. Stdio mode only. The MCP server startup command. Default: `uvx mcp-sse-shim@latest`. |\n| sse_url | String | Input parameter. SSE mode only. The SSE URL for a Langflow project's MCP server. Default for Langflow Desktop: `http://localhost:7868/`. Default for other installations: `http://localhost:7860/api/v1/mcp/sse` |\n| tools | List[Tool] | Output parameter. [`Tool`](/data-types#tool) object containing a list of tools exposed by the MCP server. |\n\n## Manage connected MCP servers\n\nTo manage all MCP server connections for your Langflow client, click  **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.\n\nTo add a new MCP server, click **Add MCP Server**, and then follow the steps in [Use the MCP Tools component](#use-the-mcp-tools-component) to configure the connection and use the server in a flow.\n\nClick  **More** to edit or delete an MCP server connection.\n\n## See also\n\n- [Use Langflow as an MCP server](/mcp-server)\n- [Use a DataStax Astra DB MCP server with the MCP Tools component](/mcp-component-astra)",
      "summary": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.",
      "keywords": [
        "add",
        "agent",
        "arguments",
        "chat",
        "client",
        "command",
        "component",
        "connect",
        "connected",
        "env",
        "environment",
        "events",
        "headers",
        "input",
        "json",
        "langflow",
        "manage",
        "mcp",
        "mode",
        "name"
      ]
    },
    "/mcp-component-astra": {
      "title": "Connect an Astra DB MCP server to Langflow",
      "slug": "/mcp-component-astra",
      "category": "agents",
      "content": "This guide demonstrates how to [use Langflow as an MCP client](/mcp-client) by using the **MCP Tools** component to run a [DataStax Astra DB MCP server](https://github.com/datastax/astra-db-mcp) in an agent flow.\n\n1. Install an LTS release of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\n2. Create an [OpenAI](https://platform.openai.com/) API key.\n\n3. Create an [Astra DB Serverless (Vector) database](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html#create-vector-database), if you don't already have one.\n\n4. Get your database's Astra DB API endpoint and an Astra application token with the **Database Administrator** role. For more information, see [Generate an application token for a database](https://docs.datastax.com/en/astra-db-serverless/administration/manage-application-tokens.html#database-token).\n\n5. To follow along with this guide, create a flow based on the **Simple Agent** template.\n\n    You can also use an existing flow or create a blank flow.\n\n6. Remove the **URL** tool, and then replace it with an [**MCP Tools** component](/mcp-client).\n\n7. Configure the **MCP Tools** component as follows:\n\n    1. Select **Stdio** mode.\n    2. In the **MCP server** field, add the following code to connect to an Astra DB MCP server:\n\n        ```bash\n        npx -y @datastax/astra-db-mcp\n        ```\n\n    3. In the **Env** fields, add variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT` with the values from your Astra database.\n\n        \n**Note:** Environment variables declared in your Langflow `.env` file can be referenced in your MCP server commands, but you cannot reference global variables declared in Langflow.\n\n        If you want to use variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT`, add them to Langflow's `.env` file, and then restart Langflow.\n        For more information, see [global variables](/configuration-global-variables).\n        \n\n        Add each variable separately.\n        To add another variable field click  **Add More**.\n\n        ```bash\n        ASTRA_DB_APPLICATION_TOKEN=AstraCS:...\n        ```\n\n        ```bash\n        ASTRA_DB_API_ENDPOINT=https://...-us-east-2.apps.astra.datastax.com\n        ```\n\n8. In the **Agent** component, add your OpenAI API key.\n\n    The default model is an OpenAI model.\n    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n    ![The Simple Agent flow with the URL tool replaced by an MCP Tools component, and the MCP Tools component launching an Astra DB MCP server](/img/component-mcp-astra-db.png)\n\n9. Open the **Playground**, and then ask the agent, `What collections are available?`\n\n    Since Langflow is connected to your Astra DB database through the MCP server, the agent chooses the correct tool and connects to your database to retrieve the answer.\n    For example:\n\n    ```text\n    The available collections in your database are:\n    collection_002\n    hardware_requirements\n    load_collection\n    nvidia_collection\n    software_requirements\n    ```",
      "summary": "This guide demonstrates how to [use Langflow as an MCP client](/mcp-client) by using the **MCP Tools** component to run a [DataStax Astra DB MCP server](https://github.com/datastax/astra-db-mcp) in an agent flow.",
      "keywords": [
        "add",
        "administrator",
        "agent",
        "api",
        "astra",
        "connect",
        "database",
        "env",
        "key",
        "langflow",
        "mcp",
        "model",
        "name",
        "note",
        "playground",
        "provider",
        "server",
        "simple",
        "stdio",
        "tools"
      ]
    },
    "mcp-component-astra": {
      "title": "Connect an Astra DB MCP server to Langflow",
      "slug": "/mcp-component-astra",
      "category": "agents",
      "content": "This guide demonstrates how to [use Langflow as an MCP client](/mcp-client) by using the **MCP Tools** component to run a [DataStax Astra DB MCP server](https://github.com/datastax/astra-db-mcp) in an agent flow.\n\n1. Install an LTS release of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\n2. Create an [OpenAI](https://platform.openai.com/) API key.\n\n3. Create an [Astra DB Serverless (Vector) database](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html#create-vector-database), if you don't already have one.\n\n4. Get your database's Astra DB API endpoint and an Astra application token with the **Database Administrator** role. For more information, see [Generate an application token for a database](https://docs.datastax.com/en/astra-db-serverless/administration/manage-application-tokens.html#database-token).\n\n5. To follow along with this guide, create a flow based on the **Simple Agent** template.\n\n    You can also use an existing flow or create a blank flow.\n\n6. Remove the **URL** tool, and then replace it with an [**MCP Tools** component](/mcp-client).\n\n7. Configure the **MCP Tools** component as follows:\n\n    1. Select **Stdio** mode.\n    2. In the **MCP server** field, add the following code to connect to an Astra DB MCP server:\n\n        ```bash\n        npx -y @datastax/astra-db-mcp\n        ```\n\n    3. In the **Env** fields, add variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT` with the values from your Astra database.\n\n        \n**Note:** Environment variables declared in your Langflow `.env` file can be referenced in your MCP server commands, but you cannot reference global variables declared in Langflow.\n\n        If you want to use variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT`, add them to Langflow's `.env` file, and then restart Langflow.\n        For more information, see [global variables](/configuration-global-variables).\n        \n\n        Add each variable separately.\n        To add another variable field click  **Add More**.\n\n        ```bash\n        ASTRA_DB_APPLICATION_TOKEN=AstraCS:...\n        ```\n\n        ```bash\n        ASTRA_DB_API_ENDPOINT=https://...-us-east-2.apps.astra.datastax.com\n        ```\n\n8. In the **Agent** component, add your OpenAI API key.\n\n    The default model is an OpenAI model.\n    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n    ![The Simple Agent flow with the URL tool replaced by an MCP Tools component, and the MCP Tools component launching an Astra DB MCP server](/img/component-mcp-astra-db.png)\n\n9. Open the **Playground**, and then ask the agent, `What collections are available?`\n\n    Since Langflow is connected to your Astra DB database through the MCP server, the agent chooses the correct tool and connects to your database to retrieve the answer.\n    For example:\n\n    ```text\n    The available collections in your database are:\n    collection_002\n    hardware_requirements\n    load_collection\n    nvidia_collection\n    software_requirements\n    ```",
      "summary": "This guide demonstrates how to [use Langflow as an MCP client](/mcp-client) by using the **MCP Tools** component to run a [DataStax Astra DB MCP server](https://github.com/datastax/astra-db-mcp) in an agent flow.",
      "keywords": [
        "add",
        "administrator",
        "agent",
        "api",
        "astra",
        "connect",
        "database",
        "env",
        "key",
        "langflow",
        "mcp",
        "model",
        "name",
        "note",
        "playground",
        "provider",
        "server",
        "simple",
        "stdio",
        "tools"
      ]
    },
    "/mcp-server": {
      "title": "Use Langflow as an MCP server",
      "slug": "/mcp-server",
      "category": "agents",
      "content": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.\n\nThis page describes how to use Langflow as an MCP server that exposes your flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use when generating responses.\n\nFor information about using Langflow as an MCP client and managing MCP server connections within flows, see [Use Langflow as an MCP client](/mcp-client).\n\n## Prerequisites\n\n* A [Langflow project](/concepts-flows#projects) with at least one flow that has a [**Chat Output** component](/components-io#chat-output).\n\n    The **Chat Output** component is required to use a flow as an MCP tool.\n\n* Any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer if you want to use MCP Inspector to [test and debug flows](#test-and-debug-flows).\n\n* [ngrok installed](https://ngrok.com/docs/getting-started/#1-install-ngrok) and an [ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken) if you want to [deploy a public Langflow server](/deployment-public-server).\n\n## Serve flows as MCP tools {#select-flows-to-serve}\n\nEach [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as tools for use by MCP clients.\n\nBy default, all flows in a project are exposed as tools on the project's MCP server.\nYou can change the exposed flows and tool metadata by managing the MCP server settings:\n\n1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.\n\n    ![MCP server projects page](/img/mcp-server.png)\n\n    The **Flows/Tools** section lists the flows that are currently being served as tools on this MCP server.\n\n2. To toggle exposed flows, click  **Edit Tools**, and then select the flows that you want exposed as tools.\nTo prevent a flow from being used as a tool, clear the checkbox in the first column.\n\n3. Close the **MCP Server Tools** dialog to save your changes.\n\n    ![MCP Server Tools](/img/mcp-server-tools.png)\n\n### Edit flow tool names and descriptions\n\nTool names and descriptions help MCP clients determine which actions your flows provide and when to use those actions.\nIt is recommended to provide clear, descriptive names and descriptions for all tools that you serve to MCP clients.\n\nTo edit the names and descriptions of flow tools on a Langflow MCP server, do the following:\n\n1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.\n\n2. Click  **Edit Tools**.\n\n3. Click the **Description** or **Tool** that you want to edit:\n\n    - **Tool name**: Enter a name that makes it clear what the flow does when used as a tool by an agent.\n\n    - **Tool description**: Enter a description that completely and accurately describes the specific actions the flow performs.\n\n4. Close the **MCP Server Tools** dialog to save your changes.\n\n#### Importance of tool names and descriptions\n\nMCP clients use tool names and descriptions to determine which actions to use when generating responses.\n\nBecause MCP clients treat your Langflow project as a single MCP server with all of your enabled flows listed as tools, unclear names and descriptions can cause the agent to select tools incorrectly or inconsistently.\n\nFor example, a flow's default tool name is the flow ID, such as `adbbf8c7-0a34-493b-90ea-5e8b42f78b66`.\nThis provides no information to an agent about the type of flow or its purpose.\n\nTo provide more context about your flows, make sure to name and describe your flows clearly when configuring your Langflow project's MCP server.\n\nThink of these names and descriptions as function names and code comments.\nUse clear statements to describe the problems your flows solve.\n\n<details>\n<summary>Example: Tool name and description usage</summary>\n\nFor example, assume you create a flow based on the **Document Q&A** template that uses an LLM to chat about resumes, and then you give the flow the following name and description:\n\n- **Tool name**: `document_qa_for_resume`\n\n- **Tool description**: `A flow for analyzing Emily's resume.`\n\nAfter connecting your Langflow MCP server to Cursor, you can ask Cursor about the resume, such as `What job experience does Emily have?`.\nUsing the context provided by your tool name and description, the agent can decide to use the `document_qa_for_resume` MCP tool to create a response about Emily's resume.\nIf necessary, the agent asks permission to use the flow tool before generating the response.\n\nIf you ask about a different resume, such as `What job experience does Alex have?`, the agent can decide that `document_qa_for_resume` isn't relevant to this request, because the tool description specifies that the flow is for Emily's resume.\nIn this case, the agent might use another available tool, or it can inform you that it doesn't have access to information about Alex's.\nFor example:\n\n```\nI notice you're asking about Alex's job experience.\nBased on the available tools, I can see there is a Document QA for Resume flow that's designed for analyzing resumes.\nHowever, the description mentions it's for \"Emily's resume\" not Alex's. I don't have access to Alex's resume or job experience information.\n```\n\n</details>\n\n{/* The anchor on this section (connect-clients-to-use-the-servers-actions) is currently a link target in the Langflow UI. Do not change. */}\n## Connect clients to your Langflow MCP server {#connect-clients-to-use-the-servers-actions}\n\nLangflow provides automatic installation and code snippets to help you deploy your Langflow MCP servers to your local MCP clients.\n\n</Tabs>\n\nOnce your MCP client is connected to your Langflow project's MCP server, your flows are registered as tools.\nCursor determines when to use tools based on your queries, and requests permissions when necessary.\nFor more information, see the MCP documentation for your client, such as [Cursor's MCP documentation](https://docs.cursor.com/context/model-context-protocol).\n\n## MCP server authentication {#authentication}\n\nEach [Langflow project](/concepts-flows#projects) has its own MCP server with its own MCP server authentication settings.\n\nTo configure authentication for a Langflow MCP server, go to the **Projects** page in Langflow, click the **MCP Server** tab, click  **Edit Auth**, and then select your preferred authentication method:\n\n</Tabs>\n\n## MCP server environment variables\n\nThe following environment variables set behaviors related to your Langflow projects' MCP servers:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_MCP_SERVER_ENABLED` | Boolean | `True` | Whether to initialize an MCP server for each of your Langflow projects. If `false`, Langflow doesn't initialize MCP servers. |\n| `LANGFLOW_MCP_SERVER_ENABLE_PROGRESS_NOTIFICATIONS` | Boolean | `False` | If `true`, Langflow MCP servers send progress notifications. |\n| `LANGFLOW_MCP_SERVER_TIMEOUT` | Integer | `20` | The number of seconds to wait before an MCP server operation expires due to poor connectivity or long-running requests. |\n| `LANGFLOW_MCP_MAX_SESSIONS_PER_SERVER` | Integer | `10` | Maximum number of MCP sessions to keep per unique server. |\n\n{/* The anchor on this section (deploy-your-server-externally) is currently a link target in the Langflow UI. Do not change. */}\n### Deploy your Langflow MCP server externally {#deploy-your-server-externally}\n\nTo deploy your Langflow MCP server externally, see [Deploy a public Langflow server](/deployment-public-server).\n\n## Use MCP Inspector to test and debug flows {#test-and-debug-flows}\n\n**Note:** Node prerequisite\nMCP Inspector requires any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer.\n\n[MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) is a common tool for testing and debugging MCP servers.\nYou can use MCP Inspector to monitor your flows and get insights into how they are being consumed by the MCP server.\n\n1. Install MCP Inspector:\n\n    ```bash\n    npx @modelcontextprotocol/inspector\n    ```\n\n    For more information about configuring MCP Inspector, including specifying a proxy port, see the [MCP Inspector GitHub project](https://github.com/modelcontextprotocol/inspector).\n\n2. Open a web browser and navigate to the MCP Inspector UI.\nThe default address is `http://localhost:6274`.\n\n3. In the MCP Inspector UI, enter the connection details for your Langflow project's MCP server.\n    The field values depend on your server's method of [authentication](#authentication).\n    \n      \n      \n    </Tabs>\n\n4. Click **Connect**.\n\n    If the connection was successful, you should see your project's flows in the **Tools** tab.\n    From this tab, you can monitor how your flows are being registered as tools by MCP, as well as test the tools with custom input values.\n\n5. To quit MCP Inspector, press <kbd>Control+C</kbd> in the same terminal window where you started it.\n\n## Troubleshoot Langflow MCP servers {#troubleshooting-mcp-server}\n\nFor troubleshooting advice for MCP servers and clients, see [Troubleshoot Langflow: MCP issues](/troubleshoot#mcp).\n\n## See also\n\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use a DataStax Astra DB MCP server](/mcp-component-astra)\n* [MCP server environment variables](/environment-variables#mcp)",
      "summary": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.",
      "keywords": [
        "actions",
        "auth",
        "authentication",
        "chat",
        "clients",
        "connect",
        "debug",
        "deploy",
        "description",
        "descriptions",
        "document",
        "edit",
        "environment",
        "externally",
        "flow",
        "flows",
        "importance",
        "inspector",
        "langflow",
        "mcp"
      ]
    },
    "mcp-server": {
      "title": "Use Langflow as an MCP server",
      "slug": "/mcp-server",
      "category": "agents",
      "content": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.\n\nThis page describes how to use Langflow as an MCP server that exposes your flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use when generating responses.\n\nFor information about using Langflow as an MCP client and managing MCP server connections within flows, see [Use Langflow as an MCP client](/mcp-client).\n\n## Prerequisites\n\n* A [Langflow project](/concepts-flows#projects) with at least one flow that has a [**Chat Output** component](/components-io#chat-output).\n\n    The **Chat Output** component is required to use a flow as an MCP tool.\n\n* Any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer if you want to use MCP Inspector to [test and debug flows](#test-and-debug-flows).\n\n* [ngrok installed](https://ngrok.com/docs/getting-started/#1-install-ngrok) and an [ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken) if you want to [deploy a public Langflow server](/deployment-public-server).\n\n## Serve flows as MCP tools {#select-flows-to-serve}\n\nEach [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as tools for use by MCP clients.\n\nBy default, all flows in a project are exposed as tools on the project's MCP server.\nYou can change the exposed flows and tool metadata by managing the MCP server settings:\n\n1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.\n\n    ![MCP server projects page](/img/mcp-server.png)\n\n    The **Flows/Tools** section lists the flows that are currently being served as tools on this MCP server.\n\n2. To toggle exposed flows, click  **Edit Tools**, and then select the flows that you want exposed as tools.\nTo prevent a flow from being used as a tool, clear the checkbox in the first column.\n\n3. Close the **MCP Server Tools** dialog to save your changes.\n\n    ![MCP Server Tools](/img/mcp-server-tools.png)\n\n### Edit flow tool names and descriptions\n\nTool names and descriptions help MCP clients determine which actions your flows provide and when to use those actions.\nIt is recommended to provide clear, descriptive names and descriptions for all tools that you serve to MCP clients.\n\nTo edit the names and descriptions of flow tools on a Langflow MCP server, do the following:\n\n1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.\n\n2. Click  **Edit Tools**.\n\n3. Click the **Description** or **Tool** that you want to edit:\n\n    - **Tool name**: Enter a name that makes it clear what the flow does when used as a tool by an agent.\n\n    - **Tool description**: Enter a description that completely and accurately describes the specific actions the flow performs.\n\n4. Close the **MCP Server Tools** dialog to save your changes.\n\n#### Importance of tool names and descriptions\n\nMCP clients use tool names and descriptions to determine which actions to use when generating responses.\n\nBecause MCP clients treat your Langflow project as a single MCP server with all of your enabled flows listed as tools, unclear names and descriptions can cause the agent to select tools incorrectly or inconsistently.\n\nFor example, a flow's default tool name is the flow ID, such as `adbbf8c7-0a34-493b-90ea-5e8b42f78b66`.\nThis provides no information to an agent about the type of flow or its purpose.\n\nTo provide more context about your flows, make sure to name and describe your flows clearly when configuring your Langflow project's MCP server.\n\nThink of these names and descriptions as function names and code comments.\nUse clear statements to describe the problems your flows solve.\n\n<details>\n<summary>Example: Tool name and description usage</summary>\n\nFor example, assume you create a flow based on the **Document Q&A** template that uses an LLM to chat about resumes, and then you give the flow the following name and description:\n\n- **Tool name**: `document_qa_for_resume`\n\n- **Tool description**: `A flow for analyzing Emily's resume.`\n\nAfter connecting your Langflow MCP server to Cursor, you can ask Cursor about the resume, such as `What job experience does Emily have?`.\nUsing the context provided by your tool name and description, the agent can decide to use the `document_qa_for_resume` MCP tool to create a response about Emily's resume.\nIf necessary, the agent asks permission to use the flow tool before generating the response.\n\nIf you ask about a different resume, such as `What job experience does Alex have?`, the agent can decide that `document_qa_for_resume` isn't relevant to this request, because the tool description specifies that the flow is for Emily's resume.\nIn this case, the agent might use another available tool, or it can inform you that it doesn't have access to information about Alex's.\nFor example:\n\n```\nI notice you're asking about Alex's job experience.\nBased on the available tools, I can see there is a Document QA for Resume flow that's designed for analyzing resumes.\nHowever, the description mentions it's for \"Emily's resume\" not Alex's. I don't have access to Alex's resume or job experience information.\n```\n\n</details>\n\n{/* The anchor on this section (connect-clients-to-use-the-servers-actions) is currently a link target in the Langflow UI. Do not change. */}\n## Connect clients to your Langflow MCP server {#connect-clients-to-use-the-servers-actions}\n\nLangflow provides automatic installation and code snippets to help you deploy your Langflow MCP servers to your local MCP clients.\n\n</Tabs>\n\nOnce your MCP client is connected to your Langflow project's MCP server, your flows are registered as tools.\nCursor determines when to use tools based on your queries, and requests permissions when necessary.\nFor more information, see the MCP documentation for your client, such as [Cursor's MCP documentation](https://docs.cursor.com/context/model-context-protocol).\n\n## MCP server authentication {#authentication}\n\nEach [Langflow project](/concepts-flows#projects) has its own MCP server with its own MCP server authentication settings.\n\nTo configure authentication for a Langflow MCP server, go to the **Projects** page in Langflow, click the **MCP Server** tab, click  **Edit Auth**, and then select your preferred authentication method:\n\n</Tabs>\n\n## MCP server environment variables\n\nThe following environment variables set behaviors related to your Langflow projects' MCP servers:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_MCP_SERVER_ENABLED` | Boolean | `True` | Whether to initialize an MCP server for each of your Langflow projects. If `false`, Langflow doesn't initialize MCP servers. |\n| `LANGFLOW_MCP_SERVER_ENABLE_PROGRESS_NOTIFICATIONS` | Boolean | `False` | If `true`, Langflow MCP servers send progress notifications. |\n| `LANGFLOW_MCP_SERVER_TIMEOUT` | Integer | `20` | The number of seconds to wait before an MCP server operation expires due to poor connectivity or long-running requests. |\n| `LANGFLOW_MCP_MAX_SESSIONS_PER_SERVER` | Integer | `10` | Maximum number of MCP sessions to keep per unique server. |\n\n{/* The anchor on this section (deploy-your-server-externally) is currently a link target in the Langflow UI. Do not change. */}\n### Deploy your Langflow MCP server externally {#deploy-your-server-externally}\n\nTo deploy your Langflow MCP server externally, see [Deploy a public Langflow server](/deployment-public-server).\n\n## Use MCP Inspector to test and debug flows {#test-and-debug-flows}\n\n**Note:** Node prerequisite\nMCP Inspector requires any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer.\n\n[MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) is a common tool for testing and debugging MCP servers.\nYou can use MCP Inspector to monitor your flows and get insights into how they are being consumed by the MCP server.\n\n1. Install MCP Inspector:\n\n    ```bash\n    npx @modelcontextprotocol/inspector\n    ```\n\n    For more information about configuring MCP Inspector, including specifying a proxy port, see the [MCP Inspector GitHub project](https://github.com/modelcontextprotocol/inspector).\n\n2. Open a web browser and navigate to the MCP Inspector UI.\nThe default address is `http://localhost:6274`.\n\n3. In the MCP Inspector UI, enter the connection details for your Langflow project's MCP server.\n    The field values depend on your server's method of [authentication](#authentication).\n    \n      \n      \n    </Tabs>\n\n4. Click **Connect**.\n\n    If the connection was successful, you should see your project's flows in the **Tools** tab.\n    From this tab, you can monitor how your flows are being registered as tools by MCP, as well as test the tools with custom input values.\n\n5. To quit MCP Inspector, press <kbd>Control+C</kbd> in the same terminal window where you started it.\n\n## Troubleshoot Langflow MCP servers {#troubleshooting-mcp-server}\n\nFor troubleshooting advice for MCP servers and clients, see [Troubleshoot Langflow: MCP issues](/troubleshoot#mcp).\n\n## See also\n\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use a DataStax Astra DB MCP server](/mcp-component-astra)\n* [MCP server environment variables](/environment-variables#mcp)",
      "summary": "Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.",
      "keywords": [
        "actions",
        "auth",
        "authentication",
        "chat",
        "clients",
        "connect",
        "debug",
        "deploy",
        "description",
        "descriptions",
        "document",
        "edit",
        "environment",
        "externally",
        "flow",
        "flows",
        "importance",
        "inspector",
        "langflow",
        "mcp"
      ]
    },
    "/bundles-cometapi": {
      "title": "CometAPI",
      "slug": "/bundles-cometapi",
      "category": "components",
      "content": "[Bundles](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **CometAPI** bundle.\n\nFor more information about CometAPI features and functionality used by CometAPI components, see the [CometAPI documentation](https://www.cometapi.com/).\n\n## CometAPI text generation\n\nThis component generates text using CometAPI's language models through the CometAPI OpenAI-compatible endpoints.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a CometAPI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Transform** component.\n\nFor more information, see [Language model components](/components-models).\n\n### CometAPI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your CometAPI API Key for authentication. |\n| model_name | String | Input parameter. The id of the CometAPI model to use. The component automatically fetches the latest available models from CometAPI when you provide a valid API key. For information about supported models, see the [CometAPI documentation](https://www.cometapi.com/). |\n| input_value | String | Input parameter. The input text to send to the model. |\n| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.1. |\n| seed | Integer | Input parameter. The seed controls the reproducibility of the job (advanced). |\n| model_kwargs | Dict | Input parameter. Additional keyword arguments to pass to the model (advanced). |\n| json_mode | Boolean | Input parameter. If True, it will output JSON regardless of passing a schema (advanced). |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: false. |\n| output_parser | OutputParser | Input parameter. The parser to use to parse the output of the model (advanced). |\n| model | LanguageModel | Output parameter. An instance of ChatOpenAI configured with CometAPI parameters. |\n\n## Use CometAPI in a flow\n\n1. Sign up for a [CometAPI account](https://www.cometapi.com/).\n2. Obtain your API key from the CometAPI dashboard.\n3. In Langflow, add the **CometAPI** component to your flow.\n4. Enter your API key in the **CometAPI API Key** field.\n5. Select your preferred model from the **Model Name** menu.\n6. Configure other parameters as needed for your use case.\n7. Add other components to your flow as needed.\n\n    To perform a basic test, add **Chat Input** and **Chat Output** components to your flow, connect them to the **CometAPI** component accordingly, and then click **Playground** to test the connection and chat with your model.\n\n    For more advanced use cases, you can connect the CometAPI component to other components like **Prompt Template**, **Agent**, or **Smart Transform** components.",
      "summary": "[Bundles](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "chat",
        "cometapi",
        "flow",
        "generation",
        "input",
        "key",
        "language",
        "model",
        "name",
        "output",
        "parameters",
        "playground",
        "prompt",
        "response",
        "smart",
        "template",
        "text",
        "transform"
      ]
    },
    "_bundles-cometapi": {
      "title": "CometAPI",
      "slug": "/bundles-cometapi",
      "category": "components",
      "content": "[Bundles](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **CometAPI** bundle.\n\nFor more information about CometAPI features and functionality used by CometAPI components, see the [CometAPI documentation](https://www.cometapi.com/).\n\n## CometAPI text generation\n\nThis component generates text using CometAPI's language models through the CometAPI OpenAI-compatible endpoints.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a CometAPI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Transform** component.\n\nFor more information, see [Language model components](/components-models).\n\n### CometAPI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your CometAPI API Key for authentication. |\n| model_name | String | Input parameter. The id of the CometAPI model to use. The component automatically fetches the latest available models from CometAPI when you provide a valid API key. For information about supported models, see the [CometAPI documentation](https://www.cometapi.com/). |\n| input_value | String | Input parameter. The input text to send to the model. |\n| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.1. |\n| seed | Integer | Input parameter. The seed controls the reproducibility of the job (advanced). |\n| model_kwargs | Dict | Input parameter. Additional keyword arguments to pass to the model (advanced). |\n| json_mode | Boolean | Input parameter. If True, it will output JSON regardless of passing a schema (advanced). |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: false. |\n| output_parser | OutputParser | Input parameter. The parser to use to parse the output of the model (advanced). |\n| model | LanguageModel | Output parameter. An instance of ChatOpenAI configured with CometAPI parameters. |\n\n## Use CometAPI in a flow\n\n1. Sign up for a [CometAPI account](https://www.cometapi.com/).\n2. Obtain your API key from the CometAPI dashboard.\n3. In Langflow, add the **CometAPI** component to your flow.\n4. Enter your API key in the **CometAPI API Key** field.\n5. Select your preferred model from the **Model Name** menu.\n6. Configure other parameters as needed for your use case.\n7. Add other components to your flow as needed.\n\n    To perform a basic test, add **Chat Input** and **Chat Output** components to your flow, connect them to the **CometAPI** component accordingly, and then click **Playground** to test the connection and chat with your model.\n\n    For more advanced use cases, you can connect the CometAPI component to other components like **Prompt Template**, **Agent**, or **Smart Transform** components.",
      "summary": "[Bundles](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "chat",
        "cometapi",
        "flow",
        "generation",
        "input",
        "key",
        "language",
        "model",
        "name",
        "output",
        "parameters",
        "playground",
        "prompt",
        "response",
        "smart",
        "template",
        "text",
        "transform"
      ]
    },
    "/bundles-aiml": {
      "title": "AI/ML API",
      "slug": "/bundles-aiml",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **AI/ML** bundle.\n\n## AI/ML API text generation\n\nThis component creates a `ChatOpenAI` model instance using the AI/ML API.\nThe output is exclusively a **Language Model** ([`LanguageModel`](/data-types#languagemodel)) that you can connect to another LLM-driven component, such as a **Smart Function** component.\n\nFor more information, see the [AI/ML API Langflow integration documentation](https://docs.aimlapi.com/integrations/langflow) and [Language model components](/components-models).\n\n### AI/ML API text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| model_name | String | Input parameter. The name of the AIML model to use. Options are predefined in `AIML_CHAT_MODELS`. |\n| aiml_api_base | String | Input parameter. The base URL of the AIML API. Defaults to `https://api.aimlapi.com`. |\n| api_key | SecretString | Input parameter. The AIML API Key to use for the model. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n\n## AI/ML API Embeddings\n\nThe **AI/ML API Embeddings** component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).\n\nThe output is [`Embeddings`](/data-types#embeddings).\nSpecifically, an instance of `AIMLEmbeddingsImpl`.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### AI/ML API Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_name | String | Input parameter. The name of the AI/ML embedding model to use. |\n| aiml_api_key | SecretString | Input parameter. The API key required for authenticating with the AI/ML service. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "smart",
        "text"
      ]
    },
    "bundles-aiml": {
      "title": "AI/ML API",
      "slug": "/bundles-aiml",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **AI/ML** bundle.\n\n## AI/ML API text generation\n\nThis component creates a `ChatOpenAI` model instance using the AI/ML API.\nThe output is exclusively a **Language Model** ([`LanguageModel`](/data-types#languagemodel)) that you can connect to another LLM-driven component, such as a **Smart Function** component.\n\nFor more information, see the [AI/ML API Langflow integration documentation](https://docs.aimlapi.com/integrations/langflow) and [Language model components](/components-models).\n\n### AI/ML API text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| model_name | String | Input parameter. The name of the AIML model to use. Options are predefined in `AIML_CHAT_MODELS`. |\n| aiml_api_base | String | Input parameter. The base URL of the AIML API. Defaults to `https://api.aimlapi.com`. |\n| api_key | SecretString | Input parameter. The AIML API Key to use for the model. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n\n## AI/ML API Embeddings\n\nThe **AI/ML API Embeddings** component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).\n\nThe output is [`Embeddings`](/data-types#embeddings).\nSpecifically, an instance of `AIMLEmbeddingsImpl`.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### AI/ML API Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_name | String | Input parameter. The name of the AI/ML embedding model to use. |\n| aiml_api_key | SecretString | Input parameter. The API key required for authenticating with the AI/ML service. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "smart",
        "text"
      ]
    },
    "/bundles-amazon": {
      "title": "Amazon",
      "slug": "/bundles-amazon",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Amazon** bundle.\n\n## Amazon Bedrock\n\nThis component generates text using [Amazon Bedrock LLMs](https://docs.aws.amazon.com/bedrock).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatBedrock`](https://docs.langchain.com/oss/python/integrations/chat/bedrock) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Amazon Bedrock model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Amazon Bedrock parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input | String | Input parameter. The input string for text generation. |\n| system_message | String | Input parameter. A system message to pass to the model. |\n| stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |\n| model_id | String | Input parameter. The Amazon Bedrock model to use. |\n| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |\n| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |\n| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |\n| credentials_profile_name | String | Input parameter. Name of the AWS credentials profile to use. |\n| region_name | String | Input parameter. AWS region where your Bedrock resources are located. Default: `us-east-1`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n| endpoint_url | String | Input parameter. Custom endpoint URL for a Bedrock service. |\n\n## Amazon Bedrock Embeddings\n\nThe **Amazon Bedrock Embeddings** component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Amazon Bedrock Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_id | String | Input parameter. The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |\n| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |\n| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |\n| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |\n| credentials_profile_name | String | Input parameter. The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |\n| region_name | String | Input parameter. The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |\n| endpoint_url | String | Input parameter. The URL to set a specific service endpoint other than the default AWS endpoint. |\n\n## S3 Bucket Uploader\n\nThe **S3 Bucket Uploader** component uploads files to an Amazon S3 bucket.\nIt is designed to process `Data` input from a **File** or **Directory** component.\nIf you upload `Data` from other components, test the results before running the flow in production.\n\nRequires the `boto3` package, which is included in your Langflow installation.\n\nThe component produces logs but it doesn't emit output to the flow.\n\n### S3 Bucket Uploader parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **AWS Access Key ID** | SecretString | Input parameter. AWS Access Key ID for authentication. |\n| **AWS Secret Key** | SecretString | Input parameter. AWS Secret Key for authentication. |\n| **Bucket Name** | String | Input parameter. The name of the S3 bucket to upload files to. |\n| **Strategy for file upload** | String | Input parameter. The file upload strategy. **Store Data** (default) iterates over `Data` inputs, logs the file path and text content, and uploads each file to the specified S3 bucket if both file path and text content are available. **Store Original File** iterates through the list of data inputs, retrieves the file path from each data item, uploads the file to the specified S3 bucket if the file path is available, and logs the file path being uploaded. |\n| **Data Inputs** | Data | Input parameter. The `Data` input to iterate over and upload as files in the specified S3 bucket. |\n| **S3 Prefix** | String | Input parameter. Optional prefix (folder path) within the S3 bucket where files will be uploaded. |\n| **Strip Path** | Boolean | Input parameter. Whether to strip the file path when uploading. Default: `false`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "access",
        "agent",
        "amazon",
        "aws",
        "bedrock",
        "bucket",
        "bundles",
        "data",
        "directory",
        "embeddings",
        "file",
        "function",
        "inputs",
        "key",
        "language",
        "model",
        "name",
        "original",
        "parameters",
        "path"
      ]
    },
    "bundles-amazon": {
      "title": "Amazon",
      "slug": "/bundles-amazon",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Amazon** bundle.\n\n## Amazon Bedrock\n\nThis component generates text using [Amazon Bedrock LLMs](https://docs.aws.amazon.com/bedrock).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatBedrock`](https://docs.langchain.com/oss/python/integrations/chat/bedrock) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Amazon Bedrock model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Amazon Bedrock parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input | String | Input parameter. The input string for text generation. |\n| system_message | String | Input parameter. A system message to pass to the model. |\n| stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |\n| model_id | String | Input parameter. The Amazon Bedrock model to use. |\n| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |\n| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |\n| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |\n| credentials_profile_name | String | Input parameter. Name of the AWS credentials profile to use. |\n| region_name | String | Input parameter. AWS region where your Bedrock resources are located. Default: `us-east-1`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n| endpoint_url | String | Input parameter. Custom endpoint URL for a Bedrock service. |\n\n## Amazon Bedrock Embeddings\n\nThe **Amazon Bedrock Embeddings** component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Amazon Bedrock Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_id | String | Input parameter. The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |\n| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |\n| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |\n| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |\n| credentials_profile_name | String | Input parameter. The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |\n| region_name | String | Input parameter. The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |\n| endpoint_url | String | Input parameter. The URL to set a specific service endpoint other than the default AWS endpoint. |\n\n## S3 Bucket Uploader\n\nThe **S3 Bucket Uploader** component uploads files to an Amazon S3 bucket.\nIt is designed to process `Data` input from a **File** or **Directory** component.\nIf you upload `Data` from other components, test the results before running the flow in production.\n\nRequires the `boto3` package, which is included in your Langflow installation.\n\nThe component produces logs but it doesn't emit output to the flow.\n\n### S3 Bucket Uploader parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **AWS Access Key ID** | SecretString | Input parameter. AWS Access Key ID for authentication. |\n| **AWS Secret Key** | SecretString | Input parameter. AWS Secret Key for authentication. |\n| **Bucket Name** | String | Input parameter. The name of the S3 bucket to upload files to. |\n| **Strategy for file upload** | String | Input parameter. The file upload strategy. **Store Data** (default) iterates over `Data` inputs, logs the file path and text content, and uploads each file to the specified S3 bucket if both file path and text content are available. **Store Original File** iterates through the list of data inputs, retrieves the file path from each data item, uploads the file to the specified S3 bucket if the file path is available, and logs the file path being uploaded. |\n| **Data Inputs** | Data | Input parameter. The `Data` input to iterate over and upload as files in the specified S3 bucket. |\n| **S3 Prefix** | String | Input parameter. Optional prefix (folder path) within the S3 bucket where files will be uploaded. |\n| **Strip Path** | Boolean | Input parameter. Whether to strip the file path when uploading. Default: `false`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "access",
        "agent",
        "amazon",
        "aws",
        "bedrock",
        "bucket",
        "bundles",
        "data",
        "directory",
        "embeddings",
        "file",
        "function",
        "inputs",
        "key",
        "language",
        "model",
        "name",
        "original",
        "parameters",
        "path"
      ]
    },
    "/bundles-anthropic": {
      "title": "Anthropic",
      "slug": "/bundles-anthropic",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Anthropic** bundle.\n\nFor more information about Anthropic features and functionality used by Anthropic components, see the [Anthropic documentation](https://docs.anthropic.com/en/docs/intro).\n\n## Anthropic text generation\n\nThe **Anthropic** component generates text using Anthropic Chat and Language models like Claude.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Anthropic model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Anthropic text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: `4096`. |\n| model | String | Input parameter. The name of the Anthropic model to use. Options include various Claude 3 models. |\n| anthropic_api_key | SecretString | Input parameter. Your Anthropic API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| anthropic_api_url | String | Input parameter. Endpoint of the Anthropic API. Defaults to `https://api.anthropic.com` if not specified. |\n| prefill | String | Input parameter. Prefill text to guide the model's response. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "anthropic",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-anthropic": {
      "title": "Anthropic",
      "slug": "/bundles-anthropic",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Anthropic** bundle.\n\nFor more information about Anthropic features and functionality used by Anthropic components, see the [Anthropic documentation](https://docs.anthropic.com/en/docs/intro).\n\n## Anthropic text generation\n\nThe **Anthropic** component generates text using Anthropic Chat and Language models like Claude.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Anthropic model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Anthropic text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: `4096`. |\n| model | String | Input parameter. The name of the Anthropic model to use. Options include various Claude 3 models. |\n| anthropic_api_key | SecretString | Input parameter. Your Anthropic API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| anthropic_api_url | String | Input parameter. Endpoint of the Anthropic API. Defaults to `https://api.anthropic.com` if not specified. |\n| prefill | String | Input parameter. Prefill text to guide the model's response. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "anthropic",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-apify": {
      "title": "Apify",
      "slug": "/bundles-apify",
      "category": "components",
      "content": "[Apify](https://apify.com/) is a web scraping and data extraction platform with more than 3,000 ready-made cloud tools called **Actors**.\n\nYour flows can use the **Apify Actors** component to run **Actors** to accomplish tasks like data extraction, content analysis, and SQL operations.\n\n## Use the Apify Actors component in a flow\n\n1. Add an **Apify Actors** component to your flow, and then configure it as follows:\n\n    * **Apify Token**: Enter your [Apify API token](https://docs.apify.com/platform/integrations/api).\n    * **Actor**: Enter the ID of the Actor you want to run from the [Apify Actor Store](https://apify.com/store). For example, the [Website Content Crawler](https://apify.com/apify/website-content-crawler) has the Actor ID `apify/website-content-crawler`.\n    * **Run Input**: Enter the [JSON input for configuring the Actor run](https://docs.apify.com/platform/actors/running-actors#input).\n    * Configure additional parameters and commands depending on the Actor you chose and your use case.\n\n2. Connect the component to other components in your flow.\nThe component can be used to perform tasks as a standalone step in a flow or as a tool for an agent.\n\n    To enable **Tool Mode** for this component, change the component's output type from **Output** to **Tool**, and then connect it to the **Tools** port on an **Agent** component.\n\n    **Apify Actors** components output the results of the Actor run as a JSON object in Langflow's [`Data` type](/data-types#data).\n\n## Example Apify Actors flows\n\nThe following scenarios provide a few examples of how you could use the **Apify Actors** components in Langflow:\n\n* **Extract website text content in Markdown**: Use the [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to extract text content in Markdown format from a website, and then connect the **Output** to a **Parser** component's input for further processing.\n\n    ![Apify Flow - Website Content Crawler](/img/apify_flow_wcc.png)\n\n* **Process web content with an agent**: Attach a [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to an **Agent** component as a tool so the agent can decide whether to extract website content based on the chat input. The agent could take the extracted data and transform it into summaries, insights, or structured responses to make the information more actionable.\n\n    ![Apify Agent Flow - Simple](/img/apify_agent_flow_simple.png)\n\n* **Analyze social media profiles with multiple actors**: Perform comprehensive social media research with multiple Apify Actors:\n\n    * Use the [Google Search Results Scraper Actor](https://apify.com/apify/google-search-scraper) to find relevant social media profiles.\n    * Use the [TikTok Data Extractor Actor](https://apify.com/clockworks/free-tiktok-scraper) to gather data and videos.\n    * Attach both actors as tools for an **Agent** component to collect the links from Google and content from TikTok, and then analyze the data to provide insights about a person, brand, or topic.\n\n    ![Apify Agent Flow](/img/apify_agent_flow.png)",
      "summary": "[Apify](https://apify.com/) is a web scraping and data extraction platform with more than 3,000 ready-made cloud tools called **Actors**.",
      "keywords": [
        "actor",
        "actors",
        "agent",
        "analyze",
        "apify",
        "component",
        "content",
        "extract",
        "flow",
        "flows",
        "input",
        "markdown",
        "media",
        "mode",
        "multiple",
        "output",
        "parser",
        "process",
        "profiles",
        "run"
      ]
    },
    "bundles-apify": {
      "title": "Apify",
      "slug": "/bundles-apify",
      "category": "components",
      "content": "[Apify](https://apify.com/) is a web scraping and data extraction platform with more than 3,000 ready-made cloud tools called **Actors**.\n\nYour flows can use the **Apify Actors** component to run **Actors** to accomplish tasks like data extraction, content analysis, and SQL operations.\n\n## Use the Apify Actors component in a flow\n\n1. Add an **Apify Actors** component to your flow, and then configure it as follows:\n\n    * **Apify Token**: Enter your [Apify API token](https://docs.apify.com/platform/integrations/api).\n    * **Actor**: Enter the ID of the Actor you want to run from the [Apify Actor Store](https://apify.com/store). For example, the [Website Content Crawler](https://apify.com/apify/website-content-crawler) has the Actor ID `apify/website-content-crawler`.\n    * **Run Input**: Enter the [JSON input for configuring the Actor run](https://docs.apify.com/platform/actors/running-actors#input).\n    * Configure additional parameters and commands depending on the Actor you chose and your use case.\n\n2. Connect the component to other components in your flow.\nThe component can be used to perform tasks as a standalone step in a flow or as a tool for an agent.\n\n    To enable **Tool Mode** for this component, change the component's output type from **Output** to **Tool**, and then connect it to the **Tools** port on an **Agent** component.\n\n    **Apify Actors** components output the results of the Actor run as a JSON object in Langflow's [`Data` type](/data-types#data).\n\n## Example Apify Actors flows\n\nThe following scenarios provide a few examples of how you could use the **Apify Actors** components in Langflow:\n\n* **Extract website text content in Markdown**: Use the [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to extract text content in Markdown format from a website, and then connect the **Output** to a **Parser** component's input for further processing.\n\n    ![Apify Flow - Website Content Crawler](/img/apify_flow_wcc.png)\n\n* **Process web content with an agent**: Attach a [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to an **Agent** component as a tool so the agent can decide whether to extract website content based on the chat input. The agent could take the extracted data and transform it into summaries, insights, or structured responses to make the information more actionable.\n\n    ![Apify Agent Flow - Simple](/img/apify_agent_flow_simple.png)\n\n* **Analyze social media profiles with multiple actors**: Perform comprehensive social media research with multiple Apify Actors:\n\n    * Use the [Google Search Results Scraper Actor](https://apify.com/apify/google-search-scraper) to find relevant social media profiles.\n    * Use the [TikTok Data Extractor Actor](https://apify.com/clockworks/free-tiktok-scraper) to gather data and videos.\n    * Attach both actors as tools for an **Agent** component to collect the links from Google and content from TikTok, and then analyze the data to provide insights about a person, brand, or topic.\n\n    ![Apify Agent Flow](/img/apify_agent_flow.png)",
      "summary": "[Apify](https://apify.com/) is a web scraping and data extraction platform with more than 3,000 ready-made cloud tools called **Actors**.",
      "keywords": [
        "actor",
        "actors",
        "agent",
        "analyze",
        "apify",
        "component",
        "content",
        "extract",
        "flow",
        "flows",
        "input",
        "markdown",
        "media",
        "mode",
        "multiple",
        "output",
        "parser",
        "process",
        "profiles",
        "run"
      ]
    },
    "/bundles-arxiv": {
      "title": "arXiv",
      "slug": "/bundles-arxiv",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **arXiv** bundle.\n\n## arXiv search\n\nThis component searches and retrieves papers from [arXiv.org](https://arXiv.org).\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### arXiv search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| search_query | String | Input parameter. The search query for arXiv papers. For example, `quantum computing`. |\n| search_type | String | Input parameter. The field to search in. |\n| max_results | Integer | Input parameter. The maximum number of results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "arxiv",
        "bundles",
        "parameters",
        "search",
        "web"
      ]
    },
    "bundles-arxiv": {
      "title": "arXiv",
      "slug": "/bundles-arxiv",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **arXiv** bundle.\n\n## arXiv search\n\nThis component searches and retrieves papers from [arXiv.org](https://arXiv.org).\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### arXiv search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| search_query | String | Input parameter. The search query for arXiv papers. For example, `quantum computing`. |\n| search_type | String | Input parameter. The field to search in. |\n| max_results | Integer | Input parameter. The maximum number of results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "arxiv",
        "bundles",
        "parameters",
        "search",
        "web"
      ]
    },
    "/bundles-assemblyai": {
      "title": "AssemblyAI",
      "slug": "/bundles-assemblyai",
      "category": "components",
      "content": "The AssemblyAI components allow you to apply powerful Speech AI models to your app for tasks like:\n\n- Transcribing audio and video files\n- Formatting transcripts\n- Generating subtitles\n- Applying LLMs to audio files\n\nFor more information about AssemblyAI features and functionality used by AssemblyAI components, see the [AssemblyAI API Docs](https://www.assemblyai.com/docs).\n\n## Prerequisites\n\n* An [AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and an AssemblyAI API key.\n\n    Enter the key in the *AssemblyAI API Key* field in all Langflow components that require the AssemblyAI key.\n\n* Optional: To use LeMUR, you need a paid AssemblyAI account because LeMUR isn't included in the free account.\n\n## Components\n\n![AssemblyAI components](/img/assemblyai-components.png)\n\n### AssemblyAI Start Transcript\n\nThis component allows you to submit an audio or video file for transcription.\n\n**Tip**: You can freeze the path of this component to only submit the file once.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Audio File: The audio or video file to transcribe.\n    - Speech Model (Optional): Select the class of models. Default is *Best*. See [speech models](https://www.assemblyai.com/docs/speech-to-text/speech-recognition#select-the-speech-model-with-best-and-nano) for more info.\n    - Automatic Language Detection (Optional): Enable automatic language detection.\n    - Language (Optional): The language of the audio file. Can be set manually if automatic language detection is disabled.\n            See [supported languages](https://www.assemblyai.com/docs/getting-started/supported-languages) for a list of supported language codes.\n    - Enable Speaker Labels (Optional): Detect speakers in an audio file and what each speaker said.\n    - Expected Number of Speakers (Optional): Set the expected number of speakers, if Speaker Labels is enabled.\n    - Audio File URL (Optional): The URL of the audio or video file to transcribe. Can be used instead of *Audio File*.\n    - Punctuate (Optional): Apply punctuation. Default is `true`.\n    - Format Text (Optional): Apply casing and text formatting. Default is `true`.\n\n- **Output**:\n    - Transcript ID: The id of the transcript\n\n### AssemblyAI Poll Transcript\n\nThis components allows you to poll the transcripts. It checks the status of the transcript every few seconds until the transcription is completed.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Polling Interval (Optional): The polling interval in seconds. Default is 3.\n\n- **Output**:\n    - Transcription Result: The AssemblyAI JSON response of a completed transcript. Contains the text and other info.\n\n### AssemblyAI Get Subtitles\n\nThis component allows you to generate subtitles in SRT or VTT format.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Transcription Result: The output of the **Poll Transcript** component.\n    - Subtitle Format: The format of the captions (SRT or VTT).\n    - Character per Caption (Optional): The maximum number of characters per caption (0 for no limit).\n\n- **Output**:\n    - Subtitles: A JSON response with the `subtitles` field containing the captions in SRT or VTT format.\n\n### AssemblyAI LeMUR\n\nThis component allows you to apply Large Language Models to spoken data using the [AssemblyAI LeMUR framework](https://www.assemblyai.com/docs/lemur).\n\nLeMUR automatically ingests the transcript as additional context, making it easy to apply LLMs to audio data. You can use it for tasks like summarizing audio, extracting insights, or asking questions.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Transcription Result: The output of the **Poll Transcript** component.\n    - Input Prompt: The text to prompt the model. You can type your prompt in this field or connect it to a **Prompt Template** component.\n    - Final Model: The model that is used for the final prompt after compression is performed. Default is Claude 3.5 Sonnet.\n    - Temperature (Optional): The temperature to use for the model. Default is 0.0.\n    - Max Output Size (Optional): Max output size in tokens, up to 4000. Default is 2000.\n    - Endpoint (Optional): The LeMUR endpoint to use. Default is \"task\". For \"summary\" and \"question-answer\", no prompt input is needed. See [LeMUR API docs](https://www.assemblyai.com/docs/api-reference/lemur/) for more info.\n    - Questions (Optional): Comma-separated list of your questions. Only used if *Endpoint* is \"question-answer\".\n    - Transcript IDs (Optional): Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the *Transcription Result* is ignored.\n\n- **Output**:\n    - LeMUR Response: The generated LLM response.\n\n### AssemblyAI List Transcripts\n\nThis component can be used as a standalone component to list all previously generated transcripts.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Limit (Optional): Maximum number of transcripts to retrieve. Default is 20, use 0 for all.\n    - Filter (Optional): Filter by transcript status.\n    - Created On (Optional): Only get transcripts created on this date (YYYY-MM-DD).\n    - Throttled Only (Optional): Only get throttled transcripts, overrides the status filter\n\n- **Output**:\n    - Transcript List: A list of all transcripts with info such as the transcript ID, the status, and the data.\n\n## Flow Process\n\n1. The user inputs an audio or video file.\n2. The user can also input an LLM prompt. In this example, we want to generate a summary of the transcript.\n3. The flow submits the audio file for transcription.\n4. The flow checks the status of the transcript every few seconds until transcription is completed.\n5. The flow parses the transcription result and outputs the transcribed text.\n6. The flow also generates subtitles.\n7. The flow applies the LLM prompt to generate a summary.\n8. As a standalone component, all transcripts can be listed.\n\n## Run the Transcription and Speech AI Flow\n\n1. Build the flow manually or import a pre-build JSON file:\n\n    * Recommended: [Download the AssemblyAI Transcription and Speech AI flow JSON](/files/AssemblyAI_Flow.json), and then [import the flow](/concepts-flows-import) into Langflow.\n    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.\n\n2. Input your AssemblyAI API key in all components that require the key (**Start Transcript**, **Poll Transcript**, **Get Subtitles**, **LeMUR**, **List Transcripts**).\n\n3. Select an audio or video file for the **Start Transcript** component.\n\n    Optional: After adding a file to the **Start Transcript** component, run and [freeze the component](/concepts-components#freeze-a-component) so you only submit the file once, no matter how many times you run the flow.\n    To do this, click  **Run component** to preload the file, and then click  **Show More** and select **Freeze** to lock the result.\n    Subsequent flow runs use the frozen component's cached output.\n\n4. Test the transcription by clicking  **Run component** on the **Parser** component. Make sure that the specified template is `{text}`.\n\n    Running one component runs all upstream components as well as the selected component and then stops the flow run.\n    In this case, the **Start Transcript** and **Poll Transcript** components are upstream from the **Parser** component.\n    If you froze the **Start Transcript** component, the flow sends the cached output from **Start Transcript**, runs the **Poll Transcript** component, to get the transcription result.\n    Check the flow logs or inspect the output of the **Parser** component to see the transcribed text result.\n\n5. To generate subtitles and run the full flow, click  **Run component** on the **List Transcript** component.\n\n## Customization\n\nThe flow can be customized by:\n\n1. Modifying the parameters in the **Start Transcript** component.\n2. Modifying the subtitle format in the **Get Subtitles** component.\n3. Modifying the LLM prompt for input of the **LeMUR** component.\n4. Modifying the LLM parameters (e.g., temperature) in the **LeMUR** component.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Ensure the API key is correctly set in all components that require the key.\n2. To use LeMUR, you need to upgrade your AssemblyAI account, since this isn't included in the free account.\n3. Verify that all components are properly connected in the flow.\n4. Review the Langflow logs for any error messages.\n5. Check the [AssemblyAI API documentation](https://www.assemblyai.com/docs/).\n6. Contact [AssemblyAI support](https://www.assemblyai.com/contact/support).",
      "summary": "The AssemblyAI components allow you to apply powerful Speech AI models to your app for tasks like:",
      "keywords": [
        "assemblyai",
        "component",
        "components",
        "customization",
        "flow",
        "freeze",
        "get",
        "input",
        "lemur",
        "list",
        "output",
        "parser",
        "poll",
        "prerequisites",
        "process",
        "prompt",
        "run",
        "show",
        "speech",
        "start"
      ]
    },
    "bundles-assemblyai": {
      "title": "AssemblyAI",
      "slug": "/bundles-assemblyai",
      "category": "components",
      "content": "The AssemblyAI components allow you to apply powerful Speech AI models to your app for tasks like:\n\n- Transcribing audio and video files\n- Formatting transcripts\n- Generating subtitles\n- Applying LLMs to audio files\n\nFor more information about AssemblyAI features and functionality used by AssemblyAI components, see the [AssemblyAI API Docs](https://www.assemblyai.com/docs).\n\n## Prerequisites\n\n* An [AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and an AssemblyAI API key.\n\n    Enter the key in the *AssemblyAI API Key* field in all Langflow components that require the AssemblyAI key.\n\n* Optional: To use LeMUR, you need a paid AssemblyAI account because LeMUR isn't included in the free account.\n\n## Components\n\n![AssemblyAI components](/img/assemblyai-components.png)\n\n### AssemblyAI Start Transcript\n\nThis component allows you to submit an audio or video file for transcription.\n\n**Tip**: You can freeze the path of this component to only submit the file once.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Audio File: The audio or video file to transcribe.\n    - Speech Model (Optional): Select the class of models. Default is *Best*. See [speech models](https://www.assemblyai.com/docs/speech-to-text/speech-recognition#select-the-speech-model-with-best-and-nano) for more info.\n    - Automatic Language Detection (Optional): Enable automatic language detection.\n    - Language (Optional): The language of the audio file. Can be set manually if automatic language detection is disabled.\n            See [supported languages](https://www.assemblyai.com/docs/getting-started/supported-languages) for a list of supported language codes.\n    - Enable Speaker Labels (Optional): Detect speakers in an audio file and what each speaker said.\n    - Expected Number of Speakers (Optional): Set the expected number of speakers, if Speaker Labels is enabled.\n    - Audio File URL (Optional): The URL of the audio or video file to transcribe. Can be used instead of *Audio File*.\n    - Punctuate (Optional): Apply punctuation. Default is `true`.\n    - Format Text (Optional): Apply casing and text formatting. Default is `true`.\n\n- **Output**:\n    - Transcript ID: The id of the transcript\n\n### AssemblyAI Poll Transcript\n\nThis components allows you to poll the transcripts. It checks the status of the transcript every few seconds until the transcription is completed.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Polling Interval (Optional): The polling interval in seconds. Default is 3.\n\n- **Output**:\n    - Transcription Result: The AssemblyAI JSON response of a completed transcript. Contains the text and other info.\n\n### AssemblyAI Get Subtitles\n\nThis component allows you to generate subtitles in SRT or VTT format.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Transcription Result: The output of the **Poll Transcript** component.\n    - Subtitle Format: The format of the captions (SRT or VTT).\n    - Character per Caption (Optional): The maximum number of characters per caption (0 for no limit).\n\n- **Output**:\n    - Subtitles: A JSON response with the `subtitles` field containing the captions in SRT or VTT format.\n\n### AssemblyAI LeMUR\n\nThis component allows you to apply Large Language Models to spoken data using the [AssemblyAI LeMUR framework](https://www.assemblyai.com/docs/lemur).\n\nLeMUR automatically ingests the transcript as additional context, making it easy to apply LLMs to audio data. You can use it for tasks like summarizing audio, extracting insights, or asking questions.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Transcription Result: The output of the **Poll Transcript** component.\n    - Input Prompt: The text to prompt the model. You can type your prompt in this field or connect it to a **Prompt Template** component.\n    - Final Model: The model that is used for the final prompt after compression is performed. Default is Claude 3.5 Sonnet.\n    - Temperature (Optional): The temperature to use for the model. Default is 0.0.\n    - Max Output Size (Optional): Max output size in tokens, up to 4000. Default is 2000.\n    - Endpoint (Optional): The LeMUR endpoint to use. Default is \"task\". For \"summary\" and \"question-answer\", no prompt input is needed. See [LeMUR API docs](https://www.assemblyai.com/docs/api-reference/lemur/) for more info.\n    - Questions (Optional): Comma-separated list of your questions. Only used if *Endpoint* is \"question-answer\".\n    - Transcript IDs (Optional): Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the *Transcription Result* is ignored.\n\n- **Output**:\n    - LeMUR Response: The generated LLM response.\n\n### AssemblyAI List Transcripts\n\nThis component can be used as a standalone component to list all previously generated transcripts.\n\n- **Input**:\n    - AssemblyAI API Key: Your API key.\n    - Limit (Optional): Maximum number of transcripts to retrieve. Default is 20, use 0 for all.\n    - Filter (Optional): Filter by transcript status.\n    - Created On (Optional): Only get transcripts created on this date (YYYY-MM-DD).\n    - Throttled Only (Optional): Only get throttled transcripts, overrides the status filter\n\n- **Output**:\n    - Transcript List: A list of all transcripts with info such as the transcript ID, the status, and the data.\n\n## Flow Process\n\n1. The user inputs an audio or video file.\n2. The user can also input an LLM prompt. In this example, we want to generate a summary of the transcript.\n3. The flow submits the audio file for transcription.\n4. The flow checks the status of the transcript every few seconds until transcription is completed.\n5. The flow parses the transcription result and outputs the transcribed text.\n6. The flow also generates subtitles.\n7. The flow applies the LLM prompt to generate a summary.\n8. As a standalone component, all transcripts can be listed.\n\n## Run the Transcription and Speech AI Flow\n\n1. Build the flow manually or import a pre-build JSON file:\n\n    * Recommended: [Download the AssemblyAI Transcription and Speech AI flow JSON](/files/AssemblyAI_Flow.json), and then [import the flow](/concepts-flows-import) into Langflow.\n    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.\n\n2. Input your AssemblyAI API key in all components that require the key (**Start Transcript**, **Poll Transcript**, **Get Subtitles**, **LeMUR**, **List Transcripts**).\n\n3. Select an audio or video file for the **Start Transcript** component.\n\n    Optional: After adding a file to the **Start Transcript** component, run and [freeze the component](/concepts-components#freeze-a-component) so you only submit the file once, no matter how many times you run the flow.\n    To do this, click  **Run component** to preload the file, and then click  **Show More** and select **Freeze** to lock the result.\n    Subsequent flow runs use the frozen component's cached output.\n\n4. Test the transcription by clicking  **Run component** on the **Parser** component. Make sure that the specified template is `{text}`.\n\n    Running one component runs all upstream components as well as the selected component and then stops the flow run.\n    In this case, the **Start Transcript** and **Poll Transcript** components are upstream from the **Parser** component.\n    If you froze the **Start Transcript** component, the flow sends the cached output from **Start Transcript**, runs the **Poll Transcript** component, to get the transcription result.\n    Check the flow logs or inspect the output of the **Parser** component to see the transcribed text result.\n\n5. To generate subtitles and run the full flow, click  **Run component** on the **List Transcript** component.\n\n## Customization\n\nThe flow can be customized by:\n\n1. Modifying the parameters in the **Start Transcript** component.\n2. Modifying the subtitle format in the **Get Subtitles** component.\n3. Modifying the LLM prompt for input of the **LeMUR** component.\n4. Modifying the LLM parameters (e.g., temperature) in the **LeMUR** component.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Ensure the API key is correctly set in all components that require the key.\n2. To use LeMUR, you need to upgrade your AssemblyAI account, since this isn't included in the free account.\n3. Verify that all components are properly connected in the flow.\n4. Review the Langflow logs for any error messages.\n5. Check the [AssemblyAI API documentation](https://www.assemblyai.com/docs/).\n6. Contact [AssemblyAI support](https://www.assemblyai.com/contact/support).",
      "summary": "The AssemblyAI components allow you to apply powerful Speech AI models to your app for tasks like:",
      "keywords": [
        "assemblyai",
        "component",
        "components",
        "customization",
        "flow",
        "freeze",
        "get",
        "input",
        "lemur",
        "list",
        "output",
        "parser",
        "poll",
        "prerequisites",
        "process",
        "prompt",
        "run",
        "show",
        "speech",
        "start"
      ]
    },
    "/bundles-azure": {
      "title": "Azure",
      "slug": "/bundles-azure",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Azure** bundle.\n\n## Azure OpenAI\n\nThis component generates text using [Azure OpenAI LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`AzureChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Azure OpenAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Azure OpenAI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Model Name | String | Input parameter. Specifies the name of the Azure OpenAI model to be used for text generation. |\n| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource. |\n| Deployment Name | String | Input parameter. Specifies the name of the deployment. |\n| API Version | String | Input parameter. Specifies the version of the Azure OpenAI API to be used. |\n| API Key | SecretString | Input parameter. Your Azure OpenAI API key. |\n| Temperature | Float | Input parameter. Specifies the sampling temperature. Defaults to `0.7`. |\n| Max Tokens | Integer | Input parameter. Specifies the maximum number of tokens to generate. Defaults to `1000`. |\n| Input Value | String | Input parameter. Specifies the input text for text generation. |\n| Stream | Boolean | Input parameter. Specifies whether to stream the response from the model. Default to `false`. |\n\n## Azure OpenAI Embeddings\n\nThe **Azure OpenAI Embeddings** component generates embeddings using Azure OpenAI models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Azure OpenAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |\n| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource, such as `https://example-resource.azure.openai.com/`. |\n| Deployment Name | String | Input parameter. The name of the deployment. |\n| API Version | String | Input parameter. The API version to use, with options including various dates. |\n| API Key | String | Input parameter. The API key required to access the Azure OpenAI service. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "azure",
        "bundles",
        "embeddings",
        "function",
        "language",
        "model",
        "openai",
        "parameters",
        "response",
        "smart"
      ]
    },
    "bundles-azure": {
      "title": "Azure",
      "slug": "/bundles-azure",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Azure** bundle.\n\n## Azure OpenAI\n\nThis component generates text using [Azure OpenAI LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`AzureChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use an Azure OpenAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Azure OpenAI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Model Name | String | Input parameter. Specifies the name of the Azure OpenAI model to be used for text generation. |\n| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource. |\n| Deployment Name | String | Input parameter. Specifies the name of the deployment. |\n| API Version | String | Input parameter. Specifies the version of the Azure OpenAI API to be used. |\n| API Key | SecretString | Input parameter. Your Azure OpenAI API key. |\n| Temperature | Float | Input parameter. Specifies the sampling temperature. Defaults to `0.7`. |\n| Max Tokens | Integer | Input parameter. Specifies the maximum number of tokens to generate. Defaults to `1000`. |\n| Input Value | String | Input parameter. Specifies the input text for text generation. |\n| Stream | Boolean | Input parameter. Specifies whether to stream the response from the model. Default to `false`. |\n\n## Azure OpenAI Embeddings\n\nThe **Azure OpenAI Embeddings** component generates embeddings using Azure OpenAI models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Azure OpenAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |\n| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource, such as `https://example-resource.azure.openai.com/`. |\n| Deployment Name | String | Input parameter. The name of the deployment. |\n| API Version | String | Input parameter. The API version to use, with options including various dates. |\n| API Key | String | Input parameter. The API key required to access the Azure OpenAI service. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "azure",
        "bundles",
        "embeddings",
        "function",
        "language",
        "model",
        "openai",
        "parameters",
        "response",
        "smart"
      ]
    },
    "/bundles-baidu": {
      "title": "Baidu",
      "slug": "/bundles-baidu",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Baidu** bundle.\n\n## Qianfan\n\nThe **Qianfan** component generates text using Qianfan's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Qianfan model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models) and the [Qianfan documentation](https://github.com/baidubce/bce-qianfan-sdk).",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "baidu",
        "bundles",
        "function",
        "language",
        "model",
        "qianfan",
        "response",
        "smart"
      ]
    },
    "bundles-baidu": {
      "title": "Baidu",
      "slug": "/bundles-baidu",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Baidu** bundle.\n\n## Qianfan\n\nThe **Qianfan** component generates text using Qianfan's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Qianfan model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models) and the [Qianfan documentation](https://github.com/baidubce/bce-qianfan-sdk).",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "baidu",
        "bundles",
        "function",
        "language",
        "model",
        "qianfan",
        "response",
        "smart"
      ]
    },
    "/bundles-bing": {
      "title": "Bing",
      "slug": "/bundles-bing",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Bing** bundle.\n\n## Bing Search API\n\nThis component allows you to call the Bing Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Bing Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| bing_subscription_key | SecretString | Input parameter. A Bing API subscription key. |\n| input_value | String | Input parameter. The search query input. |\n| bing_search_url | String | Input parameter. A custom Bing Search URL. |\n| k | Integer | Input parameter. The number of search results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**SearchApi** bundle](/bundles-searchapi)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bing",
        "bundles",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "bundles-bing": {
      "title": "Bing",
      "slug": "/bundles-bing",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Bing** bundle.\n\n## Bing Search API\n\nThis component allows you to call the Bing Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Bing Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| bing_subscription_key | SecretString | Input parameter. A Bing API subscription key. |\n| input_value | String | Input parameter. The search query input. |\n| bing_search_url | String | Input parameter. A custom Bing Search URL. |\n| k | Integer | Input parameter. The number of search results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**SearchApi** bundle](/bundles-searchapi)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bing",
        "bundles",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "/bundles-cassandra": {
      "title": "Cassandra",
      "slug": "/bundles-cassandra",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cassandra** bundle, including components that read and write to Apache Cassandra clusters, such as OSS Cassandra and Astra DB databases.\n\n## Cassandra vector store\n\nUse the **Cassandra** component to read or write to a Cassandra-based vector store using a `CassandraVectorStore` instance.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Cassandra parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see [Vector search in Cassandra](https://cassandra.apache.org/doc/latest/cassandra/vector-search/overview.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| database_ref | String | Input parameter. Contact points for the database or an Astra database ID. |\n| username | String | Input parameter. Username for the database. Leave empty for Astra DB. |\n| token | SecretString | Input parameter. User password for the database or an Astra application token. |\n| keyspace | String | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). |\n| table_name | String | Input parameter. The name of the table or collection that is the vector store. |\n| ttl_seconds | Integer | Input parameter. Time-to-live for added texts, if supported by the cluster. Only relevant for writes. |\n| batch_size | Integer | Input parameter. Amount of records to process in a single batch. |\n| setup_mode | String | Input parameter. Configuration mode for setting up a Cassandra table. |\n| cluster_kwargs | Dict | Input parameter. Additional keyword arguments for a Cassandra cluster. |\n| search_query | String | Input parameter. Query string for similarity search. Only relevant for reads. |\n| ingest_data | Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |\n| embedding | Embeddings | Input parameter. Embedding function to use. |\n| number_of_results | Integer | Input parameter. Number of results to return in search. Only relevant for reads. |\n| search_type | String | Input parameter. Type of search to perform. Only relevant for reads. |\n| search_score_threshold | Float | Input parameter. Minimum similarity score for search results. Only relevant for reads. |\n| search_filter | Dict | Input parameter. An optional dictionary of metadata search filters to apply in addition to vector search. Only relevant for reads. |\n| body_search | String | Input parameter. Document textual search terms. Only relevant for reads. |\n| enable_body_search | Boolean | Input parameter. Flag to enable body search. Only relevant for reads. |\n\n## Cassandra Chat Memory\n\nThe **Cassandra Chat Memory** component retrieves and stores chat messages using an Apache Cassandra-based database.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\nSpecifically, the component creates an instance of `CassandraChatMessageHistory`, which is a LangChain chat message history class that uses a Cassandra database for storage.\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n### Cassandra Chat Memory parameters\n\n| Name           | Type          | Description   |\n|----------------|---------------|-----------------------------|\n| database_ref   | MessageText   | Input parameter. The contact points for the Cassandra database or Astra DB database ID. Required. |\n| username       | MessageText   | Input parameter. The username for Cassandra. Leave empty for Astra DB. |\n| token          | SecretString  | Input parameter. The password for Cassandra or the token for Astra DB. Required. |\n| keyspace       | MessageText   | Input parameter. The keyspace in Cassandra or namespace in Astra DB. Required. |\n| table_name     | MessageText   | Input parameter. The name of the table or collection for storing messages. Required. |\n| session_id     | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |\n| cluster_kwargs | Dictionary    | Input parameter. Additional keyword arguments for the Cassandra cluster configuration. Optional. |\n\n## Cassandra Graph\n\nThe **Cassandra Graph** component uses `CassandraGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in a compatible Cassandra-based cluster.\nIt also supports writing to the vector store.\n\n### Cassandra Graph parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| database_ref | Contact Points / Astra Database ID | Input parameter. The contact points for the database or an Astra database ID. Required. |\n| username | Username | Input parameter. The username for the database. Leave empty for Astra DB. |\n| token | Password / Astra DB Token | Input parameter. The user password for the database or an Astra application token. Required. |\n| keyspace | Keyspace | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). Required. |\n| table_name | Table Name | Input parameter. The name of the table or collection that is the vector store. Required. |\n| setup_mode | Setup Mode | Input parameter. The configuration mode for setting up the Cassandra table. The options are `Sync` (default) or `Off`. |\n| cluster_kwargs | Cluster arguments | Input parameter. An optional dictionary of additional keyword arguments for the Cassandra cluster. |\n| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |\n| ingest_data | Ingest Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |\n| embedding | Embedding | Input parameter. The embedding model to use. |\n| number_of_results | Number of Results | Input parameter. The number of results to return in similarity search. Only relevant for reads. Default: 4. |\n| search_type | Search Type | Input parameter. The search type to use. The options are `Traversal` (default), `MMR Traversal`, `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. |\n| depth | Depth of traversal | Input parameter. The maximum depth of edges to traverse. Only relevant if **Search Type** (`search_type`) is `Traversal` or `MMR Traversal`. Default: 1. |\n| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for search results. Only relevant for reads using the `Similarity with score threshold` search type. |\n| search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata search filters to apply in addition to graph traversal and similarity search. |\n\n### See also\n\n* [**DataStax** bundle](/bundles-datastax)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "cassandra",
        "chat",
        "datastax",
        "graph",
        "history",
        "memory",
        "message",
        "name",
        "note",
        "parameters",
        "search",
        "store",
        "table",
        "type",
        "vector"
      ]
    },
    "bundles-cassandra": {
      "title": "Cassandra",
      "slug": "/bundles-cassandra",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cassandra** bundle, including components that read and write to Apache Cassandra clusters, such as OSS Cassandra and Astra DB databases.\n\n## Cassandra vector store\n\nUse the **Cassandra** component to read or write to a Cassandra-based vector store using a `CassandraVectorStore` instance.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Cassandra parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see [Vector search in Cassandra](https://cassandra.apache.org/doc/latest/cassandra/vector-search/overview.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| database_ref | String | Input parameter. Contact points for the database or an Astra database ID. |\n| username | String | Input parameter. Username for the database. Leave empty for Astra DB. |\n| token | SecretString | Input parameter. User password for the database or an Astra application token. |\n| keyspace | String | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). |\n| table_name | String | Input parameter. The name of the table or collection that is the vector store. |\n| ttl_seconds | Integer | Input parameter. Time-to-live for added texts, if supported by the cluster. Only relevant for writes. |\n| batch_size | Integer | Input parameter. Amount of records to process in a single batch. |\n| setup_mode | String | Input parameter. Configuration mode for setting up a Cassandra table. |\n| cluster_kwargs | Dict | Input parameter. Additional keyword arguments for a Cassandra cluster. |\n| search_query | String | Input parameter. Query string for similarity search. Only relevant for reads. |\n| ingest_data | Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |\n| embedding | Embeddings | Input parameter. Embedding function to use. |\n| number_of_results | Integer | Input parameter. Number of results to return in search. Only relevant for reads. |\n| search_type | String | Input parameter. Type of search to perform. Only relevant for reads. |\n| search_score_threshold | Float | Input parameter. Minimum similarity score for search results. Only relevant for reads. |\n| search_filter | Dict | Input parameter. An optional dictionary of metadata search filters to apply in addition to vector search. Only relevant for reads. |\n| body_search | String | Input parameter. Document textual search terms. Only relevant for reads. |\n| enable_body_search | Boolean | Input parameter. Flag to enable body search. Only relevant for reads. |\n\n## Cassandra Chat Memory\n\nThe **Cassandra Chat Memory** component retrieves and stores chat messages using an Apache Cassandra-based database.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\nSpecifically, the component creates an instance of `CassandraChatMessageHistory`, which is a LangChain chat message history class that uses a Cassandra database for storage.\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n### Cassandra Chat Memory parameters\n\n| Name           | Type          | Description   |\n|----------------|---------------|-----------------------------|\n| database_ref   | MessageText   | Input parameter. The contact points for the Cassandra database or Astra DB database ID. Required. |\n| username       | MessageText   | Input parameter. The username for Cassandra. Leave empty for Astra DB. |\n| token          | SecretString  | Input parameter. The password for Cassandra or the token for Astra DB. Required. |\n| keyspace       | MessageText   | Input parameter. The keyspace in Cassandra or namespace in Astra DB. Required. |\n| table_name     | MessageText   | Input parameter. The name of the table or collection for storing messages. Required. |\n| session_id     | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |\n| cluster_kwargs | Dictionary    | Input parameter. Additional keyword arguments for the Cassandra cluster configuration. Optional. |\n\n## Cassandra Graph\n\nThe **Cassandra Graph** component uses `CassandraGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in a compatible Cassandra-based cluster.\nIt also supports writing to the vector store.\n\n### Cassandra Graph parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| database_ref | Contact Points / Astra Database ID | Input parameter. The contact points for the database or an Astra database ID. Required. |\n| username | Username | Input parameter. The username for the database. Leave empty for Astra DB. |\n| token | Password / Astra DB Token | Input parameter. The user password for the database or an Astra application token. Required. |\n| keyspace | Keyspace | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). Required. |\n| table_name | Table Name | Input parameter. The name of the table or collection that is the vector store. Required. |\n| setup_mode | Setup Mode | Input parameter. The configuration mode for setting up the Cassandra table. The options are `Sync` (default) or `Off`. |\n| cluster_kwargs | Cluster arguments | Input parameter. An optional dictionary of additional keyword arguments for the Cassandra cluster. |\n| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |\n| ingest_data | Ingest Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |\n| embedding | Embedding | Input parameter. The embedding model to use. |\n| number_of_results | Number of Results | Input parameter. The number of results to return in similarity search. Only relevant for reads. Default: 4. |\n| search_type | Search Type | Input parameter. The search type to use. The options are `Traversal` (default), `MMR Traversal`, `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. |\n| depth | Depth of traversal | Input parameter. The maximum depth of edges to traverse. Only relevant if **Search Type** (`search_type`) is `Traversal` or `MMR Traversal`. Default: 1. |\n| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for search results. Only relevant for reads using the `Similarity with score threshold` search type. |\n| search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata search filters to apply in addition to graph traversal and similarity search. |\n\n### See also\n\n* [**DataStax** bundle](/bundles-datastax)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "cassandra",
        "chat",
        "datastax",
        "graph",
        "history",
        "memory",
        "message",
        "name",
        "note",
        "parameters",
        "search",
        "store",
        "table",
        "type",
        "vector"
      ]
    },
    "/bundles-chroma": {
      "title": "Chroma",
      "slug": "/bundles-chroma",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Chroma** bundle.\n\n## Chroma DB\n\nYou can use the **Chroma DB** component to read and write to a Chroma database using an instance of `Chroma` vector store.\nIncludes support for remote or in-memory instances with or without persistence.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\nWhen writing, the component can create a new database or collection at the specified location.\n\n**Note:** An ephemeral (non-persistent) local Chroma vector store is helpful for testing vector search flows where you don't need to retain the database.\n\n### Use the Chroma DB component in a flow\n\nThe following example flow uses one **Chroma DB** component for both reads and writes:\n\n![ChromaDB receiving split text](/img/component-chroma-db.png)\n\n* When writing, it splits `Data` from a [**URL** component](/components-data#url) into chunks, computes embeddings with attached **Embedding Model** component, and then loads the chunks and embeddings into the Chroma vector store.\nTo trigger writes, click  **Run component** on the **Chroma DB** component.\n\n* When reading, it uses chat input to perform a similarity search on the vector store, and then print the search results to the chat.\nTo trigger reads, open the **Playground** and enter a chat message.\n\nAfter running the flow once, you can click  **Inspect Output** on each component to understand how the data transformed as it passed from component to component.\n\n### Chroma DB parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the provider's documentation or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Collection Name** (`collection_name`) | String | Input parameter.  The name of your Chroma vector store collection. Default: `langflow`. |\n| **Persist Directory** (`persist_directory`) | String | Input parameter. To persist the Chroma database, enter a relative or absolute path to a directory to store the `chroma.sqlite3` file. Leave empty for an ephemeral database. When reading or writing to an existing persistent database, specify the path to the persistent directory. |\n| **Ingest Data** (`ingest_data`) | Data or DataFrame | Input parameter. `Data` or `DataFrame` input containing the records to write to the vector store. Only relevant for writes. |\n| **Search Query** (`search_query`) | String | Input parameter. The query to use for vector search. Only relevant for reads. |\n| **Cache Vector Store** (`cache_vector_store`) | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| **Embedding** (`embedding`) | Embeddings | Input parameter. The embedding function to use for the vector store. By default, Chroma DB uses its built-in embeddings model, or you can attach an **Embedding Model** component to use a different provider or model. |\n| **CORS Allow Origins** (`chroma_server_cors_allow_origins`) | String | Input parameter. The allowed CORS origins for the Chroma server. |\n| **Chroma Server Host** (`chroma_server_host`) | String | Input parameter. The host for the Chroma server. |\n| **Chroma Server HTTP Port** (`chroma_server_http_port`) | Integer | Input parameter. The HTTP port for the Chroma server. |\n| **Chroma Server gRPC Port** (`chroma_server_grpc_port`) | Integer | Input parameter. The gRPC port for the Chroma server. |\n| **Chroma Server SSL Enabled** (`chroma_server_ssl_enabled`) | Boolean | Input parameter. Enable SSL for the Chroma server. |\n| **Allow Duplicates** (`allow_duplicates`) | Boolean | Input parameter. If `true` (default), writes don't check for existing duplicates in the collection, allowing you to store multiple copies of the same content. If `false`, writes won't add documents that match existing documents already present in the collection. If `false`, it can strictly enforce deduplication by searching the entire collection or only search the number of records, specified in `limit`. Only relevant for writes.|\n| **Search Type** (`search_type`) | String | Input parameter. The type of search to perform, either `Similarity` or `MMR`. Only relevant for reads. |\n| **Number of Results** (`number_of_results`) | Integer | Input parameter. The number of search results to return. Default: `10`. Only relevant for reads. |\n| **Limit** (`limit`) | Integer | Input parameter. Limit the number of records to compare when **Allow Duplicates** is `false`. This can help improve performance when writing to large collections, but it can result in some duplicate records. Only relevant for writes. |\n\n## See also\n\n* [**Local DB** component](/components-bundle-components#vector-stores-bundle)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "allow",
        "bundles",
        "cache",
        "chroma",
        "collection",
        "component",
        "cors",
        "data",
        "directory",
        "duplicates",
        "embedding",
        "enabled",
        "flow",
        "grpc",
        "host",
        "http",
        "ingest",
        "inspect",
        "limit",
        "local"
      ]
    },
    "bundles-chroma": {
      "title": "Chroma",
      "slug": "/bundles-chroma",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Chroma** bundle.\n\n## Chroma DB\n\nYou can use the **Chroma DB** component to read and write to a Chroma database using an instance of `Chroma` vector store.\nIncludes support for remote or in-memory instances with or without persistence.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\nWhen writing, the component can create a new database or collection at the specified location.\n\n**Note:** An ephemeral (non-persistent) local Chroma vector store is helpful for testing vector search flows where you don't need to retain the database.\n\n### Use the Chroma DB component in a flow\n\nThe following example flow uses one **Chroma DB** component for both reads and writes:\n\n![ChromaDB receiving split text](/img/component-chroma-db.png)\n\n* When writing, it splits `Data` from a [**URL** component](/components-data#url) into chunks, computes embeddings with attached **Embedding Model** component, and then loads the chunks and embeddings into the Chroma vector store.\nTo trigger writes, click  **Run component** on the **Chroma DB** component.\n\n* When reading, it uses chat input to perform a similarity search on the vector store, and then print the search results to the chat.\nTo trigger reads, open the **Playground** and enter a chat message.\n\nAfter running the flow once, you can click  **Inspect Output** on each component to understand how the data transformed as it passed from component to component.\n\n### Chroma DB parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the provider's documentation or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Collection Name** (`collection_name`) | String | Input parameter.  The name of your Chroma vector store collection. Default: `langflow`. |\n| **Persist Directory** (`persist_directory`) | String | Input parameter. To persist the Chroma database, enter a relative or absolute path to a directory to store the `chroma.sqlite3` file. Leave empty for an ephemeral database. When reading or writing to an existing persistent database, specify the path to the persistent directory. |\n| **Ingest Data** (`ingest_data`) | Data or DataFrame | Input parameter. `Data` or `DataFrame` input containing the records to write to the vector store. Only relevant for writes. |\n| **Search Query** (`search_query`) | String | Input parameter. The query to use for vector search. Only relevant for reads. |\n| **Cache Vector Store** (`cache_vector_store`) | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| **Embedding** (`embedding`) | Embeddings | Input parameter. The embedding function to use for the vector store. By default, Chroma DB uses its built-in embeddings model, or you can attach an **Embedding Model** component to use a different provider or model. |\n| **CORS Allow Origins** (`chroma_server_cors_allow_origins`) | String | Input parameter. The allowed CORS origins for the Chroma server. |\n| **Chroma Server Host** (`chroma_server_host`) | String | Input parameter. The host for the Chroma server. |\n| **Chroma Server HTTP Port** (`chroma_server_http_port`) | Integer | Input parameter. The HTTP port for the Chroma server. |\n| **Chroma Server gRPC Port** (`chroma_server_grpc_port`) | Integer | Input parameter. The gRPC port for the Chroma server. |\n| **Chroma Server SSL Enabled** (`chroma_server_ssl_enabled`) | Boolean | Input parameter. Enable SSL for the Chroma server. |\n| **Allow Duplicates** (`allow_duplicates`) | Boolean | Input parameter. If `true` (default), writes don't check for existing duplicates in the collection, allowing you to store multiple copies of the same content. If `false`, writes won't add documents that match existing documents already present in the collection. If `false`, it can strictly enforce deduplication by searching the entire collection or only search the number of records, specified in `limit`. Only relevant for writes.|\n| **Search Type** (`search_type`) | String | Input parameter. The type of search to perform, either `Similarity` or `MMR`. Only relevant for reads. |\n| **Number of Results** (`number_of_results`) | Integer | Input parameter. The number of search results to return. Default: `10`. Only relevant for reads. |\n| **Limit** (`limit`) | Integer | Input parameter. Limit the number of records to compare when **Allow Duplicates** is `false`. This can help improve performance when writing to large collections, but it can result in some duplicate records. Only relevant for writes. |\n\n## See also\n\n* [**Local DB** component](/components-bundle-components#vector-stores-bundle)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "allow",
        "bundles",
        "cache",
        "chroma",
        "collection",
        "component",
        "cors",
        "data",
        "directory",
        "duplicates",
        "embedding",
        "enabled",
        "flow",
        "grpc",
        "host",
        "http",
        "ingest",
        "inspect",
        "limit",
        "local"
      ]
    },
    "/bundles-cleanlab": {
      "title": "Cleanlab",
      "slug": "/bundles-cleanlab",
      "category": "components",
      "content": "[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.\n\nUse the Cleanlab components to integrate Cleanlab Evaluations with Langflow and unlock trustworthy agentic, RAG, and LLM pipelines with Cleanlab's evaluation and remediation suite.\n\nYou can use these components to quantify the trustworthiness of any LLM response with a score between `0` and `1`, and explain why a response may be good or bad. For RAG or agent pipelines with context, you can evaluate context sufficiency, groundedness, helpfulness, and query clarity with quantitative scores. Additionally, you can remediate low-trust responses with warnings or fallback answers.\n\nAuthentication is required with a Cleanlab API key.\n\n## Cleanlab Evaluator\n\nThe **Cleanlab Evaluator** component evaluates and explains the trustworthiness of a prompt and response pair using Cleanlab. For more information on how the score works, see the [Cleanlab documentation](https://help.cleanlab.ai/tlm/).\n\n### Cleanlab Evaluator parameters\n\n| Name                    | Type       | Description                        |\n|-------------------------|------------|------------------------------------|\n| system_prompt           | Message    | Input parameter. The system message prepended to the prompt. Optional. |\n| prompt                  | Message    | Input parameter. The user-facing input to the LLM.  |\n| response                | Message    | Input parameter. The model's response to evaluate.    |\n| cleanlab_api_key        | Secret     | Input parameter. Your Cleanlab API key.  |\n| cleanlab_evaluation_model | Dropdown   | Input parameter. Evaluation model used by Cleanlab, such as GPT-4 or Claude. This doesn't need to be the same model that generated the response. |\n| quality_preset          | Dropdown   | Input parameter. Tradeoff between evaluation speed and accuracy. |\n\n### Cleanlab Evaluator outputs\n\nThe **Cleanlab Evaluator** component has three possible outputs.\n\n| Name                    | Type       | Description            |\n|-------------------------|------------|-------------------------|\n| score                   | number, float | Displays the trust score between 0 and 1.  |\n| explanation             | `Message`    | Provides an explanation of the trust score. |\n| response                | `Message`    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |\n\n## Cleanlab Remediator\n\nThe **Cleanlab Remediator** component uses the trust score from the [**Cleanlab Evaluator** component](#cleanlab-evaluator) to determine whether to show, warn about, or replace an LLM response.\n\nThis component has parameters for the score threshold, warning text, and fallback message that you can customize as needed.\n\nThe output is **Remediated Response** (`remediated_response`), which is a `Message` containing the final message shown to the user after remediation logic is applied.\n\n### Cleanlab Remediator parameters\n\n| Name                        | Type       | Description |\n|-----------------------------|------------|---------|\n| response                    | Message    | Input parameter. The response to potentially remediate.  |\n| score                       | Number     | Input parameter. The trust score from `CleanlabEvaluator`. |\n| explanation                 | Message    | Input parameter. The explanation to append if a warning is shown. Optional.|\n| threshold                   | Float      | Input parameter. The minimum trust score to pass a response unchanged.  |\n| show_untrustworthy_response | Boolean      | Input parameter. Whether to display or hide the original response with a warning if a response is deemed untrustworthy. |\n| untrustworthy_warning_text  | Prompt     | Input parameter. The warning text for untrustworthy responses. |\n| fallback_text              | Prompt     | Input parameter. The fallback message if the response is hidden. |\n\n## Cleanlab RAG Evaluator\n\nThe **Cleanlab RAG Evaluator** component evaluates RAG and LLM pipeline outputs for trustworthiness, context sufficiency, response groundedness, helpfulness, and query ease using [Cleanlab's evaluation metrics](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/).\n\nYou can pair this component with the [**Cleanlab Remediator** component](#cleanlab-remediator) to remediate low-trust responses coming from the RAG pipeline.\n\n### Cleanlab RAG Evaluator parameters\n\n| Name                        | Type       | Description |\n|-----------------------------|------------|------------|\n| cleanlab_api_key           | Secret     | Input parameter. Your Cleanlab API key.    |\n| cleanlab_evaluation_model  | Dropdown   | Input parameter. The evaluation model used by Cleanlab, such as GPT-4, or Claude. This doesn't need to be the same model that generated the response. |\n| quality_preset             | Dropdown   | Input parameter. The tradeoff between evaluation speed and accuracy.  |\n| context                    | Message    | Input parameter. The retrieved context from your RAG system.   |\n| query                      | Message    | Input parameter. The original user query.   |\n| response                   | Message    | Input parameter. The model's response based on the context and query. |\n| run_context_sufficiency    | Boolean      | Input parameter. Evaluate whether context supports answering the query.  |\n| run_response_groundedness  | Boolean      | Input parameter. Evaluate whether the response is grounded in the context. |\n| run_response_helpfulness   | Boolean      | Input parameter. Evaluate how helpful the response is.  |\n| run_query_ease            | Boolean      | Input parameter. Evaluate if the query is vague, complex, or adversarial. |\n\n### Cleanlab RAG Evaluator outputs\n\nThe **Cleanlab RAG Evaluator** component has the following output options:\n\n| Name               | Type       | Description              |\n|--------------------|------------|--------------------------|\n| trust_score        | Number     | The overall trust score. |\n| trust_explanation  | Message    | The explanation for the trust score. |\n| other_scores       | Dictionary | A dictionary of optional enabled RAG evaluation metrics. |\n| evaluation_summary | Message    | A Markdown summary of query, context, response, and evaluation results. |\n| response           | Message    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |\n\n## Example Cleanlab flows\n\nThe following example flows show how to use the **Cleanlab Evaluator** and **Cleanlab Remediator** components to evaluate and remediate responses from any LLM, and how to use the **Cleanlab RAG Evaluator** component to evaluate RAG pipeline outputs.\n\n### Evaluate and remediate responses from an LLM\n\nThis flow evaluates and remediates the trustworthiness of a response from any LLM using the **Cleanlab Evaluator** and **Cleanlab Remediator** components.\n\nYou can [download the Evaluate and Remediate flow](/files/eval_and_remediate_cleanlab.json), and then [import the flow](/concepts-flows-import) to your Langflow instance.\nOr, you can build the flow from scratch by connecting the following components:\n\n* Connect the `Message` output from any **Language Model** or **Agent** component to the **Response** input of the **Cleanlab Evaluator** component.\n* Connect a **Prompt Template** component to the **Cleanlab Evaluator** component's **Prompt** input.\n\n![Evaluate response trustworthiness](/img/eval_response.png)\n\nWhen you run the flow, the **Cleanlab Evaluator** component returns a trust score and explanation from the flow.\n\nThe **Cleanlab Remediator** component uses this trust score to determine whether to output the original response, warn about it, or replace it with a fallback answer.\n\nThis example shows a response that was determined to be untrustworthy (a score of `.09`) and flagged with a warning by the **Cleanlab Remediator** component.\n\n![Cleanlab Remediator Example](/img/cleanlab_remediator_example.png)\n\nTo hide untrustworthy responses, configure the **Cleanlab Remediator** component to replace the response with a fallback message.\n\n![Cleanlab Remediator Example](/img/cleanlab_remediator_example_fallback.png)\n\n### Evaluate RAG pipeline\n\nAs an example, create a flow based on the **Vector Store RAG** template, and then add the **Cleanlab RAG Evaluator** component to evaluate the flow's context, query, and response.\nConnect the **context**, **query**, and **response** outputs from the other components in the RAG flow to the **Cleanlab RAG Evaluator** component.\n\n![Evaluate RAG pipeline](/img/eval_rag.png)\n\nHere is an example of the `Evaluation Summary` output from the **Cleanlab RAG Evaluator** component:\n\n![Evaluate RAG pipeline](/img/eval_summary_rag.png)\n\nThe `Evaluation Summary` includes the query, context, response, and all evaluation results. In this example, the `Context Sufficiency` and `Response Groundedness` scores are low (a score of `0.002`) because the context doesn't contain information about the query, and the response isn't grounded in the context.",
      "summary": "[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.",
      "keywords": [
        "agent",
        "cleanlab",
        "context",
        "evaluate",
        "evaluator",
        "flows",
        "language",
        "llm",
        "model",
        "outputs",
        "parameters",
        "pipeline",
        "prompt",
        "query",
        "rag",
        "remediate",
        "remediated",
        "remediator",
        "response",
        "responses"
      ]
    },
    "bundles-cleanlab": {
      "title": "Cleanlab",
      "slug": "/bundles-cleanlab",
      "category": "components",
      "content": "[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.\n\nUse the Cleanlab components to integrate Cleanlab Evaluations with Langflow and unlock trustworthy agentic, RAG, and LLM pipelines with Cleanlab's evaluation and remediation suite.\n\nYou can use these components to quantify the trustworthiness of any LLM response with a score between `0` and `1`, and explain why a response may be good or bad. For RAG or agent pipelines with context, you can evaluate context sufficiency, groundedness, helpfulness, and query clarity with quantitative scores. Additionally, you can remediate low-trust responses with warnings or fallback answers.\n\nAuthentication is required with a Cleanlab API key.\n\n## Cleanlab Evaluator\n\nThe **Cleanlab Evaluator** component evaluates and explains the trustworthiness of a prompt and response pair using Cleanlab. For more information on how the score works, see the [Cleanlab documentation](https://help.cleanlab.ai/tlm/).\n\n### Cleanlab Evaluator parameters\n\n| Name                    | Type       | Description                        |\n|-------------------------|------------|------------------------------------|\n| system_prompt           | Message    | Input parameter. The system message prepended to the prompt. Optional. |\n| prompt                  | Message    | Input parameter. The user-facing input to the LLM.  |\n| response                | Message    | Input parameter. The model's response to evaluate.    |\n| cleanlab_api_key        | Secret     | Input parameter. Your Cleanlab API key.  |\n| cleanlab_evaluation_model | Dropdown   | Input parameter. Evaluation model used by Cleanlab, such as GPT-4 or Claude. This doesn't need to be the same model that generated the response. |\n| quality_preset          | Dropdown   | Input parameter. Tradeoff between evaluation speed and accuracy. |\n\n### Cleanlab Evaluator outputs\n\nThe **Cleanlab Evaluator** component has three possible outputs.\n\n| Name                    | Type       | Description            |\n|-------------------------|------------|-------------------------|\n| score                   | number, float | Displays the trust score between 0 and 1.  |\n| explanation             | `Message`    | Provides an explanation of the trust score. |\n| response                | `Message`    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |\n\n## Cleanlab Remediator\n\nThe **Cleanlab Remediator** component uses the trust score from the [**Cleanlab Evaluator** component](#cleanlab-evaluator) to determine whether to show, warn about, or replace an LLM response.\n\nThis component has parameters for the score threshold, warning text, and fallback message that you can customize as needed.\n\nThe output is **Remediated Response** (`remediated_response`), which is a `Message` containing the final message shown to the user after remediation logic is applied.\n\n### Cleanlab Remediator parameters\n\n| Name                        | Type       | Description |\n|-----------------------------|------------|---------|\n| response                    | Message    | Input parameter. The response to potentially remediate.  |\n| score                       | Number     | Input parameter. The trust score from `CleanlabEvaluator`. |\n| explanation                 | Message    | Input parameter. The explanation to append if a warning is shown. Optional.|\n| threshold                   | Float      | Input parameter. The minimum trust score to pass a response unchanged.  |\n| show_untrustworthy_response | Boolean      | Input parameter. Whether to display or hide the original response with a warning if a response is deemed untrustworthy. |\n| untrustworthy_warning_text  | Prompt     | Input parameter. The warning text for untrustworthy responses. |\n| fallback_text              | Prompt     | Input parameter. The fallback message if the response is hidden. |\n\n## Cleanlab RAG Evaluator\n\nThe **Cleanlab RAG Evaluator** component evaluates RAG and LLM pipeline outputs for trustworthiness, context sufficiency, response groundedness, helpfulness, and query ease using [Cleanlab's evaluation metrics](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/).\n\nYou can pair this component with the [**Cleanlab Remediator** component](#cleanlab-remediator) to remediate low-trust responses coming from the RAG pipeline.\n\n### Cleanlab RAG Evaluator parameters\n\n| Name                        | Type       | Description |\n|-----------------------------|------------|------------|\n| cleanlab_api_key           | Secret     | Input parameter. Your Cleanlab API key.    |\n| cleanlab_evaluation_model  | Dropdown   | Input parameter. The evaluation model used by Cleanlab, such as GPT-4, or Claude. This doesn't need to be the same model that generated the response. |\n| quality_preset             | Dropdown   | Input parameter. The tradeoff between evaluation speed and accuracy.  |\n| context                    | Message    | Input parameter. The retrieved context from your RAG system.   |\n| query                      | Message    | Input parameter. The original user query.   |\n| response                   | Message    | Input parameter. The model's response based on the context and query. |\n| run_context_sufficiency    | Boolean      | Input parameter. Evaluate whether context supports answering the query.  |\n| run_response_groundedness  | Boolean      | Input parameter. Evaluate whether the response is grounded in the context. |\n| run_response_helpfulness   | Boolean      | Input parameter. Evaluate how helpful the response is.  |\n| run_query_ease            | Boolean      | Input parameter. Evaluate if the query is vague, complex, or adversarial. |\n\n### Cleanlab RAG Evaluator outputs\n\nThe **Cleanlab RAG Evaluator** component has the following output options:\n\n| Name               | Type       | Description              |\n|--------------------|------------|--------------------------|\n| trust_score        | Number     | The overall trust score. |\n| trust_explanation  | Message    | The explanation for the trust score. |\n| other_scores       | Dictionary | A dictionary of optional enabled RAG evaluation metrics. |\n| evaluation_summary | Message    | A Markdown summary of query, context, response, and evaluation results. |\n| response           | Message    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |\n\n## Example Cleanlab flows\n\nThe following example flows show how to use the **Cleanlab Evaluator** and **Cleanlab Remediator** components to evaluate and remediate responses from any LLM, and how to use the **Cleanlab RAG Evaluator** component to evaluate RAG pipeline outputs.\n\n### Evaluate and remediate responses from an LLM\n\nThis flow evaluates and remediates the trustworthiness of a response from any LLM using the **Cleanlab Evaluator** and **Cleanlab Remediator** components.\n\nYou can [download the Evaluate and Remediate flow](/files/eval_and_remediate_cleanlab.json), and then [import the flow](/concepts-flows-import) to your Langflow instance.\nOr, you can build the flow from scratch by connecting the following components:\n\n* Connect the `Message` output from any **Language Model** or **Agent** component to the **Response** input of the **Cleanlab Evaluator** component.\n* Connect a **Prompt Template** component to the **Cleanlab Evaluator** component's **Prompt** input.\n\n![Evaluate response trustworthiness](/img/eval_response.png)\n\nWhen you run the flow, the **Cleanlab Evaluator** component returns a trust score and explanation from the flow.\n\nThe **Cleanlab Remediator** component uses this trust score to determine whether to output the original response, warn about it, or replace it with a fallback answer.\n\nThis example shows a response that was determined to be untrustworthy (a score of `.09`) and flagged with a warning by the **Cleanlab Remediator** component.\n\n![Cleanlab Remediator Example](/img/cleanlab_remediator_example.png)\n\nTo hide untrustworthy responses, configure the **Cleanlab Remediator** component to replace the response with a fallback message.\n\n![Cleanlab Remediator Example](/img/cleanlab_remediator_example_fallback.png)\n\n### Evaluate RAG pipeline\n\nAs an example, create a flow based on the **Vector Store RAG** template, and then add the **Cleanlab RAG Evaluator** component to evaluate the flow's context, query, and response.\nConnect the **context**, **query**, and **response** outputs from the other components in the RAG flow to the **Cleanlab RAG Evaluator** component.\n\n![Evaluate RAG pipeline](/img/eval_rag.png)\n\nHere is an example of the `Evaluation Summary` output from the **Cleanlab RAG Evaluator** component:\n\n![Evaluate RAG pipeline](/img/eval_summary_rag.png)\n\nThe `Evaluation Summary` includes the query, context, response, and all evaluation results. In this example, the `Context Sufficiency` and `Response Groundedness` scores are low (a score of `0.002`) because the context doesn't contain information about the query, and the response isn't grounded in the context.",
      "summary": "[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.",
      "keywords": [
        "agent",
        "cleanlab",
        "context",
        "evaluate",
        "evaluator",
        "flows",
        "language",
        "llm",
        "model",
        "outputs",
        "parameters",
        "pipeline",
        "prompt",
        "query",
        "rag",
        "remediate",
        "remediated",
        "remediator",
        "response",
        "responses"
      ]
    },
    "/bundles-clickhouse": {
      "title": "ClickHouse",
      "slug": "/bundles-clickhouse",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **ClickHouse** bundle.\n\n## ClickHouse vector store\n\nThe **ClickHouse** component reads and writes to a ClickHouse vector store using an instance of `ClickHouse` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### ClickHouse parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [ClickHouse Documentation](https://clickhouse.com/docs/en/intro) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| host | hostname | Input parameter. The ClickHouse server hostname. Required. Default: `localhost`. |\n| port | port | Input parameter. The ClickHouse server port. Required. Default: `8123`. |\n| database | database | Input parameter. The ClickHouse database name. Required. |\n| table | Table name | Input parameter. The ClickHouse table name. Required. |\n| username | Username | Input parameter. ClickHouse username for authentication. Required. |\n| password | Password | Input parameter. ClickHouse password for authentication. Required. |\n| index_type | index_type | Input parameter. Type of the index, either `annoy` (default) or `vector_similarity`. |\n| metric | metric | Input parameter. Metric to compute distance for similarity search. The options are `angular` (default), `euclidean`, `manhattan`, `hamming`, `dot`. |\n| secure | Use HTTPS/TLS | Input parameter. If `true`, enables HTTPS/TLS for the ClickHouse server and overrides inferred values for interface or port arguments. Default: `false`. |\n| index_param | Param of the index | Input parameter. Index parameters. Default: `100,'L2Distance'`. |\n| index_query_params | index query params | Input parameter. Additional index query parameters. |\n| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |\n| ingest_data | Ingest Data | Input parameter. The records to load into the vector store. |\n| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embedding | Input parameter. The embedding model to use. |\n| number_of_results | Number of Results | Input parameter. The number of search results to return. Default: `4`. Only relevant for reads. |\n| score_threshold | Score threshold | Input parameter. The threshold for similarity score comparison. Default: Unset (no threshold). Only relevant for reads. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "clickhouse",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "bundles-clickhouse": {
      "title": "ClickHouse",
      "slug": "/bundles-clickhouse",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **ClickHouse** bundle.\n\n## ClickHouse vector store\n\nThe **ClickHouse** component reads and writes to a ClickHouse vector store using an instance of `ClickHouse` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### ClickHouse parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [ClickHouse Documentation](https://clickhouse.com/docs/en/intro) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| host | hostname | Input parameter. The ClickHouse server hostname. Required. Default: `localhost`. |\n| port | port | Input parameter. The ClickHouse server port. Required. Default: `8123`. |\n| database | database | Input parameter. The ClickHouse database name. Required. |\n| table | Table name | Input parameter. The ClickHouse table name. Required. |\n| username | Username | Input parameter. ClickHouse username for authentication. Required. |\n| password | Password | Input parameter. ClickHouse password for authentication. Required. |\n| index_type | index_type | Input parameter. Type of the index, either `annoy` (default) or `vector_similarity`. |\n| metric | metric | Input parameter. Metric to compute distance for similarity search. The options are `angular` (default), `euclidean`, `manhattan`, `hamming`, `dot`. |\n| secure | Use HTTPS/TLS | Input parameter. If `true`, enables HTTPS/TLS for the ClickHouse server and overrides inferred values for interface or port arguments. Default: `false`. |\n| index_param | Param of the index | Input parameter. Index parameters. Default: `100,'L2Distance'`. |\n| index_query_params | index query params | Input parameter. Additional index query parameters. |\n| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |\n| ingest_data | Ingest Data | Input parameter. The records to load into the vector store. |\n| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embedding | Input parameter. The embedding model to use. |\n| number_of_results | Number of Results | Input parameter. The number of search results to return. Default: `4`. Only relevant for reads. |\n| score_threshold | Score threshold | Input parameter. The threshold for similarity score comparison. Default: Unset (no threshold). Only relevant for reads. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "clickhouse",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "/bundles-cloudflare": {
      "title": "Cloudflare",
      "slug": "/bundles-cloudflare",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cloudflare** bundle.\n\n## Cloudflare Workers AI Embeddings\n\nThe **Cloudflare Workers AI Embeddings** component generates embeddings using [Cloudflare Workers AI models](https://developers.cloudflare.com/workers-ai/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Cloudflare Workers AI Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| account_id | Cloudflare account ID | Input parameter. Your [Cloudflare account ID](https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages). |\n| api_token | Cloudflare API token | Input parameter. Your [Cloudflare API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/). |\n| model_name | Model Name | Input parameter. A [supported model](https://developers.cloudflare.com/workers-ai/models/#text-embeddings) for embedding generation. |\n| strip_new_lines | Strip New Lines | Input parameter. Whether to strip new lines from the input text. |\n| batch_size | Batch Size | Input parameter. The number of texts to embed in each batch. |\n| api_base_url | Cloudflare API base URL | Input parameter. The base URL for the Cloudflare API. |\n| headers | Headers | Input parameter. Additional headers for the embedding generation API request. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "cloudflare",
        "embeddings",
        "parameters",
        "workers"
      ]
    },
    "bundles-cloudflare": {
      "title": "Cloudflare",
      "slug": "/bundles-cloudflare",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cloudflare** bundle.\n\n## Cloudflare Workers AI Embeddings\n\nThe **Cloudflare Workers AI Embeddings** component generates embeddings using [Cloudflare Workers AI models](https://developers.cloudflare.com/workers-ai/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Cloudflare Workers AI Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| account_id | Cloudflare account ID | Input parameter. Your [Cloudflare account ID](https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages). |\n| api_token | Cloudflare API token | Input parameter. Your [Cloudflare API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/). |\n| model_name | Model Name | Input parameter. A [supported model](https://developers.cloudflare.com/workers-ai/models/#text-embeddings) for embedding generation. |\n| strip_new_lines | Strip New Lines | Input parameter. Whether to strip new lines from the input text. |\n| batch_size | Batch Size | Input parameter. The number of texts to embed in each batch. |\n| api_base_url | Cloudflare API base URL | Input parameter. The base URL for the Cloudflare API. |\n| headers | Headers | Input parameter. Additional headers for the embedding generation API request. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "cloudflare",
        "embeddings",
        "parameters",
        "workers"
      ]
    },
    "/bundles-cohere": {
      "title": "Cohere",
      "slug": "/bundles-cohere",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cohere** bundle.\n\nFor more information about Cohere features and functionality used by Cohere components, see the [Cohere documentation](https://cohere.ai/).\n\n## Cohere text generation\n\nThis component generates text using Cohere's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Cohere model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Cohere text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Input | String | Input parameter. Specifies the input text for text generation. |\n| System Message | String | Input parameter. A [system message](https://docs.cohere.com/docs/system-instructions) to pass to the model. |\n| Stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |\n| Cohere API Key | SecretString | Input parameter. Your Cohere API key. |\n| Temperature | Float | Input parameter. Specifies the randomness of sampling. Lower values (near 0) are more deterministic, and higher values (near 1) are more creative. Defaults to `0.75`. |\n\n## Cohere Embeddings\n\nThe **Cohere Embeddings** component is used to load embedding models from Cohere.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Cohere Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| cohere_api_key | SecretString | Input parameter. The API key required to authenticate with the Cohere service. |\n| model | String | Input parameter. The language model used for embedding text documents and performing queries. Default: `embed-english-v2.0` |\n| truncate | Boolean | Input parameter. How to handle input that exceeds the model's token limit. One of `NONE`, `START`, or `END` (default). For more information, see the [Cohere `truncate` API reference](https://docs.cohere.com/reference/embed#request.body.truncate). |\n| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: `3` |\n| user_agent | String | Input parameter. A user agent string to include in requests. Default: `langchain`|\n| request_timeout | Float | Input parameter. The timeout duration for requests in seconds. Default: None |\n\n## Cohere Rerank\n\nThis component finds and reranks documents using the Cohere API.\n\nOutputs `Data` containing the reranked documents, limited by the **Top N** parameter.\n\n### Cohere Rerank parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Search Query** | String | Input parameter. The search query for reranking documents. |\n| **Search Results** | Data | Input parameter. Connect search results output from a vector store component. Use this parameter to apply reranking after running a similarity search on your vector database. |\n| **Top N** | Integer | Input parameter. The number of documents to return after reranking. Default: `3`. |\n| **Cohere API Key** | SecretString | Input parameter. Your Cohere API key. |\n| **Model** | String | Input parameter. The re-ranker model to use. Default: `rerank-english-v3.0` |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "bundles",
        "cohere",
        "embeddings",
        "function",
        "generation",
        "key",
        "language",
        "model",
        "parameters",
        "query",
        "rerank",
        "response",
        "results",
        "search",
        "smart",
        "text",
        "top"
      ]
    },
    "bundles-cohere": {
      "title": "Cohere",
      "slug": "/bundles-cohere",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Cohere** bundle.\n\nFor more information about Cohere features and functionality used by Cohere components, see the [Cohere documentation](https://cohere.ai/).\n\n## Cohere text generation\n\nThis component generates text using Cohere's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Cohere model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Cohere text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Input | String | Input parameter. Specifies the input text for text generation. |\n| System Message | String | Input parameter. A [system message](https://docs.cohere.com/docs/system-instructions) to pass to the model. |\n| Stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |\n| Cohere API Key | SecretString | Input parameter. Your Cohere API key. |\n| Temperature | Float | Input parameter. Specifies the randomness of sampling. Lower values (near 0) are more deterministic, and higher values (near 1) are more creative. Defaults to `0.75`. |\n\n## Cohere Embeddings\n\nThe **Cohere Embeddings** component is used to load embedding models from Cohere.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Cohere Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| cohere_api_key | SecretString | Input parameter. The API key required to authenticate with the Cohere service. |\n| model | String | Input parameter. The language model used for embedding text documents and performing queries. Default: `embed-english-v2.0` |\n| truncate | Boolean | Input parameter. How to handle input that exceeds the model's token limit. One of `NONE`, `START`, or `END` (default). For more information, see the [Cohere `truncate` API reference](https://docs.cohere.com/reference/embed#request.body.truncate). |\n| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: `3` |\n| user_agent | String | Input parameter. A user agent string to include in requests. Default: `langchain`|\n| request_timeout | Float | Input parameter. The timeout duration for requests in seconds. Default: None |\n\n## Cohere Rerank\n\nThis component finds and reranks documents using the Cohere API.\n\nOutputs `Data` containing the reranked documents, limited by the **Top N** parameter.\n\n### Cohere Rerank parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Search Query** | String | Input parameter. The search query for reranking documents. |\n| **Search Results** | Data | Input parameter. Connect search results output from a vector store component. Use this parameter to apply reranking after running a similarity search on your vector database. |\n| **Top N** | Integer | Input parameter. The number of documents to return after reranking. Default: `3`. |\n| **Cohere API Key** | SecretString | Input parameter. Your Cohere API key. |\n| **Model** | String | Input parameter. The re-ranker model to use. Default: `rerank-english-v3.0` |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "bundles",
        "cohere",
        "embeddings",
        "function",
        "generation",
        "key",
        "language",
        "model",
        "parameters",
        "query",
        "rerank",
        "response",
        "results",
        "search",
        "smart",
        "text",
        "top"
      ]
    },
    "/bundles-composio": {
      "title": "Composio",
      "slug": "/bundles-composio",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Composio** bundle.\n\nFor information about specific Composio functionality, see the [Composio documentation](https://docs.composio.dev/introduction/intro/overview).\n\n## Use Composio components in a flow\n\nComposio components are primarily used as [tools for agents](/agents-tools).\n\nThe **Composio** bundle includes an aggregate **Composio Tools** component and the following single-service components:\n\n- **Dropbox**\n- **GitHub**\n- **Gmail**\n- **Google Calendar**\n- **Google Meet**\n- **Google Tasks**\n- **Linear**\n- **Outlook**\n- **Reddit**\n- **Slack** (your Slack account)\n- **Slackbot** (bot integration)\n- **Supabase**\n- **Todoist**\n- **Youtube**\n\nThe **Composio Tools** component is an access point for multiple Composio services (tools).\nHowever, most of these services are also available as single-service components, which are recommended over the **Composio Tools** component.\n\nAlthough you can use single-service components for non-agentic actions in your flows, they are primarily used in **Tool Mode** with **Agent** components.\nIn contrast, the **Composio Tools** component can _only_ be a tool for agents; it doesn't support non-agentic use.\n\nThe following example demonstrates how to use the Composio **Gmail** component as a tool for an **Agent** component.\nThis allows the agent to use Composio Gmail functionality, if necessary, when generating responses.\nYou can connect other Composio components in the same way.\n\n1. Create a flow based on the **Simple Agent** template.\n\n2. In  **Bundles**, find the **Composio** bundle, and then add the **Gmail** component to the flow.\n\n3. In the **Composio API Key** field, enter your Composio API key or use the `COMPOSIO_API_KEY` global variable.\nFor more information, see [Composio authentication](#composio-authentication).\n\n   If the key is valid, the  **Alert** is replaced by a  **Success** indicator, and the **Actions** list populates with actions available to your API key.\n\n4. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode**.\n\n    If you are using the **Composio Tools** component, skip this step because the component is already configured as a tool.\n\n5. In the **Actions** list, configure the Gmail actions that you want to provide to the agent.\nYou can select the actions you want to allow, and you can edit each action's slug (agentic label) and description, which help the agent decide which tools to use.\n\n6. Connect the **Gmail** component's **Toolset** output to the **Agent** component's **Tools** input.\n\n7. In the **Agent** component, enter your OpenAI API key or configure the **Agent** component to use a different LLM.\nFor more information, see [Language model components](/components-models).\n\n    At this point, your flow has four connected components:\n\n    * The **Chat Input** component is connected to the **Agent** component's **Input** port.\n    This allows the flow to be triggered by an incoming prompt from a user or application.\n    * The **Gmail** component is connected to the **Agent** component as a tool.\n    The agent may not use this tool for every request; the agent only uses this connection if it decides the Gmail tool can help respond to the prompt.\n    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.\n\n    ![Simple Agent flow with a Composio Tools component](/img/composio-create-flow.png)\n\n8. To test the flow, click **Playground**, and then ask the LLM about the tools available to it.\nThe agent should provide a list of connected tools, including the **Gmail** tool and the built-in tools in the **Agent** component.\nFor example:\n\n    ```text\n    User:\n    What tools are available to you?\n\n    I have access to a variety of tools that allow me to help you with different tasks. Here are the main categories of tools available to me:\n\n    1. Gmail Tools:\n       - Fetch emails, search, and filter messages.\n       - Fetch specific email details by message ID.\n       - Create email drafts (with attachments, HTML, CC/BCC, etc.).\n       - Delete email drafts or specific messages.\n\n    2. Date & Time Tools:\n       - Get the current date and time in any timezone.\n\n    3. Multi-Tool Use:\n       - Run multiple tools in parallel for efficiency.\n\n    If you have a specific task in mind, let me know and I can tell you which tool I would use or demonstrate how I can help!\n    ```\n\n9. To test a specific tool or function, tell the agent to perform an action that uses that tool.\nFor example, ask the agent to write a draft email:\n\n    ```text\n    Create a draft email with the subject line \"Greetings from Composio\"\n    recipient: \"your.email@address.com\"\n    Body content: \"Hello from composio!\"\n    ```\n\n    The **Playground** prints the logic as the agent chooses the `GMAIL_CREATE_EMAIL_DRAFT` tool to create the email draft.\n    The following example response is abbreviated:\n\n    ```text\n    The draft email with the subject \"Greetings from Composio\" and body \"Hello from composio!\" has been successfully created.\n    ```\n\n    ```json\n    {\n      \"recipient_email\": \"your.email@address.com\",\n      \"subject\": \"Greetings from Composio\",\n      \"body\": \"Hello from composio!\",\n      \"is_html\": false\n    }\n\n    {\n      \"data\": {\n        \"response_data\": {\n          \"id\": \"r-237981011463568567\",\n          \"message\": {\n            \"id\": \"195dd80528171132\",\n            \"threadId\": \"195dd80528171132\",\n            \"labelIds\": [\n              \"DRAFT\"\n            ]\n          }\n        }\n      },\n      \"error\": null,\n      \"successfull\": true,\n      \"successful\": true\n    }\n    ```\n\n    For further confirmation, you can go to your Gmail account and find the message in your drafts folder.\n\n10. Optional: To add more Composio services, repeat these steps to add more Composio components to your flow.\nFor each component, provide the necessary credentials, enable **Tool Mode**, configure the actions, and then connect it to the **Agent** component's **Tools** port.\n\n## Composio parameters\n\nAll single-service Composio components have the same parameters, and the **Composio Tools** component has one additional parameter:\n\n| Name | Type | Description |\n|------|------|-------------|\n| entity_id | String | Input parameter. The entity ID for the Composio account. Default: `default`. This parameter is hidden by default in the visual editor. If you need to set this parameter, you can access it through the  **Controls** in the [component's header menu](/concepts-components#component-menus). |\n| api_key | SecretString | Input parameter. The Composio API key for authentication with the Composio platform. Make sure the key authorizes the specific service that you want to use. For more information, see [Composio authentication](#composio-authentication). |\n| tool_name | Connection | Input parameter for the **Composio Tools** component only. Select the Composio service (tool) to connect to. |\n| action | List | Input parameter. Select actions to use. Available actions vary by service. Some actions might require premium access to a particular service. |\n\n## Composio authentication\n\nComposio components require authentication to the Composio platform with a Composio API key.\n\nYou can provide this key directly in your components, or you can use the `COMPOSIO_API_KEY` [global variable](/configuration-global-variables), which Langflow can automatically load from your `.env` file.\n\n**Note:** The Composio API key _only_ handles the connection to Composio.\nService provider authentication is managed through the Composio platform for each service that you want to use.\n\nMake sure that your Composio API key provides access to the required services for the components in your flow.\nFor example, if you are using the Composio **Gmail** component, your Composio API key must have access to the Gmail service.\n\n## Composio output\n\nWhen used as tools for an agent, Composio components output [`Tools`](/data-types#tool), which is a list of tools for use by an agent.\nWhen called by the agent, the response from the Composio service is ingested by the agent, not passed directly as output to the user or application.\n\nIn non-agentic use cases, the output is a [`DataFrame`](/data-types#dataframe) containing the response from the specified Composio service, depending on the component and action used in the flow.\n\nBecause the **Composio Tools** component supports _only_ agentic use, it cannot output `DataFrame`.\nAll single-service Composio components can output either `DataFrame` or `Tools`.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "actions",
        "agent",
        "alert",
        "api",
        "authentication",
        "bundles",
        "calendar",
        "chat",
        "components",
        "composio",
        "controls",
        "dropbox",
        "flow",
        "github",
        "gmail",
        "google",
        "input",
        "key",
        "linear",
        "meet"
      ]
    },
    "bundles-composio": {
      "title": "Composio",
      "slug": "/bundles-composio",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Composio** bundle.\n\nFor information about specific Composio functionality, see the [Composio documentation](https://docs.composio.dev/introduction/intro/overview).\n\n## Use Composio components in a flow\n\nComposio components are primarily used as [tools for agents](/agents-tools).\n\nThe **Composio** bundle includes an aggregate **Composio Tools** component and the following single-service components:\n\n- **Dropbox**\n- **GitHub**\n- **Gmail**\n- **Google Calendar**\n- **Google Meet**\n- **Google Tasks**\n- **Linear**\n- **Outlook**\n- **Reddit**\n- **Slack** (your Slack account)\n- **Slackbot** (bot integration)\n- **Supabase**\n- **Todoist**\n- **Youtube**\n\nThe **Composio Tools** component is an access point for multiple Composio services (tools).\nHowever, most of these services are also available as single-service components, which are recommended over the **Composio Tools** component.\n\nAlthough you can use single-service components for non-agentic actions in your flows, they are primarily used in **Tool Mode** with **Agent** components.\nIn contrast, the **Composio Tools** component can _only_ be a tool for agents; it doesn't support non-agentic use.\n\nThe following example demonstrates how to use the Composio **Gmail** component as a tool for an **Agent** component.\nThis allows the agent to use Composio Gmail functionality, if necessary, when generating responses.\nYou can connect other Composio components in the same way.\n\n1. Create a flow based on the **Simple Agent** template.\n\n2. In  **Bundles**, find the **Composio** bundle, and then add the **Gmail** component to the flow.\n\n3. In the **Composio API Key** field, enter your Composio API key or use the `COMPOSIO_API_KEY` global variable.\nFor more information, see [Composio authentication](#composio-authentication).\n\n   If the key is valid, the  **Alert** is replaced by a  **Success** indicator, and the **Actions** list populates with actions available to your API key.\n\n4. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode**.\n\n    If you are using the **Composio Tools** component, skip this step because the component is already configured as a tool.\n\n5. In the **Actions** list, configure the Gmail actions that you want to provide to the agent.\nYou can select the actions you want to allow, and you can edit each action's slug (agentic label) and description, which help the agent decide which tools to use.\n\n6. Connect the **Gmail** component's **Toolset** output to the **Agent** component's **Tools** input.\n\n7. In the **Agent** component, enter your OpenAI API key or configure the **Agent** component to use a different LLM.\nFor more information, see [Language model components](/components-models).\n\n    At this point, your flow has four connected components:\n\n    * The **Chat Input** component is connected to the **Agent** component's **Input** port.\n    This allows the flow to be triggered by an incoming prompt from a user or application.\n    * The **Gmail** component is connected to the **Agent** component as a tool.\n    The agent may not use this tool for every request; the agent only uses this connection if it decides the Gmail tool can help respond to the prompt.\n    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.\n\n    ![Simple Agent flow with a Composio Tools component](/img/composio-create-flow.png)\n\n8. To test the flow, click **Playground**, and then ask the LLM about the tools available to it.\nThe agent should provide a list of connected tools, including the **Gmail** tool and the built-in tools in the **Agent** component.\nFor example:\n\n    ```text\n    User:\n    What tools are available to you?\n\n    I have access to a variety of tools that allow me to help you with different tasks. Here are the main categories of tools available to me:\n\n    1. Gmail Tools:\n       - Fetch emails, search, and filter messages.\n       - Fetch specific email details by message ID.\n       - Create email drafts (with attachments, HTML, CC/BCC, etc.).\n       - Delete email drafts or specific messages.\n\n    2. Date & Time Tools:\n       - Get the current date and time in any timezone.\n\n    3. Multi-Tool Use:\n       - Run multiple tools in parallel for efficiency.\n\n    If you have a specific task in mind, let me know and I can tell you which tool I would use or demonstrate how I can help!\n    ```\n\n9. To test a specific tool or function, tell the agent to perform an action that uses that tool.\nFor example, ask the agent to write a draft email:\n\n    ```text\n    Create a draft email with the subject line \"Greetings from Composio\"\n    recipient: \"your.email@address.com\"\n    Body content: \"Hello from composio!\"\n    ```\n\n    The **Playground** prints the logic as the agent chooses the `GMAIL_CREATE_EMAIL_DRAFT` tool to create the email draft.\n    The following example response is abbreviated:\n\n    ```text\n    The draft email with the subject \"Greetings from Composio\" and body \"Hello from composio!\" has been successfully created.\n    ```\n\n    ```json\n    {\n      \"recipient_email\": \"your.email@address.com\",\n      \"subject\": \"Greetings from Composio\",\n      \"body\": \"Hello from composio!\",\n      \"is_html\": false\n    }\n\n    {\n      \"data\": {\n        \"response_data\": {\n          \"id\": \"r-237981011463568567\",\n          \"message\": {\n            \"id\": \"195dd80528171132\",\n            \"threadId\": \"195dd80528171132\",\n            \"labelIds\": [\n              \"DRAFT\"\n            ]\n          }\n        }\n      },\n      \"error\": null,\n      \"successfull\": true,\n      \"successful\": true\n    }\n    ```\n\n    For further confirmation, you can go to your Gmail account and find the message in your drafts folder.\n\n10. Optional: To add more Composio services, repeat these steps to add more Composio components to your flow.\nFor each component, provide the necessary credentials, enable **Tool Mode**, configure the actions, and then connect it to the **Agent** component's **Tools** port.\n\n## Composio parameters\n\nAll single-service Composio components have the same parameters, and the **Composio Tools** component has one additional parameter:\n\n| Name | Type | Description |\n|------|------|-------------|\n| entity_id | String | Input parameter. The entity ID for the Composio account. Default: `default`. This parameter is hidden by default in the visual editor. If you need to set this parameter, you can access it through the  **Controls** in the [component's header menu](/concepts-components#component-menus). |\n| api_key | SecretString | Input parameter. The Composio API key for authentication with the Composio platform. Make sure the key authorizes the specific service that you want to use. For more information, see [Composio authentication](#composio-authentication). |\n| tool_name | Connection | Input parameter for the **Composio Tools** component only. Select the Composio service (tool) to connect to. |\n| action | List | Input parameter. Select actions to use. Available actions vary by service. Some actions might require premium access to a particular service. |\n\n## Composio authentication\n\nComposio components require authentication to the Composio platform with a Composio API key.\n\nYou can provide this key directly in your components, or you can use the `COMPOSIO_API_KEY` [global variable](/configuration-global-variables), which Langflow can automatically load from your `.env` file.\n\n**Note:** The Composio API key _only_ handles the connection to Composio.\nService provider authentication is managed through the Composio platform for each service that you want to use.\n\nMake sure that your Composio API key provides access to the required services for the components in your flow.\nFor example, if you are using the Composio **Gmail** component, your Composio API key must have access to the Gmail service.\n\n## Composio output\n\nWhen used as tools for an agent, Composio components output [`Tools`](/data-types#tool), which is a list of tools for use by an agent.\nWhen called by the agent, the response from the Composio service is ingested by the agent, not passed directly as output to the user or application.\n\nIn non-agentic use cases, the output is a [`DataFrame`](/data-types#dataframe) containing the response from the specified Composio service, depending on the component and action used in the flow.\n\nBecause the **Composio Tools** component supports _only_ agentic use, it cannot output `DataFrame`.\nAll single-service Composio components can output either `DataFrame` or `Tools`.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "actions",
        "agent",
        "alert",
        "api",
        "authentication",
        "bundles",
        "calendar",
        "chat",
        "components",
        "composio",
        "controls",
        "dropbox",
        "flow",
        "github",
        "gmail",
        "google",
        "input",
        "key",
        "linear",
        "meet"
      ]
    },
    "/bundles-couchbase": {
      "title": "Couchbase",
      "slug": "/bundles-couchbase",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Couchbase** bundle.\n\n## Couchbase vector store\n\nThe **Couchbase** component reads and writes to a Couchbase vector store using an instance of `CouchbaseSearchVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Couchbase parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Couchbase documentation](https://docs.couchbase.com/home/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| couchbase_connection_string | SecretString | Input parameter. Couchbase Cluster connection string. Required. |\n| couchbase_username | String | Input parameter. Couchbase username for authentication. Required. |\n| couchbase_password | SecretString | Input parameter. Couchbase password for authentication. Required. |\n| bucket_name | String | Input parameter. Name of the Couchbase bucket. Required. |\n| scope_name | String | Input parameter. Name of the Couchbase scope. Required. |\n| collection_name | String | Input parameter. Name of the Couchbase collection. Required. |\n| index_name | String | Input parameter. Name of the Couchbase index. Required. |\n| ingest_data | Data | Input parameter. The records to load into the vector store. Only relevant for writes. |\n| search_query | String | Input parameter. The query string for vector search. Only relevant for reads. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embeddings | Input parameter. The embedding function to use for the vector store. |\n| number_of_results | Integer | Input parameter. Maximum number of search results to return. Default: 4. Only relevant for reads. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "couchbase",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "bundles-couchbase": {
      "title": "Couchbase",
      "slug": "/bundles-couchbase",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Couchbase** bundle.\n\n## Couchbase vector store\n\nThe **Couchbase** component reads and writes to a Couchbase vector store using an instance of `CouchbaseSearchVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Couchbase parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Couchbase documentation](https://docs.couchbase.com/home/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| couchbase_connection_string | SecretString | Input parameter. Couchbase Cluster connection string. Required. |\n| couchbase_username | String | Input parameter. Couchbase username for authentication. Required. |\n| couchbase_password | SecretString | Input parameter. Couchbase password for authentication. Required. |\n| bucket_name | String | Input parameter. Name of the Couchbase bucket. Required. |\n| scope_name | String | Input parameter. Name of the Couchbase scope. Required. |\n| collection_name | String | Input parameter. Name of the Couchbase collection. Required. |\n| index_name | String | Input parameter. Name of the Couchbase index. Required. |\n| ingest_data | Data | Input parameter. The records to load into the vector store. Only relevant for writes. |\n| search_query | String | Input parameter. The query string for vector search. Only relevant for reads. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embeddings | Input parameter. The embedding function to use for the vector store. |\n| number_of_results | Integer | Input parameter. Maximum number of search results to return. Default: 4. Only relevant for reads. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "couchbase",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "/bundles-datastax": {
      "title": "DataStax",
      "slug": "/bundles-datastax",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DataStax** bundle, including components that read and write to Astra DB databases.\n\n## Astra DB\n\n**Note:** It is recommended that you create any databases, keyspaces, and collections you need before configuring the **Astra DB** component.\n\nYou can create new databases and collections through this component, but this is only possible in the Langflow visual editor (not at runtime), and you must wait while the database or collection initializes before proceeding with flow configuration.\nAdditionally, not all database and collection configuration options are available through the **Astra DB** component, such as hybrid search options, PCU groups, vectorize integration management, and multi-region deployments.\n\nThe **Astra DB** component reads and writes to Astra DB Serverless databases, using an instance of `AstraDBVectorStore` to call the Data API and DevOps API.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Astra DB parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |\n| environment | Environment | Input parameter. The environment for the Astra DB API endpoint. Typically always `prod`. |\n| database_name | Database | Input parameter. The name of the database that you want this component to connect to. Or, you can select **New Database** to create a new database, and then wait for the database to initialize before setting the remaining parameters. |\n| endpoint | Astra DB API Endpoint | Input parameter. For multi-region databases, select the API endpoint for your nearest datacenter. To get the list of regions for a multi-region database, see [List database regions](https://docs.datastax.com/en/astra-db-serverless/databases/manage-regions.html#list-db-regions). This field is automatically populated when you select a database, and it defaults to the primary region's endpoint. |\n| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |\n| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. Or, select **New Collection** to create a new collection with limited configuration options. To ensure your collection is configured with the correct embedding provider and search capabilities, it is recommended to create the collection in the Astra Portal or with the Data API *before* configuring this component. For more information, see [Manage collections in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/manage-collections.html). |\n| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. Only available if the specified collection doesn't have a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). If a vectorize integration exists, the component automatically uses the collection's integrated model. |\n| ingest_data | Ingest Data | Input parameter. The documents to load into the specified collection. Accepts `Data` or `DataFrame` input. |\n| search_query | Search Query | Input parameter. The query string for vector search. |\n| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |\n| search_method | Search Method | Input parameter. The search methods to use, either `Hybrid Search` or `Vector Search`. Your collection must be configured to support the chosen option, and the default depends on what your collection supports. All vector-enabled collections in Astra DB Serverless (Vector) databases support vector search, but hybrid search requires that you set specific collection settings when creating the collection. These options are only available when creating a collection programmatically. For more information, see [Ways to find data in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/about-search.html) and [Create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid). |\n| reranker | Reranker | Input parameter. The re-ranker model to use for hybrid search, depending on the collection configuration. This parameter is only available for collections that support hybrid search. To determine if a collection supports hybrid search, [get collection metadata](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/list-collection-metadata.html), and then check that `lexical` and `rerank` both have `\"enabled\": true`. |\n| lexical_terms | Lexical Terms | Input parameter. A space-separated string of keywords for hybrid search, like `features, data, attributes, characteristics`. This parameter is only available if the collection supports hybrid search. For more information, see the [Hybrid search example](#astra-db-examples). |\n| number_of_results | Number of Search Results | Input parameter. The number of search results to return. Default: 4. |\n| search_type | Search Type | Input parameter. The search type to use, either `Similarity` (default), `Similarity with score threshold`, and `MMR (Max Marginal Relevance)`. |\n| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for vector search results with the `Similarity with score threshold` search type. Default: 0. |\n| advanced_search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata filters to apply in addition to vector or hybrid search. |\n| autodetect_collection | Autodetect Collection | Input parameter. Whether to automatically fetch a list of available collections after providing an application token and API endpoint. |\n| content_field | Content Field | Input parameter. For writes, this parameter specifies the name of the field in the documents that contains text strings for which you want to generate embeddings. |\n| deletion_field | Deletion Based On Field | Input parameter. When provided, documents in the target collection with metadata field values matching the input metadata field value are deleted before new records are loaded. Use this setting for writes with upserts (overwrites). |\n| ignore_invalid_documents | Ignore Invalid Documents | Input parameter. Whether to ignore invalid documents during writes. If disabled (`false`), then an error is raised for invalid documents. Default: Enabled (`true`). |\n| astradb_vectorstore_kwargs | AstraDBVectorStore Parameters | Input parameter. An optional dictionary of additional parameters for the `AstraDBVectorStore` instance. |\n\n### Astra DB examples\n\n<details>\n<summary>Example: Vector RAG</summary>\n\n</details>\n\n<details>\n<summary>Example: Hybrid search</summary>\n\nThe **Astra DB** component supports the Data API's [hybrid search](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html) feature.\nHybrid search performs a vector similarity search and a lexical search, compares the results of both searches, and then returns the most relevant results overall.\n\nTo use hybrid search through the **Astra DB** component, do the following:\n\n1. Use the Data API to [create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid) if you don't already have one.\n\n    Although you can create a collection through the **Astra DB** component, you have more control and insight into the collection settings when using the Data API for this operation.\n\n2. Create a flow based on the **Hybrid Search RAG** template, which includes an **Astra DB** component that is pre-configured for hybrid search.\n\n    After loading the template, check for **Upgrade available** alerts on the components.\n    If any components have an upgrade pending, upgrade and reconnect them before continuing.\n\n3. In the **Language Model** components, add your OpenAI API key.\nIf you want to use a different provider or model, see [Language model components](/components-models).\n\n4. Delete the **Language Model** component that is connected to the **Structured Output** component's **Input Message** port, and then connect the **Chat Input** component to that port.\n\n5. Configure the **Astra DB** vector store component:\n\n    1. Enter your Astra DB application token.\n    2. In the **Database** field, select your database.\n    3. In the **Collection** field, select your collection with hybrid search enabled.\n\n    Once you select a collection that supports hybrid search, the other parameters automatically update to allow hybrid search options.\n\n6. Connect the first **Parser** component's **Parsed Text** output to the **Astra DB** component's **Lexical Terms** input.\nThis input only appears after connecting a collection that support hybrid search with reranking.\n\n7. Update the **Structured Output** template:\n\n    1. Click the **Structured Output** component to expose the [component's header menu](/concepts-components#component-menus), and then click  **Controls**.\n    2. Find the **Format Instructions** row, click  **Expand**, and then replace the prompt with the following text:\n\n        ```text\n        You are a database query planner that takes a user's requests, and then converts to a search against the subject matter in question.\n        You should convert the query into:\n        1. A list of keywords to use against a Lucene text analyzer index, no more than 4. Strictly unigrams.\n        2. A question to use as the basis for a QA embedding engine.\n        Avoid common keywords associated with the user's subject matter.\n        ```\n\n    3. Click **Finish Editing**, and then click **Close** to save your changes to the component.\n\n8. Open the **Playground**, and then enter a natural language question that you would ask about your database.\n\n    In this example, your input is sent to both the **Astra DB** and **Structured Output** components:\n\n    * The input sent directly to the **Astra DB** component's **Search Query** port is used as a string for similarity search.\n    An embedding is generated from the query string using the collection's Astra DB vectorize integration.\n\n    * The input sent to the **Structured Output** component is processed by the **Structured Output**, **Language Model**, and **Parser** components to extract space-separated `keywords` used for the lexical search portion of the hybrid search.\n\n    The complete hybrid search query is executed against your database using the Data API's `find_and_rerank` command.\n    The API's response is output as a `DataFrame` that is transformed into a text string `Message` by another **Parser** component.\n    Finally, the **Chat Output** component prints the `Message` response to the **Playground**.\n\n9. Optional: Exit the **Playground**, and then click  **Inspect Output** on each individual component to understand how lexical keywords were constructed and view the raw response from the Data API.\nThis is helpful for debugging flows where a certain component isn't receiving input as expected from another component.\n\n    * **Structured Output component**: The output is the `Data` object produced by applying the output schema to the LLM's response to the input message and format instructions.\n    The following example is based on the aforementioned instructions for keyword extraction:\n\n        ```\n        1. Keywords: features, data, attributes, characteristics\n        2. Question: What characteristics can be identified in my data?\n        ```\n\n    * **Parser component**: The output is the string of keywords extracted from the structured output `Data`, and then used as lexical terms for the hybrid search.\n\n    * **Astra DB component**: The output is the `DataFrame` containing the results of the hybrid search as returned by the Data API.\n\n</details>\n\n### Astra DB output\n\n<details>\n<summary>Vector Store Connection port</summary>\n\nThe **Astra DB** component has an additional **Vector Store Connection** output.\nThis output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.\n\nThe only non-legacy component that supports this input is the [**Graph RAG** component](#graph-rag), which can be a Graph RAG extension to the **Astra DB** component.\nInstead, use the **Astra DB Graph** component that includes both the vector store connection and Graph RAG functionality.\n\n</details>\n\n## Astra DB CQL\n\nThe **Astra DB CQL** component allows agents to query data from CQL tables in Astra DB.\n\nThe output is a list of [`Data`](/data-types#data) objects containing the query results from the Astra DB CQL table. Each Data object contains the document fields specified by the projection fields. Limited by the `number_of_results` parameter.\n\n### Astra DB CQL parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Tool Name | String | Input parameter. The name used to reference the tool in the agent's prompt. |\n| Tool Description | String | Input parameter. A brief description of the tool to guide the model in using it. |\n| Keyspace | String | Input parameter. The name of the keyspace. |\n| Table Name | String | Input parameter. The name of the Astra DB CQL table to query. |\n| Token | SecretString | Input parameter. The authentication token for Astra DB. |\n| API Endpoint | String | Input parameter. The Astra DB API endpoint. |\n| Projection Fields | String | Input parameter. The attributes to return, separated by commas. Default: \"*\". |\n| Partition Keys | Dict | Input parameter. Required parameters that the model must fill to query the tool. |\n| Clustering Keys | Dict | Input parameter. Optional parameters the model can fill to refine the query. Required parameters should be marked with an exclamation mark, for example, `!customer_id`. |\n| Static Filters | Dict | Input parameter. Attribute-value pairs used to filter query results. |\n| Limit | String | Input parameter. The number of records to return. |\n\n## Astra DB Tool\n\nThe **Astra DB Tool** component enables searching data in Astra DB collections, including hybrid search, vector search, and regular filter-based search.\nSpecialized searches require that the collection is pre-configured with the required parameters.\n\nOutputs a list of [`Data`](/data-types#data) objects containing the query results from Astra DB. Each `Data` object contains the document fields specified by the projection attributes. Limited by the `number_of_results` parameter and the upper limit of the Astra DB Data API, depending on the type of search.\n\nYou can use the component to execute queries directly as isolated steps in a flow, or you can connect it as a [tool for an agent](/agents-tools) to allow the agent to query data from Astra DB collections as needed to respond to user queries.\nFor more information, see [Use Langflow agents](/agents).\n\n![Astra DB Tool component connected as a tool to an Agent component](/img/component-astra-db-json-tool.png)\n\n### Astra DB Tool parameters\n\nThe following parameters are for the **Astra DB Tool** component overall.\n\nThe values for **Collection Name**, **Astra DB Application Token**, and **Astra DB API Endpoint** are found in your Astra DB deployment. For more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).\n\n| Name              | Type   | Description   |\n|-------------------|--------|--------|\n| Tool Name         | String | Input parameter. The name used to reference the tool in the agent's prompt.  |\n| Tool Description  | String | Input parameter. A brief description of the tool. This helps the model decide when to use it. |\n| Keyspace Name     | String | Input parameter. The name of the keyspace in Astra DB. Default: `default_keyspace` |\n| Collection Name   | String | Input parameter. The name of the Astra DB collection to query.    |\n| Token             | SecretString | Input parameter. The authentication token for accessing Astra DB.  |\n| API Endpoint      | String | Input parameter. The Astra DB API endpoint.  |\n| Projection Fields | String | Input parameter. Comma-separated list of attributes to return from matching documents. The default is the default projection, `*`, which returns all attributes except reserved fields like `$vector`.  |\n| Tool Parameters  | Dict   | Input parameter. [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters) that become tools for an agent. These Filters _may_ be used in a search, if the agent selects them. See [Define tool-specific parameters](#define-tool-specific-parameters). |\n| Static Filters    | Dict   | Input parameter. Attribute-value pairs used to filter query results. Equivalent to [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters). **Static Filters** are included with _every_ query. Use **Static Filters** without semantic search to perform a regular filter search. |\n| Number of Results  | Int | Input parameter. The maximum number of documents to return.   |\n| Semantic Search | Boolean | Input parameter. Whether to run a similarity search by generating a vector embedding from the chat input and following the **Semantic Search Instruction**. Default: `false`. If `true`, you must attach an [embedding model component](/components-embedding-models) or have vectorize pre-enabled on your collection. |\n| Use Astra DB Vectorize | Boolean | Input parameter. Whether to use the Astra DB vectorize feature for embedding generation when running a semantic search. Default: `false`. If `true`, you must have vectorize pre-enabled on your collection. |\n| Embedding Model | Embedding | Input parameter. A port to attach an embedding model component to generate a vector from input text for semantic search. This can be used when **Semantic Search** is `true`, with or without vectorize. Be sure to use a model that aligns with the dimensions of the embeddings already present in the collection. |\n| Semantic Search Instruction | String | Input parameter. The query to use for similarity search. Default: `\"Find documents similar to the query.\"`. This instruction is used to guide the model in performing semantic search. |\n\n### Define tool-specific parameters\n\n**Note:** **Tool Parameters** are small functions that you create within the **Astra DB Tool** component.\nThey give the LLM pre-defined ways to interact with the data in your collection.\n\nWithout these filters, the LLM has no concept of the data in your collection or which attributes are important.\n\nAt runtime, the LLM can decide which filters are relevant to the current query.\n\nFilters in **Tool Parameters** aren't always applied.\nIf you want to enforce filters for _every_ query, use the **Static Filters** parameter.\nYou can use both **Tool Parameters** and **Static Filters** to set some required filters and some optional filters.\n\nIn the **Astra DB Tool** component's **Tool Parameters** field, you can create filters to query documents in your collection.\n\nWhen used in **Tool Mode** with an agent, these filters tell the agent which document attributes are most important, which are required in searches, and which operators to use on certain attributes.\nThe filters become available as parameters that the LLM can use when calling the tool, with a better understanding of each parameter provided by the **Description** field.\n\nIn the **Tool Parameters** pane, click  **Add a new row**, and then edit each cell in the row.\nFor example, the following filter allows an LLM to filter by unique `customer_id` values:\n\n    * Name: `customer_id`\n    * Attribute Name: Leave empty if the attribute matches the field name in the database.\n    * Description: `\"The unique identifier of the customer to filter by\"`.\n    * Is Metadata: Select **False** unless the value is stored in the metadata field.\n    * Is Mandatory: Set to **True** to make the filter required.\n    * Is Timestamp: For this example, select **False** because the value is an ID, not a timestamp.\n    * Operator: `$eq` to look for an exact match.\n\nThe following fields are available for each row in the **Tool Parameters** pane:\n\n| Parameter | Description |\n|-----------|-------------|\n| Name | The name of the parameter that is exposed to the LLM. It can be the same as the underlying field name or a more descriptive label. The LLM uses this name, along with the description, to infer what value to provide during execution. |\n| Attribute Name | When the parameter name shown to the LLM differs from the actual field or property in the database, use this setting to map the user-facing name to the correct attribute. For example, to apply a range filter to the timestamp field, define two separate parameters, such as `start_date` and `end_date`, that both reference the same timestamp attribute. |\n| Description | Provides instructions to the LLM on how the parameter should be used. Clear and specific guidance helps the LLM provide valid input. For example, if a field such as `specialty` is stored in lowercase, the description should indicate that the input must be lowercase. |\n| Is Metadata | When loading data using LangChain or Langflow, additional attributes may be stored under a metadata object. If the target attribute is stored this way, enable this option. It adjusts the query by generating a filter in the format: `{\"metadata.<attribute_name>\": \"<value>\"}` |\n| Is Timestamp | For date or time-based filters, enable this option to automatically convert values to the timestamp format that the Astrapy client expects. This ensures compatibility with the underlying API without requiring manual formatting. |\n| Operator | Defines the filtering logic applied to the attribute. You can use any valid [Data API filter operator](https://docs.datastax.com/en/astra-db-serverless/api-reference/filter-operator-collections.html). For example, to filter a time range on the timestamp attribute, use two parameters: one with the `$gt` operator for \"greater than\", and another with the `$lt` operator for \"less than\". |\n\n## Astra DB Graph\n\nThe **Astra DB Graph** component uses `AstraDBGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in an Astra DB collection. It also supports writing to the vector store.\nFor more information, see [Build a Graph RAG system with LangChain and GraphRetriever](https://docs.datastax.com/en/astra-db-serverless/tutorials/graph-rag.html).\n\n### Astra DB Graph parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |\n| api_endpoint | API Endpoint | Input parameter. Your database's API endpoint. |\n| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |\n| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. For write operations, if a matching collection doesn't exist, a new one is created. |\n| metadata_incoming_links_key | Metadata Incoming Links Key | Input parameter. The metadata key for the incoming links in the vector store. |\n| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |\n| search_input | Search Query | Input parameter. Query string for similarity search. Only relevant for reads. |\n| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |\n| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. If the collection has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html), don't attach an embedding model component. |\n| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine` (default), `dot_product`, or `euclidean`. This is a collection setting. |\n| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |\n| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |\n| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |\n| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |\n| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default) or `Off`. |\n| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. Default: Disabled (`false`). |\n| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |\n| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |\n| search_type | Search Type | Input parameter. Search type to use, either `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`, `Graph Traversal`, or `MMR (Max Marginal Relevance) Graph Traversal` (default). Only relevant to reads. |\n| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |\n| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |\n\n## Graph RAG\n\nThe **Graph RAG** component uses an instance of [`GraphRetriever`](https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/) for Graph RAG traversal enabling graph-based document retrieval in an Astra DB vector store.\nFor more information, see the [DataStax Graph RAG documentation](https://datastax.github.io/graph-rag/).\n\n**Note:** This component can be a Graph RAG extension for the [**Astra DB** vector store component](#astra-db).\nHowever, the [**Astra DB Graph** component](#astra-db-graph) includes both the vector store connection and Graph RAG functionality.\n\n### Graph RAG parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| embedding_model | Embedding Model | Input parameter. Specify the embedding model to use. Not required if the connected vector store has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). |\n| vector_store | Vector Store Connection | Input parameter. An instance of `AstraDbVectorStore` inherited from the [**Astra DB** component](#astra-db)'s **Vector Store Connection** output. |\n| edge_definition | Edge Definition | Input parameter. [Edge definition](https://datastax.github.io/graph-rag/reference/graph_retriever/edges/) for the graph traversal. |\n| strategy | Traversal Strategies | Input parameter. The strategy to use for graph traversal. Strategy options are dynamically loaded from available strategies. |\n| search_query | Search Query | Input parameter. The query to search for in the vector store. |\n| graphrag_strategy_kwargs | Strategy Parameters | Input parameter. Optional dictionary of additional parameters for the [retrieval strategy](https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/). |\n| search_results | **Search Results** or **DataFrame** | Output parameter. The results of the graph-based document retrieval as a list of [`Data`](/data-types#data) objects or as a tabular [`DataFrame`](/data-types#dataframe). You can set the desired output type near the component's output port. |\n\n## Hyper-Converged Database (HCD)\n\nThe **Hyper-Converged Database (HCD)** component uses your cluster's Data API server to read and write to your HCD vector store.\nBecause the underlying functions call the Data API, which originated from Astra DB, the component uses an instance of `AstraDBVectorStore`.\n\n![A flow using the HCD component to load vector data.](/img/component-hcd-example-flow.png)\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\nFor more information about HCD, see [Get started with HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/get-started/get-started-hcd.html) and [Get started with the Data API in HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/dataapiclient.html).\n\n### HCD parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| collection_name | Collection Name | Input parameter. The name of a vector store collection in HCD. For write operations, if the collection doesn't exist, then a new one is created. Required. |\n| username | HCD Username | Input parameter. Username for authenticating to your HCD deployment. Default: `hcd-superuser`. Required. |\n| password | HCD Password | Input parameter. Password for authenticating to your HCD deployment. Required. |\n| api_endpoint | HCD API Endpoint | Input parameter. Your deployment's HCD Data API endpoint, formatted as `http[s]://CLUSTER_HOST:GATEWAY_PORT` where `CLUSTER_HOST` is the IP address of any node in your cluster and `GATEWAY_PORT` is the port number for your API gateway service. For example, `http://192.0.2.250:8181`. Required. |\n| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |\n| search_input | Search Input | Input parameter. Query string for similarity search. Only relevant for reads. |\n| namespace | Namespace | Input parameter. The namespace in HCD that contains or will contain the collection specified in `collection_name`. Default: `default_namespace`. |\n| ca_certificate | CA Certificate | Input parameter. Optional CA certificate for TLS connections to HCD. |\n| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine`, `dot_product`, or `euclidean`. This is a collection setting. If calling an existing collection, leave unset to use the collection's metric. If a write operation creates a new collection, specify the desired similarity metric setting. |\n| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |\n| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |\n| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |\n| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |\n| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default), `Async`, or `Off`. |\n| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. |\n| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |\n| embedding | Embedding or Astra Vectorize | Input parameter. The embedding model to use by attaching an **Embedding Model** component. This component doesn't support additional vectorize authentication headers, so it isn't possible to use a vectorize integration with this component, even if you have enabled one on an existing HCD collection. |\n| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |\n| search_type | Search Type | Input parameter. Search type to use, either `Similarity` (default), `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. Only relevant to reads. |\n| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |\n| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |\n\n## Other DataStax components\n\nThe following components are also included in the **DataStax** bundle.\n\n### Astra DB Chat Memory\n\nThe **Astra DB Chat Memory** component retrieves and stores chat messages using an Astra DB database.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\nSpecifically, the component creates an instance of `AstraDBChatMessageHistory`, which is a LangChain chat message history class that uses Astra DB for storage.\n\n**Note:** The **Astra DB Chat Memory** component isn't recommended for most memory storage because memories tend to be long JSON objects or strings, often exceeding the maximum size of a document or object supported by Astra DB.\n\nHowever, Langflow's **Agent** component includes built-in chat memory that is enabled by default.\nYour agentic flows don't need an external database to store chat memory.\nFor more information, see [Memory management options](/memory).\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n#### Astra DB Chat Memory parameters\n\n| Name             | Type          | Description                                                           |\n|------------------|---------------|-----------------------------------------------------------------------|\n| collection_name  | String        | Input parameter. The name of the Astra DB collection for storing messages. Required. |\n| token            | SecretString  | Input parameter. The authentication token for Astra DB access. Required. |\n| api_endpoint     | SecretString  | Input parameter. The API endpoint URL for the Astra DB service. Required. |\n| namespace        | String        | Input parameter. The optional namespace within Astra DB for the collection. |\n| session_id       | MessageText   | Input parameter. The unique identifier for the chat session. Uses the current session ID if not provided. |\n\n### Assistants API\n\nThe following DataStax components are used to create and manage Assistants API functions in a flow:\n\n* **Astra Assistant Agent**\n* **Create Assistant**\n* **Create Assistant Thread**\n* **Get Assistant Name**\n* **List Assistants**\n* **Run Assistant**\n\n## Environment variables\n\nThe following DataStax components are used to load and retrieve environment variables in a flow:\n\n* **Dotenv**\n* **Get Environment Variable**\n\n## Legacy DataStax components\n\nThe following DataStax components are in legacy status:\n\n<details>\n<summary>Astra Vectorize</summary>\n\nThis component was deprecated in Langflow version 1.1.2.\nReplace it with the [**Astra DB** component](#astra-db).\n\nThe **Astra DB Vectorize** component was used to generate embeddings with Astra DB's vectorize feature in conjunction with an **Astra DB** component.\n\nThe vectorize functionality is now built into the **Astra DB** component.\nYou no longer need a separate component for vectorize embedding generation.\n\n</details>\n\n## See also\n\n* [**Cassandra** bundle](/bundles-cassandra)\n* [Create a vector RAG chatbot](/chat-with-rag)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "add",
        "agent",
        "api",
        "application",
        "assistant",
        "assistants",
        "astra",
        "available",
        "bundles",
        "cassandra",
        "chat",
        "close",
        "collection",
        "component",
        "components",
        "connection",
        "controls",
        "converged",
        "cql",
        "create"
      ]
    },
    "bundles-datastax": {
      "title": "DataStax",
      "slug": "/bundles-datastax",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DataStax** bundle, including components that read and write to Astra DB databases.\n\n## Astra DB\n\n**Note:** It is recommended that you create any databases, keyspaces, and collections you need before configuring the **Astra DB** component.\n\nYou can create new databases and collections through this component, but this is only possible in the Langflow visual editor (not at runtime), and you must wait while the database or collection initializes before proceeding with flow configuration.\nAdditionally, not all database and collection configuration options are available through the **Astra DB** component, such as hybrid search options, PCU groups, vectorize integration management, and multi-region deployments.\n\nThe **Astra DB** component reads and writes to Astra DB Serverless databases, using an instance of `AstraDBVectorStore` to call the Data API and DevOps API.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Astra DB parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |\n| environment | Environment | Input parameter. The environment for the Astra DB API endpoint. Typically always `prod`. |\n| database_name | Database | Input parameter. The name of the database that you want this component to connect to. Or, you can select **New Database** to create a new database, and then wait for the database to initialize before setting the remaining parameters. |\n| endpoint | Astra DB API Endpoint | Input parameter. For multi-region databases, select the API endpoint for your nearest datacenter. To get the list of regions for a multi-region database, see [List database regions](https://docs.datastax.com/en/astra-db-serverless/databases/manage-regions.html#list-db-regions). This field is automatically populated when you select a database, and it defaults to the primary region's endpoint. |\n| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |\n| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. Or, select **New Collection** to create a new collection with limited configuration options. To ensure your collection is configured with the correct embedding provider and search capabilities, it is recommended to create the collection in the Astra Portal or with the Data API *before* configuring this component. For more information, see [Manage collections in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/manage-collections.html). |\n| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. Only available if the specified collection doesn't have a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). If a vectorize integration exists, the component automatically uses the collection's integrated model. |\n| ingest_data | Ingest Data | Input parameter. The documents to load into the specified collection. Accepts `Data` or `DataFrame` input. |\n| search_query | Search Query | Input parameter. The query string for vector search. |\n| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |\n| search_method | Search Method | Input parameter. The search methods to use, either `Hybrid Search` or `Vector Search`. Your collection must be configured to support the chosen option, and the default depends on what your collection supports. All vector-enabled collections in Astra DB Serverless (Vector) databases support vector search, but hybrid search requires that you set specific collection settings when creating the collection. These options are only available when creating a collection programmatically. For more information, see [Ways to find data in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/about-search.html) and [Create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid). |\n| reranker | Reranker | Input parameter. The re-ranker model to use for hybrid search, depending on the collection configuration. This parameter is only available for collections that support hybrid search. To determine if a collection supports hybrid search, [get collection metadata](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/list-collection-metadata.html), and then check that `lexical` and `rerank` both have `\"enabled\": true`. |\n| lexical_terms | Lexical Terms | Input parameter. A space-separated string of keywords for hybrid search, like `features, data, attributes, characteristics`. This parameter is only available if the collection supports hybrid search. For more information, see the [Hybrid search example](#astra-db-examples). |\n| number_of_results | Number of Search Results | Input parameter. The number of search results to return. Default: 4. |\n| search_type | Search Type | Input parameter. The search type to use, either `Similarity` (default), `Similarity with score threshold`, and `MMR (Max Marginal Relevance)`. |\n| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for vector search results with the `Similarity with score threshold` search type. Default: 0. |\n| advanced_search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata filters to apply in addition to vector or hybrid search. |\n| autodetect_collection | Autodetect Collection | Input parameter. Whether to automatically fetch a list of available collections after providing an application token and API endpoint. |\n| content_field | Content Field | Input parameter. For writes, this parameter specifies the name of the field in the documents that contains text strings for which you want to generate embeddings. |\n| deletion_field | Deletion Based On Field | Input parameter. When provided, documents in the target collection with metadata field values matching the input metadata field value are deleted before new records are loaded. Use this setting for writes with upserts (overwrites). |\n| ignore_invalid_documents | Ignore Invalid Documents | Input parameter. Whether to ignore invalid documents during writes. If disabled (`false`), then an error is raised for invalid documents. Default: Enabled (`true`). |\n| astradb_vectorstore_kwargs | AstraDBVectorStore Parameters | Input parameter. An optional dictionary of additional parameters for the `AstraDBVectorStore` instance. |\n\n### Astra DB examples\n\n<details>\n<summary>Example: Vector RAG</summary>\n\n</details>\n\n<details>\n<summary>Example: Hybrid search</summary>\n\nThe **Astra DB** component supports the Data API's [hybrid search](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html) feature.\nHybrid search performs a vector similarity search and a lexical search, compares the results of both searches, and then returns the most relevant results overall.\n\nTo use hybrid search through the **Astra DB** component, do the following:\n\n1. Use the Data API to [create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid) if you don't already have one.\n\n    Although you can create a collection through the **Astra DB** component, you have more control and insight into the collection settings when using the Data API for this operation.\n\n2. Create a flow based on the **Hybrid Search RAG** template, which includes an **Astra DB** component that is pre-configured for hybrid search.\n\n    After loading the template, check for **Upgrade available** alerts on the components.\n    If any components have an upgrade pending, upgrade and reconnect them before continuing.\n\n3. In the **Language Model** components, add your OpenAI API key.\nIf you want to use a different provider or model, see [Language model components](/components-models).\n\n4. Delete the **Language Model** component that is connected to the **Structured Output** component's **Input Message** port, and then connect the **Chat Input** component to that port.\n\n5. Configure the **Astra DB** vector store component:\n\n    1. Enter your Astra DB application token.\n    2. In the **Database** field, select your database.\n    3. In the **Collection** field, select your collection with hybrid search enabled.\n\n    Once you select a collection that supports hybrid search, the other parameters automatically update to allow hybrid search options.\n\n6. Connect the first **Parser** component's **Parsed Text** output to the **Astra DB** component's **Lexical Terms** input.\nThis input only appears after connecting a collection that support hybrid search with reranking.\n\n7. Update the **Structured Output** template:\n\n    1. Click the **Structured Output** component to expose the [component's header menu](/concepts-components#component-menus), and then click  **Controls**.\n    2. Find the **Format Instructions** row, click  **Expand**, and then replace the prompt with the following text:\n\n        ```text\n        You are a database query planner that takes a user's requests, and then converts to a search against the subject matter in question.\n        You should convert the query into:\n        1. A list of keywords to use against a Lucene text analyzer index, no more than 4. Strictly unigrams.\n        2. A question to use as the basis for a QA embedding engine.\n        Avoid common keywords associated with the user's subject matter.\n        ```\n\n    3. Click **Finish Editing**, and then click **Close** to save your changes to the component.\n\n8. Open the **Playground**, and then enter a natural language question that you would ask about your database.\n\n    In this example, your input is sent to both the **Astra DB** and **Structured Output** components:\n\n    * The input sent directly to the **Astra DB** component's **Search Query** port is used as a string for similarity search.\n    An embedding is generated from the query string using the collection's Astra DB vectorize integration.\n\n    * The input sent to the **Structured Output** component is processed by the **Structured Output**, **Language Model**, and **Parser** components to extract space-separated `keywords` used for the lexical search portion of the hybrid search.\n\n    The complete hybrid search query is executed against your database using the Data API's `find_and_rerank` command.\n    The API's response is output as a `DataFrame` that is transformed into a text string `Message` by another **Parser** component.\n    Finally, the **Chat Output** component prints the `Message` response to the **Playground**.\n\n9. Optional: Exit the **Playground**, and then click  **Inspect Output** on each individual component to understand how lexical keywords were constructed and view the raw response from the Data API.\nThis is helpful for debugging flows where a certain component isn't receiving input as expected from another component.\n\n    * **Structured Output component**: The output is the `Data` object produced by applying the output schema to the LLM's response to the input message and format instructions.\n    The following example is based on the aforementioned instructions for keyword extraction:\n\n        ```\n        1. Keywords: features, data, attributes, characteristics\n        2. Question: What characteristics can be identified in my data?\n        ```\n\n    * **Parser component**: The output is the string of keywords extracted from the structured output `Data`, and then used as lexical terms for the hybrid search.\n\n    * **Astra DB component**: The output is the `DataFrame` containing the results of the hybrid search as returned by the Data API.\n\n</details>\n\n### Astra DB output\n\n<details>\n<summary>Vector Store Connection port</summary>\n\nThe **Astra DB** component has an additional **Vector Store Connection** output.\nThis output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.\n\nThe only non-legacy component that supports this input is the [**Graph RAG** component](#graph-rag), which can be a Graph RAG extension to the **Astra DB** component.\nInstead, use the **Astra DB Graph** component that includes both the vector store connection and Graph RAG functionality.\n\n</details>\n\n## Astra DB CQL\n\nThe **Astra DB CQL** component allows agents to query data from CQL tables in Astra DB.\n\nThe output is a list of [`Data`](/data-types#data) objects containing the query results from the Astra DB CQL table. Each Data object contains the document fields specified by the projection fields. Limited by the `number_of_results` parameter.\n\n### Astra DB CQL parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Tool Name | String | Input parameter. The name used to reference the tool in the agent's prompt. |\n| Tool Description | String | Input parameter. A brief description of the tool to guide the model in using it. |\n| Keyspace | String | Input parameter. The name of the keyspace. |\n| Table Name | String | Input parameter. The name of the Astra DB CQL table to query. |\n| Token | SecretString | Input parameter. The authentication token for Astra DB. |\n| API Endpoint | String | Input parameter. The Astra DB API endpoint. |\n| Projection Fields | String | Input parameter. The attributes to return, separated by commas. Default: \"*\". |\n| Partition Keys | Dict | Input parameter. Required parameters that the model must fill to query the tool. |\n| Clustering Keys | Dict | Input parameter. Optional parameters the model can fill to refine the query. Required parameters should be marked with an exclamation mark, for example, `!customer_id`. |\n| Static Filters | Dict | Input parameter. Attribute-value pairs used to filter query results. |\n| Limit | String | Input parameter. The number of records to return. |\n\n## Astra DB Tool\n\nThe **Astra DB Tool** component enables searching data in Astra DB collections, including hybrid search, vector search, and regular filter-based search.\nSpecialized searches require that the collection is pre-configured with the required parameters.\n\nOutputs a list of [`Data`](/data-types#data) objects containing the query results from Astra DB. Each `Data` object contains the document fields specified by the projection attributes. Limited by the `number_of_results` parameter and the upper limit of the Astra DB Data API, depending on the type of search.\n\nYou can use the component to execute queries directly as isolated steps in a flow, or you can connect it as a [tool for an agent](/agents-tools) to allow the agent to query data from Astra DB collections as needed to respond to user queries.\nFor more information, see [Use Langflow agents](/agents).\n\n![Astra DB Tool component connected as a tool to an Agent component](/img/component-astra-db-json-tool.png)\n\n### Astra DB Tool parameters\n\nThe following parameters are for the **Astra DB Tool** component overall.\n\nThe values for **Collection Name**, **Astra DB Application Token**, and **Astra DB API Endpoint** are found in your Astra DB deployment. For more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).\n\n| Name              | Type   | Description   |\n|-------------------|--------|--------|\n| Tool Name         | String | Input parameter. The name used to reference the tool in the agent's prompt.  |\n| Tool Description  | String | Input parameter. A brief description of the tool. This helps the model decide when to use it. |\n| Keyspace Name     | String | Input parameter. The name of the keyspace in Astra DB. Default: `default_keyspace` |\n| Collection Name   | String | Input parameter. The name of the Astra DB collection to query.    |\n| Token             | SecretString | Input parameter. The authentication token for accessing Astra DB.  |\n| API Endpoint      | String | Input parameter. The Astra DB API endpoint.  |\n| Projection Fields | String | Input parameter. Comma-separated list of attributes to return from matching documents. The default is the default projection, `*`, which returns all attributes except reserved fields like `$vector`.  |\n| Tool Parameters  | Dict   | Input parameter. [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters) that become tools for an agent. These Filters _may_ be used in a search, if the agent selects them. See [Define tool-specific parameters](#define-tool-specific-parameters). |\n| Static Filters    | Dict   | Input parameter. Attribute-value pairs used to filter query results. Equivalent to [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters). **Static Filters** are included with _every_ query. Use **Static Filters** without semantic search to perform a regular filter search. |\n| Number of Results  | Int | Input parameter. The maximum number of documents to return.   |\n| Semantic Search | Boolean | Input parameter. Whether to run a similarity search by generating a vector embedding from the chat input and following the **Semantic Search Instruction**. Default: `false`. If `true`, you must attach an [embedding model component](/components-embedding-models) or have vectorize pre-enabled on your collection. |\n| Use Astra DB Vectorize | Boolean | Input parameter. Whether to use the Astra DB vectorize feature for embedding generation when running a semantic search. Default: `false`. If `true`, you must have vectorize pre-enabled on your collection. |\n| Embedding Model | Embedding | Input parameter. A port to attach an embedding model component to generate a vector from input text for semantic search. This can be used when **Semantic Search** is `true`, with or without vectorize. Be sure to use a model that aligns with the dimensions of the embeddings already present in the collection. |\n| Semantic Search Instruction | String | Input parameter. The query to use for similarity search. Default: `\"Find documents similar to the query.\"`. This instruction is used to guide the model in performing semantic search. |\n\n### Define tool-specific parameters\n\n**Note:** **Tool Parameters** are small functions that you create within the **Astra DB Tool** component.\nThey give the LLM pre-defined ways to interact with the data in your collection.\n\nWithout these filters, the LLM has no concept of the data in your collection or which attributes are important.\n\nAt runtime, the LLM can decide which filters are relevant to the current query.\n\nFilters in **Tool Parameters** aren't always applied.\nIf you want to enforce filters for _every_ query, use the **Static Filters** parameter.\nYou can use both **Tool Parameters** and **Static Filters** to set some required filters and some optional filters.\n\nIn the **Astra DB Tool** component's **Tool Parameters** field, you can create filters to query documents in your collection.\n\nWhen used in **Tool Mode** with an agent, these filters tell the agent which document attributes are most important, which are required in searches, and which operators to use on certain attributes.\nThe filters become available as parameters that the LLM can use when calling the tool, with a better understanding of each parameter provided by the **Description** field.\n\nIn the **Tool Parameters** pane, click  **Add a new row**, and then edit each cell in the row.\nFor example, the following filter allows an LLM to filter by unique `customer_id` values:\n\n    * Name: `customer_id`\n    * Attribute Name: Leave empty if the attribute matches the field name in the database.\n    * Description: `\"The unique identifier of the customer to filter by\"`.\n    * Is Metadata: Select **False** unless the value is stored in the metadata field.\n    * Is Mandatory: Set to **True** to make the filter required.\n    * Is Timestamp: For this example, select **False** because the value is an ID, not a timestamp.\n    * Operator: `$eq` to look for an exact match.\n\nThe following fields are available for each row in the **Tool Parameters** pane:\n\n| Parameter | Description |\n|-----------|-------------|\n| Name | The name of the parameter that is exposed to the LLM. It can be the same as the underlying field name or a more descriptive label. The LLM uses this name, along with the description, to infer what value to provide during execution. |\n| Attribute Name | When the parameter name shown to the LLM differs from the actual field or property in the database, use this setting to map the user-facing name to the correct attribute. For example, to apply a range filter to the timestamp field, define two separate parameters, such as `start_date` and `end_date`, that both reference the same timestamp attribute. |\n| Description | Provides instructions to the LLM on how the parameter should be used. Clear and specific guidance helps the LLM provide valid input. For example, if a field such as `specialty` is stored in lowercase, the description should indicate that the input must be lowercase. |\n| Is Metadata | When loading data using LangChain or Langflow, additional attributes may be stored under a metadata object. If the target attribute is stored this way, enable this option. It adjusts the query by generating a filter in the format: `{\"metadata.<attribute_name>\": \"<value>\"}` |\n| Is Timestamp | For date or time-based filters, enable this option to automatically convert values to the timestamp format that the Astrapy client expects. This ensures compatibility with the underlying API without requiring manual formatting. |\n| Operator | Defines the filtering logic applied to the attribute. You can use any valid [Data API filter operator](https://docs.datastax.com/en/astra-db-serverless/api-reference/filter-operator-collections.html). For example, to filter a time range on the timestamp attribute, use two parameters: one with the `$gt` operator for \"greater than\", and another with the `$lt` operator for \"less than\". |\n\n## Astra DB Graph\n\nThe **Astra DB Graph** component uses `AstraDBGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in an Astra DB collection. It also supports writing to the vector store.\nFor more information, see [Build a Graph RAG system with LangChain and GraphRetriever](https://docs.datastax.com/en/astra-db-serverless/tutorials/graph-rag.html).\n\n### Astra DB Graph parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |\n| api_endpoint | API Endpoint | Input parameter. Your database's API endpoint. |\n| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |\n| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. For write operations, if a matching collection doesn't exist, a new one is created. |\n| metadata_incoming_links_key | Metadata Incoming Links Key | Input parameter. The metadata key for the incoming links in the vector store. |\n| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |\n| search_input | Search Query | Input parameter. Query string for similarity search. Only relevant for reads. |\n| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |\n| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. If the collection has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html), don't attach an embedding model component. |\n| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine` (default), `dot_product`, or `euclidean`. This is a collection setting. |\n| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |\n| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |\n| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |\n| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |\n| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default) or `Off`. |\n| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. Default: Disabled (`false`). |\n| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |\n| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |\n| search_type | Search Type | Input parameter. Search type to use, either `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`, `Graph Traversal`, or `MMR (Max Marginal Relevance) Graph Traversal` (default). Only relevant to reads. |\n| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |\n| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |\n\n## Graph RAG\n\nThe **Graph RAG** component uses an instance of [`GraphRetriever`](https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/) for Graph RAG traversal enabling graph-based document retrieval in an Astra DB vector store.\nFor more information, see the [DataStax Graph RAG documentation](https://datastax.github.io/graph-rag/).\n\n**Note:** This component can be a Graph RAG extension for the [**Astra DB** vector store component](#astra-db).\nHowever, the [**Astra DB Graph** component](#astra-db-graph) includes both the vector store connection and Graph RAG functionality.\n\n### Graph RAG parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| embedding_model | Embedding Model | Input parameter. Specify the embedding model to use. Not required if the connected vector store has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). |\n| vector_store | Vector Store Connection | Input parameter. An instance of `AstraDbVectorStore` inherited from the [**Astra DB** component](#astra-db)'s **Vector Store Connection** output. |\n| edge_definition | Edge Definition | Input parameter. [Edge definition](https://datastax.github.io/graph-rag/reference/graph_retriever/edges/) for the graph traversal. |\n| strategy | Traversal Strategies | Input parameter. The strategy to use for graph traversal. Strategy options are dynamically loaded from available strategies. |\n| search_query | Search Query | Input parameter. The query to search for in the vector store. |\n| graphrag_strategy_kwargs | Strategy Parameters | Input parameter. Optional dictionary of additional parameters for the [retrieval strategy](https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/). |\n| search_results | **Search Results** or **DataFrame** | Output parameter. The results of the graph-based document retrieval as a list of [`Data`](/data-types#data) objects or as a tabular [`DataFrame`](/data-types#dataframe). You can set the desired output type near the component's output port. |\n\n## Hyper-Converged Database (HCD)\n\nThe **Hyper-Converged Database (HCD)** component uses your cluster's Data API server to read and write to your HCD vector store.\nBecause the underlying functions call the Data API, which originated from Astra DB, the component uses an instance of `AstraDBVectorStore`.\n\n![A flow using the HCD component to load vector data.](/img/component-hcd-example-flow.png)\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\nFor more information about HCD, see [Get started with HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/get-started/get-started-hcd.html) and [Get started with the Data API in HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/dataapiclient.html).\n\n### HCD parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| collection_name | Collection Name | Input parameter. The name of a vector store collection in HCD. For write operations, if the collection doesn't exist, then a new one is created. Required. |\n| username | HCD Username | Input parameter. Username for authenticating to your HCD deployment. Default: `hcd-superuser`. Required. |\n| password | HCD Password | Input parameter. Password for authenticating to your HCD deployment. Required. |\n| api_endpoint | HCD API Endpoint | Input parameter. Your deployment's HCD Data API endpoint, formatted as `http[s]://CLUSTER_HOST:GATEWAY_PORT` where `CLUSTER_HOST` is the IP address of any node in your cluster and `GATEWAY_PORT` is the port number for your API gateway service. For example, `http://192.0.2.250:8181`. Required. |\n| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |\n| search_input | Search Input | Input parameter. Query string for similarity search. Only relevant for reads. |\n| namespace | Namespace | Input parameter. The namespace in HCD that contains or will contain the collection specified in `collection_name`. Default: `default_namespace`. |\n| ca_certificate | CA Certificate | Input parameter. Optional CA certificate for TLS connections to HCD. |\n| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine`, `dot_product`, or `euclidean`. This is a collection setting. If calling an existing collection, leave unset to use the collection's metric. If a write operation creates a new collection, specify the desired similarity metric setting. |\n| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |\n| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |\n| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |\n| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |\n| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default), `Async`, or `Off`. |\n| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. |\n| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |\n| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |\n| embedding | Embedding or Astra Vectorize | Input parameter. The embedding model to use by attaching an **Embedding Model** component. This component doesn't support additional vectorize authentication headers, so it isn't possible to use a vectorize integration with this component, even if you have enabled one on an existing HCD collection. |\n| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |\n| search_type | Search Type | Input parameter. Search type to use, either `Similarity` (default), `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. Only relevant to reads. |\n| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |\n| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |\n\n## Other DataStax components\n\nThe following components are also included in the **DataStax** bundle.\n\n### Astra DB Chat Memory\n\nThe **Astra DB Chat Memory** component retrieves and stores chat messages using an Astra DB database.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\nSpecifically, the component creates an instance of `AstraDBChatMessageHistory`, which is a LangChain chat message history class that uses Astra DB for storage.\n\n**Note:** The **Astra DB Chat Memory** component isn't recommended for most memory storage because memories tend to be long JSON objects or strings, often exceeding the maximum size of a document or object supported by Astra DB.\n\nHowever, Langflow's **Agent** component includes built-in chat memory that is enabled by default.\nYour agentic flows don't need an external database to store chat memory.\nFor more information, see [Memory management options](/memory).\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n#### Astra DB Chat Memory parameters\n\n| Name             | Type          | Description                                                           |\n|------------------|---------------|-----------------------------------------------------------------------|\n| collection_name  | String        | Input parameter. The name of the Astra DB collection for storing messages. Required. |\n| token            | SecretString  | Input parameter. The authentication token for Astra DB access. Required. |\n| api_endpoint     | SecretString  | Input parameter. The API endpoint URL for the Astra DB service. Required. |\n| namespace        | String        | Input parameter. The optional namespace within Astra DB for the collection. |\n| session_id       | MessageText   | Input parameter. The unique identifier for the chat session. Uses the current session ID if not provided. |\n\n### Assistants API\n\nThe following DataStax components are used to create and manage Assistants API functions in a flow:\n\n* **Astra Assistant Agent**\n* **Create Assistant**\n* **Create Assistant Thread**\n* **Get Assistant Name**\n* **List Assistants**\n* **Run Assistant**\n\n## Environment variables\n\nThe following DataStax components are used to load and retrieve environment variables in a flow:\n\n* **Dotenv**\n* **Get Environment Variable**\n\n## Legacy DataStax components\n\nThe following DataStax components are in legacy status:\n\n<details>\n<summary>Astra Vectorize</summary>\n\nThis component was deprecated in Langflow version 1.1.2.\nReplace it with the [**Astra DB** component](#astra-db).\n\nThe **Astra DB Vectorize** component was used to generate embeddings with Astra DB's vectorize feature in conjunction with an **Astra DB** component.\n\nThe vectorize functionality is now built into the **Astra DB** component.\nYou no longer need a separate component for vectorize embedding generation.\n\n</details>\n\n## See also\n\n* [**Cassandra** bundle](/bundles-cassandra)\n* [Create a vector RAG chatbot](/chat-with-rag)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "add",
        "agent",
        "api",
        "application",
        "assistant",
        "assistants",
        "astra",
        "available",
        "bundles",
        "cassandra",
        "chat",
        "close",
        "collection",
        "component",
        "components",
        "connection",
        "controls",
        "converged",
        "cql",
        "create"
      ]
    },
    "/bundles-deepseek": {
      "title": "DeepSeek",
      "slug": "/bundles-deepseek",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DeepSeek** bundle.\n\nFor more information about DeepSeek features and functionality used by DeepSeek components, see the [DeepSeek documentation](https://api-docs.deepseek.com/).\n\n## DeepSeek text generation\n\nThe **DeepSeek** component generates text using DeepSeek's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a DeepSeek model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### DeepSeek text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |\n| model_name | String | Input parameter. The DeepSeek model to use. Default: `deepseek-chat`. |\n| api_base | String | Input parameter. Base URL for API requests. Default: `https://api.deepseek.com`. |\n| api_key | SecretString | Input parameter. Your DeepSeek API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 2.0]`. Default: `1.0`. |\n| seed | Integer | Input parameter. Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "deepseek",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-deepseek": {
      "title": "DeepSeek",
      "slug": "/bundles-deepseek",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DeepSeek** bundle.\n\nFor more information about DeepSeek features and functionality used by DeepSeek components, see the [DeepSeek documentation](https://api-docs.deepseek.com/).\n\n## DeepSeek text generation\n\nThe **DeepSeek** component generates text using DeepSeek's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a DeepSeek model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### DeepSeek text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |\n| model_name | String | Input parameter. The DeepSeek model to use. Default: `deepseek-chat`. |\n| api_base | String | Input parameter. Base URL for API requests. Default: `https://api.deepseek.com`. |\n| api_key | SecretString | Input parameter. Your DeepSeek API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 2.0]`. Default: `1.0`. |\n| seed | Integer | Input parameter. Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "deepseek",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-docling": {
      "title": "Docling",
      "slug": "/bundles-docling",
      "category": "components",
      "content": "Langflow integrates with [Docling](https://docling-project.github.io/docling/) through a bundle of components for parsing documents.\n\n## Prerequisites\n\n* **Enable Developer Mode for Windows**:\n\n* **Install Docling dependency**:\nThe Docling dependency is required to use the Docling components in Langflow.\n\n    * **Langflow version 1.6 and later**: The Docling dependency is included by default for all operating systems except macOS Intel (x86_64).\n\n        For macOS Intel (x86_64), use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.\n\n    * **Earlier versions**: Langflow versions earlier than 1.6 don't include the Docling dependency.\n    For Langflow OSS, install the Docling extra with `uv pip install 'langflow[docling]'`.\n    For Langflow Desktop, add the Docling dependency to Langflow Desktop's `requirements.txt`.\n    For more information, see [Install custom dependencies](/install-custom-dependencies).\n\n## Use Docling components in a flow\n\n**Note:** To learn more about content extraction with Docling, see the video tutorial [Docling + Langflow: Document Processing for AI Workflows](https://www.youtube.com/watch?v=5DuS6uRI5OM).\n\nThis example demonstrates how to use Docling components to split a PDF in a flow:\n\n1. Connect a **Docling** and an **Export DoclingDocument** component to a [**Split Text** component](/components-processing#split-text).\n\n    The **Docling** component loads the document, and the **Export DoclingDocument** component converts the `DoclingDocument` into the format you select. This example converts the document to Markdown, with images represented as placeholders.\n    The **Split Text** component will split the Markdown into chunks for the vector database to store in the next part of the flow.\n\n2. Connect a [**Chroma DB** vector store component](/bundles-chroma#chroma-db) to the **Split Text** component's **Chunks** output.\n3. Connect an [embedding model component](/components-embedding-models) to the **Chroma DB** component's **Embedding** port and a **Chat Output** component to view the extracted [`DataFrame`](/data-types#dataframe).\n4. In the embedding model component, select your preferred model, provide credentials, and configure other settings as needed.\n\n    ![Docling and ExportDoclingDocument extracting and splitting text to vector database](/img/integrations-docling-split-text.png)\n\n5. Add a file to the **Docling** component.\n6. To run the flow, click  **Playground**.\n\n    The chunked document is loaded as vectors into your vector database.\n\n## Docling components\n\nThe following sections describe the purpose and configuration options for each component in the **Docling** bundle.\n\n### Docling language model\n\nThe **Docling** language model component ingest documents, and then uses Docling to process them by running the Docling models locally.\n\nIt outputs `files`, which is the processed files with `DoclingDocument` data.\n\nFor more information, see the [Docling IBM models project repository](https://github.com/docling-project/docling-ibm-models).\n\n#### Docling parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| files | File | The files to process. |\n| pipeline | String | Docling pipeline to use (standard, vlm). |\n| ocr_engine | String | OCR engine to use (easyocr, tesserocr, rapidocr, ocrmac). |\n\n### Docling Serve\n\nThe **Docling Serve** component runs Docling as an API service.\n\nIt outputs `files`, which is the processed files with `DoclingDocument` data.\n\nFor more information, see the [Docling serve project repository](https://github.com/docling-project/docling-serve).\n\n#### Docling Serve parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| files | File | The files to process. |\n| api_url | String | URL of the Docling Serve instance. |\n| max_concurrency | Integer | Maximum number of concurrent requests for the server. |\n| max_poll_timeout | Float | Maximum waiting time for the document conversion to complete. |\n| api_headers | Dict | Optional dictionary of additional headers required for connecting to Docling Serve. |\n| docling_serve_opts | Dict | Optional dictionary of additional options for Docling Serve. |\n\n### Chunk DoclingDocument\n\nThe **Chunk DoclingDocument** component uses the `DoclingDocument` chunkers to split a document into chunks.\n\nIt outputs the chunked documents as a [`DataFrame`](/data-types#dataframe).\n\nFor more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).\n\n#### Chunk DoclingDocument parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| data_inputs | Data/DataFrame | The data with documents to split in chunks. |\n| chunker | String | Which chunker to use (HybridChunker, HierarchicalChunker). |\n| provider | String | Which tokenizer provider (Hugging Face, OpenAI). |\n| hf_model_name | String | Model name of the tokenizer to use with the HybridChunker when Hugging Face is chosen. |\n| openai_model_name | String | Model name of the tokenizer to use with the HybridChunker when OpenAI is chosen. |\n| max_tokens | Integer | Maximum number of tokens for the HybridChunker. |\n| doc_key | String | The key to use for the `DoclingDocument` column. |\n\n### Export DoclingDocument\n\nThe **Export DoclingDocument** component exports `DoclingDocument` to Markdown, HTML, and other formats.\n\nIt can output the exported data as either [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe).\n\nFor more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).\n\n#### Export DoclingDocument parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| data_inputs | Data/DataFrame | The data with documents to export. |\n| export_format | String | Select the export format to convert the input (Markdown, HTML, Plaintext, DocTags). |\n| image_mode | String | Specify how images are exported in the output (placeholder, embedded). |\n| md_image_placeholder | String | Specify the image placeholder for markdown exports. |\n| md_page_break_placeholder | String | Add this placeholder between pages in the markdown output. |\n| doc_key | String | The key to use for the `DoclingDocument` column. |\n\n## See also\n\n* [**File** component](/components-data#file)",
      "summary": "Langflow integrates with [Docling](https://docling-project.github.io/docling/) through a bundle of components for parsing documents.",
      "keywords": [
        "chat",
        "chroma",
        "chunk",
        "chunks",
        "components",
        "dependency",
        "developer",
        "docling",
        "doclingdocument",
        "earlier",
        "embedding",
        "enable",
        "export",
        "file",
        "flow",
        "install",
        "langflow",
        "language",
        "later",
        "mode"
      ]
    },
    "bundles-docling": {
      "title": "Docling",
      "slug": "/bundles-docling",
      "category": "components",
      "content": "Langflow integrates with [Docling](https://docling-project.github.io/docling/) through a bundle of components for parsing documents.\n\n## Prerequisites\n\n* **Enable Developer Mode for Windows**:\n\n* **Install Docling dependency**:\nThe Docling dependency is required to use the Docling components in Langflow.\n\n    * **Langflow version 1.6 and later**: The Docling dependency is included by default for all operating systems except macOS Intel (x86_64).\n\n        For macOS Intel (x86_64), use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.\n\n    * **Earlier versions**: Langflow versions earlier than 1.6 don't include the Docling dependency.\n    For Langflow OSS, install the Docling extra with `uv pip install 'langflow[docling]'`.\n    For Langflow Desktop, add the Docling dependency to Langflow Desktop's `requirements.txt`.\n    For more information, see [Install custom dependencies](/install-custom-dependencies).\n\n## Use Docling components in a flow\n\n**Note:** To learn more about content extraction with Docling, see the video tutorial [Docling + Langflow: Document Processing for AI Workflows](https://www.youtube.com/watch?v=5DuS6uRI5OM).\n\nThis example demonstrates how to use Docling components to split a PDF in a flow:\n\n1. Connect a **Docling** and an **Export DoclingDocument** component to a [**Split Text** component](/components-processing#split-text).\n\n    The **Docling** component loads the document, and the **Export DoclingDocument** component converts the `DoclingDocument` into the format you select. This example converts the document to Markdown, with images represented as placeholders.\n    The **Split Text** component will split the Markdown into chunks for the vector database to store in the next part of the flow.\n\n2. Connect a [**Chroma DB** vector store component](/bundles-chroma#chroma-db) to the **Split Text** component's **Chunks** output.\n3. Connect an [embedding model component](/components-embedding-models) to the **Chroma DB** component's **Embedding** port and a **Chat Output** component to view the extracted [`DataFrame`](/data-types#dataframe).\n4. In the embedding model component, select your preferred model, provide credentials, and configure other settings as needed.\n\n    ![Docling and ExportDoclingDocument extracting and splitting text to vector database](/img/integrations-docling-split-text.png)\n\n5. Add a file to the **Docling** component.\n6. To run the flow, click  **Playground**.\n\n    The chunked document is loaded as vectors into your vector database.\n\n## Docling components\n\nThe following sections describe the purpose and configuration options for each component in the **Docling** bundle.\n\n### Docling language model\n\nThe **Docling** language model component ingest documents, and then uses Docling to process them by running the Docling models locally.\n\nIt outputs `files`, which is the processed files with `DoclingDocument` data.\n\nFor more information, see the [Docling IBM models project repository](https://github.com/docling-project/docling-ibm-models).\n\n#### Docling parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| files | File | The files to process. |\n| pipeline | String | Docling pipeline to use (standard, vlm). |\n| ocr_engine | String | OCR engine to use (easyocr, tesserocr, rapidocr, ocrmac). |\n\n### Docling Serve\n\nThe **Docling Serve** component runs Docling as an API service.\n\nIt outputs `files`, which is the processed files with `DoclingDocument` data.\n\nFor more information, see the [Docling serve project repository](https://github.com/docling-project/docling-serve).\n\n#### Docling Serve parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| files | File | The files to process. |\n| api_url | String | URL of the Docling Serve instance. |\n| max_concurrency | Integer | Maximum number of concurrent requests for the server. |\n| max_poll_timeout | Float | Maximum waiting time for the document conversion to complete. |\n| api_headers | Dict | Optional dictionary of additional headers required for connecting to Docling Serve. |\n| docling_serve_opts | Dict | Optional dictionary of additional options for Docling Serve. |\n\n### Chunk DoclingDocument\n\nThe **Chunk DoclingDocument** component uses the `DoclingDocument` chunkers to split a document into chunks.\n\nIt outputs the chunked documents as a [`DataFrame`](/data-types#dataframe).\n\nFor more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).\n\n#### Chunk DoclingDocument parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| data_inputs | Data/DataFrame | The data with documents to split in chunks. |\n| chunker | String | Which chunker to use (HybridChunker, HierarchicalChunker). |\n| provider | String | Which tokenizer provider (Hugging Face, OpenAI). |\n| hf_model_name | String | Model name of the tokenizer to use with the HybridChunker when Hugging Face is chosen. |\n| openai_model_name | String | Model name of the tokenizer to use with the HybridChunker when OpenAI is chosen. |\n| max_tokens | Integer | Maximum number of tokens for the HybridChunker. |\n| doc_key | String | The key to use for the `DoclingDocument` column. |\n\n### Export DoclingDocument\n\nThe **Export DoclingDocument** component exports `DoclingDocument` to Markdown, HTML, and other formats.\n\nIt can output the exported data as either [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe).\n\nFor more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).\n\n#### Export DoclingDocument parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| data_inputs | Data/DataFrame | The data with documents to export. |\n| export_format | String | Select the export format to convert the input (Markdown, HTML, Plaintext, DocTags). |\n| image_mode | String | Specify how images are exported in the output (placeholder, embedded). |\n| md_image_placeholder | String | Specify the image placeholder for markdown exports. |\n| md_page_break_placeholder | String | Add this placeholder between pages in the markdown output. |\n| doc_key | String | The key to use for the `DoclingDocument` column. |\n\n## See also\n\n* [**File** component](/components-data#file)",
      "summary": "Langflow integrates with [Docling](https://docling-project.github.io/docling/) through a bundle of components for parsing documents.",
      "keywords": [
        "chat",
        "chroma",
        "chunk",
        "chunks",
        "components",
        "dependency",
        "developer",
        "docling",
        "doclingdocument",
        "earlier",
        "embedding",
        "enable",
        "export",
        "file",
        "flow",
        "install",
        "langflow",
        "language",
        "later",
        "mode"
      ]
    },
    "/bundles-duckduckgo": {
      "title": "DuckDuckGo",
      "slug": "/bundles-duckduckgo",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DuckDuckGo** bundle.\n\n## DuckDuckGo Search\n\nThis component performs web searches using the [DuckDuckGo](https://www.duckduckgo.com) search engine with result-limiting capabilities.\n\nIt outputs a list of search results as a [`DataFrame`](/data-types#dataframe) with a `text` key containing the search results as a single string.\n\n### DuckDuckGo Search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input_value | String | Input parameter. The search query to execute with DuckDuckGo. |\n| max_results | Integer | Input parameter. The maximum number of search results to return. Default: 5. |\n| max_snippet_length | Integer | Input parameter. The maximum length of each result snippet. Default: 100. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**SearchApi** bundle](/bundles-searchapi)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "duckduckgo",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "bundles-duckduckgo": {
      "title": "DuckDuckGo",
      "slug": "/bundles-duckduckgo",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **DuckDuckGo** bundle.\n\n## DuckDuckGo Search\n\nThis component performs web searches using the [DuckDuckGo](https://www.duckduckgo.com) search engine with result-limiting capabilities.\n\nIt outputs a list of search results as a [`DataFrame`](/data-types#dataframe) with a `text` key containing the search results as a single string.\n\n### DuckDuckGo Search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input_value | String | Input parameter. The search query to execute with DuckDuckGo. |\n| max_results | Integer | Input parameter. The maximum number of search results to return. Default: 5. |\n| max_snippet_length | Integer | Input parameter. The maximum length of each result snippet. Default: 100. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**SearchApi** bundle](/bundles-searchapi)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "duckduckgo",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "/bundles-elastic": {
      "title": "Elastic",
      "slug": "/bundles-elastic",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Elastic** bundle.\n\n## Elasticsearch\n\nThe **Elasticsearch** component reads and writes to an Elasticsearch instance using [`ElasticsearchStore`](https://docs.langchain.com/oss/python/integrations/vectorstores/elasticsearch).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Elasticsearch parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| elasticsearch_url | String | Input parameter. Elasticsearch server URL. |\n| cloud_id | String | Input parameter. Elasticsearch Cloud ID. |\n| index_name | String | Input parameter. Name of the Elasticsearch index. |\n| ingest_data | Data | Input parameter. Records to load into the vector store. |\n| search_query | String | Input parameter. Query string for similarity search. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| username | String | Input parameter. Username for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty. |\n| password | SecretString | Input parameter. Password for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty |\n| embedding | Embeddings | Input parameter. The embedding model to use. |\n| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default) or `mmr`. |\n| number_of_results | Integer | Input parameter. Number of search results to return. Default: 4. |\n| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |\n| api_key | SecretString | Input parameter. API key for Elastic Cloud authentication. If provided, `username` and `password` aren't required. |\n| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates when connecting to Elasticsearch. Default: Enabled (`true`). |\n\n## OpenSearch\n\nThe **OpenSearch** component reads and writes to OpenSearch instances using [`OpenSearchVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/opensearch).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### OpenSearch parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.;\n\nFor information about accepted values and functionality, see the [OpenSearch documentation](https://opensearch.org/platform/search/vector-database.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| opensearch_url | String | Input parameter. URL for OpenSearch cluster, such as `https://192.168.1.1:9200`. |\n| index_name | String | Input parameter. The index name where the vectors are stored in OpenSearch cluster. Default: `langflow`. |\n| ingest_data | Data | Input parameter. The data to be ingested into the vector store. |\n| search_input | String | Input parameter. Enter a search query. Leave empty to retrieve all documents or if hybrid search is being used. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embeddings | Input parameter. Attach an [embedding model component](/components-embedding-models) to use to generate an embedding from the search query. |\n| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default), `similarity_score_threshold`, `mmr`. |\n| number_of_results | Integer | Input parameter. The number of results to return in search. Default: 4. |\n| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |\n| username | String | Input parameter. The username for the OpenSearch cluster. Default: `admin`.|\n| password | SecretString | Input parameter. The password for the OpenSearch cluster. |\n| use_ssl | Boolean | Input parameter. Whether to use SSL. Default: Enabled (`true`). |\n| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates. Default: Disabled (`false`). |\n| hybrid_search_query | String | Input parameter. Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching. |\n\n### OpenSearch output\n\n<details>\n<summary>Vector Store Connection port</summary>\n\nThe **OpenSearch** component has an additional deprecated **Vector Store Connection** output.\nThis output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.\n\nThe **OpenSearch** component doesn't require a separate Graph RAG component because OpenSearch instances support Graph traversal through built-in RAG functionality and plugins.\n\n</details>",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "connection",
        "elastic",
        "elasticsearch",
        "opensearch",
        "output",
        "parameters",
        "store",
        "vector"
      ]
    },
    "bundles-elastic": {
      "title": "Elastic",
      "slug": "/bundles-elastic",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Elastic** bundle.\n\n## Elasticsearch\n\nThe **Elasticsearch** component reads and writes to an Elasticsearch instance using [`ElasticsearchStore`](https://docs.langchain.com/oss/python/integrations/vectorstores/elasticsearch).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Elasticsearch parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| elasticsearch_url | String | Input parameter. Elasticsearch server URL. |\n| cloud_id | String | Input parameter. Elasticsearch Cloud ID. |\n| index_name | String | Input parameter. Name of the Elasticsearch index. |\n| ingest_data | Data | Input parameter. Records to load into the vector store. |\n| search_query | String | Input parameter. Query string for similarity search. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| username | String | Input parameter. Username for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty. |\n| password | SecretString | Input parameter. Password for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty |\n| embedding | Embeddings | Input parameter. The embedding model to use. |\n| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default) or `mmr`. |\n| number_of_results | Integer | Input parameter. Number of search results to return. Default: 4. |\n| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |\n| api_key | SecretString | Input parameter. API key for Elastic Cloud authentication. If provided, `username` and `password` aren't required. |\n| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates when connecting to Elasticsearch. Default: Enabled (`true`). |\n\n## OpenSearch\n\nThe **OpenSearch** component reads and writes to OpenSearch instances using [`OpenSearchVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/opensearch).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### OpenSearch parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.;\n\nFor information about accepted values and functionality, see the [OpenSearch documentation](https://opensearch.org/platform/search/vector-database.html) or inspect [component code](/concepts-components#component-code).\n\n| Name | Type | Description |\n|------|------|-------------|\n| opensearch_url | String | Input parameter. URL for OpenSearch cluster, such as `https://192.168.1.1:9200`. |\n| index_name | String | Input parameter. The index name where the vectors are stored in OpenSearch cluster. Default: `langflow`. |\n| ingest_data | Data | Input parameter. The data to be ingested into the vector store. |\n| search_input | String | Input parameter. Enter a search query. Leave empty to retrieve all documents or if hybrid search is being used. |\n| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding | Embeddings | Input parameter. Attach an [embedding model component](/components-embedding-models) to use to generate an embedding from the search query. |\n| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default), `similarity_score_threshold`, `mmr`. |\n| number_of_results | Integer | Input parameter. The number of results to return in search. Default: 4. |\n| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |\n| username | String | Input parameter. The username for the OpenSearch cluster. Default: `admin`.|\n| password | SecretString | Input parameter. The password for the OpenSearch cluster. |\n| use_ssl | Boolean | Input parameter. Whether to use SSL. Default: Enabled (`true`). |\n| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates. Default: Disabled (`false`). |\n| hybrid_search_query | String | Input parameter. Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching. |\n\n### OpenSearch output\n\n<details>\n<summary>Vector Store Connection port</summary>\n\nThe **OpenSearch** component has an additional deprecated **Vector Store Connection** output.\nThis output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.\n\nThe **OpenSearch** component doesn't require a separate Graph RAG component because OpenSearch instances support Graph traversal through built-in RAG functionality and plugins.\n\n</details>",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "connection",
        "elastic",
        "elasticsearch",
        "opensearch",
        "output",
        "parameters",
        "store",
        "vector"
      ]
    },
    "/bundles-exa": {
      "title": "Exa",
      "slug": "/bundles-exa",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Exa** bundle.\n\n## Exa Search\n\nThis component provides an [Exa Search](https://exa.ai/) toolkit for search and content retrieval by a Langflow [**Agent** component](/agents) or [MCP client](/mcp-client).\n\nThe output is exclusively [`Tools`](/data-types#tool).\n\n### Exa Search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Exa Search API Key (`metaphor_api_key`) | SecretString | Input parameter. An API key for Exa Search. |\n| Use Autoprompt (`use_autoprompt`) | Boolean | Input parameter. Whether to use the autoprompt feature. Default: `true`. |\n| Search Number of Results (`search_num_results`) | Integer | Input parameter. The number of results to return for search. Default: 5. |\n| Similar Number of Results (`similar_num_results`) | Integer | Input parameter. The number of similar results to return. Default: 5. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "exa",
        "parameters",
        "search"
      ]
    },
    "bundles-exa": {
      "title": "Exa",
      "slug": "/bundles-exa",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Exa** bundle.\n\n## Exa Search\n\nThis component provides an [Exa Search](https://exa.ai/) toolkit for search and content retrieval by a Langflow [**Agent** component](/agents) or [MCP client](/mcp-client).\n\nThe output is exclusively [`Tools`](/data-types#tool).\n\n### Exa Search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Exa Search API Key (`metaphor_api_key`) | SecretString | Input parameter. An API key for Exa Search. |\n| Use Autoprompt (`use_autoprompt`) | Boolean | Input parameter. Whether to use the autoprompt feature. Default: `true`. |\n| Search Number of Results (`search_num_results`) | Integer | Input parameter. The number of results to return for search. Default: 5. |\n| Similar Number of Results (`similar_num_results`) | Integer | Input parameter. The number of similar results to return. Default: 5. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "exa",
        "parameters",
        "search"
      ]
    },
    "/bundles-faiss": {
      "title": "FAISS",
      "slug": "/bundles-faiss",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **FAISS** bundle.\n\n## FAISS vector store\n\nThe **FAISS** component provides access to the Facebook AI Similarity Search (FAISS) library through an instance of `FAISS` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### FAISS vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [FAISS documentation](https://faiss.ai/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name                      | Type          | Description                                      |\n|---------------------------|---------------|--------------------------------------------------|\n| index_name                 | String        | Input parameter. The name of the FAISS index. Default: \"langflow_index\". |\n| persist_directory          | String        | Input parameter. Path to save the FAISS index. It is relative to where Langflow is running. |\n| search_query               | String        | Input parameter. The query to search for in the vector store.     |\n| ingest_data                | Data          | Input parameter. The list of data to ingest into the vector store. |\n| allow_dangerous_deserialization | Boolean  | Input parameter. Set to `True` to allow loading pickle files from untrusted sources. Default: `True`. |\n| embedding                  | Embeddings    | Input parameter. The embedding function to use for the vector store. |\n| number_of_results          | Integer       | Input parameter. Number of results to return from the search. Default: 4. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "faiss",
        "parameters",
        "store",
        "vector"
      ]
    },
    "bundles-faiss": {
      "title": "FAISS",
      "slug": "/bundles-faiss",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **FAISS** bundle.\n\n## FAISS vector store\n\nThe **FAISS** component provides access to the Facebook AI Similarity Search (FAISS) library through an instance of `FAISS` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### FAISS vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [FAISS documentation](https://faiss.ai/index.html) or inspect [component code](/concepts-components#component-code).\n\n| Name                      | Type          | Description                                      |\n|---------------------------|---------------|--------------------------------------------------|\n| index_name                 | String        | Input parameter. The name of the FAISS index. Default: \"langflow_index\". |\n| persist_directory          | String        | Input parameter. Path to save the FAISS index. It is relative to where Langflow is running. |\n| search_query               | String        | Input parameter. The query to search for in the vector store.     |\n| ingest_data                | Data          | Input parameter. The list of data to ingest into the vector store. |\n| allow_dangerous_deserialization | Boolean  | Input parameter. Set to `True` to allow loading pickle files from untrusted sources. Default: `True`. |\n| embedding                  | Embeddings    | Input parameter. The embedding function to use for the vector store. |\n| number_of_results          | Integer       | Input parameter. Number of results to return from the search. Default: 4. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "faiss",
        "parameters",
        "store",
        "vector"
      ]
    },
    "/bundles-glean": {
      "title": "Glean",
      "slug": "/bundles-glean",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Glean** bundle.\n\n## Glean Search API\n\nThis component allows you to call the Glean Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Glean Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| glean_api_url | String | Input parameter. The URL of the Glean API. |\n| glean_access_token | SecretString | Input parameter. An access token for Glean API authentication. |\n| query | String | Input parameter. The search query input. |\n| page_size | Integer | Input parameter. The number of results per page. Default: 10. |\n| request_options | Dict | Input parameter. Additional options for the API request. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "glean",
        "parameters",
        "search",
        "web"
      ]
    },
    "bundles-glean": {
      "title": "Glean",
      "slug": "/bundles-glean",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Glean** bundle.\n\n## Glean Search API\n\nThis component allows you to call the Glean Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Glean Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| glean_api_url | String | Input parameter. The URL of the Glean API. |\n| glean_access_token | SecretString | Input parameter. An access token for Glean API authentication. |\n| query | String | Input parameter. The search query input. |\n| page_size | Integer | Input parameter. The number of results per page. Default: 10. |\n| request_options | Dict | Input parameter. Additional options for the API request. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "glean",
        "parameters",
        "search",
        "web"
      ]
    },
    "/bundles-google": {
      "title": "Google components",
      "slug": "/bundles-google",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Google** bundle.\n\n## BigQuery\n\nLangflow integrates with [Google BigQuery](https://cloud.google.com/bigquery) through the **BigQuery** component in the [**Google** bundle](/bundles-google), allowing you to execute SQL queries and retrieve data from your BigQuery datasets.\n\n### Use the BigQuery component in a flow\n\nTo use the **BigQuery** component in a flow, you need the following:\n\n* A [Google Cloud project](https://developers.google.com/workspace/guides/create-project) with the BigQuery API enabled\n* A [service account](https://developers.google.com/workspace/guides/create-credentials#service-account) with the **BigQuery Job User** role\n* A [BigQuery dataset and table](https://cloud.google.com/bigquery/docs/datasets-intro)\n* A [running Langflow server](/get-started-installation)\n\n#### Create a service account with BigQuery access\n\n1. Select and enable your Google Cloud project.\nFor more information, see [Create a Google Cloud project](https://developers.google.com/workspace/guides/create-project).\n2. Create a service account in your Google Cloud project.\nFor more information, see [Create a service account](https://developers.google.com/workspace/guides/create-credentials#service-account).\n3. Assign the **BigQuery Job User** role to your new account.\nThis role allows Langflow to access BigQuery resources with the service account.\nYou may also need to allow access to your BigQuery dataset.\nFor more information, see [BigQuery access control with IAM](https://cloud.google.com/bigquery/docs/access-control).\n4. To generate a new JSON key for the service account, navigate to your service account.\n5. Click **Add Key**, and then click **Create new key**.\n6. Under **Key type**, select **JSON**, and then click **Create**.\nA JSON private key file is downloaded to your machine.\nNow that you have a service account and a JSON private key, you need to configure the credentials in the Langflow **BigQuery** component.\n\n#### Configure credentials in the Langflow component\n\nWith your service account configured and your credentials JSON file created, follow these steps to authenticate the Langflow application.\n\n1. Create a new flow in Langflow.\n2. In  **Bundles**, find the Google **BigQuery** component, and then add it to your flow.\n3. In the **BigQuery** component's **Upload Service Account JSON** field, click **Select file**.\n4. In the **My Files** pane, select **Click or drag files here**.\nYour file browser opens.\n5. In your file browser, select the service account JSON file, and then click **Open**.\n6. In the **My Files** pane, select your service account JSON file, and then click **Select files**.\nThe **BigQuery** component can now query your datasets and tables using your service account JSON file.\n\n#### Query a BigQuery dataset\n\nWith your component credentials configured, query your BigQuery datasets and tables to confirm connectivity.\n\n1. Connect **Chat Input** and **Chat Output** components to the **BigQuery** component.\n\n    ![BigQuery component connected to chat input and output](/img/integrations-bigquery.png)\n\n2. Open the **Playground**, and then submit a valid SQL query.\n\n    This example queries a table of Oscar winners stored within a BigQuery dataset called `the_oscar_award`:\n\n    ```sql\n    SELECT film, category, year_film\n    FROM `big-query-langflow-project.the_oscar_award.oscar_winners`\n    WHERE winner = TRUE\n    LIMIT 10\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```text\n    film\tcategory\tyear_film\n    The Last Command\tACTOR\t1927\n    7th Heaven\tACTRESS\t1927\n    The Dove;\tART DIRECTION\t1927\n    Sunrise\tCINEMATOGRAPHY\t1927\n    Sunrise\tCINEMATOGRAPHY\t1927\n    Two Arabian Knights\tDIRECTING (Comedy Picture)\t1927\n    7th Heaven\tDIRECTING (Dramatic Picture)\t1927\n    Wings\tENGINEERING EFFECTS\t1927\n    Wings\tOUTSTANDING PICTURE\t1927\n    Sunrise\tUNIQUE AND ARTISTIC PICTURE\t1927\n    ```\n\n    </details>\n\n    A successful chat confirms the component can access the BigQuery table.\n\n## Google Generative AI\n\nThis component generates text using [Google Generative AI models](https://cloud.google.com/vertex-ai/docs/).\n\n### Google Generative AI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Google API Key | SecretString | Input parameter. Your Google API key to use for the Google Generative AI. |\n| Model | String | Input parameter. The name of the model to use, such as `\"gemini-pro\"`. |\n| Max Output Tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| Temperature | Float | Input parameter. Run inference with this temperature. |\n| Top K | Integer | Input parameter. Consider the set of top K most probable tokens. |\n| Top P | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |\n| N | Integer | Input parameter. Number of chat completions to generate for each prompt. |\n| model | LanguageModel | Output parameter. An instance of ChatGoogleGenerativeAI configured with the specified parameters. |\n\n## Google Generative AI Embeddings\n\nThe **Google Generative AI Embeddings** component connects to Google's generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the `langchain-google-genai` package.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Google Generative AI Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| api_key | API Key | Input parameter. The secret API key for accessing Google's generative AI service. Required. |\n| model_name | Model Name | Input parameter. The name of the embedding model to use. Default: \"models/text-embedding-004\". |\n| embeddings | Embeddings | Output parameter. The built GoogleGenerativeAIEmbeddings object. |\n\n## Google Search API\n\nThis component allows you to call the Google Search API.\n\n### Google Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| google_api_key | SecretString | Input parameter. A Google API key for authentication. |\n| google_cse_id | SecretString | Input parameter. A Google Custom Search Engine ID. |\n| input_value | String | Input parameter. The search query input. |\n| k | Integer | Input parameter. The number of search results to return. |\n| results | List[Data] | Output parameter. A list of search results. |\n| tool | Tool | Output parameter. A Google Search tool for use in LangChain. |\n\n### Other Google Search components\n\nLangflow includes multiple components that support Google Search, such as the following:\n\n* [**Apify Actors** component](/bundles-apify)\n* [**SearchApi** component](/bundles-searchapi)\n* [**Serper Google Search API** component](/bundles-serper)\n* [**News Search** component](/components-data#news-search)\n* [**Web Search** component](/components-data#web-search)\n\n## Google Vertex AI\n\nFor information about Vertex AI components, see the [**Vertex AI** bundle](/bundles-vertexai).\n\n## Legacy Google components\n\nThe following Google components are in legacy status:\n\n<details>\n<summary>Google OAuth Token</summary>\n\nThe **Google OAuth Token** component was deprecated in Langflow 1.4.0.\n\nTo connect your flows to Google OAuth services, use [Composio components](/bundles-composio).\n\n</details>\n\n<details>\n<summary>Gmail Loader</summary>\n\nThis component loads emails from Gmail using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and label ID filters.\n\nAs an alternative, you can use [Composio components](/bundles-composio) to connect your flows to Google services.\n\n</details>\n\n<details>\n<summary>Google Drive Loader</summary>\n\nThis component loads documents from Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and document ID filters.\n\nWhile there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.\n\n</details>\n\n<details>\n<summary>Google Drive Search</summary>\n\nThis component searches Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and various query strings and filters.\n\nWhile there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.\n\n</details>\n\n## See also\n\n- [**Composio** bundle](/bundles-composio)\n- [**Vertex AI** bundle](/bundles-vertexai)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "access",
        "account",
        "actors",
        "add",
        "api",
        "apify",
        "bigquery",
        "bundles",
        "chat",
        "click",
        "component",
        "components",
        "composio",
        "configure",
        "create",
        "credentials",
        "dataset",
        "drag",
        "embeddings",
        "file"
      ]
    },
    "bundles-google": {
      "title": "Google components",
      "slug": "/bundles-google",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Google** bundle.\n\n## BigQuery\n\nLangflow integrates with [Google BigQuery](https://cloud.google.com/bigquery) through the **BigQuery** component in the [**Google** bundle](/bundles-google), allowing you to execute SQL queries and retrieve data from your BigQuery datasets.\n\n### Use the BigQuery component in a flow\n\nTo use the **BigQuery** component in a flow, you need the following:\n\n* A [Google Cloud project](https://developers.google.com/workspace/guides/create-project) with the BigQuery API enabled\n* A [service account](https://developers.google.com/workspace/guides/create-credentials#service-account) with the **BigQuery Job User** role\n* A [BigQuery dataset and table](https://cloud.google.com/bigquery/docs/datasets-intro)\n* A [running Langflow server](/get-started-installation)\n\n#### Create a service account with BigQuery access\n\n1. Select and enable your Google Cloud project.\nFor more information, see [Create a Google Cloud project](https://developers.google.com/workspace/guides/create-project).\n2. Create a service account in your Google Cloud project.\nFor more information, see [Create a service account](https://developers.google.com/workspace/guides/create-credentials#service-account).\n3. Assign the **BigQuery Job User** role to your new account.\nThis role allows Langflow to access BigQuery resources with the service account.\nYou may also need to allow access to your BigQuery dataset.\nFor more information, see [BigQuery access control with IAM](https://cloud.google.com/bigquery/docs/access-control).\n4. To generate a new JSON key for the service account, navigate to your service account.\n5. Click **Add Key**, and then click **Create new key**.\n6. Under **Key type**, select **JSON**, and then click **Create**.\nA JSON private key file is downloaded to your machine.\nNow that you have a service account and a JSON private key, you need to configure the credentials in the Langflow **BigQuery** component.\n\n#### Configure credentials in the Langflow component\n\nWith your service account configured and your credentials JSON file created, follow these steps to authenticate the Langflow application.\n\n1. Create a new flow in Langflow.\n2. In  **Bundles**, find the Google **BigQuery** component, and then add it to your flow.\n3. In the **BigQuery** component's **Upload Service Account JSON** field, click **Select file**.\n4. In the **My Files** pane, select **Click or drag files here**.\nYour file browser opens.\n5. In your file browser, select the service account JSON file, and then click **Open**.\n6. In the **My Files** pane, select your service account JSON file, and then click **Select files**.\nThe **BigQuery** component can now query your datasets and tables using your service account JSON file.\n\n#### Query a BigQuery dataset\n\nWith your component credentials configured, query your BigQuery datasets and tables to confirm connectivity.\n\n1. Connect **Chat Input** and **Chat Output** components to the **BigQuery** component.\n\n    ![BigQuery component connected to chat input and output](/img/integrations-bigquery.png)\n\n2. Open the **Playground**, and then submit a valid SQL query.\n\n    This example queries a table of Oscar winners stored within a BigQuery dataset called `the_oscar_award`:\n\n    ```sql\n    SELECT film, category, year_film\n    FROM `big-query-langflow-project.the_oscar_award.oscar_winners`\n    WHERE winner = TRUE\n    LIMIT 10\n    ```\n\n    <details>\n    <summary>Result</summary>\n\n    ```text\n    film\tcategory\tyear_film\n    The Last Command\tACTOR\t1927\n    7th Heaven\tACTRESS\t1927\n    The Dove;\tART DIRECTION\t1927\n    Sunrise\tCINEMATOGRAPHY\t1927\n    Sunrise\tCINEMATOGRAPHY\t1927\n    Two Arabian Knights\tDIRECTING (Comedy Picture)\t1927\n    7th Heaven\tDIRECTING (Dramatic Picture)\t1927\n    Wings\tENGINEERING EFFECTS\t1927\n    Wings\tOUTSTANDING PICTURE\t1927\n    Sunrise\tUNIQUE AND ARTISTIC PICTURE\t1927\n    ```\n\n    </details>\n\n    A successful chat confirms the component can access the BigQuery table.\n\n## Google Generative AI\n\nThis component generates text using [Google Generative AI models](https://cloud.google.com/vertex-ai/docs/).\n\n### Google Generative AI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Google API Key | SecretString | Input parameter. Your Google API key to use for the Google Generative AI. |\n| Model | String | Input parameter. The name of the model to use, such as `\"gemini-pro\"`. |\n| Max Output Tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| Temperature | Float | Input parameter. Run inference with this temperature. |\n| Top K | Integer | Input parameter. Consider the set of top K most probable tokens. |\n| Top P | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |\n| N | Integer | Input parameter. Number of chat completions to generate for each prompt. |\n| model | LanguageModel | Output parameter. An instance of ChatGoogleGenerativeAI configured with the specified parameters. |\n\n## Google Generative AI Embeddings\n\nThe **Google Generative AI Embeddings** component connects to Google's generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the `langchain-google-genai` package.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Google Generative AI Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| api_key | API Key | Input parameter. The secret API key for accessing Google's generative AI service. Required. |\n| model_name | Model Name | Input parameter. The name of the embedding model to use. Default: \"models/text-embedding-004\". |\n| embeddings | Embeddings | Output parameter. The built GoogleGenerativeAIEmbeddings object. |\n\n## Google Search API\n\nThis component allows you to call the Google Search API.\n\n### Google Search API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| google_api_key | SecretString | Input parameter. A Google API key for authentication. |\n| google_cse_id | SecretString | Input parameter. A Google Custom Search Engine ID. |\n| input_value | String | Input parameter. The search query input. |\n| k | Integer | Input parameter. The number of search results to return. |\n| results | List[Data] | Output parameter. A list of search results. |\n| tool | Tool | Output parameter. A Google Search tool for use in LangChain. |\n\n### Other Google Search components\n\nLangflow includes multiple components that support Google Search, such as the following:\n\n* [**Apify Actors** component](/bundles-apify)\n* [**SearchApi** component](/bundles-searchapi)\n* [**Serper Google Search API** component](/bundles-serper)\n* [**News Search** component](/components-data#news-search)\n* [**Web Search** component](/components-data#web-search)\n\n## Google Vertex AI\n\nFor information about Vertex AI components, see the [**Vertex AI** bundle](/bundles-vertexai).\n\n## Legacy Google components\n\nThe following Google components are in legacy status:\n\n<details>\n<summary>Google OAuth Token</summary>\n\nThe **Google OAuth Token** component was deprecated in Langflow 1.4.0.\n\nTo connect your flows to Google OAuth services, use [Composio components](/bundles-composio).\n\n</details>\n\n<details>\n<summary>Gmail Loader</summary>\n\nThis component loads emails from Gmail using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and label ID filters.\n\nAs an alternative, you can use [Composio components](/bundles-composio) to connect your flows to Google services.\n\n</details>\n\n<details>\n<summary>Google Drive Loader</summary>\n\nThis component loads documents from Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and document ID filters.\n\nWhile there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.\n\n</details>\n\n<details>\n<summary>Google Drive Search</summary>\n\nThis component searches Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and various query strings and filters.\n\nWhile there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.\n\n</details>\n\n## See also\n\n- [**Composio** bundle](/bundles-composio)\n- [**Vertex AI** bundle](/bundles-vertexai)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "access",
        "account",
        "actors",
        "add",
        "api",
        "apify",
        "bigquery",
        "bundles",
        "chat",
        "click",
        "component",
        "components",
        "composio",
        "configure",
        "create",
        "credentials",
        "dataset",
        "drag",
        "embeddings",
        "file"
      ]
    },
    "/bundles-groq": {
      "title": "Groq",
      "slug": "/bundles-groq",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Groq** bundle.\n\nFor more information, see the [Groq documentation](https://groq.com/).\n\n## Groq text generation\n\nThis component generates text using Groq's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatGroq`](https://docs.langchain.com/oss/python/integrations/chat/groq) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use a Groq model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n![Groq component in a basic prompting flow](/img/component-groq.png)\n\n### Groq text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| groq_api_key | SecretString | Your Groq API key. |\n| groq_api_base | String | Base URL path for API requests. Default: `https://api.groq.com`. |\n| max_tokens | Integer | The maximum number of tokens to generate. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.1`. |\n| n | Integer | Number of chat completions to generate for each prompt. |\n| model_name | String | The name of the Groq model to use. Options are dynamically fetched from the Groq API after entering your API key and URL. To refresh your list of models, click  **Refresh**. |\n| tool_mode_enabled | Boolean | If enabled, the component only displays models that work with tools. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "groq",
        "language",
        "model",
        "parameters",
        "refresh",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-groq": {
      "title": "Groq",
      "slug": "/bundles-groq",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Groq** bundle.\n\nFor more information, see the [Groq documentation](https://groq.com/).\n\n## Groq text generation\n\nThis component generates text using Groq's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatGroq`](https://docs.langchain.com/oss/python/integrations/chat/groq) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use a Groq model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n![Groq component in a basic prompting flow](/img/component-groq.png)\n\n### Groq text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| groq_api_key | SecretString | Your Groq API key. |\n| groq_api_base | String | Base URL path for API requests. Default: `https://api.groq.com`. |\n| max_tokens | Integer | The maximum number of tokens to generate. |\n| temperature | Float | Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.1`. |\n| n | Integer | Number of chat completions to generate for each prompt. |\n| model_name | String | The name of the Groq model to use. Options are dynamically fetched from the Groq API after entering your API key and URL. To refresh your list of models, click  **Refresh**. |\n| tool_mode_enabled | Boolean | If enabled, the component only displays models that work with tools. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "groq",
        "language",
        "model",
        "parameters",
        "refresh",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-huggingface": {
      "title": "Hugging Face",
      "slug": "/bundles-huggingface",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe components in the **Hugging Face** bundle require access to Hugging Face APIs.\n\nFor more information about Hugging Face features and functionality used by Hugging Face components, see the [Hugging Face documentation](https://huggingface.co/docs).\n\n## Hugging Face text generation\n\nThe **Hugging Face** component generates text using a specified model by sending requests to the Hugging Face API, which is a hosted inference API for models hosted on Hugging Face.\nAuthentication is required.\n\nThis component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatHuggingFace`](https://docs.langchain.com/oss/python/integrations/chat/huggingface) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use a Hugging Face model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Hugging Face text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_id | String | Input parameter. The model ID from Hugging Face Hub. For example, \"gpt2\", \"facebook/bart-large\". |\n| huggingfacehub_api_token | SecretString | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| max_new_tokens | Integer | Input parameter. Maximum number of tokens to generate. Default: 512. |\n| top_p | Float | Input parameter. Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95. |\n| top_k | Integer | Input parameter. Top-k sampling parameter. Default: 50. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n\n## Hugging Face Embeddings Inference\n\nUse the **Hugging Face Embeddings Inference** component to create embeddings with Hugging Face's hosted models or with your own locally hosted models.\n\nThe component generates embeddings using [Hugging Face Inference API models](https://huggingface.co/models).\nAuthentication is required when not using a local model.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models) and [Use a local Hugging Face embeddings model](#local-hugging-face-model).\n\n### Hugging Face Embeddings Inference parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| api_key | API Key | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for accessing the Hugging Face Inference API, if required. Local inference models don't require an API key. |\n| api_url | API URL | Input parameter. The URL of the Hugging Face Inference API. |\n| model_name | Model Name | Input parameter. The name of the model to use for embeddings. |\n\n### Use a local Hugging Face embeddings model {#local-hugging-face-model}\n\nTo connect the local Hugging Face model to the **Hugging Face Embeddings Inference** component and use it in a flow, follow these steps:\n\n1. Run a [local Hugging Face embeddings inference](https://huggingface.co/docs/text-embeddings-inference/local_cpu).\n\n2. For this example, create a flow from the **Vector Store RAG** template.\n\n3. Replace the two **OpenAI Embeddings** components with **Hugging Face Embeddings Inference** components.\n\n    Make sure to reconnect the **Embedding Model** ports from each **Embeddings Inference** component to its corresponding **Astra DB** component.\n\n4. Configure the **Astra DB** components to connect to your Astra organization, or replace both **Astra DB** components with other vector store components.\n\n5. Connect each **Hugging Face Embeddings Inference** component to your local inference model:\n\n    * **Inference Endpoint**: Enter the URL of your local inference model.\n    * **API Key**: Can be empty for local inference.\n    * **Model Name**: Enter the name of your local inference model if it isn't automatically detected.\n\n6. To test the flow, click **Playground**, and then enter some text to generate an embedding.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "astra",
        "bundles",
        "embedding",
        "embeddings",
        "endpoint",
        "face",
        "function",
        "generation",
        "hugging",
        "inference",
        "key",
        "language",
        "local",
        "model",
        "name",
        "openai",
        "parameters",
        "playground"
      ]
    },
    "bundles-huggingface": {
      "title": "Hugging Face",
      "slug": "/bundles-huggingface",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe components in the **Hugging Face** bundle require access to Hugging Face APIs.\n\nFor more information about Hugging Face features and functionality used by Hugging Face components, see the [Hugging Face documentation](https://huggingface.co/docs).\n\n## Hugging Face text generation\n\nThe **Hugging Face** component generates text using a specified model by sending requests to the Hugging Face API, which is a hosted inference API for models hosted on Hugging Face.\nAuthentication is required.\n\nThis component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\nSpecifically, the **Language Model** output is an instance of [`ChatHuggingFace`](https://docs.langchain.com/oss/python/integrations/chat/huggingface) configured according to the component's parameters.\n\nUse the **Language Model** output when you want to use a Hugging Face model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Hugging Face text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_id | String | Input parameter. The model ID from Hugging Face Hub. For example, \"gpt2\", \"facebook/bart-large\". |\n| huggingfacehub_api_token | SecretString | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| max_new_tokens | Integer | Input parameter. Maximum number of tokens to generate. Default: 512. |\n| top_p | Float | Input parameter. Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95. |\n| top_k | Integer | Input parameter. Top-k sampling parameter. Default: 50. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n\n## Hugging Face Embeddings Inference\n\nUse the **Hugging Face Embeddings Inference** component to create embeddings with Hugging Face's hosted models or with your own locally hosted models.\n\nThe component generates embeddings using [Hugging Face Inference API models](https://huggingface.co/models).\nAuthentication is required when not using a local model.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models) and [Use a local Hugging Face embeddings model](#local-hugging-face-model).\n\n### Hugging Face Embeddings Inference parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| api_key | API Key | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for accessing the Hugging Face Inference API, if required. Local inference models don't require an API key. |\n| api_url | API URL | Input parameter. The URL of the Hugging Face Inference API. |\n| model_name | Model Name | Input parameter. The name of the model to use for embeddings. |\n\n### Use a local Hugging Face embeddings model {#local-hugging-face-model}\n\nTo connect the local Hugging Face model to the **Hugging Face Embeddings Inference** component and use it in a flow, follow these steps:\n\n1. Run a [local Hugging Face embeddings inference](https://huggingface.co/docs/text-embeddings-inference/local_cpu).\n\n2. For this example, create a flow from the **Vector Store RAG** template.\n\n3. Replace the two **OpenAI Embeddings** components with **Hugging Face Embeddings Inference** components.\n\n    Make sure to reconnect the **Embedding Model** ports from each **Embeddings Inference** component to its corresponding **Astra DB** component.\n\n4. Configure the **Astra DB** components to connect to your Astra organization, or replace both **Astra DB** components with other vector store components.\n\n5. Connect each **Hugging Face Embeddings Inference** component to your local inference model:\n\n    * **Inference Endpoint**: Enter the URL of your local inference model.\n    * **API Key**: Can be empty for local inference.\n    * **Model Name**: Enter the name of your local inference model if it isn't automatically detected.\n\n6. To test the flow, click **Playground**, and then enter some text to generate an embedding.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "api",
        "astra",
        "bundles",
        "embedding",
        "embeddings",
        "endpoint",
        "face",
        "function",
        "generation",
        "hugging",
        "inference",
        "key",
        "language",
        "local",
        "model",
        "name",
        "openai",
        "parameters",
        "playground"
      ]
    },
    "/bundles-ibm": {
      "title": "IBM",
      "slug": "/bundles-ibm",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe **IBM** bundle provides access to IBM watsonx.ai models for text and embedding generation.\nThese components require an IBM watsonx.ai deployment and watsonx API credentials.\n\n## IBM watsonx.ai\n\nThe **IBM watsonx.ai** component generates text using [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai).\nTo use gateway models, use the [**OpenAI** text generation component](/bundles-openai) with the gateway model's OpenAI-compatible endpoint.\n\nYou can use the **IBM watsonx.ai** component anywhere you need a language model in a flow.\n\n![A basic prompting flow using the IBM watsonx.ai component as the central Language Model component.](/img/component-watsonx-model.png)\n\n### IBM watsonx.ai parameters {#ibm-watsonxai-parameters}\n\n| Name | Type | Description |\n|------|------|-------------|\n| url | String | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |\n| project_id | String | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |\n| api_key | SecretString | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |\n| model_name | String | Input parameter. The name of the watsonx model to use. Options are dynamically fetched from the API. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Default: `1000`. |\n| stop_sequence | String | Input parameter. The sequence where generation should stop. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| top_p | Float | Input parameter. Controls nucleus sampling, which limits the model to tokens whose probability is below the `top_p` value. Range: Default: `0.9`. |\n| frequency_penalty | Float | Input parameter. Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: `0.5`. |\n| presence_penalty | Float | Input parameter. Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: `0.3`. |\n| seed | Integer | Input parameter. A random seed for the model. Default: `8`. |\n| logprobs | Boolean | Input parameter. Whether to return log probabilities of output tokens or not. Default: `true`. |\n| top_logprobs | Integer | Input parameter. The number of most likely tokens to return at each position. Default: `3`. |\n| logit_bias | String | Input parameter. A JSON string of token IDs to bias or suppress. |\n\n### IBM watsonx.ai output\n\nThe **IBM watsonx.ai** component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an IBM watsonx.ai model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\nFor more information, see [Language model components](/components-models).\n\nThe `LanguageModel` output from the **IBM watsonx.ai** component is an instance of `[ChatWatsonx](https://docs.langchain.com/oss/python/integrations/chat/ibm_watsonx)` configured according to the [component's parameters](#ibm-watsonxai-parameters).\n\n## IBM watsonx.ai Embeddings\n\nThe **IBM watsonx.ai Embeddings** component uses the [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) for embedding generation.\n\nThe output is [`Embeddings`](/data-types#embeddings) generated with [`WatsonxEmbeddings`](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n![A basic embedding generation flow using the IBM watsonx.ai Embeddings component](/img/component-watsonx-embeddings-chroma.png)\n\n### IBM watsonx.ai Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| url | watsonx API Endpoint | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |\n| project_id | watsonx project id | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |\n| api_key | API Key | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |\n| model_name | Model Name | Input parameter. The name of the embedding model to use. Supports [default embedding models](#default-embedding-models) and automatically updates after connecting to your watsonx.ai deployment. |\n| truncate_input_tokens | Truncate Input Tokens | Input parameter. The maximum number of tokens to process. Default: `200`. |\n| input_text | Include the original text in the output | Input parameter. Determines if the original text is included in the output. Default: `true`. |\n\n### Default embedding models\n\nBy default, the **IBM watsonx.ai Embeddings** component supports the following default models:\n\n- `sentence-transformers/all-minilm-l12-v2`: 384-dimensional embeddings\n- `ibm/slate-125m-english-rtrvr-v2`: 768-dimensional embeddings\n- `ibm/slate-30m-english-rtrvr-v2`: 768-dimensional embeddings\n- `intfloat/multilingual-e5-large`: 1024-dimensional embeddings\n\nAfter entering your API endpoint and credentials, the component automatically fetches the list of available models from your watsonx.ai deployment.\n\n## See also\n\n* [IBM documentation](https://cloud.ibm.com/docs)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "default",
        "embedding",
        "embeddings",
        "function",
        "ibm",
        "language",
        "model",
        "models",
        "openai",
        "output",
        "parameters",
        "response",
        "smart",
        "watsonx",
        "watsonxai"
      ]
    },
    "bundles-ibm": {
      "title": "IBM",
      "slug": "/bundles-ibm",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe **IBM** bundle provides access to IBM watsonx.ai models for text and embedding generation.\nThese components require an IBM watsonx.ai deployment and watsonx API credentials.\n\n## IBM watsonx.ai\n\nThe **IBM watsonx.ai** component generates text using [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai).\nTo use gateway models, use the [**OpenAI** text generation component](/bundles-openai) with the gateway model's OpenAI-compatible endpoint.\n\nYou can use the **IBM watsonx.ai** component anywhere you need a language model in a flow.\n\n![A basic prompting flow using the IBM watsonx.ai component as the central Language Model component.](/img/component-watsonx-model.png)\n\n### IBM watsonx.ai parameters {#ibm-watsonxai-parameters}\n\n| Name | Type | Description |\n|------|------|-------------|\n| url | String | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |\n| project_id | String | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |\n| api_key | SecretString | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |\n| model_name | String | Input parameter. The name of the watsonx model to use. Options are dynamically fetched from the API. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Default: `1000`. |\n| stop_sequence | String | Input parameter. The sequence where generation should stop. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| top_p | Float | Input parameter. Controls nucleus sampling, which limits the model to tokens whose probability is below the `top_p` value. Range: Default: `0.9`. |\n| frequency_penalty | Float | Input parameter. Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: `0.5`. |\n| presence_penalty | Float | Input parameter. Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: `0.3`. |\n| seed | Integer | Input parameter. A random seed for the model. Default: `8`. |\n| logprobs | Boolean | Input parameter. Whether to return log probabilities of output tokens or not. Default: `true`. |\n| top_logprobs | Integer | Input parameter. The number of most likely tokens to return at each position. Default: `3`. |\n| logit_bias | String | Input parameter. A JSON string of token IDs to bias or suppress. |\n\n### IBM watsonx.ai output\n\nThe **IBM watsonx.ai** component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an IBM watsonx.ai model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\nFor more information, see [Language model components](/components-models).\n\nThe `LanguageModel` output from the **IBM watsonx.ai** component is an instance of `[ChatWatsonx](https://docs.langchain.com/oss/python/integrations/chat/ibm_watsonx)` configured according to the [component's parameters](#ibm-watsonxai-parameters).\n\n## IBM watsonx.ai Embeddings\n\nThe **IBM watsonx.ai Embeddings** component uses the [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) for embedding generation.\n\nThe output is [`Embeddings`](/data-types#embeddings) generated with [`WatsonxEmbeddings`](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n![A basic embedding generation flow using the IBM watsonx.ai Embeddings component](/img/component-watsonx-embeddings-chroma.png)\n\n### IBM watsonx.ai Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| url | watsonx API Endpoint | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |\n| project_id | watsonx project id | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |\n| api_key | API Key | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |\n| model_name | Model Name | Input parameter. The name of the embedding model to use. Supports [default embedding models](#default-embedding-models) and automatically updates after connecting to your watsonx.ai deployment. |\n| truncate_input_tokens | Truncate Input Tokens | Input parameter. The maximum number of tokens to process. Default: `200`. |\n| input_text | Include the original text in the output | Input parameter. Determines if the original text is included in the output. Default: `true`. |\n\n### Default embedding models\n\nBy default, the **IBM watsonx.ai Embeddings** component supports the following default models:\n\n- `sentence-transformers/all-minilm-l12-v2`: 384-dimensional embeddings\n- `ibm/slate-125m-english-rtrvr-v2`: 768-dimensional embeddings\n- `ibm/slate-30m-english-rtrvr-v2`: 768-dimensional embeddings\n- `intfloat/multilingual-e5-large`: 1024-dimensional embeddings\n\nAfter entering your API endpoint and credentials, the component automatically fetches the list of available models from your watsonx.ai deployment.\n\n## See also\n\n* [IBM documentation](https://cloud.ibm.com/docs)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "default",
        "embedding",
        "embeddings",
        "function",
        "ibm",
        "language",
        "model",
        "models",
        "openai",
        "output",
        "parameters",
        "response",
        "smart",
        "watsonx",
        "watsonxai"
      ]
    },
    "/bundles-icosacomputing": {
      "title": "Icosa Computing",
      "slug": "/bundles-icosacomputing",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe **Icosa Computing** components require access to Icosa Computing services.\nFor more information and to request access, see the [Icosa Computing site](https://www.icosacomputing.com/).\n\n## Combinatorial Reasoner\n\nThe **Combinatorial Reasoner** component runs Icosa's Combinatorial Reasoning (CR) pipeline on an input to create an optimized prompt with embedded reasons.\n\n### Combinatorial Reasoner parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| prompt | String | Input parameter. The input to run CR on. |\n| openai_api_key | SecretString | Input parameter. An OpenAI API key for authentication to OpenAI, which is used for reason generation. |\n| model_name | String | Input parameter. The OpenAI LLM to use for reason generation. |\n| username | String | Input parameter. A username for authentication to the Icosa API. |\n| password | SecretString | Input parameter. A password for authentication to the Icosa API. |\n\n### Combinatorial Reasoner outputs\n\nThe **Combinatorial Reasoner** component outputs one of the following:\n\n* **Optimized Prompt (`optimized_prompt`)**: A [`Message`](/data-types#message) object containing the optimized prompt with embedded reasons.\n* **Selected Reasons (`reasons`)**: A [`Data`](/data-types#data) object containing a list of strings where each string is a reason that was selected and embedded in the optimized prompt.\n\nYou can toggle the output type near the component's output port.\n\n## See also\n\n* [**Prompt Template** component](/components-prompts)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "combinatorial",
        "computing",
        "icosa",
        "optimized",
        "outputs",
        "parameters",
        "prompt",
        "reasoner",
        "reasons",
        "selected",
        "template"
      ]
    },
    "bundles-icosacomputing": {
      "title": "Icosa Computing",
      "slug": "/bundles-icosacomputing",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe **Icosa Computing** components require access to Icosa Computing services.\nFor more information and to request access, see the [Icosa Computing site](https://www.icosacomputing.com/).\n\n## Combinatorial Reasoner\n\nThe **Combinatorial Reasoner** component runs Icosa's Combinatorial Reasoning (CR) pipeline on an input to create an optimized prompt with embedded reasons.\n\n### Combinatorial Reasoner parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| prompt | String | Input parameter. The input to run CR on. |\n| openai_api_key | SecretString | Input parameter. An OpenAI API key for authentication to OpenAI, which is used for reason generation. |\n| model_name | String | Input parameter. The OpenAI LLM to use for reason generation. |\n| username | String | Input parameter. A username for authentication to the Icosa API. |\n| password | SecretString | Input parameter. A password for authentication to the Icosa API. |\n\n### Combinatorial Reasoner outputs\n\nThe **Combinatorial Reasoner** component outputs one of the following:\n\n* **Optimized Prompt (`optimized_prompt`)**: A [`Message`](/data-types#message) object containing the optimized prompt with embedded reasons.\n* **Selected Reasons (`reasons`)**: A [`Data`](/data-types#data) object containing a list of strings where each string is a reason that was selected and embedded in the optimized prompt.\n\nYou can toggle the output type near the component's output port.\n\n## See also\n\n* [**Prompt Template** component](/components-prompts)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "combinatorial",
        "computing",
        "icosa",
        "optimized",
        "outputs",
        "parameters",
        "prompt",
        "reasoner",
        "reasons",
        "selected",
        "template"
      ]
    },
    "/bundles-langchain": {
      "title": "LangChain",
      "slug": "/bundles-langchain",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **LangChain** bundle.\n\n## CSV Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates a CSV agent from a CSV file and LLM.\nFor more information, see the [LangChain CSV agent documentation](https://python.langchain.com/api_reference/experimental/agents/langchain_experimental.agents.agent_toolkits.csv.base.create_csv_agent.html).\n\n### CSV Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use for the agent. |\n| path | File | Input parameter. The path to the CSV file. |\n| agent_type | String | Input parameter. The type of agent to create. |\n| agent | AgentExecutor | Output parameter. The CSV agent instance. |\n\n## OpenAI Tools Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an OpenAI Tools Agent.\nFor more information, see the [LangChain OpenAI agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html).\n\n### OpenAI Tools Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| tools | List of Tools | Input parameter. The tools to give the agent access to. |\n| system_prompt | String | Input parameter. The system prompt to provide context to the agent. |\n| input_value | String | Input parameter. The user's input to the agent. |\n| memory | Memory | Input parameter. The memory for the agent to use for context persistence. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to allow the agent to execute. |\n| verbose | Boolean | Input parameter. This determines whether to print out the agent's intermediate steps. |\n| handle_parsing_errors | Boolean | Input parameter. This determines whether to handle parsing errors in the agent. |\n| agent | AgentExecutor | Output parameter. The OpenAI Tools agent instance. |\n| output | String | Output parameter. The output from executing the agent on the input. |\n\n## OpenAPI Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for interacting with OpenAPI services.\nFor more information, see the [LangChain OpenAPI toolkit documentation](https://docs.langchain.com/oss/python/integrations/tools/openapi).\n\n### OpenAPI Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| openapi_spec | String | Input parameter. The OpenAPI specification for the service. |\n| base_url | String | Input parameter. The base URL for the API. |\n| headers | Dict | Input parameter. The optional headers for API requests. |\n| agent_executor_kwargs | Dict | Input parameter. The optional parameters for the agent executor. |\n| agent | AgentExecutor | Output parameter.The OpenAPI agent instance. |\n\n## Prompt Hub\n\nThis component fetches prompts from the [LangChain Hub](https://docs.langchain.com/langsmith/manage-prompts#public-prompt-hub).\n\nLike the [**Prompt Template** core component](/components-prompts), additional fields are added to the component for each variable in the prompt.\nFor example, the default prompt `efriis/my-first-prompt` adds fields for `profession` and `question`.\n\n### Prompt Hub parameters\n\n| Name                | Display Name              | Description                              |\n|---------------------|---------------------------|------------------------------------------|\n| langchain_api_key   | Your LangChain API Key    | Input parameter. The LangChain API Key to use. |\n| langchain_hub_prompt| LangChain Hub Prompt      | Input parameter. The LangChain Hub prompt to use.  |\n| prompt              | Build Prompt              | Output parameter. The built prompt message returned by the `build_prompt` method.   |\n\n## SQL Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for interacting with SQL databases.\nFor more information, see the [LangChain SQL agent documentation](https://docs.langchain.com/oss/python/langchain/sql-agent).\n\n### SQL Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| database | Database | Input parameter. The SQL database connection. |\n| top_k | Integer | Input parameter. The number of results to return from a SELECT query. |\n| use_tools | Boolean | Input parameter. This determines whether to use tools for query execution. |\n| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |\n| early_stopping_method | String | Input parameter. The method to use for early stopping. |\n| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |\n| agent | AgentExecutor | Output parameter. The SQL agent instance. |\n\n## SQL Database\n\nThe LangChain **SQL Database** component establishes a connection to an SQL database.\n\nThis component is different from the [**SQL Database** core component](/components-data#sql-database), which executes SQL queries on SQLAlchemy-compatible databases.\n\n## Text Splitters\n\nThe **LangChain** bundle includes the following text splitter components:\n\n- **Character Text Splitter**\n- **Language Recursive Text Splitter**\n- **Natural Language Text Splitter**\n- **Recursive Character Text Splitter**\n- **Semantic Text Splitter**\n\n## Tool Calling Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for structured tool calling with various language models.\nFor more information, see the [LangChain tool calling documentation](https://docs.langchain.com/oss/python/langchain/agents#tools).\n\n### Tool Calling Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| tools | List[Tool] | Input parameter. The list of tools available to the agent. |\n| system_message | String | Input parameter. The system message to use for the agent. |\n| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |\n| early_stopping_method | String | Input parameter. The method to use for early stopping. |\n| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |\n| agent | AgentExecutor | Output parameter. The tool calling agent instance. |\n\n## XML Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an XML Agent using LangChain.\nThe agent uses XML formatting for tool instructions to the LLM.\nFor more information, see the [LangChain XML Agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.xml.base.create_xml_agent.html).\n\n### XML Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use for the agent. |\n| user_prompt | String | Input parameter. The custom prompt template for the agent with XML formatting instructions. |\n| tools | List[Tool] | Input parameter. The list of tools available to the agent. |\n| agent | AgentExecutor | Output parameter. The XML Agent instance. |\n\n## Other LangChain components\n\nOther components in the **LangChain** bundle include the following:\n\n- **Fake Embeddings**\n- **HTML Link Extractor**\n- **Runnable Executor**\n- **Spider Web Crawler & Scraper**\n\n## Legacy LangChain components\n\nThe following LangChain components are in legacy status:\n\n* **Conversation Chain**\n* **LLM Checker Chain**\n* **LLM Math Chain**\n* **Natural Language to SQL**\n* **Retrieval QA**\n* **Self Query Retriever**\n* **JSON Agent**\n* **Vector Store Info/Agent**\n* **VectorStoreRouterAgent**\n\nTo replace these components, consider other components in the **LangChain** bundle or general Langflow components, such as the [**Agent** component](/components-agents) or the [**SQL Database** component](/components-data#sql-database).",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "calling",
        "chain",
        "character",
        "checker",
        "components",
        "conversation",
        "crawler",
        "csv",
        "database",
        "embeddings",
        "executor",
        "extractor",
        "fake",
        "html",
        "hub",
        "info",
        "json",
        "langchain"
      ]
    },
    "bundles-langchain": {
      "title": "LangChain",
      "slug": "/bundles-langchain",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **LangChain** bundle.\n\n## CSV Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates a CSV agent from a CSV file and LLM.\nFor more information, see the [LangChain CSV agent documentation](https://python.langchain.com/api_reference/experimental/agents/langchain_experimental.agents.agent_toolkits.csv.base.create_csv_agent.html).\n\n### CSV Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use for the agent. |\n| path | File | Input parameter. The path to the CSV file. |\n| agent_type | String | Input parameter. The type of agent to create. |\n| agent | AgentExecutor | Output parameter. The CSV agent instance. |\n\n## OpenAI Tools Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an OpenAI Tools Agent.\nFor more information, see the [LangChain OpenAI agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html).\n\n### OpenAI Tools Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| tools | List of Tools | Input parameter. The tools to give the agent access to. |\n| system_prompt | String | Input parameter. The system prompt to provide context to the agent. |\n| input_value | String | Input parameter. The user's input to the agent. |\n| memory | Memory | Input parameter. The memory for the agent to use for context persistence. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to allow the agent to execute. |\n| verbose | Boolean | Input parameter. This determines whether to print out the agent's intermediate steps. |\n| handle_parsing_errors | Boolean | Input parameter. This determines whether to handle parsing errors in the agent. |\n| agent | AgentExecutor | Output parameter. The OpenAI Tools agent instance. |\n| output | String | Output parameter. The output from executing the agent on the input. |\n\n## OpenAPI Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for interacting with OpenAPI services.\nFor more information, see the [LangChain OpenAPI toolkit documentation](https://docs.langchain.com/oss/python/integrations/tools/openapi).\n\n### OpenAPI Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| openapi_spec | String | Input parameter. The OpenAPI specification for the service. |\n| base_url | String | Input parameter. The base URL for the API. |\n| headers | Dict | Input parameter. The optional headers for API requests. |\n| agent_executor_kwargs | Dict | Input parameter. The optional parameters for the agent executor. |\n| agent | AgentExecutor | Output parameter.The OpenAPI agent instance. |\n\n## Prompt Hub\n\nThis component fetches prompts from the [LangChain Hub](https://docs.langchain.com/langsmith/manage-prompts#public-prompt-hub).\n\nLike the [**Prompt Template** core component](/components-prompts), additional fields are added to the component for each variable in the prompt.\nFor example, the default prompt `efriis/my-first-prompt` adds fields for `profession` and `question`.\n\n### Prompt Hub parameters\n\n| Name                | Display Name              | Description                              |\n|---------------------|---------------------------|------------------------------------------|\n| langchain_api_key   | Your LangChain API Key    | Input parameter. The LangChain API Key to use. |\n| langchain_hub_prompt| LangChain Hub Prompt      | Input parameter. The LangChain Hub prompt to use.  |\n| prompt              | Build Prompt              | Output parameter. The built prompt message returned by the `build_prompt` method.   |\n\n## SQL Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for interacting with SQL databases.\nFor more information, see the [LangChain SQL agent documentation](https://docs.langchain.com/oss/python/langchain/sql-agent).\n\n### SQL Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| database | Database | Input parameter. The SQL database connection. |\n| top_k | Integer | Input parameter. The number of results to return from a SELECT query. |\n| use_tools | Boolean | Input parameter. This determines whether to use tools for query execution. |\n| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |\n| early_stopping_method | String | Input parameter. The method to use for early stopping. |\n| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |\n| agent | AgentExecutor | Output parameter. The SQL agent instance. |\n\n## SQL Database\n\nThe LangChain **SQL Database** component establishes a connection to an SQL database.\n\nThis component is different from the [**SQL Database** core component](/components-data#sql-database), which executes SQL queries on SQLAlchemy-compatible databases.\n\n## Text Splitters\n\nThe **LangChain** bundle includes the following text splitter components:\n\n- **Character Text Splitter**\n- **Language Recursive Text Splitter**\n- **Natural Language Text Splitter**\n- **Recursive Character Text Splitter**\n- **Semantic Text Splitter**\n\n## Tool Calling Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an agent for structured tool calling with various language models.\nFor more information, see the [LangChain tool calling documentation](https://docs.langchain.com/oss/python/langchain/agents#tools).\n\n### Tool Calling Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use. |\n| tools | List[Tool] | Input parameter. The list of tools available to the agent. |\n| system_message | String | Input parameter. The system message to use for the agent. |\n| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |\n| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |\n| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |\n| early_stopping_method | String | Input parameter. The method to use for early stopping. |\n| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |\n| agent | AgentExecutor | Output parameter. The tool calling agent instance. |\n\n## XML Agent\n\nThis component is based on the [**Agent** core component](/agents).\n\nThis component creates an XML Agent using LangChain.\nThe agent uses XML formatting for tool instructions to the LLM.\nFor more information, see the [LangChain XML Agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.xml.base.create_xml_agent.html).\n\n### XML Agent parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| llm | LanguageModel | Input parameter. The language model to use for the agent. |\n| user_prompt | String | Input parameter. The custom prompt template for the agent with XML formatting instructions. |\n| tools | List[Tool] | Input parameter. The list of tools available to the agent. |\n| agent | AgentExecutor | Output parameter. The XML Agent instance. |\n\n## Other LangChain components\n\nOther components in the **LangChain** bundle include the following:\n\n- **Fake Embeddings**\n- **HTML Link Extractor**\n- **Runnable Executor**\n- **Spider Web Crawler & Scraper**\n\n## Legacy LangChain components\n\nThe following LangChain components are in legacy status:\n\n* **Conversation Chain**\n* **LLM Checker Chain**\n* **LLM Math Chain**\n* **Natural Language to SQL**\n* **Retrieval QA**\n* **Self Query Retriever**\n* **JSON Agent**\n* **Vector Store Info/Agent**\n* **VectorStoreRouterAgent**\n\nTo replace these components, consider other components in the **LangChain** bundle or general Langflow components, such as the [**Agent** component](/components-agents) or the [**SQL Database** component](/components-data#sql-database).",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "calling",
        "chain",
        "character",
        "checker",
        "components",
        "conversation",
        "crawler",
        "csv",
        "database",
        "embeddings",
        "executor",
        "extractor",
        "fake",
        "html",
        "hub",
        "info",
        "json",
        "langchain"
      ]
    },
    "/bundles-lmstudio": {
      "title": "LM Studio",
      "slug": "/bundles-lmstudio",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe components in the **LM Studio** bundle let you use models from a local or hosted instance of LM Studio.\nComponents can require authentication with an LM Studio API key. For information about LM Studio models, connections, and credentials, see the [LM Studio documentation](https://lmstudio.ai/docs).\n\n## LM Studio text generation\n\nThe **LM Studio** component generates text using LM Studio's local language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an LM Studio model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### LM Studio text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| base_url | String | Input parameter. The URL where LM Studio is running. Default: `\"http://localhost:1234\"`. |\n| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio, if required. |\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate in the response. Default: `512`. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.7`. |\n| top_p | Float | Input parameter. Controls diversity with nucleus sampling. Range: `[0.0, 1.0]`. Default: `1.0`. |\n| stop | List[String] | Input parameter. List of strings that stop generation when encountered. |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |\n| presence_penalty | Float | Input parameter. Penalizes repeated tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n| frequency_penalty | Float | Input parameter. Penalizes frequent tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n\n## LM Studio Embeddings\n\nThe **LM Studio Embeddings** component generates embeddings using LM Studio models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### LM Studio Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| model | Model | Input parameter. The LM Studio model to use for generating embeddings. |\n| base_url | LM Studio Base URL | Input parameter. The base URL for the LM Studio API. |\n| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio. |\n| temperature | Model Temperature | Input parameter. The temperature setting for the model. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "studio",
        "text"
      ]
    },
    "bundles-lmstudio": {
      "title": "LM Studio",
      "slug": "/bundles-lmstudio",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThe components in the **LM Studio** bundle let you use models from a local or hosted instance of LM Studio.\nComponents can require authentication with an LM Studio API key. For information about LM Studio models, connections, and credentials, see the [LM Studio documentation](https://lmstudio.ai/docs).\n\n## LM Studio text generation\n\nThe **LM Studio** component generates text using LM Studio's local language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an LM Studio model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### LM Studio text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| base_url | String | Input parameter. The URL where LM Studio is running. Default: `\"http://localhost:1234\"`. |\n| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio, if required. |\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate in the response. Default: `512`. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.7`. |\n| top_p | Float | Input parameter. Controls diversity with nucleus sampling. Range: `[0.0, 1.0]`. Default: `1.0`. |\n| stop | List[String] | Input parameter. List of strings that stop generation when encountered. |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |\n| presence_penalty | Float | Input parameter. Penalizes repeated tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n| frequency_penalty | Float | Input parameter. Penalizes frequent tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |\n\n## LM Studio Embeddings\n\nThe **LM Studio Embeddings** component generates embeddings using LM Studio models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### LM Studio Embeddings parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| model | Model | Input parameter. The LM Studio model to use for generating embeddings. |\n| base_url | LM Studio Base URL | Input parameter. The base URL for the LM Studio API. |\n| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio. |\n| temperature | Model Temperature | Input parameter. The temperature setting for the model. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "studio",
        "text"
      ]
    },
    "/bundles-maritalk": {
      "title": "MariTalk",
      "slug": "/bundles-maritalk",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MariTalk** bundle.\n\nFor more information, see the [MariTalk documentation](https://www.maritalk.com/).\n\n## MariTalk text generation\n\nThe **MariTalk** component generates text using MariTalk LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a MariTalk model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### MariTalk text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. Default: `512`. |\n| model_name | String | Input parameter. The name of the MariTalk model to use. Options: `sabia-2-small`, `sabia-2-medium`. Default: `sabia-2-small`. |\n| api_key | SecretString | Input parameter. The MariTalk API Key to use for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.5`. |\n| endpoint_url | String | Input parameter. The MariTalk API endpoint. Default: `https://api.maritalk.com`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "maritalk",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-maritalk": {
      "title": "MariTalk",
      "slug": "/bundles-maritalk",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MariTalk** bundle.\n\nFor more information, see the [MariTalk documentation](https://www.maritalk.com/).\n\n## MariTalk text generation\n\nThe **MariTalk** component generates text using MariTalk LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a MariTalk model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### MariTalk text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. Default: `512`. |\n| model_name | String | Input parameter. The name of the MariTalk model to use. Options: `sabia-2-small`, `sabia-2-medium`. Default: `sabia-2-small`. |\n| api_key | SecretString | Input parameter. The MariTalk API Key to use for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.5`. |\n| endpoint_url | String | Input parameter. The MariTalk API endpoint. Default: `https://api.maritalk.com`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "maritalk",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-mem0": {
      "title": "Mem0",
      "slug": "/bundles-mem0",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Mem0** bundle.\n\n## Mem0 Chat Memory\n\nThe **Mem0 Chat Memory** component retrieves and stores chat messages using Mem0 memory storage.\n\n### Mem0 Chat Memory parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| mem0_config | Mem0 Configuration | Input parameter. The configuration dictionary for initializing the Mem0 memory instance. |\n| ingest_message | Message to Ingest | Input parameter. The message content to be ingested into Mem0 memory. |\n| existing_memory | Existing Memory Instance | Input parameter. An optional existing Mem0 memory instance. |\n| user_id | User ID | Input parameter. The identifier for the user associated with the messages. |\n| search_query | Search Query | Input parameter. The input text for searching related memories in Mem0. |\n| mem0_api_key | Mem0 API Key | Input parameter. The API key for the Mem0 platform. Leave empty to use the local version. |\n| metadata | Metadata | Input parameter. The additional metadata to associate with the ingested message. |\n| openai_api_key | OpenAI API Key | Input parameter. The API key for OpenAI. Required when using OpenAI embeddings without a provided configuration. |\n\n### Mem0 Chat Memory output\n\nThe **Mem0 Chat Memory** component can output either **Mem0 Memory** ([`Memory`](/data-types#memory)) or **Search Results** ([`Data`](/data-types#data)).\nYou can select the output type near the component's output port.\n\nUse **Mem0 Chat Memory** for memory storage and retrieval operations with the [**Message History** component](/components-helpers#message-history).\n\nUse the **Search Results** output to retrieve specific memories based on a search query.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "chat",
        "history",
        "memory",
        "message",
        "output",
        "parameters",
        "results",
        "search"
      ]
    },
    "bundles-mem0": {
      "title": "Mem0",
      "slug": "/bundles-mem0",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Mem0** bundle.\n\n## Mem0 Chat Memory\n\nThe **Mem0 Chat Memory** component retrieves and stores chat messages using Mem0 memory storage.\n\n### Mem0 Chat Memory parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| mem0_config | Mem0 Configuration | Input parameter. The configuration dictionary for initializing the Mem0 memory instance. |\n| ingest_message | Message to Ingest | Input parameter. The message content to be ingested into Mem0 memory. |\n| existing_memory | Existing Memory Instance | Input parameter. An optional existing Mem0 memory instance. |\n| user_id | User ID | Input parameter. The identifier for the user associated with the messages. |\n| search_query | Search Query | Input parameter. The input text for searching related memories in Mem0. |\n| mem0_api_key | Mem0 API Key | Input parameter. The API key for the Mem0 platform. Leave empty to use the local version. |\n| metadata | Metadata | Input parameter. The additional metadata to associate with the ingested message. |\n| openai_api_key | OpenAI API Key | Input parameter. The API key for OpenAI. Required when using OpenAI embeddings without a provided configuration. |\n\n### Mem0 Chat Memory output\n\nThe **Mem0 Chat Memory** component can output either **Mem0 Memory** ([`Memory`](/data-types#memory)) or **Search Results** ([`Data`](/data-types#data)).\nYou can select the output type near the component's output port.\n\nUse **Mem0 Chat Memory** for memory storage and retrieval operations with the [**Message History** component](/components-helpers#message-history).\n\nUse the **Search Results** output to retrieve specific memories based on a search query.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "chat",
        "history",
        "memory",
        "message",
        "output",
        "parameters",
        "results",
        "search"
      ]
    },
    "/bundles-milvus": {
      "title": "Milvus",
      "slug": "/bundles-milvus",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Milvus** bundle.\n\n## Milvus vector store\n\nThe **Milvus** component reads and writes to Milvus vector stores using an instance of `Milvus` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Milvus vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Milvus documentation](https://milvus.io/docs) or inspect [component code](/concepts-components#component-code).\n\n| Name                    | Type          | Description                                      |\n|-------------------------|---------------|--------------------------------------------------|\n| collection_name          | String        | Input parameter. Name of the Milvus collection.                   |\n| collection_description   | String        | Input parameter. Description of the Milvus collection.            |\n| uri                      | String        | Input parameter. Connection URI for Milvus.                       |\n| password                 | SecretString  | Input parameter. Password for Milvus.                             |\n| username                 | SecretString  | Input parameter. Username for Milvus.                             |\n| batch_size               | Integer       | Input parameter. Number of data to process in a single batch.     |\n| search_query             | String        | Input parameter. Query for similarity search.                     |\n| ingest_data              | Data          | Input parameter. Data to be ingested into the vector store.       |\n| embedding                | Embeddings    | Input parameter. Embedding function to use.                       |\n| number_of_results        | Integer       | Input parameter. Number of results to return in search.           |\n| search_type              | String        | Input parameter. Type of search to perform.                       |\n| search_score_threshold   | Float         | Input parameter. Minimum similarity score for search results.     |\n| search_filter            | Dict          | Input parameter. Metadata filters for search query.               |\n| setup_mode               | String        | Input parameter. Configuration mode for setting up the vector store. |\n| vector_dimensions        | Integer       | Input parameter. Number of dimensions of the vectors.             |\n| pre_delete_collection    | Boolean       | Input parameter. Whether to delete the collection before creating a new one. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "milvus",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "bundles-milvus": {
      "title": "Milvus",
      "slug": "/bundles-milvus",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Milvus** bundle.\n\n## Milvus vector store\n\nThe **Milvus** component reads and writes to Milvus vector stores using an instance of `Milvus` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Milvus vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Milvus documentation](https://milvus.io/docs) or inspect [component code](/concepts-components#component-code).\n\n| Name                    | Type          | Description                                      |\n|-------------------------|---------------|--------------------------------------------------|\n| collection_name          | String        | Input parameter. Name of the Milvus collection.                   |\n| collection_description   | String        | Input parameter. Description of the Milvus collection.            |\n| uri                      | String        | Input parameter. Connection URI for Milvus.                       |\n| password                 | SecretString  | Input parameter. Password for Milvus.                             |\n| username                 | SecretString  | Input parameter. Username for Milvus.                             |\n| batch_size               | Integer       | Input parameter. Number of data to process in a single batch.     |\n| search_query             | String        | Input parameter. Query for similarity search.                     |\n| ingest_data              | Data          | Input parameter. Data to be ingested into the vector store.       |\n| embedding                | Embeddings    | Input parameter. Embedding function to use.                       |\n| number_of_results        | Integer       | Input parameter. Number of results to return in search.           |\n| search_type              | String        | Input parameter. Type of search to perform.                       |\n| search_score_threshold   | Float         | Input parameter. Minimum similarity score for search results.     |\n| search_filter            | Dict          | Input parameter. Metadata filters for search query.               |\n| setup_mode               | String        | Input parameter. Configuration mode for setting up the vector store. |\n| vector_dimensions        | Integer       | Input parameter. Number of dimensions of the vectors.             |\n| pre_delete_collection    | Boolean       | Input parameter. Whether to delete the collection before creating a new one. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "milvus",
        "note",
        "parameters",
        "store",
        "vector"
      ]
    },
    "/bundles-mistralai": {
      "title": "MistralAI",
      "slug": "/bundles-mistralai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MistralAI** bundle.\n\nFor more information about MistralAI features and functionality used by MistralAI components, see the [MistralAI documentation](https://docs.mistral.ai).\n\n## MistralAI text generation\n\nThe **MistralAI** component generates text using MistralAI LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a MistralAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### MistralAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| model_name | String | Input parameter. The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |\n| mistral_api_base | String | Input parameter. The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1`. |\n| api_key | SecretString | Input parameter. The Mistral API Key to use for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.5. |\n| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 5. |\n| timeout | Integer | Input parameter. Timeout for API calls in seconds. Default: 60. |\n| max_concurrent_requests | Integer | Input parameter. Maximum number of concurrent API requests. Default: 3. |\n| top_p | Float | Input parameter. Nucleus sampling parameter. Default: 1. |\n| random_seed | Integer | Input parameter. Seed for random number generation. Default: 1. |\n| safe_mode | Boolean | Input parameter. Enables safe mode for content generation. |\n\n## MistralAI Embeddings\n\nThe **MistralAI Embeddings** component generates embeddings using MistralAI models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### MistralAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | String | Input parameter. The MistralAI model to use. Default: \"mistral-embed\". |\n| mistral_api_key | SecretString | Input parameter. The API key for authenticating with MistralAI. |\n| max_concurrent_requests | Integer | Input parameter. The maximum number of concurrent API requests. Default: 64. |\n| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: 5. |\n| timeout | Integer | Input parameter. The request timeout in seconds. Default: 120. |\n| endpoint | String | Input parameter. The custom API endpoint URL. Default: `https://api.mistral.ai/v1/`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "mistralai",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-mistralai": {
      "title": "MistralAI",
      "slug": "/bundles-mistralai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MistralAI** bundle.\n\nFor more information about MistralAI features and functionality used by MistralAI components, see the [MistralAI documentation](https://docs.mistral.ai).\n\n## MistralAI text generation\n\nThe **MistralAI** component generates text using MistralAI LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a MistralAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### MistralAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| model_name | String | Input parameter. The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |\n| mistral_api_base | String | Input parameter. The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1`. |\n| api_key | SecretString | Input parameter. The Mistral API Key to use for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.5. |\n| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 5. |\n| timeout | Integer | Input parameter. Timeout for API calls in seconds. Default: 60. |\n| max_concurrent_requests | Integer | Input parameter. Maximum number of concurrent API requests. Default: 3. |\n| top_p | Float | Input parameter. Nucleus sampling parameter. Default: 1. |\n| random_seed | Integer | Input parameter. Seed for random number generation. Default: 1. |\n| safe_mode | Boolean | Input parameter. Enables safe mode for content generation. |\n\n## MistralAI Embeddings\n\nThe **MistralAI Embeddings** component generates embeddings using MistralAI models.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### MistralAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | String | Input parameter. The MistralAI model to use. Default: \"mistral-embed\". |\n| mistral_api_key | SecretString | Input parameter. The API key for authenticating with MistralAI. |\n| max_concurrent_requests | Integer | Input parameter. The maximum number of concurrent API requests. Default: 64. |\n| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: 5. |\n| timeout | Integer | Input parameter. The request timeout in seconds. Default: 120. |\n| endpoint | String | Input parameter. The custom API endpoint URL. Default: `https://api.mistral.ai/v1/`. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embeddings",
        "function",
        "generation",
        "language",
        "mistralai",
        "model",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-mongodb": {
      "title": "MongoDB",
      "slug": "/bundles-mongodb",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MongoDB** bundle.\n\n## MongoDB Atlas\n\nThe **MongoDB Atlas** component reads and writes to MongoDB Atlas vector stores using an instance of [`MongoDBAtlasVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/mongodb_atlas).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### MongoDB Atlas parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/) or inspect [component code](/concepts-components#component-code).\n\n| Name                      | Type         | Description                               |\n| ------------------------- | ------------ | ----------------------------------------- |\n| mongodb_atlas_cluster_uri | SecretString | Input parameter. The connection URI for your MongoDB Atlas cluster. Required. |\n| enable_mtls               | Boolean      | Input parameter. Enable mutual TLS authentication. Default: `false`. |\n| mongodb_atlas_client_cert | SecretString | Input parameter. Client certificate combined with private key for mTLS authentication. Required if mTLS is enabled. |\n| db_name                   | String       | Input parameter. The name of the database to use. Required. |\n| collection_name           | String       | Input parameter. The name of the collection to use. Required. |\n| index_name                | String       | Input parameter. The name of the Atlas Search index, it should be a Vector Search. Required. |\n| insert_mode               | String       | Input parameter. How to insert new documents into the collection. The options are \"append\" or \"overwrite\". Default: \"append\". |\n| embedding                 | Embeddings   | Input parameter. The embedding model to use. |\n| number_of_results         | Integer      | Input parameter. Number of results to return in similarity search. Default: 4. |\n| index_field               | String       | Input parameter. The field to index. Default: \"embedding\". |\n| filter_field              | String       | Input parameter. The field to filter the index. |\n| number_dimensions         | Integer      | Input parameter. Embedding vector dimension count. Default: 1536. |\n| similarity                | String       | Input parameter. The method used to measure similarity between vectors. The options are \"cosine\", \"euclidean\", or \"dotProduct\". Default: \"cosine\". |\n| quantization              | String       | Input parameter. Quantization reduces memory costs by converting 32-bit floats to smaller data types. The options are \"scalar\" or \"binary\". |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "atlas",
        "bundles",
        "mongodb",
        "parameters"
      ]
    },
    "bundles-mongodb": {
      "title": "MongoDB",
      "slug": "/bundles-mongodb",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **MongoDB** bundle.\n\n## MongoDB Atlas\n\nThe **MongoDB Atlas** component reads and writes to MongoDB Atlas vector stores using an instance of [`MongoDBAtlasVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/mongodb_atlas).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### MongoDB Atlas parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/) or inspect [component code](/concepts-components#component-code).\n\n| Name                      | Type         | Description                               |\n| ------------------------- | ------------ | ----------------------------------------- |\n| mongodb_atlas_cluster_uri | SecretString | Input parameter. The connection URI for your MongoDB Atlas cluster. Required. |\n| enable_mtls               | Boolean      | Input parameter. Enable mutual TLS authentication. Default: `false`. |\n| mongodb_atlas_client_cert | SecretString | Input parameter. Client certificate combined with private key for mTLS authentication. Required if mTLS is enabled. |\n| db_name                   | String       | Input parameter. The name of the database to use. Required. |\n| collection_name           | String       | Input parameter. The name of the collection to use. Required. |\n| index_name                | String       | Input parameter. The name of the Atlas Search index, it should be a Vector Search. Required. |\n| insert_mode               | String       | Input parameter. How to insert new documents into the collection. The options are \"append\" or \"overwrite\". Default: \"append\". |\n| embedding                 | Embeddings   | Input parameter. The embedding model to use. |\n| number_of_results         | Integer      | Input parameter. Number of results to return in similarity search. Default: 4. |\n| index_field               | String       | Input parameter. The field to index. Default: \"embedding\". |\n| filter_field              | String       | Input parameter. The field to filter the index. |\n| number_dimensions         | Integer      | Input parameter. Embedding vector dimension count. Default: 1536. |\n| similarity                | String       | Input parameter. The method used to measure similarity between vectors. The options are \"cosine\", \"euclidean\", or \"dotProduct\". Default: \"cosine\". |\n| quantization              | String       | Input parameter. Quantization reduces memory costs by converting 32-bit floats to smaller data types. The options are \"scalar\" or \"binary\". |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "atlas",
        "bundles",
        "mongodb",
        "parameters"
      ]
    },
    "/bundles-notion": {
      "title": "Notion",
      "slug": "/bundles-notion",
      "category": "components",
      "content": "To use Notion components in Langflow, you first need to create a Notion integration and configure it with the necessary capabilities. This guide will walk you through the process of setting up a Notion integration and granting it access to your Notion databases, and then provide two examples of flows with Notion components.\n\n## Set up a Notion app\n\nTo create a Notion app, you need the following:\n\n- A Notion account with access to the workspace where you want to use the integration.\n- Admin permissions in the Notion workspace to create and manage integrations.\n\n### Create a Notion Integration\n\n1. Go to the [Notion Integrations](https://www.notion.com/my-integrations) page.\n2. Click **New Integration**.\n3. Enter an integration name, and then select the workspace where you want to use it.\n4. Optional: Enable the [necessary Notion integration capabilities](https://developers.notion.com/reference/capabilities) based on your requirements.\n5. Click **Submit** to create the integration.\n\n### Configure Integration Capabilities\n\nAfter creating the integration, you need to configure its capabilities to define what actions it can perform and what data it can access.\n\n1. In the integration settings page, go to the **Capabilities** tab.\n2. Enable the required capabilities for your integration. For example:\n   - If your integration needs to read data from Notion, enable the \"Read content\" capability.\n   - If your integration needs to create new content in Notion, enable the \"Insert content\" capability.\n   - If your integration needs to update existing content in Notion, enable the \"Update content\" capability.\n3. Configure the user information access level based on your integration's requirements.\n4. Save the changes.\n\n### Obtain Integration Token\n\n**Note:** Your integration token is a sensitive piece of information. Make sure to keep it secure and never share it publicly. Store it safely in your Langflow configuration or environment variables.\n\nTo authenticate your integration with Notion, you need to obtain an integration token.\n\n1. In the integration settings page, go to the \"Secrets\" tab.\n2. Copy the \"Internal Integration Token\" value. This token will be used to authenticate your integration with Notion.\n\n### Grant Integration Access to Notion Databases\n\nFor your integration to interact with Notion databases, you need to grant it access to the specific databases it must work with:\n\n1. Open the Notion database that you want your integration to access.\n2. Click **Share**.\n3. In the **Invite** section, select your integration from the list.\n4. Click **Invite** to grant the integration access to the database.\n\n   If your database contains references to other databases, you need to grant the integration access to those referenced databases as well.\n   Repeat this step for each referenced database that your integration must access.\n\n## Build with Notion components in Langflow\n\nOnce you have set up your Notion integration and granted it access to the required databases, you can start using the Notion components in Langflow:\n\n- **Add Content to Page**: Converts markdown text to Notion blocks and appends them to a specified Notion page.\n- **Create Page**: Creates a new page in a specified Notion database with the provided properties.\n- **List Database Properties**: Retrieves the properties of a specified Notion database.\n- **List Pages**: Queries a Notion database with filtering and sorting options.\n- **List Users**: Retrieves a list of users from the Notion workspace.\n- **Page Content Viewer**: Retrieves the content of a Notion page as plain text.\n- **Search**: Searches all pages and databases that have been shared with the integration. You can filter results to either pages or databases and specify the sort direction.\n- **Update Page Property**: Updates the properties of an existing Notion page.\n\nEach of these components can output `Data` and `Tool` [data types](/data-types).\n\n## Notion Meeting Notes Agent\n\nThe Notion Agent for Meeting Notes is an AI-powered tool that automatically processes meeting transcripts and updates your Notion workspace. It identifies tasks, action items, and key points from your meetings, then creates new tasks or updates existing ones in Notion without manual input.\n\nTo build this flow, you need the following:\n\n- [A Notion App](#set-up-a-notion-app)\n- [A Notion API key](https://www.notion.so/my-integrations)\n- [An OpenAI API key](https://platform.openai.com/account/api-keys)\n- Recommended: [Download the Meeting Agent Flow JSON](/files/Meeting_Notes_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.\n\n**Note:** Treat all keys and other credentials as sensitive information. Use secure references, and don't share them publicly.\n\n### Components used in this flow\n\n![Notion Meeting Agent Part 1](/img/notion_meeting_agent_part_1.png)\n\n* Meeting Transcript (text input): This component allows users to input the meeting transcript directly into the flow.\n* List Users (Notion component): Retrieves a list of users from the Notion workspace. Requires a Notion Secret (API key).\n* List Databases (Notion component): Searches and lists all databases in the Notion workspace. Requires the following inputs:\n    - Notion Secret (API key)\n    - Query (optional)\n    - Filter Type (default: database)\n    - Sort Direction\n* Prompt Template: This component creates a dynamic prompt template using the following inputs:\n    - Meeting Transcript\n    - List of Users\n    - List of Databases\n    - Current Date\n* Meeting Summarizer (Tool Calling Agent): Analyzes the meeting transcript and identifies tasks and action items. Requires the following inputs:\n    - System Prompt (from the **Prompt Template** component)\n    - Language Model (OpenAI)\n    - Tools:\n        - Notion Search\n        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.\n        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.\n        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.\n        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.\n\n![Notion Meeting Agent Part 2](/img/notion_meeting_agent_part_2.png)\n\n* Notion Agent (Tool Calling Agent): Executes actions in Notion based on the meeting summary/ Requires the following inputs:\n    - System Prompt (from the second **Prompt Template** component)\n    - Language Model (OpenAI)\n    - Tools:\n        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.\n        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.\n        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.\n        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.\n* Chat Output: Displays the final output of the Notion Agent in the Playground.\n\n### Flow Process\n\n1. The user inputs a meeting transcript.\n2. The flow retrieves the list of Notion users and databases.\n3. A prompt is generated using the transcript, user list, database list, and current date.\n4. The Meeting Summarizer analyzes the transcript and identifies tasks and action items.\n5. The Notion Agent uses the meeting summary to:\n   - Create new pages for new tasks\n   - Update existing pages for existing tasks\n   - Add content to pages with meeting notes\n6. The Chat Output displays a summary of actions taken in Notion.\n\n### Run the Notion Meeting Notes flow\n\n1. Create a flow manually or import a pre-built flow JSON file:\n\n    * Recommended: [Download the Meeting Agent flow JSON](/files/Meeting_Notes_Agent.json) and then [import the flow](/concepts-flows-import) into Langflow.\n    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.\n\n2. Input the Notion and OpenAI API keys in their respective components.\n3. Paste your meeting transcript into the **Meeting Transcript** component.\n\n    For optimal results, use detailed meeting transcripts. The quality of the output depends on the comprehensiveness of the input provided.\n\n4. Run the flow by clicking  **Run component** on the **Chat Output** component or open the **Playground**.\n5. Review the output summarizing the actions taken in your Notion workspace.\n6. Optional: The flow can be customized to meet your team's specific needs.\nCustomize this flow by:\n    * Adjusting the system prompt to change the agent's behavior or context.\n    * Adding or removing Notion tools based on your specific needs.\n    * Modifying the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.\n\n## Notion Conversational Agent\n\nThe Notion Conversational Agent is an AI-powered assistant that interacts with your Notion workspace through natural language conversations. This flow performs Notion-related tasks like creating pages, searching for information, and managing content, all through a chat interface.\n\n![Notion Components Toolkit](/img/notion_conversational_agent_tools.png)\n\nTo build this flow, you need the following:\n\n- [A Notion App](#set-up-a-notion-app)\n- [A Notion account and API key](https://www.notion.so/my-integrations)\n- [An OpenAI API key](https://platform.openai.com/account/api-keys)\n- Recommended: [Download the Conversation Agent Flow JSON](/files/Conversational_Notion_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.\n\n### Components used in this flow\n\n- **Chat Input**: Accepts user queries and commands\n- **Chat Output**: Displays the agent's responses\n- **Language Model**: Processes user input and generates responses with an OpenAI model\n- **Tool Calling Agent**: Coordinates the use of various Notion tools based on user input\n- **Toolkit**: Combines multiple Notion-specific tools into a single toolkit\n- **Notion Tools**: Various components for interacting with Notion, including:\n  - List Users\n  - List Database Properties\n  - List Pages\n  - Page Content Viewer\n  - Create Page\n  - Update Page Property\n  - Add Content to Page\n  - Search\n- **Message History**: Stores conversation history\n- **Prompt Template**: Provides system instructions and context for the agent\n- **Current Date**: Supplies the current date and time for context\n\n### Run the Conversational Notion Agent\n\n1. Open Langflow and create a new flow.\n2. Add the components listed above to your flow canvas, or Download the [Conversation Agent Flow](/files/Conversational_Notion_Agent.json)(Download link) and **Import** the JSON file into Langflow.\n3. Connect the components as shown in the flow diagram.\n4. Input the Notion and OpenAI API keys in their respective components.\n5. To start the flow, click the \"Play\" button in Langflow.\n6. In the chat interface, enter a Notion-related request or question. For example:\n   - \"Create a new page in my task database\"\n   - \"Search for pages about project management\"\n   - \"List all users in my Notion workspace\"\n7. The agent will process your request, use the appropriate Notion tools, and provide a response.\n8. Continue the conversation, asking follow-up questions or giving new commands as needed.\n\n   <details>\n   <summary>Example Conversation</summary>\n\n   ```\n   User: List all the users in my Notion workspace.\n\n   Agent: Certainly! I'll use the Notion User List tool to retrieve that information for you. One moment please.\n\n   [Agent uses the Notion User List tool]\n\n   Here's a list of users in your Notion workspace:\n\n   1. John Doe (john@example.com)\n   2. Jane Smith (jane@example.com)\n   3. Alex Johnson (alex@example.com)\n\n   Is there anything specific you'd like to know about these users or any other Notion-related task I can help you with?\n\n   User: Create a new page in my Projects database with the title \"Website Redesign\".\n\n   Agent: Certainly! I'd be happy to create a new page in your Projects database with the title \"Website Redesign\". I'll use the Notion Page Creator tool to do this. First, let me check the properties of your Projects database to ensure we include all necessary information.\n\n   [Agent uses the Notion Database Properties tool]\n\n   I've retrieved the properties for your Projects database. Now, I'll create the new page with the title \"Website Redesign\". One moment please.\n\n   [Agent uses the Notion Page Creator tool]\n\n   Great news! I've successfully created a new page in your Projects database with the title \"Website Redesign\". Here are the details:\n\n   - Title: Website Redesign\n   - Page ID: [page_id_here]\n   - URL: [page_url_here]\n\n   Is there any additional information you'd like me to add to this new project page, such as a description, due date, or assigned team members?\n\n   User: Yes, please add a description: \"Redesign company website to improve user experience and modernize the look.\"\n\n   Agent: Certainly! I'd be happy to add that description to the \"Website Redesign\" project page. I'll use the Add Content to Page tool to update the page with this information.\n\n   [Agent uses the Add Content to Page tool]\n\n   I've successfully added the description to your \"Website Redesign\" project page. Here's a summary of what I've done:\n\n   1. Located the \"Website Redesign\" page using the page ID from our previous interaction.\n   2. Added the following content to the page:\n\n      Description: Redesign company website to improve user experience and modernize the look.\n\n   The description has been added as a new text block on the page. Is there anything else you'd like me to add or modify on this project page?\n   ```\n\n   </details>\n\n9. Optional: The flow can be customized to meet your team's specific needs.\nFor example:\n   * Adjust the system prompt to change the agent's behavior or context.\n   * Add or remove Notion tools based on your specific needs.\n   * Modify the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Ensure all API keys are correctly set and have the necessary permissions.\n2. Check that your Notion integration has access to the relevant pages and databases.\n3. Verify that all components are properly connected in the flow.\n4. Review the Langflow logs for any error messages.\n\nFor more advanced usage and integration options, refer to the [Notion API documentation](https://developers.notion.com/) and [Langflow documentation](/).",
      "summary": "To use Notion components in Langflow, you first need to create a Notion integration and configure it with the necessary capabilities. This guide will walk you through the process of setting up a Notion integration and granting it access to your Notion databases, and then provide two examples of...",
      "keywords": [
        "access",
        "add",
        "agent",
        "app",
        "build",
        "calling",
        "capabilities",
        "chat",
        "component",
        "components",
        "configure",
        "content",
        "conversational",
        "create",
        "current",
        "database",
        "databases",
        "date",
        "flow",
        "grant"
      ]
    },
    "bundles-notion": {
      "title": "Notion",
      "slug": "/bundles-notion",
      "category": "components",
      "content": "To use Notion components in Langflow, you first need to create a Notion integration and configure it with the necessary capabilities. This guide will walk you through the process of setting up a Notion integration and granting it access to your Notion databases, and then provide two examples of flows with Notion components.\n\n## Set up a Notion app\n\nTo create a Notion app, you need the following:\n\n- A Notion account with access to the workspace where you want to use the integration.\n- Admin permissions in the Notion workspace to create and manage integrations.\n\n### Create a Notion Integration\n\n1. Go to the [Notion Integrations](https://www.notion.com/my-integrations) page.\n2. Click **New Integration**.\n3. Enter an integration name, and then select the workspace where you want to use it.\n4. Optional: Enable the [necessary Notion integration capabilities](https://developers.notion.com/reference/capabilities) based on your requirements.\n5. Click **Submit** to create the integration.\n\n### Configure Integration Capabilities\n\nAfter creating the integration, you need to configure its capabilities to define what actions it can perform and what data it can access.\n\n1. In the integration settings page, go to the **Capabilities** tab.\n2. Enable the required capabilities for your integration. For example:\n   - If your integration needs to read data from Notion, enable the \"Read content\" capability.\n   - If your integration needs to create new content in Notion, enable the \"Insert content\" capability.\n   - If your integration needs to update existing content in Notion, enable the \"Update content\" capability.\n3. Configure the user information access level based on your integration's requirements.\n4. Save the changes.\n\n### Obtain Integration Token\n\n**Note:** Your integration token is a sensitive piece of information. Make sure to keep it secure and never share it publicly. Store it safely in your Langflow configuration or environment variables.\n\nTo authenticate your integration with Notion, you need to obtain an integration token.\n\n1. In the integration settings page, go to the \"Secrets\" tab.\n2. Copy the \"Internal Integration Token\" value. This token will be used to authenticate your integration with Notion.\n\n### Grant Integration Access to Notion Databases\n\nFor your integration to interact with Notion databases, you need to grant it access to the specific databases it must work with:\n\n1. Open the Notion database that you want your integration to access.\n2. Click **Share**.\n3. In the **Invite** section, select your integration from the list.\n4. Click **Invite** to grant the integration access to the database.\n\n   If your database contains references to other databases, you need to grant the integration access to those referenced databases as well.\n   Repeat this step for each referenced database that your integration must access.\n\n## Build with Notion components in Langflow\n\nOnce you have set up your Notion integration and granted it access to the required databases, you can start using the Notion components in Langflow:\n\n- **Add Content to Page**: Converts markdown text to Notion blocks and appends them to a specified Notion page.\n- **Create Page**: Creates a new page in a specified Notion database with the provided properties.\n- **List Database Properties**: Retrieves the properties of a specified Notion database.\n- **List Pages**: Queries a Notion database with filtering and sorting options.\n- **List Users**: Retrieves a list of users from the Notion workspace.\n- **Page Content Viewer**: Retrieves the content of a Notion page as plain text.\n- **Search**: Searches all pages and databases that have been shared with the integration. You can filter results to either pages or databases and specify the sort direction.\n- **Update Page Property**: Updates the properties of an existing Notion page.\n\nEach of these components can output `Data` and `Tool` [data types](/data-types).\n\n## Notion Meeting Notes Agent\n\nThe Notion Agent for Meeting Notes is an AI-powered tool that automatically processes meeting transcripts and updates your Notion workspace. It identifies tasks, action items, and key points from your meetings, then creates new tasks or updates existing ones in Notion without manual input.\n\nTo build this flow, you need the following:\n\n- [A Notion App](#set-up-a-notion-app)\n- [A Notion API key](https://www.notion.so/my-integrations)\n- [An OpenAI API key](https://platform.openai.com/account/api-keys)\n- Recommended: [Download the Meeting Agent Flow JSON](/files/Meeting_Notes_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.\n\n**Note:** Treat all keys and other credentials as sensitive information. Use secure references, and don't share them publicly.\n\n### Components used in this flow\n\n![Notion Meeting Agent Part 1](/img/notion_meeting_agent_part_1.png)\n\n* Meeting Transcript (text input): This component allows users to input the meeting transcript directly into the flow.\n* List Users (Notion component): Retrieves a list of users from the Notion workspace. Requires a Notion Secret (API key).\n* List Databases (Notion component): Searches and lists all databases in the Notion workspace. Requires the following inputs:\n    - Notion Secret (API key)\n    - Query (optional)\n    - Filter Type (default: database)\n    - Sort Direction\n* Prompt Template: This component creates a dynamic prompt template using the following inputs:\n    - Meeting Transcript\n    - List of Users\n    - List of Databases\n    - Current Date\n* Meeting Summarizer (Tool Calling Agent): Analyzes the meeting transcript and identifies tasks and action items. Requires the following inputs:\n    - System Prompt (from the **Prompt Template** component)\n    - Language Model (OpenAI)\n    - Tools:\n        - Notion Search\n        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.\n        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.\n        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.\n        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.\n\n![Notion Meeting Agent Part 2](/img/notion_meeting_agent_part_2.png)\n\n* Notion Agent (Tool Calling Agent): Executes actions in Notion based on the meeting summary/ Requires the following inputs:\n    - System Prompt (from the second **Prompt Template** component)\n    - Language Model (OpenAI)\n    - Tools:\n        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.\n        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.\n        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.\n        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.\n* Chat Output: Displays the final output of the Notion Agent in the Playground.\n\n### Flow Process\n\n1. The user inputs a meeting transcript.\n2. The flow retrieves the list of Notion users and databases.\n3. A prompt is generated using the transcript, user list, database list, and current date.\n4. The Meeting Summarizer analyzes the transcript and identifies tasks and action items.\n5. The Notion Agent uses the meeting summary to:\n   - Create new pages for new tasks\n   - Update existing pages for existing tasks\n   - Add content to pages with meeting notes\n6. The Chat Output displays a summary of actions taken in Notion.\n\n### Run the Notion Meeting Notes flow\n\n1. Create a flow manually or import a pre-built flow JSON file:\n\n    * Recommended: [Download the Meeting Agent flow JSON](/files/Meeting_Notes_Agent.json) and then [import the flow](/concepts-flows-import) into Langflow.\n    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.\n\n2. Input the Notion and OpenAI API keys in their respective components.\n3. Paste your meeting transcript into the **Meeting Transcript** component.\n\n    For optimal results, use detailed meeting transcripts. The quality of the output depends on the comprehensiveness of the input provided.\n\n4. Run the flow by clicking  **Run component** on the **Chat Output** component or open the **Playground**.\n5. Review the output summarizing the actions taken in your Notion workspace.\n6. Optional: The flow can be customized to meet your team's specific needs.\nCustomize this flow by:\n    * Adjusting the system prompt to change the agent's behavior or context.\n    * Adding or removing Notion tools based on your specific needs.\n    * Modifying the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.\n\n## Notion Conversational Agent\n\nThe Notion Conversational Agent is an AI-powered assistant that interacts with your Notion workspace through natural language conversations. This flow performs Notion-related tasks like creating pages, searching for information, and managing content, all through a chat interface.\n\n![Notion Components Toolkit](/img/notion_conversational_agent_tools.png)\n\nTo build this flow, you need the following:\n\n- [A Notion App](#set-up-a-notion-app)\n- [A Notion account and API key](https://www.notion.so/my-integrations)\n- [An OpenAI API key](https://platform.openai.com/account/api-keys)\n- Recommended: [Download the Conversation Agent Flow JSON](/files/Conversational_Notion_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.\n\n### Components used in this flow\n\n- **Chat Input**: Accepts user queries and commands\n- **Chat Output**: Displays the agent's responses\n- **Language Model**: Processes user input and generates responses with an OpenAI model\n- **Tool Calling Agent**: Coordinates the use of various Notion tools based on user input\n- **Toolkit**: Combines multiple Notion-specific tools into a single toolkit\n- **Notion Tools**: Various components for interacting with Notion, including:\n  - List Users\n  - List Database Properties\n  - List Pages\n  - Page Content Viewer\n  - Create Page\n  - Update Page Property\n  - Add Content to Page\n  - Search\n- **Message History**: Stores conversation history\n- **Prompt Template**: Provides system instructions and context for the agent\n- **Current Date**: Supplies the current date and time for context\n\n### Run the Conversational Notion Agent\n\n1. Open Langflow and create a new flow.\n2. Add the components listed above to your flow canvas, or Download the [Conversation Agent Flow](/files/Conversational_Notion_Agent.json)(Download link) and **Import** the JSON file into Langflow.\n3. Connect the components as shown in the flow diagram.\n4. Input the Notion and OpenAI API keys in their respective components.\n5. To start the flow, click the \"Play\" button in Langflow.\n6. In the chat interface, enter a Notion-related request or question. For example:\n   - \"Create a new page in my task database\"\n   - \"Search for pages about project management\"\n   - \"List all users in my Notion workspace\"\n7. The agent will process your request, use the appropriate Notion tools, and provide a response.\n8. Continue the conversation, asking follow-up questions or giving new commands as needed.\n\n   <details>\n   <summary>Example Conversation</summary>\n\n   ```\n   User: List all the users in my Notion workspace.\n\n   Agent: Certainly! I'll use the Notion User List tool to retrieve that information for you. One moment please.\n\n   [Agent uses the Notion User List tool]\n\n   Here's a list of users in your Notion workspace:\n\n   1. John Doe (john@example.com)\n   2. Jane Smith (jane@example.com)\n   3. Alex Johnson (alex@example.com)\n\n   Is there anything specific you'd like to know about these users or any other Notion-related task I can help you with?\n\n   User: Create a new page in my Projects database with the title \"Website Redesign\".\n\n   Agent: Certainly! I'd be happy to create a new page in your Projects database with the title \"Website Redesign\". I'll use the Notion Page Creator tool to do this. First, let me check the properties of your Projects database to ensure we include all necessary information.\n\n   [Agent uses the Notion Database Properties tool]\n\n   I've retrieved the properties for your Projects database. Now, I'll create the new page with the title \"Website Redesign\". One moment please.\n\n   [Agent uses the Notion Page Creator tool]\n\n   Great news! I've successfully created a new page in your Projects database with the title \"Website Redesign\". Here are the details:\n\n   - Title: Website Redesign\n   - Page ID: [page_id_here]\n   - URL: [page_url_here]\n\n   Is there any additional information you'd like me to add to this new project page, such as a description, due date, or assigned team members?\n\n   User: Yes, please add a description: \"Redesign company website to improve user experience and modernize the look.\"\n\n   Agent: Certainly! I'd be happy to add that description to the \"Website Redesign\" project page. I'll use the Add Content to Page tool to update the page with this information.\n\n   [Agent uses the Add Content to Page tool]\n\n   I've successfully added the description to your \"Website Redesign\" project page. Here's a summary of what I've done:\n\n   1. Located the \"Website Redesign\" page using the page ID from our previous interaction.\n   2. Added the following content to the page:\n\n      Description: Redesign company website to improve user experience and modernize the look.\n\n   The description has been added as a new text block on the page. Is there anything else you'd like me to add or modify on this project page?\n   ```\n\n   </details>\n\n9. Optional: The flow can be customized to meet your team's specific needs.\nFor example:\n   * Adjust the system prompt to change the agent's behavior or context.\n   * Add or remove Notion tools based on your specific needs.\n   * Modify the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.\n\n## Troubleshooting\n\nIf you encounter issues:\n\n1. Ensure all API keys are correctly set and have the necessary permissions.\n2. Check that your Notion integration has access to the relevant pages and databases.\n3. Verify that all components are properly connected in the flow.\n4. Review the Langflow logs for any error messages.\n\nFor more advanced usage and integration options, refer to the [Notion API documentation](https://developers.notion.com/) and [Langflow documentation](/).",
      "summary": "To use Notion components in Langflow, you first need to create a Notion integration and configure it with the necessary capabilities. This guide will walk you through the process of setting up a Notion integration and granting it access to your Notion databases, and then provide two examples of...",
      "keywords": [
        "access",
        "add",
        "agent",
        "app",
        "build",
        "calling",
        "capabilities",
        "chat",
        "component",
        "components",
        "configure",
        "content",
        "conversational",
        "create",
        "current",
        "database",
        "databases",
        "date",
        "flow",
        "grant"
      ]
    },
    "/bundles-novita": {
      "title": "Novita",
      "slug": "/bundles-novita",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Novita** bundle.\n\n## Novita AI\n\nThis component generates text using [Novita's language models](https://novita.ai/docs/guides/llm-api).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Novita model as the LLM for another LLM-driven component, such as a **Language Model** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Novita AI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your Novita AI API Key. |\n| model | String | Input parameter. The id of the Novita AI model to use. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "function",
        "language",
        "model",
        "novita",
        "parameters",
        "response",
        "smart"
      ]
    },
    "bundles-novita": {
      "title": "Novita",
      "slug": "/bundles-novita",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Novita** bundle.\n\n## Novita AI\n\nThis component generates text using [Novita's language models](https://novita.ai/docs/guides/llm-api).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Novita model as the LLM for another LLM-driven component, such as a **Language Model** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Novita AI parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your Novita AI API Key. |\n| model | String | Input parameter. The id of the Novita AI model to use. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "function",
        "language",
        "model",
        "novita",
        "parameters",
        "response",
        "smart"
      ]
    },
    "/bundles-nvidia": {
      "title": "NVIDIA",
      "slug": "/bundles-nvidia",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **NVIDIA** bundle.\n\n## NVIDIA\n\nThis component generates text using NVIDIA LLMs.\nFor more information about NVIDIA LLMs, see the [NVIDIA AI documentation](https://developer.nvidia.com/generative-ai).\n\n### NVIDIA parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. |\n| model_name | String | Input parameter. The name of the NVIDIA model to use. Default: `mistralai/mixtral-8x7b-instruct-v0.1`. |\n| base_url | String | Input parameter. The base URL of the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | Input parameter. The NVIDIA API Key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| seed | Integer | Input parameter. The seed controls the reproducibility of the job. Default: `1`. |\n| model | LanguageModel | Output parameter. An instance of ChatNVIDIA configured with the specified parameters. |\n\n### NVIDIA NIM on WSL2\n\n[NVIDIA NIM (NVIDIA Inference Microservices)](https://docs.nvidia.com/nim/index.html) provides containers to self-host GPU-accelerated inferencing microservices.\n\nYou can use the **NVIDIA** component to connect Langflow with NVIDIA NIM on an RTX Windows system with [Windows Subsystem for Linux 2 (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) installed.\nThe following example connects an **NVIDIA** language model component in Langflow to a deployed `mistral-nemo-12b-instruct` NIM on an **RTX Windows system** with **WSL2**.\n\n1. Prepare your system:\n\n    * [NVIDIA NIM WSL2 installed](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html)\n    * A NIM container deployed according to the model's instructions\n\n        Prerequisites vary between models.\n        For example, to deploy the `mistral-nemo-12b-instruct` NIM, follow the instructions for **Windows on RTX AI PCs (Beta)** on your [model's deployment overview](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md).\n\n    * Windows 11 build 23H2 or later\n    * At least 12 GB of RAM\n\n2. Create a flow based on the **Basic Prompting** template.\n3. Replace the **OpenAI** model component with the **NVIDIA** component.\n4. In the **NVIDIA** component's **Base URL** field, add the URL where your NIM is accessible. If you followed your model's [deployment instructions](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md), the value is `http://localhost:8000/v1`.\n5. In the **NVIDIA** component's **NVIDIA API Key** field, add your NVIDIA API Key.\n6. Select your model from the **Model Name** field.\n7. Open the **Playground** and chat with your **NIM** model.\n\n## NVIDIA Embeddings\n\nThe **NVIDIA Embeddings** component generates embeddings using [NVIDIA models](https://docs.nvidia.com).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### NVIDIA Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | String | Input parameter. The NVIDIA model to use for embeddings, such as `nvidia/nv-embed-v1`. |\n| base_url | String | Input parameter. The base URL for the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | Input parameter. The API key for authenticating with NVIDIA's service. |\n| temperature | Float | Input parameter. The model temperature for embedding generation. Default: `0.1`. |\n| embeddings | Embeddings | Output parameter. An `NVIDIAEmbeddings` instance for generating embeddings. |\n\n**Note:** Tokenization considerations\nBe aware of your embedding model's chunk size limit.\nTokenization errors can occur if your text chunks are too large.\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\n\n## NVIDIA Rerank\n\nThis component finds and reranks documents using the NVIDIA API.\n\n## NVIDIA Retriever Extraction\n\nThe **NVIDIA Retriever Extraction** component integrates with the [NVIDIA nv-ingest](https://github.com/NVIDIA/nv-ingest) microservice for data ingestion, processing, and extraction of text files.\n\nThe `nv-ingest` service supports multiple extraction methods for PDF, DOCX, and PPTX file types, and includes pre-  and post-processing services like splitting, chunking, and embedding generation. The extractor service's High Resolution mode uses the `nemoretriever-parse` extraction method for better quality extraction from scanned PDF documents. This feature is only available for PDF files.\n\nThe **NVIDIA Retriever Extraction** component imports the NVIDIA `Ingestor` client, ingests files with requests to the NVIDIA ingest endpoint, and outputs the processed content as a list of [`Data`](/data-types#data) objects. `Ingestor` accepts additional configuration options for data extraction from other text formats. To configure these options, see the [parameters](#parameters).\n\n**Note:** NVIDIA Retriever Extraction is also known as NV-Ingest and NeMo Retriever Extraction.\n\n### Use the NVIDIA Retriever Extraction component in a flow\n\nThe **NVIDIA Retriever Extraction** component accepts `Message` inputs, and then outputs `Data`. The component calls an NVIDIA Ingest microservice's endpoint to ingest a local file and extract the text.\n\nTo use the **NVIDIA Retriever Extraction** component in your flow, follow these steps:\n\n1. Prepare your system:\n\n    * An NVIDIA Ingest endpoint. For more information on setting up an NVIDIA Ingest endpoint, see the [NVIDIA Ingest quickstart](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart).\n\n    * The **NVIDIA Retriever Extraction** component requires the installation of additional dependencies to your Langflow environment. To install the dependencies in a virtual environment, run the following commands.\n\n        * If you have the Langflow repository cloned and installed from source:\n        ```bash\n        source **YOUR_LANGFLOW_VENV**/bin/activate\n        uv sync --extra nv-ingest\n        uv run langflow run\n        ```\n\n        * If you are installing Langflow from the Python Package Index:\n        ```bash\n        source **YOUR_LANGFLOW_VENV**/bin/activate\n        uv pip install --prerelease=allow 'langflow[nv-ingest]'\n        uv run langflow run\n        ```\n\n2. Add the **NVIDIA Retriever Extraction** component to your flow.\n3. In the **Base URL** field, enter the URL of the NVIDIA Ingest endpoint.\nYou can also store the URL as a [global variable](/configuration-global-variables) to reuse it in multiple components and flows.\n4. Click **Select Files** to select a file to ingest.\n5. Select which text type to extract from the file: text, charts, tables, images, or infographics.\n6. Optional: For PDF files, enable **High Resolution Mode** for better quality extraction from scanned documents.\n7. Select whether to split the text into chunks.\n\n    \n\n8. Click  **Run component** to ingest the file, and then click **Logs** or  **Inspect output** to confirm the component ingested the file.\n9. To store the processed data in a vector database, add a vector store component to your flow, and then connect the **NVIDIA Retriever Extraction** component's `Data` output to the vector store component's input.\n\n    When you run the flow with a vector store component, the processed data is stored in the vector database.\n    You can query your database to retrieve the uploaded data.\n\n    ![NVIDIA Retriever Extraction component flow](/img/nvidia-component-ingest-astra.png)\n\n### NVIDIA Retriever Extraction parameters {#parameters}\n\nThe **NVIDIA Retriever Extraction** component has the following parameters.\n\nFor more information, see the [NV-Ingest documentation](https://nvidia.github.io/nv-ingest/user-guide/).\n\n#### Inputs\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| base_url | NVIDIA Ingestion URL | The URL of the NVIDIA Ingestion API. |\n| path | Path | File path to process. |\n| extract_text | Extract Text | Extract text from documents. Default: `true`. |\n| extract_charts | Extract Charts | Extract text from charts. Default: `false`. |\n| extract_tables | Extract Tables | Extract text from tables. Default: `true`. |\n| extract_images | Extract Images | Extract images from document. Default: `true`. |\n| extract_infographics | Extract Infographics | Extract infographics from document. Default: `false`. |\n| text_depth | Text Depth | The level at which text is extracted. Options: 'document', 'page', 'block', 'line', 'span'. Default: `page`. |\n| split_text | Split Text | Split text into smaller chunks. Default: `true`. |\n| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. Make sure the chunk size is compatible with your embedding model. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |\n| chunk_overlap | Chunk Overlap | Number of tokens to overlap from previous chunk. Default: `150`. |\n| filter_images | Filter Images | Filter images (see advanced options for filtering criteria). Default: `false`. |\n| min_image_size | Minimum Image Size Filter | Minimum image width/length in pixels. Default: `128`. |\n| min_aspect_ratio | Minimum Aspect Ratio Filter | Minimum allowed aspect ratio (width / height). Default: `0.2`. |\n| max_aspect_ratio | Maximum Aspect Ratio Filter | Maximum allowed aspect ratio (width / height). Default: `5.0`. |\n| dedup_images | Deduplicate Images | Filter duplicated images. Default: `true`. |\n| caption_images | Caption Images | Generate captions for images using the NVIDIA captioning model. Default: `true`. |\n| high_resolution | High Resolution (PDF only) | Process PDF in high-resolution mode for better quality extraction from scanned PDF. Default: `false`. |\n\n#### Outputs\n\nThe **NVIDIA Retriever Extraction** component outputs a list of [`Data`](/data-types#data) objects where each object contains:\n- `text`: The extracted content.\n  - For text documents: The extracted text content.\n  - For tables and charts: The extracted table/chart content.\n  - For images: The image caption.\n  - For infographics: The extracted infographic content.\n- `file_path`: The source file name and path.\n- `document_type`: The type of the document, which can be `text`, `structured`, or `image`.\n- `description`: Additional description of the content.\n\nThe output varies based on the `document_type`:\n\n- Documents with `document_type: \"text\"` contain:\n  - Raw text content extracted from documents, for example, paragraphs from PDFs or DOCX files.\n  - Content stored directly in the `text` field.\n  - Content extracted using the `extract_text` parameter.\n\n- Documents with `document_type: \"structured\"` contain:\n  - Text extracted from tables, charts, and infographics and processed to preserve structural information.\n  - Content extracted using the `extract_tables`, `extract_charts`, and `extract_infographics` parameters.\n  - Content stored in the `text` field after being processed from the `table_content` metadata.\n\n- Documents with `document_type: \"image\"` contain:\n  - Image content extracted from documents.\n  - Caption text stored in the `text` field when `caption_images` is enabled.\n  - Content extracted using the `extract_images` parameter.\n\n## NVIDIA System-Assist\n\nThe **NVIDIA System-Assist** component integrates your flows with NVIDIA G-Assist, enabling interaction with NVIDIA GPU drivers through natural language prompts.\nFor example, prompt G-Assist with `\"What is my current GPU temperature?\"` or `\"Show me the available GPU memory\"` to get information, and then tell G-Assist to modify your GPU settings.\nFor more information, see the [NVIDIA G-Assist repository](https://github.com/NVIDIA/g-assist).\n\n1. Prepare your system:\n\n    * The **NVIDIA System-Assist** component requires an NVIDIA GPU on a Windows operating system.\n    * It uses the `gassist.rise` package, which is installed with all Langflow versions that include this component.\n\n2. Create a flow with a **Chat Input** component, **NVIDIA System-Assist** component, and **Chat Output** components.\n\n    This is a simplified example that uses only three components.\n    Depending on your use case, your flow might use more components or different inputs and outputs.\n\n3. Connect the **Chat Input** component to the **NVIDIA System-Assist** component's **Prompt** input.\n\n    The **Prompt** parameter accepts a natural language prompt that is processed by the NVIDIA G-Assist AI Assistant.\n    In this example, you'll provide the prompt as chat input.\n    You could also enter a prompt directly in the **Prompt** input or connect another input component.\n\n4. Connect the **NVIDIA System-Assist** component's output to the **Chat Output** component.\n\n5. To test the flow, open the **Playground**, and then ask a question about your GPU.\nFor example, `\"What is my current GPU temperature?\"`.\n\n    Through the **NVIDIA System-Assist** component, NVIDIA G-Assist queries your GPU based on the prompt, and then prints the response to the **Playground**.\n\n    The component's output is a [`Message`](/data-types#message) containing the NVIDIA G-Assist response.\n    The string response with the completed operation result is available in the `text` key in the `Message` object.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "assist",
        "base",
        "basic",
        "beta",
        "bundles",
        "chat",
        "component",
        "embeddings",
        "extraction",
        "files",
        "flow",
        "high",
        "input",
        "inputs",
        "inspect",
        "key",
        "logs",
        "mode",
        "model"
      ]
    },
    "bundles-nvidia": {
      "title": "NVIDIA",
      "slug": "/bundles-nvidia",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **NVIDIA** bundle.\n\n## NVIDIA\n\nThis component generates text using NVIDIA LLMs.\nFor more information about NVIDIA LLMs, see the [NVIDIA AI documentation](https://developer.nvidia.com/generative-ai).\n\n### NVIDIA parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. |\n| model_name | String | Input parameter. The name of the NVIDIA model to use. Default: `mistralai/mixtral-8x7b-instruct-v0.1`. |\n| base_url | String | Input parameter. The base URL of the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | Input parameter. The NVIDIA API Key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |\n| seed | Integer | Input parameter. The seed controls the reproducibility of the job. Default: `1`. |\n| model | LanguageModel | Output parameter. An instance of ChatNVIDIA configured with the specified parameters. |\n\n### NVIDIA NIM on WSL2\n\n[NVIDIA NIM (NVIDIA Inference Microservices)](https://docs.nvidia.com/nim/index.html) provides containers to self-host GPU-accelerated inferencing microservices.\n\nYou can use the **NVIDIA** component to connect Langflow with NVIDIA NIM on an RTX Windows system with [Windows Subsystem for Linux 2 (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) installed.\nThe following example connects an **NVIDIA** language model component in Langflow to a deployed `mistral-nemo-12b-instruct` NIM on an **RTX Windows system** with **WSL2**.\n\n1. Prepare your system:\n\n    * [NVIDIA NIM WSL2 installed](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html)\n    * A NIM container deployed according to the model's instructions\n\n        Prerequisites vary between models.\n        For example, to deploy the `mistral-nemo-12b-instruct` NIM, follow the instructions for **Windows on RTX AI PCs (Beta)** on your [model's deployment overview](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md).\n\n    * Windows 11 build 23H2 or later\n    * At least 12 GB of RAM\n\n2. Create a flow based on the **Basic Prompting** template.\n3. Replace the **OpenAI** model component with the **NVIDIA** component.\n4. In the **NVIDIA** component's **Base URL** field, add the URL where your NIM is accessible. If you followed your model's [deployment instructions](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md), the value is `http://localhost:8000/v1`.\n5. In the **NVIDIA** component's **NVIDIA API Key** field, add your NVIDIA API Key.\n6. Select your model from the **Model Name** field.\n7. Open the **Playground** and chat with your **NIM** model.\n\n## NVIDIA Embeddings\n\nThe **NVIDIA Embeddings** component generates embeddings using [NVIDIA models](https://docs.nvidia.com).\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### NVIDIA Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | String | Input parameter. The NVIDIA model to use for embeddings, such as `nvidia/nv-embed-v1`. |\n| base_url | String | Input parameter. The base URL for the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |\n| nvidia_api_key | SecretString | Input parameter. The API key for authenticating with NVIDIA's service. |\n| temperature | Float | Input parameter. The model temperature for embedding generation. Default: `0.1`. |\n| embeddings | Embeddings | Output parameter. An `NVIDIAEmbeddings` instance for generating embeddings. |\n\n**Note:** Tokenization considerations\nBe aware of your embedding model's chunk size limit.\nTokenization errors can occur if your text chunks are too large.\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\n\n## NVIDIA Rerank\n\nThis component finds and reranks documents using the NVIDIA API.\n\n## NVIDIA Retriever Extraction\n\nThe **NVIDIA Retriever Extraction** component integrates with the [NVIDIA nv-ingest](https://github.com/NVIDIA/nv-ingest) microservice for data ingestion, processing, and extraction of text files.\n\nThe `nv-ingest` service supports multiple extraction methods for PDF, DOCX, and PPTX file types, and includes pre-  and post-processing services like splitting, chunking, and embedding generation. The extractor service's High Resolution mode uses the `nemoretriever-parse` extraction method for better quality extraction from scanned PDF documents. This feature is only available for PDF files.\n\nThe **NVIDIA Retriever Extraction** component imports the NVIDIA `Ingestor` client, ingests files with requests to the NVIDIA ingest endpoint, and outputs the processed content as a list of [`Data`](/data-types#data) objects. `Ingestor` accepts additional configuration options for data extraction from other text formats. To configure these options, see the [parameters](#parameters).\n\n**Note:** NVIDIA Retriever Extraction is also known as NV-Ingest and NeMo Retriever Extraction.\n\n### Use the NVIDIA Retriever Extraction component in a flow\n\nThe **NVIDIA Retriever Extraction** component accepts `Message` inputs, and then outputs `Data`. The component calls an NVIDIA Ingest microservice's endpoint to ingest a local file and extract the text.\n\nTo use the **NVIDIA Retriever Extraction** component in your flow, follow these steps:\n\n1. Prepare your system:\n\n    * An NVIDIA Ingest endpoint. For more information on setting up an NVIDIA Ingest endpoint, see the [NVIDIA Ingest quickstart](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart).\n\n    * The **NVIDIA Retriever Extraction** component requires the installation of additional dependencies to your Langflow environment. To install the dependencies in a virtual environment, run the following commands.\n\n        * If you have the Langflow repository cloned and installed from source:\n        ```bash\n        source **YOUR_LANGFLOW_VENV**/bin/activate\n        uv sync --extra nv-ingest\n        uv run langflow run\n        ```\n\n        * If you are installing Langflow from the Python Package Index:\n        ```bash\n        source **YOUR_LANGFLOW_VENV**/bin/activate\n        uv pip install --prerelease=allow 'langflow[nv-ingest]'\n        uv run langflow run\n        ```\n\n2. Add the **NVIDIA Retriever Extraction** component to your flow.\n3. In the **Base URL** field, enter the URL of the NVIDIA Ingest endpoint.\nYou can also store the URL as a [global variable](/configuration-global-variables) to reuse it in multiple components and flows.\n4. Click **Select Files** to select a file to ingest.\n5. Select which text type to extract from the file: text, charts, tables, images, or infographics.\n6. Optional: For PDF files, enable **High Resolution Mode** for better quality extraction from scanned documents.\n7. Select whether to split the text into chunks.\n\n    \n\n8. Click  **Run component** to ingest the file, and then click **Logs** or  **Inspect output** to confirm the component ingested the file.\n9. To store the processed data in a vector database, add a vector store component to your flow, and then connect the **NVIDIA Retriever Extraction** component's `Data` output to the vector store component's input.\n\n    When you run the flow with a vector store component, the processed data is stored in the vector database.\n    You can query your database to retrieve the uploaded data.\n\n    ![NVIDIA Retriever Extraction component flow](/img/nvidia-component-ingest-astra.png)\n\n### NVIDIA Retriever Extraction parameters {#parameters}\n\nThe **NVIDIA Retriever Extraction** component has the following parameters.\n\nFor more information, see the [NV-Ingest documentation](https://nvidia.github.io/nv-ingest/user-guide/).\n\n#### Inputs\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| base_url | NVIDIA Ingestion URL | The URL of the NVIDIA Ingestion API. |\n| path | Path | File path to process. |\n| extract_text | Extract Text | Extract text from documents. Default: `true`. |\n| extract_charts | Extract Charts | Extract text from charts. Default: `false`. |\n| extract_tables | Extract Tables | Extract text from tables. Default: `true`. |\n| extract_images | Extract Images | Extract images from document. Default: `true`. |\n| extract_infographics | Extract Infographics | Extract infographics from document. Default: `false`. |\n| text_depth | Text Depth | The level at which text is extracted. Options: 'document', 'page', 'block', 'line', 'span'. Default: `page`. |\n| split_text | Split Text | Split text into smaller chunks. Default: `true`. |\n| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. Make sure the chunk size is compatible with your embedding model. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |\n| chunk_overlap | Chunk Overlap | Number of tokens to overlap from previous chunk. Default: `150`. |\n| filter_images | Filter Images | Filter images (see advanced options for filtering criteria). Default: `false`. |\n| min_image_size | Minimum Image Size Filter | Minimum image width/length in pixels. Default: `128`. |\n| min_aspect_ratio | Minimum Aspect Ratio Filter | Minimum allowed aspect ratio (width / height). Default: `0.2`. |\n| max_aspect_ratio | Maximum Aspect Ratio Filter | Maximum allowed aspect ratio (width / height). Default: `5.0`. |\n| dedup_images | Deduplicate Images | Filter duplicated images. Default: `true`. |\n| caption_images | Caption Images | Generate captions for images using the NVIDIA captioning model. Default: `true`. |\n| high_resolution | High Resolution (PDF only) | Process PDF in high-resolution mode for better quality extraction from scanned PDF. Default: `false`. |\n\n#### Outputs\n\nThe **NVIDIA Retriever Extraction** component outputs a list of [`Data`](/data-types#data) objects where each object contains:\n- `text`: The extracted content.\n  - For text documents: The extracted text content.\n  - For tables and charts: The extracted table/chart content.\n  - For images: The image caption.\n  - For infographics: The extracted infographic content.\n- `file_path`: The source file name and path.\n- `document_type`: The type of the document, which can be `text`, `structured`, or `image`.\n- `description`: Additional description of the content.\n\nThe output varies based on the `document_type`:\n\n- Documents with `document_type: \"text\"` contain:\n  - Raw text content extracted from documents, for example, paragraphs from PDFs or DOCX files.\n  - Content stored directly in the `text` field.\n  - Content extracted using the `extract_text` parameter.\n\n- Documents with `document_type: \"structured\"` contain:\n  - Text extracted from tables, charts, and infographics and processed to preserve structural information.\n  - Content extracted using the `extract_tables`, `extract_charts`, and `extract_infographics` parameters.\n  - Content stored in the `text` field after being processed from the `table_content` metadata.\n\n- Documents with `document_type: \"image\"` contain:\n  - Image content extracted from documents.\n  - Caption text stored in the `text` field when `caption_images` is enabled.\n  - Content extracted using the `extract_images` parameter.\n\n## NVIDIA System-Assist\n\nThe **NVIDIA System-Assist** component integrates your flows with NVIDIA G-Assist, enabling interaction with NVIDIA GPU drivers through natural language prompts.\nFor example, prompt G-Assist with `\"What is my current GPU temperature?\"` or `\"Show me the available GPU memory\"` to get information, and then tell G-Assist to modify your GPU settings.\nFor more information, see the [NVIDIA G-Assist repository](https://github.com/NVIDIA/g-assist).\n\n1. Prepare your system:\n\n    * The **NVIDIA System-Assist** component requires an NVIDIA GPU on a Windows operating system.\n    * It uses the `gassist.rise` package, which is installed with all Langflow versions that include this component.\n\n2. Create a flow with a **Chat Input** component, **NVIDIA System-Assist** component, and **Chat Output** components.\n\n    This is a simplified example that uses only three components.\n    Depending on your use case, your flow might use more components or different inputs and outputs.\n\n3. Connect the **Chat Input** component to the **NVIDIA System-Assist** component's **Prompt** input.\n\n    The **Prompt** parameter accepts a natural language prompt that is processed by the NVIDIA G-Assist AI Assistant.\n    In this example, you'll provide the prompt as chat input.\n    You could also enter a prompt directly in the **Prompt** input or connect another input component.\n\n4. Connect the **NVIDIA System-Assist** component's output to the **Chat Output** component.\n\n5. To test the flow, open the **Playground**, and then ask a question about your GPU.\nFor example, `\"What is my current GPU temperature?\"`.\n\n    Through the **NVIDIA System-Assist** component, NVIDIA G-Assist queries your GPU based on the prompt, and then prints the response to the **Playground**.\n\n    The component's output is a [`Message`](/data-types#message) containing the NVIDIA G-Assist response.\n    The string response with the completed operation result is available in the `text` key in the `Message` object.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "assist",
        "base",
        "basic",
        "beta",
        "bundles",
        "chat",
        "component",
        "embeddings",
        "extraction",
        "files",
        "flow",
        "high",
        "input",
        "inputs",
        "inspect",
        "key",
        "logs",
        "mode",
        "model"
      ]
    },
    "/bundles-ollama": {
      "title": "Ollama",
      "slug": "/bundles-ollama",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Ollama** bundle.\n\nFor more information about Ollama features and functionality used by Ollama components, see the [Ollama documentation](https://ollama.com/).\n\n## Ollama text generation\n\nThis component generates text using [Ollama's language models](https://ollama.com/library).\n\nTo use the **Ollama** component in a flow, connect Langflow to your locally running Ollama server and select a model:\n\n1. Add the **Ollama** component to your flow.\n\n2. In the **Base URL** field, enter the address for your locally running Ollama server.\n\n    This value is set as the `OLLAMA_HOST` environment variable in Ollama.\n    The default base URL is `http://127.0.0.1:11434`.\n\n3. Once the connection is established, select a model in the **Model Name** field, such as `llama3.2:latest`.\n\n    To refresh the server's list of models, click  **Refresh**.\n\n4. Optional: To configure additional parameters, such as temperature or max tokens, click  **Controls** in the [component's header menu](/concepts-components#component-menus).\n\n5. Connect the **Ollama** component to other components in the flow, depending on how you want to use the model.\n\n    Language model components can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)). Use the **Language Model** output when you want to use an Ollama model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component. For more information, see [Language model components](/components-models).\n\n    In the following example, the flow uses `LanguageModel` output to use an Ollama model as the LLM for an [**Agent** component](/components-agents).\n\n    ![Ollama component used as the LLM in an agent flow](/img/component-ollama-model.png)\n\n## Ollama Embeddings\n\nThe **Ollama Embeddings** component generates embeddings using [Ollama embedding models](https://ollama.com/search?c=embedding).\n\nTo use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model:\n\n1. Add the **Ollama Embeddings** component to your flow.\n\n2. In the **Ollama Base URL** field, enter the address for your locally running Ollama server.\n\n    This value is set as the `OLLAMA_HOST` environment variable in Ollama.\n    The default base URL is `http://127.0.0.1:11434`.\n\n3. Once the connection is established, select a model in the **Ollama Model** field, such as `all-minilm:latest`.\n\n    To refresh the server's list of models, click  **Refresh**.\n\n4. Optional: To configure additional parameters, such as temperature or max tokens, click  **Controls** in the [component's header menu](/concepts-components#component-menus).\nAvailable parameters depend on the selected model.\n\n5. Connect the **Ollama Embeddings** component to other components in the flow.\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n    This example connects the **Ollama Embeddings** component to generate embeddings for text chunks extracted from a PDF file, and then stores the embeddings and chunks in a Chroma DB vector store.\n\n    ![Ollama Embeddings component in an embedding generation flow](/img/component-ollama-embeddings-chromadb.png)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "base",
        "bundles",
        "controls",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "name",
        "ollama",
        "refresh",
        "response",
        "smart",
        "text",
        "url"
      ]
    },
    "bundles-ollama": {
      "title": "Ollama",
      "slug": "/bundles-ollama",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Ollama** bundle.\n\nFor more information about Ollama features and functionality used by Ollama components, see the [Ollama documentation](https://ollama.com/).\n\n## Ollama text generation\n\nThis component generates text using [Ollama's language models](https://ollama.com/library).\n\nTo use the **Ollama** component in a flow, connect Langflow to your locally running Ollama server and select a model:\n\n1. Add the **Ollama** component to your flow.\n\n2. In the **Base URL** field, enter the address for your locally running Ollama server.\n\n    This value is set as the `OLLAMA_HOST` environment variable in Ollama.\n    The default base URL is `http://127.0.0.1:11434`.\n\n3. Once the connection is established, select a model in the **Model Name** field, such as `llama3.2:latest`.\n\n    To refresh the server's list of models, click  **Refresh**.\n\n4. Optional: To configure additional parameters, such as temperature or max tokens, click  **Controls** in the [component's header menu](/concepts-components#component-menus).\n\n5. Connect the **Ollama** component to other components in the flow, depending on how you want to use the model.\n\n    Language model components can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)). Use the **Language Model** output when you want to use an Ollama model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component. For more information, see [Language model components](/components-models).\n\n    In the following example, the flow uses `LanguageModel` output to use an Ollama model as the LLM for an [**Agent** component](/components-agents).\n\n    ![Ollama component used as the LLM in an agent flow](/img/component-ollama-model.png)\n\n## Ollama Embeddings\n\nThe **Ollama Embeddings** component generates embeddings using [Ollama embedding models](https://ollama.com/search?c=embedding).\n\nTo use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model:\n\n1. Add the **Ollama Embeddings** component to your flow.\n\n2. In the **Ollama Base URL** field, enter the address for your locally running Ollama server.\n\n    This value is set as the `OLLAMA_HOST` environment variable in Ollama.\n    The default base URL is `http://127.0.0.1:11434`.\n\n3. Once the connection is established, select a model in the **Ollama Model** field, such as `all-minilm:latest`.\n\n    To refresh the server's list of models, click  **Refresh**.\n\n4. Optional: To configure additional parameters, such as temperature or max tokens, click  **Controls** in the [component's header menu](/concepts-components#component-menus).\nAvailable parameters depend on the selected model.\n\n5. Connect the **Ollama Embeddings** component to other components in the flow.\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n    This example connects the **Ollama Embeddings** component to generate embeddings for text chunks extracted from a PDF file, and then stores the embeddings and chunks in a Chroma DB vector store.\n\n    ![Ollama Embeddings component in an embedding generation flow](/img/component-ollama-embeddings-chromadb.png)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "base",
        "bundles",
        "controls",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "name",
        "ollama",
        "refresh",
        "response",
        "smart",
        "text",
        "url"
      ]
    },
    "/bundles-openai": {
      "title": "OpenAI",
      "slug": "/bundles-openai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **OpenAI** bundle.\n\nFor more information about OpenAI features and functionality used by OpenAI components, see the [OpenAI documentation](https://platform.openai.com/docs/overview).\n\n## OpenAI text generation\n\nThe **OpenAI** component generates text using [OpenAI's language models](https://platform.openai.com/docs/models).\n\nIt provides access to the same OpenAI models that are available in the core **Language Model** component, but the **OpenAI** component provides additional parameters for customizing the request to the OpenAI API.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a specific OpenAI model configuration as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### OpenAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your OpenAI API Key. |\n| model | String | Input parameter. The name of the OpenAI model to use. Options include \"gpt-3.5-turbo\" and \"gpt-4\". |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |\n\n## OpenAI Embeddings\n\nThe **OpenAI Embeddings** component uses [OpenAI embedding models](https://platform.openai.com/docs/guides/embeddings) for embedding generation.\n\nIt provides access to the same OpenAI models that are available in the core **Embedding Model** component, but the **OpenAI Embeddings** component provides additional parameters for customizing the request to the OpenAI embedding API.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### OpenAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| OpenAI API Key | String | Input parameter. The API key to use for accessing the OpenAI API. |\n| Default Headers | Dict | Input parameter. The default headers for the HTTP requests. |\n| Default Query | NestedDict | Input parameter. The default query parameters for the HTTP requests. |\n| Allowed Special | List | Input parameter. The special tokens allowed for processing. Default: `[]`. |\n| Disallowed Special | List | Input parameter. The special tokens disallowed for processing. Default: `[\"all\"]`. |\n| Chunk Size | Integer | Input parameter. The chunk size for processing. Default: `1000`. |\n| Client | Any | Input parameter. The HTTP client for making requests. |\n| Deployment | String | Input parameter. The deployment name for the model. Default: `text-embedding-3-small`. |\n| Embedding Context Length | Integer | Input parameter. The length of embedding context. Default: `8191`. |\n| Max Retries | Integer | Input parameter. The maximum number of retries for failed requests. Default: `6`. |\n| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |\n| Model Kwargs | NestedDict | Input parameter. Additional keyword arguments for the model. |\n| OpenAI API Base | String | Input parameter. The base URL of the OpenAI API. |\n| OpenAI API Type | String | Input parameter. The type of the OpenAI API. |\n| OpenAI API Version | String | Input parameter. The version of the OpenAI API. |\n| OpenAI Organization | String | Input parameter. The organization associated with the API key. |\n| OpenAI Proxy | String | Input parameter. The proxy server for the requests. |\n| Request Timeout | Float | Input parameter. The timeout for the HTTP requests. |\n| Show Progress Bar | Boolean | Input parameter. Whether to show a progress bar for processing. Default: `false`. |\n| Skip Empty | Boolean | Input parameter. Whether to skip empty inputs. Default: `false`. |\n| TikToken Enable | Boolean | Input parameter. Whether to enable TikToken. Default: `true`. |\n| TikToken Model Name | String | Input parameter. The name of the TikToken model. |\n\n## See also\n\n* [**Agent** component](/components-agents)\n* [LangChain **OpenAI Tools Agent** component](/bundles-langchain#openai-tools-agent)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embedding",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "openai",
        "parameters",
        "response",
        "smart",
        "text",
        "tools"
      ]
    },
    "bundles-openai": {
      "title": "OpenAI",
      "slug": "/bundles-openai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **OpenAI** bundle.\n\nFor more information about OpenAI features and functionality used by OpenAI components, see the [OpenAI documentation](https://platform.openai.com/docs/overview).\n\n## OpenAI text generation\n\nThe **OpenAI** component generates text using [OpenAI's language models](https://platform.openai.com/docs/models).\n\nIt provides access to the same OpenAI models that are available in the core **Language Model** component, but the **OpenAI** component provides additional parameters for customizing the request to the OpenAI API.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a specific OpenAI model configuration as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### OpenAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your OpenAI API Key. |\n| model | String | Input parameter. The name of the OpenAI model to use. Options include \"gpt-3.5-turbo\" and \"gpt-4\". |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |\n| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |\n| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |\n| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |\n\n## OpenAI Embeddings\n\nThe **OpenAI Embeddings** component uses [OpenAI embedding models](https://platform.openai.com/docs/guides/embeddings) for embedding generation.\n\nIt provides access to the same OpenAI models that are available in the core **Embedding Model** component, but the **OpenAI Embeddings** component provides additional parameters for customizing the request to the OpenAI embedding API.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### OpenAI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| OpenAI API Key | String | Input parameter. The API key to use for accessing the OpenAI API. |\n| Default Headers | Dict | Input parameter. The default headers for the HTTP requests. |\n| Default Query | NestedDict | Input parameter. The default query parameters for the HTTP requests. |\n| Allowed Special | List | Input parameter. The special tokens allowed for processing. Default: `[]`. |\n| Disallowed Special | List | Input parameter. The special tokens disallowed for processing. Default: `[\"all\"]`. |\n| Chunk Size | Integer | Input parameter. The chunk size for processing. Default: `1000`. |\n| Client | Any | Input parameter. The HTTP client for making requests. |\n| Deployment | String | Input parameter. The deployment name for the model. Default: `text-embedding-3-small`. |\n| Embedding Context Length | Integer | Input parameter. The length of embedding context. Default: `8191`. |\n| Max Retries | Integer | Input parameter. The maximum number of retries for failed requests. Default: `6`. |\n| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |\n| Model Kwargs | NestedDict | Input parameter. Additional keyword arguments for the model. |\n| OpenAI API Base | String | Input parameter. The base URL of the OpenAI API. |\n| OpenAI API Type | String | Input parameter. The type of the OpenAI API. |\n| OpenAI API Version | String | Input parameter. The version of the OpenAI API. |\n| OpenAI Organization | String | Input parameter. The organization associated with the API key. |\n| OpenAI Proxy | String | Input parameter. The proxy server for the requests. |\n| Request Timeout | Float | Input parameter. The timeout for the HTTP requests. |\n| Show Progress Bar | Boolean | Input parameter. Whether to show a progress bar for processing. Default: `false`. |\n| Skip Empty | Boolean | Input parameter. Whether to skip empty inputs. Default: `false`. |\n| TikToken Enable | Boolean | Input parameter. Whether to enable TikToken. Default: `true`. |\n| TikToken Model Name | String | Input parameter. The name of the TikToken model. |\n\n## See also\n\n* [**Agent** component](/components-agents)\n* [LangChain **OpenAI Tools Agent** component](/bundles-langchain#openai-tools-agent)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "embedding",
        "embeddings",
        "function",
        "generation",
        "language",
        "model",
        "openai",
        "parameters",
        "response",
        "smart",
        "text",
        "tools"
      ]
    },
    "/bundles-openrouter": {
      "title": "OpenRouter",
      "slug": "/bundles-openrouter",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **OpenRouter** bundle.\n\nFor more information about OpenRouter features and functionality used by OpenRouter components, see the [OpenRouter documentation](https://openrouter.ai/docs).\n\n## OpenRouter text generation\n\nThis component generates text using OpenRouter's unified API for multiple AI models from different providers.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an OpenRouter model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### OpenRouter text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your OpenRouter API key for authentication. |\n| site_url | String | Input parameter. Your site URL for OpenRouter rankings. |\n| app_name | String | Input parameter. Your app name for OpenRouter rankings. |\n| provider | String | Input parameter. The AI model provider to use. |\n| model_name | String | Input parameter. The specific model to use for chat completion. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "openrouter",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-openrouter": {
      "title": "OpenRouter",
      "slug": "/bundles-openrouter",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **OpenRouter** bundle.\n\nFor more information about OpenRouter features and functionality used by OpenRouter components, see the [OpenRouter documentation](https://openrouter.ai/docs).\n\n## OpenRouter text generation\n\nThis component generates text using OpenRouter's unified API for multiple AI models from different providers.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an OpenRouter model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### OpenRouter text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your OpenRouter API key for authentication. |\n| site_url | String | Input parameter. Your site URL for OpenRouter rankings. |\n| app_name | String | Input parameter. Your app name for OpenRouter rankings. |\n| provider | String | Input parameter. The AI model provider to use. |\n| model_name | String | Input parameter. The specific model to use for chat completion. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "openrouter",
        "parameters",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-perplexity": {
      "title": "Perplexity",
      "slug": "/bundles-perplexity",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Perplexity** bundle.\n\nFor more information about Perplexity features and functionality used by Perplexity components, see the [Perplexity documentation](https://perplexity.ai/).\n\n## Perplexity text generation\n\nThis component generates text using Perplexity's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Perplexity model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Perplexity text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_name | String | Input parameter. The name of the Perplexity model to use. Options include various Llama 3.1 models. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| api_key | SecretString | Input parameter. The Perplexity API Key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.75. |\n| top_p | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |\n| n | Integer | Input parameter. Number of chat completions to generate for each prompt. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "perplexity",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-perplexity": {
      "title": "Perplexity",
      "slug": "/bundles-perplexity",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Perplexity** bundle.\n\nFor more information about Perplexity features and functionality used by Perplexity components, see the [Perplexity documentation](https://perplexity.ai/).\n\n## Perplexity text generation\n\nThis component generates text using Perplexity's language models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Perplexity model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Perplexity text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model_name | String | Input parameter. The name of the Perplexity model to use. Options include various Llama 3.1 models. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| api_key | SecretString | Input parameter. The Perplexity API Key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.75. |\n| top_p | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |\n| n | Integer | Input parameter. Number of chat completions to generate for each prompt. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "perplexity",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-pgvector": {
      "title": "pgvector",
      "slug": "/bundles-pgvector",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **pgvector** bundle.\n\n## pgvector vector store\n\nThe **PGVector** component reads and writes to PostgreSQL vector stores using an instance of [`PGVector`](https://docs.langchain.com/oss/python/integrations/vectorstores/pgvector).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### pgvector vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [PGVector documentation](https://github.com/pgvector/pgvector) or inspect [component code](/concepts-components#component-code).\n\n| Name            | Type         | Description                               |\n| --------------- | ------------ | ----------------------------------------- |\n| pg_server_url   | SecretString | Input parameter. The PostgreSQL server connection string.       |\n| collection_name | String       | Input parameter. The table name for the vector store.           |\n| search_query    | String       | Input parameter. The query for similarity search.               |\n| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding       | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results | Integer    | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "pgvector",
        "store",
        "vector"
      ]
    },
    "bundles-pgvector": {
      "title": "pgvector",
      "slug": "/bundles-pgvector",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **pgvector** bundle.\n\n## pgvector vector store\n\nThe **PGVector** component reads and writes to PostgreSQL vector stores using an instance of [`PGVector`](https://docs.langchain.com/oss/python/integrations/vectorstores/pgvector).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### pgvector vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [PGVector documentation](https://github.com/pgvector/pgvector) or inspect [component code](/concepts-components#component-code).\n\n| Name            | Type         | Description                               |\n| --------------- | ------------ | ----------------------------------------- |\n| pg_server_url   | SecretString | Input parameter. The PostgreSQL server connection string.       |\n| collection_name | String       | Input parameter. The table name for the vector store.           |\n| search_query    | String       | Input parameter. The query for similarity search.               |\n| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding       | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results | Integer    | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "pgvector",
        "store",
        "vector"
      ]
    },
    "/bundles-pinecone": {
      "title": "Pinecone",
      "slug": "/bundles-pinecone",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Pinecone** bundle.\n\n## Pinecone vector store\n\nThe **Pinecone** component reads and writes to Pinecone vector stores using an instance of `PineconeVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Pinecone vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Pinecone documentation](https://docs.pinecone.io/home) or inspect [component code](/concepts-components#component-code).\n\n| Name              | Type         | Description                               |\n| ----------------- | ------------ | ----------------------------------------- |\n| index_name        | String       | Input parameter. The name of the Pinecone index.                |\n| namespace         | String       | Input parameter. The namespace for the index.                   |\n| distance_strategy | String       | Input parameter. The strategy for calculating distance between vectors. |\n| pinecone_api_key  | SecretString | Input parameter. The API key for Pinecone.                      |\n| text_key          | String       | Input parameter. The key in the record to use as text.          |\n| search_query      | String       | Input parameter. The query for similarity search.               |\n| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding         | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "pinecone",
        "store",
        "vector"
      ]
    },
    "bundles-pinecone": {
      "title": "Pinecone",
      "slug": "/bundles-pinecone",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Pinecone** bundle.\n\n## Pinecone vector store\n\nThe **Pinecone** component reads and writes to Pinecone vector stores using an instance of `PineconeVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Pinecone vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Pinecone documentation](https://docs.pinecone.io/home) or inspect [component code](/concepts-components#component-code).\n\n| Name              | Type         | Description                               |\n| ----------------- | ------------ | ----------------------------------------- |\n| index_name        | String       | Input parameter. The name of the Pinecone index.                |\n| namespace         | String       | Input parameter. The namespace for the index.                   |\n| distance_strategy | String       | Input parameter. The strategy for calculating distance between vectors. |\n| pinecone_api_key  | SecretString | Input parameter. The API key for Pinecone.                      |\n| text_key          | String       | Input parameter. The key in the record to use as text.          |\n| search_query      | String       | Input parameter. The query for similarity search.               |\n| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding         | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "pinecone",
        "store",
        "vector"
      ]
    },
    "/bundles-qdrant": {
      "title": "Qdrant",
      "slug": "/bundles-qdrant",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Qdrant** bundle.\n\n## Qdrant vector store\n\nThe **Qdrant** component reads and writes to Qdrant vector stores using an instance of `QdrantVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Qdrant vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Qdrant documentation](https://qdrant.tech/documentation/) or inspect [component code](/concepts-components#component-code).\n\n| Name                 | Type         | Description                               |\n| -------------------- | ------------ | ----------------------------------------- |\n| collection_name      | String       | Input parameter. The name of the Qdrant collection.             |\n| host                 | String       | Input parameter. The Qdrant server host.                        |\n| port                 | Integer      | Input parameter. The Qdrant server port.                        |\n| grpc_port            | Integer      | Input parameter. The Qdrant gRPC port.                          |\n| api_key              | SecretString | Input parameter. The API key for Qdrant.                        |\n| prefix               | String       | Input parameter. The prefix for Qdrant.                         |\n| timeout              | Integer      | Input parameter. The timeout for Qdrant operations.             |\n| path                 | String       | Input parameter. The path for Qdrant.                           |\n| url                  | String       | Input parameter. The URL for Qdrant.                            |\n| distance_func        | String       | Input parameter. The distance function for vector similarity.   |\n| content_payload_key  | String       | Input parameter. The content payload key.                       |\n| metadata_payload_key | String       | Input parameter. The metadata payload key.                      |\n| search_query         | String       | Input parameter. The query for similarity search.               |\n| ingest_data          | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding            | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results    | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "qdrant",
        "store",
        "vector"
      ]
    },
    "bundles-qdrant": {
      "title": "Qdrant",
      "slug": "/bundles-qdrant",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Qdrant** bundle.\n\n## Qdrant vector store\n\nThe **Qdrant** component reads and writes to Qdrant vector stores using an instance of `QdrantVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Qdrant vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Qdrant documentation](https://qdrant.tech/documentation/) or inspect [component code](/concepts-components#component-code).\n\n| Name                 | Type         | Description                               |\n| -------------------- | ------------ | ----------------------------------------- |\n| collection_name      | String       | Input parameter. The name of the Qdrant collection.             |\n| host                 | String       | Input parameter. The Qdrant server host.                        |\n| port                 | Integer      | Input parameter. The Qdrant server port.                        |\n| grpc_port            | Integer      | Input parameter. The Qdrant gRPC port.                          |\n| api_key              | SecretString | Input parameter. The API key for Qdrant.                        |\n| prefix               | String       | Input parameter. The prefix for Qdrant.                         |\n| timeout              | Integer      | Input parameter. The timeout for Qdrant operations.             |\n| path                 | String       | Input parameter. The path for Qdrant.                           |\n| url                  | String       | Input parameter. The URL for Qdrant.                            |\n| distance_func        | String       | Input parameter. The distance function for vector similarity.   |\n| content_payload_key  | String       | Input parameter. The content payload key.                       |\n| metadata_payload_key | String       | Input parameter. The metadata payload key.                      |\n| search_query         | String       | Input parameter. The query for similarity search.               |\n| ingest_data          | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding            | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results    | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "qdrant",
        "store",
        "vector"
      ]
    },
    "/bundles-queryrouter": {
      "title": "QueryRouter",
      "slug": "/bundles-queryrouter",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **QueryRouter** bundle.\n\nFor more information about QueryRouter features and functionality, see your services API documentation (OpenAI-compatible).\n\n## QueryRouter text generation\n\nThis component generates text using QueryRouters OpenAI-compatible API.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a QueryRouter model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### QueryRouter text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your QueryRouter API key for authentication. |\n| openai_api_base | String | Fixed to `https://api.queryrouter.ru/v1` in this bundle. |\n| model_name | String | Input parameter. The model to use for generation. Loaded dynamically from `/v1/models`. |\n| model_kwargs | Dict | Input parameter. Additional keyword arguments passed to the model. |\n| temperature | Float | Input parameter. Controls randomness in the output. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| json_mode | Bool | Input parameter. If true, enforces JSON output. |\n| seed | Integer | Input parameter. Seed for reproducibility. |\n| max_retries | Integer | Input parameter. Maximum number of request retries. |\n| timeout | Integer | Input parameter. Request timeout (seconds). |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "queryrouter",
        "response",
        "smart",
        "text"
      ]
    },
    "bundles-queryrouter": {
      "title": "QueryRouter",
      "slug": "/bundles-queryrouter",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **QueryRouter** bundle.\n\nFor more information about QueryRouter features and functionality, see your services API documentation (OpenAI-compatible).\n\n## QueryRouter text generation\n\nThis component generates text using QueryRouters OpenAI-compatible API.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a QueryRouter model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### QueryRouter text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| api_key | SecretString | Input parameter. Your QueryRouter API key for authentication. |\n| openai_api_base | String | Fixed to `https://api.queryrouter.ru/v1` in this bundle. |\n| model_name | String | Input parameter. The model to use for generation. Loaded dynamically from `/v1/models`. |\n| model_kwargs | Dict | Input parameter. Additional keyword arguments passed to the model. |\n| temperature | Float | Input parameter. Controls randomness in the output. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| json_mode | Bool | Input parameter. If true, enforces JSON output. |\n| seed | Integer | Input parameter. Seed for reproducibility. |\n| max_retries | Integer | Input parameter. Maximum number of request retries. |\n| timeout | Integer | Input parameter. Request timeout (seconds). |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "queryrouter",
        "response",
        "smart",
        "text"
      ]
    },
    "/bundles-redis": {
      "title": "Redis",
      "slug": "/bundles-redis",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Redis** bundle.\n\n## Redis Chat Memory\n\nThe **Redis Chat Memory** component retrieves and stores chat messages using Redis memory storage.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n### Redis Chat Memory parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| host | hostname | Input parameter. The IP address or hostname. |\n| port | port | Input parameter. The Redis Port Number. |\n| database | database | Input parameter. The Redis database. |\n| username | Username | Input parameter. The Redis username. |\n| password | Password | Input parameter. The password for the username. |\n| key_prefix | Key prefix | Input parameter. The key prefix. |\n| session_id | Session ID | Input parameter. The unique session identifier for the message. |\n\n## Redis vector store\n\nThe **Redis** vector store component reads and writes to Redis vector stores using an instance of [`RedisVectorStore`](https://python.langchain.com/docs/integrations/vectorstores/redis/).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Redis vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Redis documentation](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/) or inspect [component code](/concepts-components#component-code).\n\n| Name              | Type         | Description                               |\n| ----------------- | ------------ | ----------------------------------------- |\n| redis_server_url  | SecretString | Input parameter. The Redis server connection string. |\n| redis_index_name  | String       | Input parameter. The name of the Redis index. |\n| code              | String       | Input parameter. Additional custom code for Redis, if supported. |\n| schema            | String       | Input parameter. The schema for Redis index. |\n| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |\n| search_query      | String       | Input parameter. The query for similarity search. |\n| embedding         | Embeddings   | Input parameter. The embedding function to use. |\n| number_of_results | Integer      | Input parameter. The number of results to return in search. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "chat",
        "history",
        "memory",
        "message",
        "note",
        "parameters",
        "redis",
        "store",
        "vector"
      ]
    },
    "bundles-redis": {
      "title": "Redis",
      "slug": "/bundles-redis",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Redis** bundle.\n\n## Redis Chat Memory\n\nThe **Redis Chat Memory** component retrieves and stores chat messages using Redis memory storage.\n\nChat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.\n\nFor more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).\n\n### Redis Chat Memory parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| host | hostname | Input parameter. The IP address or hostname. |\n| port | port | Input parameter. The Redis Port Number. |\n| database | database | Input parameter. The Redis database. |\n| username | Username | Input parameter. The Redis username. |\n| password | Password | Input parameter. The password for the username. |\n| key_prefix | Key prefix | Input parameter. The key prefix. |\n| session_id | Session ID | Input parameter. The unique session identifier for the message. |\n\n## Redis vector store\n\nThe **Redis** vector store component reads and writes to Redis vector stores using an instance of [`RedisVectorStore`](https://python.langchain.com/docs/integrations/vectorstores/redis/).\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Redis vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Redis documentation](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/) or inspect [component code](/concepts-components#component-code).\n\n| Name              | Type         | Description                               |\n| ----------------- | ------------ | ----------------------------------------- |\n| redis_server_url  | SecretString | Input parameter. The Redis server connection string. |\n| redis_index_name  | String       | Input parameter. The name of the Redis index. |\n| code              | String       | Input parameter. Additional custom code for Redis, if supported. |\n| schema            | String       | Input parameter. The schema for Redis index. |\n| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |\n| search_query      | String       | Input parameter. The query for similarity search. |\n| embedding         | Embeddings   | Input parameter. The embedding function to use. |\n| number_of_results | Integer      | Input parameter. The number of results to return in search. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "chat",
        "history",
        "memory",
        "message",
        "note",
        "parameters",
        "redis",
        "store",
        "vector"
      ]
    },
    "/bundles-sambanova": {
      "title": "SambaNova",
      "slug": "/bundles-sambanova",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **SambaNova** bundle.\n\nFor more information about SambaNova features and functionality used by SambaNova components, see the [SambaNova Cloud documentation](https://cloud.sambanova.ai/).\n\n## SambaNova text generation\n\nThis component generates text using SambaNova LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a SambaNova model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### SambaNova text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| sambanova_url | String | Input parameter. Base URL path for API requests. Default: `https://api.sambanova.ai/v1/chat/completions`. |\n| sambanova_api_key | SecretString | Input parameter. Your SambaNova API Key. |\n| model_name | String | Input parameter. The name of the SambaNova model to use. Options include various Llama models. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "sambanova",
        "smart",
        "text"
      ]
    },
    "bundles-sambanova": {
      "title": "SambaNova",
      "slug": "/bundles-sambanova",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **SambaNova** bundle.\n\nFor more information about SambaNova features and functionality used by SambaNova components, see the [SambaNova Cloud documentation](https://cloud.sambanova.ai/).\n\n## SambaNova text generation\n\nThis component generates text using SambaNova LLMs.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a SambaNova model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### SambaNova text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| sambanova_url | String | Input parameter. Base URL path for API requests. Default: `https://api.sambanova.ai/v1/chat/completions`. |\n| sambanova_api_key | SecretString | Input parameter. Your SambaNova API Key. |\n| model_name | String | Input parameter. The name of the SambaNova model to use. Options include various Llama models. |\n| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "sambanova",
        "smart",
        "text"
      ]
    },
    "/bundles-searchapi": {
      "title": "SearchApi",
      "slug": "/bundles-searchapi",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **SearchApi** bundle.\n\nFor more information, see the [SearchApi documentation](https://www.searchapi.io/docs/google).\n\n## SearchApi web search\n\nThis component calls the SearchApi API to run Google, Bing, and DuckDuckGo web searches.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### SearchApi web search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| engine | String | Input parameter. The search engine to use. Default: `google`. |\n| api_key | SecretString | Input parameter. The API key for authenticating with SearchApi. |\n| input_value | String | Input parameter. The search query or input for the API call. |\n| max_results | Integer | Input parameter. The maximum number of search results to return. Default: `5`. |\n| max_snippet_length | Integer | Input parameter. The maximum length of the snippet to return. Default: `100`. |\n| search_params | Dict | Input parameter. Additional key-value pairs to customize the request. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**Google** bundle](/bundles-google)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bing",
        "bundles",
        "duckduckgo",
        "google",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "bundles-searchapi": {
      "title": "SearchApi",
      "slug": "/bundles-searchapi",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **SearchApi** bundle.\n\nFor more information, see the [SearchApi documentation](https://www.searchapi.io/docs/google).\n\n## SearchApi web search\n\nThis component calls the SearchApi API to run Google, Bing, and DuckDuckGo web searches.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### SearchApi web search parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| engine | String | Input parameter. The search engine to use. Default: `google`. |\n| api_key | SecretString | Input parameter. The API key for authenticating with SearchApi. |\n| input_value | String | Input parameter. The search query or input for the API call. |\n| max_results | Integer | Input parameter. The maximum number of search results to return. Default: `5`. |\n| max_snippet_length | Integer | Input parameter. The maximum length of the snippet to return. Default: `100`. |\n| search_params | Dict | Input parameter. Additional key-value pairs to customize the request. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**Google** bundle](/bundles-google)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bing",
        "bundles",
        "duckduckgo",
        "google",
        "parameters",
        "search",
        "searchapi",
        "web"
      ]
    },
    "/bundles-serper": {
      "title": "Serper",
      "slug": "/bundles-serper",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Serper** bundle.\n\nFor more information, see the [Serper documentation](https://serper.dev/).\n\n## Serper Google Search API\n\nThis component allows you to call the Serper.dev Google Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Google Serper API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Serper API Key** (`serper_api_key`) | SecretString | Input parameter. An API key for Serper.dev API authentication. |\n| **Input Value** (`input_value`) | String | Input parameter. The search query input. |\n| **Number of Results** (`k`) | Integer | Input parameter. The number of search results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**Google** bundle](/bundles-google)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bing",
        "bundles",
        "duckduckgo",
        "google",
        "input",
        "key",
        "number",
        "parameters",
        "results",
        "search",
        "serper",
        "value",
        "web"
      ]
    },
    "bundles-serper": {
      "title": "Serper",
      "slug": "/bundles-serper",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Serper** bundle.\n\nFor more information, see the [Serper documentation](https://serper.dev/).\n\n## Serper Google Search API\n\nThis component allows you to call the Serper.dev Google Search API.\n\nIt returns a list of search results as a [`DataFrame`](/data-types#dataframe).\n\n### Google Serper API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| **Serper API Key** (`serper_api_key`) | SecretString | Input parameter. An API key for Serper.dev API authentication. |\n| **Input Value** (`input_value`) | String | Input parameter. The search query input. |\n| **Number of Results** (`k`) | Integer | Input parameter. The number of search results to return. |\n\n## See also\n\n* [**Web Search** component](/components-data#web-search)\n* [**Google** bundle](/bundles-google)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bing",
        "bundles",
        "duckduckgo",
        "google",
        "input",
        "key",
        "number",
        "parameters",
        "results",
        "search",
        "serper",
        "value",
        "web"
      ]
    },
    "/bundles-supabase": {
      "title": "Supabase",
      "slug": "/bundles-supabase",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Supabase** bundle.\n\n## Supabase vector store\n\nThe **Supabase** component reads and writes to Supabase vector stores using an instance of `SupabaseVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Supabase vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Supabase documentation](https://supabase.com/docs/guides/ai) or inspect [component code](/concepts-components#component-code).\n\n| Name                | Type         | Description                               |\n| ------------------- | ------------ | ----------------------------------------- |\n| supabase_url        | String       | Input parameter. The URL of the Supabase instance.              |\n| supabase_service_key| SecretString | Input parameter. The service key for Supabase authentication.   |\n| table_name          | String       | Input parameter. The name of the table in Supabase.             |\n| query_name          | String       | Input parameter. The name of the query to use.                  |\n| search_query        | String       | Input parameter. The query for similarity search.               |\n| ingest_data         | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding           | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results   | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "supabase",
        "vector"
      ]
    },
    "bundles-supabase": {
      "title": "Supabase",
      "slug": "/bundles-supabase",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Supabase** bundle.\n\n## Supabase vector store\n\nThe **Supabase** component reads and writes to Supabase vector stores using an instance of `SupabaseVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Supabase vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Supabase documentation](https://supabase.com/docs/guides/ai) or inspect [component code](/concepts-components#component-code).\n\n| Name                | Type         | Description                               |\n| ------------------- | ------------ | ----------------------------------------- |\n| supabase_url        | String       | Input parameter. The URL of the Supabase instance.              |\n| supabase_service_key| SecretString | Input parameter. The service key for Supabase authentication.   |\n| table_name          | String       | Input parameter. The name of the table in Supabase.             |\n| query_name          | String       | Input parameter. The name of the query to use.                  |\n| search_query        | String       | Input parameter. The query for similarity search.               |\n| ingest_data         | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding           | Embeddings   | Input parameter. The embedding function to use.                 |\n| number_of_results   | Integer      | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "supabase",
        "vector"
      ]
    },
    "/bundles-upstash": {
      "title": "Upstash",
      "slug": "/bundles-upstash",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Upstash** bundle.\n\n## Upstash vector store\n\nThe **Upstash** component reads and writes to Upstash vector stores using an instance of `UpstashVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Upstash vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Upstash documentation](https://upstash.com/docs/introduction) or inspect [component code](/concepts-components#component-code).\n\n| Name            | Type         | Description                               |\n| --------------- | ------------ | ----------------------------------------- |\n| index_url       | String       | Input parameter. The URL of the Upstash index.              |\n| index_token     | SecretString | Input parameter. The token for the Upstash index.           |\n| text_key        | String       | Input parameter. The key in the record to use as text.      |\n| namespace       | String       | Input parameter. The namespace for the index.                   |\n| search_query    | String       | Input parameter. The query for similarity search.               |\n| metadata_filter | String       | Input parameter. Filter documents by metadata.             |\n| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding       | Embeddings   | Input parameter. The embedding function to use.      |\n| number_of_results | Integer    | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "upstash",
        "vector"
      ]
    },
    "bundles-upstash": {
      "title": "Upstash",
      "slug": "/bundles-upstash",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Upstash** bundle.\n\n## Upstash vector store\n\nThe **Upstash** component reads and writes to Upstash vector stores using an instance of `UpstashVectorStore`.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Upstash vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Upstash documentation](https://upstash.com/docs/introduction) or inspect [component code](/concepts-components#component-code).\n\n| Name            | Type         | Description                               |\n| --------------- | ------------ | ----------------------------------------- |\n| index_url       | String       | Input parameter. The URL of the Upstash index.              |\n| index_token     | SecretString | Input parameter. The token for the Upstash index.           |\n| text_key        | String       | Input parameter. The key in the record to use as text.      |\n| namespace       | String       | Input parameter. The namespace for the index.                   |\n| search_query    | String       | Input parameter. The query for similarity search.               |\n| metadata_filter | String       | Input parameter. Filter documents by metadata.             |\n| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |\n| embedding       | Embeddings   | Input parameter. The embedding function to use.      |\n| number_of_results | Integer    | Input parameter. The number of results to return in search.     |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "upstash",
        "vector"
      ]
    },
    "/bundles-vectara": {
      "title": "Vectara",
      "slug": "/bundles-vectara",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Vectara** bundle.\n\n## Vectara vector store\n\nThe **Vectara** component reads and writes to Vectara vector stores using an instance of `Vectara` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Vectara vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Vectara documentation](https://docs.vectara.com/docs/) or inspect [component code](/concepts-components#component-code).\n\n| Name             | Type         | Description                               |\n| ---------------- | ------------ | ----------------------------------------- |\n| vectara_customer_id | String     | Input parameter. The Vectara customer ID.                       |\n| vectara_corpus_id   | String     | Input parameter. The Vectara corpus ID.                         |\n| vectara_api_key   | SecretString | Input parameter. The Vectara API key.                           |\n| embedding         | Embeddings   | Input parameter. The embedding function to use (optional).      |\n| ingest_data       | List[Document/Data] | Input parameter. The data to be ingested into the vector store. |\n| search_query      | String       | Input parameter. The query for similarity search.               |\n| number_of_results | Integer      | Input parameter. The number of results to return in search.     |\n\n## Vectara RAG\n\nThis component enables Vectara's full end-to-end RAG capabilities with reranking options.\n\nThis component uses a `Vectara` vector store to execute the vector search and reranking functions, and then outputs an **Answer** string in [`Message`](/data-types#message) format.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "answer",
        "bundles",
        "parameters",
        "rag",
        "store",
        "vectara",
        "vector"
      ]
    },
    "bundles-vectara": {
      "title": "Vectara",
      "slug": "/bundles-vectara",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Vectara** bundle.\n\n## Vectara vector store\n\nThe **Vectara** component reads and writes to Vectara vector stores using an instance of `Vectara` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n### Vectara vector store parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Vectara documentation](https://docs.vectara.com/docs/) or inspect [component code](/concepts-components#component-code).\n\n| Name             | Type         | Description                               |\n| ---------------- | ------------ | ----------------------------------------- |\n| vectara_customer_id | String     | Input parameter. The Vectara customer ID.                       |\n| vectara_corpus_id   | String     | Input parameter. The Vectara corpus ID.                         |\n| vectara_api_key   | SecretString | Input parameter. The Vectara API key.                           |\n| embedding         | Embeddings   | Input parameter. The embedding function to use (optional).      |\n| ingest_data       | List[Document/Data] | Input parameter. The data to be ingested into the vector store. |\n| search_query      | String       | Input parameter. The query for similarity search.               |\n| number_of_results | Integer      | Input parameter. The number of results to return in search.     |\n\n## Vectara RAG\n\nThis component enables Vectara's full end-to-end RAG capabilities with reranking options.\n\nThis component uses a `Vectara` vector store to execute the vector search and reranking functions, and then outputs an **Answer** string in [`Message`](/data-types#message) format.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "answer",
        "bundles",
        "parameters",
        "rag",
        "store",
        "vectara",
        "vector"
      ]
    },
    "/bundles-vertexai": {
      "title": "Vertex AI",
      "slug": "/bundles-vertexai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Vertex AI** bundle.\n\nFor more information about Vertex AI features and functionality used by Vertex AI components, see the [Vertex AI documentation](https://cloud.google.com/vertex-ai).\n\nFor other Google components, see the [**Google** bundle](/bundles-google).\n\n## Vertex AI text generation\n\nThe **Vertex AI** component generates text using Google Vertex AI models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Vertex AI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Vertex AI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| credentials | File | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |\n| model_name | String | Input parameter. The name of the Vertex AI model to use. Default: \"gemini-1.5-pro\". |\n| project | String | Input parameter. The project ID. |\n| location | String | Input parameter. The location for the Vertex AI API. Default: \"us-central1\". |\n| max_output_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 1. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.0. |\n| top_k | Integer | Input parameter. The number of highest-probability vocabulary tokens to keep for top-k-filtering. |\n| top_p | Float | Input parameter. The cumulative probability of all highest-probability vocabulary tokens that are kept for nucleus sampling. Default: 0.95. |\n| verbose | Boolean | Input parameter. Whether to print verbose output. Default: `false`. |\n\nFor more information about Vertex AI text generation parameters, see the [Vertex AI content generation parameters documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters#token-sampling-parameters).\n\n## Vertex AI Embeddings\n\nThe **Vertex AI Embeddings** component is a wrapper around the [Google Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) for embedding generation.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Vertex AI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| credentials | Credentials | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |\n| location | String | Input parameter. The default location to use when making API calls. Default: `us-central1`. |\n| max_output_tokens | Integer | Input parameter. The token limit determines the maximum amount of text output from one prompt. Default: `128`. |\n| model_name | String | Input parameter. The name of the Vertex AI large language model. Default: `text-bison`. |\n| project | String | Input parameter. The default GCP project to use when making Vertex API calls. |\n| request_parallelism | Integer | Input parameter. The amount of parallelism allowed for requests issued to Vertex AI models. Default: `5`. |\n| temperature | Float | Input parameter. Tunes the degree of randomness in text generations. Should be a non-negative value. Default: `0`. |\n| top_k | Integer | Input parameter. How the model selects tokens for output. The next token is selected from the top `k` tokens. Default: `40`. |\n| top_p | Float | Input parameter. Tokens are selected from the most probable to least until the sum of their probabilities exceeds the top `p` value. Default: `0.95`. |\n| tuned_model_name | String | Input parameter. The name of a tuned model. If provided, `model_name` is ignored. |\n| verbose | Boolean | Input parameter. This parameter controls the level of detail in the output. When set to `true`, it prints internal states of the chain to help debug. Default: `false`. |\n| embeddings | Embeddings | Output parameter. An instance for generating embeddings using Vertex AI. |\n\n## Create a service account for Vertex AI authentication {#vertexai-credentials}\n\nThe **Vertex AI Embeddings** and **Vertex AI** components authenticate with the [Google Vertex AI API](https://console.cloud.google.com/marketplace/product/google/aiplatform.googleapis.com) using a service account JSON file.\n\nThese components require that you provide a service account JSON file in the **Credentials** parameter (`credentials`).\nYou can either provide the file directly in your component's settings or you can use a [`GOOGLE_APPLICATION_CREDENTIALS` environment variable](https://cloud.google.com/docs/authentication/application-default-credentials) to reference the path to your JSON file.\n\nThe following steps explain how to create a service account for Langflow, create a service account JSON key file, and then use the JSON key file in Langflow:\n\n1. If you haven't done so already, [enable the Vertex AI APIs in your Google Cloud project](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n\n2. Create a [service account](https://cloud.google.com/iam/docs/service-account-overview) in your Google Cloud project.\n\n    It is recommended that you [create a custom service account for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/custom-service-account) because Langflow uses this account to access the Vertex AI APIs.\n\n3. Assign the **Vertex AI Service Agent** role to the service account.\n\n    This role allows Langflow to access Vertex AI resources.\n    For more information, see [Vertex AI access control with IAM](https://cloud.google.com/vertex-ai/docs/general/access-control).\n\n4. [Create a service account key](https://cloud.google.com/iam/docs/keys-create-delete#console) in JSON format for your Langflow Vertex AI service account.\n\n    When you click **Create**, the JSON key file is downloaded.\n\n5. In Langflow, create or open a flow with the **Vertex AI** or **Vertex AI Embeddings** component.\n\n6. In the component's **Credentials** field, do one of the following:\n\n    * Select your service account JSON file directly.\n\n    * Leave the **Credentials** field empty if you want to pull the key file from an environment variable.\n    Then, you must provide the path to your JSON file in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable that is set somewhere relative to your Langflow instance.\n    For example, you can set the environment variable in your terminal, your Langflow `.env` file, or otherwise in the environment where your Langflow server or application runs.\n\n    ![Select a local JSON key file for the Vertex AI component](/img/configure-vertex-ai-credentials-in-langflow.gif)\n\n7. Run your flow to verify that Langflow can use the service account credentials successfully.\n\n    For example, in a flow with **Chat Input**, **Chat Output**, and **Vertex AI** components, you can open the **Playground** and try chatting with the LLM.\n    If the LLM responds, then the credentials are configured correctly.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "account",
        "agent",
        "authentication",
        "bundles",
        "chat",
        "create",
        "credentials",
        "embeddings",
        "function",
        "generation",
        "google",
        "input",
        "language",
        "model",
        "output",
        "parameters",
        "playground",
        "response",
        "service",
        "smart"
      ]
    },
    "bundles-vertexai": {
      "title": "Vertex AI",
      "slug": "/bundles-vertexai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Vertex AI** bundle.\n\nFor more information about Vertex AI features and functionality used by Vertex AI components, see the [Vertex AI documentation](https://cloud.google.com/vertex-ai).\n\nFor other Google components, see the [**Google** bundle](/bundles-google).\n\n## Vertex AI text generation\n\nThe **Vertex AI** component generates text using Google Vertex AI models.\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use a Vertex AI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### Vertex AI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| credentials | File | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |\n| model_name | String | Input parameter. The name of the Vertex AI model to use. Default: \"gemini-1.5-pro\". |\n| project | String | Input parameter. The project ID. |\n| location | String | Input parameter. The location for the Vertex AI API. Default: \"us-central1\". |\n| max_output_tokens | Integer | Input parameter. The maximum number of tokens to generate. |\n| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 1. |\n| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.0. |\n| top_k | Integer | Input parameter. The number of highest-probability vocabulary tokens to keep for top-k-filtering. |\n| top_p | Float | Input parameter. The cumulative probability of all highest-probability vocabulary tokens that are kept for nucleus sampling. Default: 0.95. |\n| verbose | Boolean | Input parameter. Whether to print verbose output. Default: `false`. |\n\nFor more information about Vertex AI text generation parameters, see the [Vertex AI content generation parameters documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters#token-sampling-parameters).\n\n## Vertex AI Embeddings\n\nThe **Vertex AI Embeddings** component is a wrapper around the [Google Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) for embedding generation.\n\nFor more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).\n\n### Vertex AI Embeddings parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| credentials | Credentials | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |\n| location | String | Input parameter. The default location to use when making API calls. Default: `us-central1`. |\n| max_output_tokens | Integer | Input parameter. The token limit determines the maximum amount of text output from one prompt. Default: `128`. |\n| model_name | String | Input parameter. The name of the Vertex AI large language model. Default: `text-bison`. |\n| project | String | Input parameter. The default GCP project to use when making Vertex API calls. |\n| request_parallelism | Integer | Input parameter. The amount of parallelism allowed for requests issued to Vertex AI models. Default: `5`. |\n| temperature | Float | Input parameter. Tunes the degree of randomness in text generations. Should be a non-negative value. Default: `0`. |\n| top_k | Integer | Input parameter. How the model selects tokens for output. The next token is selected from the top `k` tokens. Default: `40`. |\n| top_p | Float | Input parameter. Tokens are selected from the most probable to least until the sum of their probabilities exceeds the top `p` value. Default: `0.95`. |\n| tuned_model_name | String | Input parameter. The name of a tuned model. If provided, `model_name` is ignored. |\n| verbose | Boolean | Input parameter. This parameter controls the level of detail in the output. When set to `true`, it prints internal states of the chain to help debug. Default: `false`. |\n| embeddings | Embeddings | Output parameter. An instance for generating embeddings using Vertex AI. |\n\n## Create a service account for Vertex AI authentication {#vertexai-credentials}\n\nThe **Vertex AI Embeddings** and **Vertex AI** components authenticate with the [Google Vertex AI API](https://console.cloud.google.com/marketplace/product/google/aiplatform.googleapis.com) using a service account JSON file.\n\nThese components require that you provide a service account JSON file in the **Credentials** parameter (`credentials`).\nYou can either provide the file directly in your component's settings or you can use a [`GOOGLE_APPLICATION_CREDENTIALS` environment variable](https://cloud.google.com/docs/authentication/application-default-credentials) to reference the path to your JSON file.\n\nThe following steps explain how to create a service account for Langflow, create a service account JSON key file, and then use the JSON key file in Langflow:\n\n1. If you haven't done so already, [enable the Vertex AI APIs in your Google Cloud project](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n\n2. Create a [service account](https://cloud.google.com/iam/docs/service-account-overview) in your Google Cloud project.\n\n    It is recommended that you [create a custom service account for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/custom-service-account) because Langflow uses this account to access the Vertex AI APIs.\n\n3. Assign the **Vertex AI Service Agent** role to the service account.\n\n    This role allows Langflow to access Vertex AI resources.\n    For more information, see [Vertex AI access control with IAM](https://cloud.google.com/vertex-ai/docs/general/access-control).\n\n4. [Create a service account key](https://cloud.google.com/iam/docs/keys-create-delete#console) in JSON format for your Langflow Vertex AI service account.\n\n    When you click **Create**, the JSON key file is downloaded.\n\n5. In Langflow, create or open a flow with the **Vertex AI** or **Vertex AI Embeddings** component.\n\n6. In the component's **Credentials** field, do one of the following:\n\n    * Select your service account JSON file directly.\n\n    * Leave the **Credentials** field empty if you want to pull the key file from an environment variable.\n    Then, you must provide the path to your JSON file in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable that is set somewhere relative to your Langflow instance.\n    For example, you can set the environment variable in your terminal, your Langflow `.env` file, or otherwise in the environment where your Langflow server or application runs.\n\n    ![Select a local JSON key file for the Vertex AI component](/img/configure-vertex-ai-credentials-in-langflow.gif)\n\n7. Run your flow to verify that Langflow can use the service account credentials successfully.\n\n    For example, in a flow with **Chat Input**, **Chat Output**, and **Vertex AI** components, you can open the **Playground** and try chatting with the LLM.\n    If the LLM responds, then the credentials are configured correctly.",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "account",
        "agent",
        "authentication",
        "bundles",
        "chat",
        "create",
        "credentials",
        "embeddings",
        "function",
        "generation",
        "google",
        "input",
        "language",
        "model",
        "output",
        "parameters",
        "playground",
        "response",
        "service",
        "smart"
      ]
    },
    "/bundles-weaviate": {
      "title": "Weaviate",
      "slug": "/bundles-weaviate",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Weaviate** bundle.\n\n## Weaviate vector store\n\nThe **Weaviate** component reads and writes to Weaviate vector stores using an instance of `Weaviate` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Weaviate parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Weaviate documentation](https://weaviate.io/developers/weaviate) or inspect [component code](/concepts-components#component-code).\n\n| Name          | Type         | Description                               |\n|---------------|--------------|-------------------------------------------|\n| weaviate_url  | String | Input parameter. The default instance URL. |\n| api_key       | SecretString | Input parameter. The optional API key for authentication. |\n| index_name    | String | Input parameter. The optional index name. |\n| text_key      | String | Input parameter. The default text extraction key. |\n| input         | Data or DataFrame | Input parameter. The document or record.  |\n| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding     | Embeddings | Input parameter. Connect an [embedding model component](/components-embedding-models). |\n| number_of_results | Integer | Input parameter. The number of search results to return. Default: `4`. |\n| search_by_text | Boolean | Input parameter. Indicates whether to search by text. Default: Disabled (`false`). |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "vector",
        "weaviate"
      ]
    },
    "bundles-weaviate": {
      "title": "Weaviate",
      "slug": "/bundles-weaviate",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Weaviate** bundle.\n\n## Weaviate vector store\n\nThe **Weaviate** component reads and writes to Weaviate vector stores using an instance of `Weaviate` vector store.\n\n<details>\n<summary>About vector store instances</summary>\n\n</details>\n\n**Note:** For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).\n\n### Weaviate parameters\n\nYou can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.\n\nFor information about accepted values and functionality, see the [Weaviate documentation](https://weaviate.io/developers/weaviate) or inspect [component code](/concepts-components#component-code).\n\n| Name          | Type         | Description                               |\n|---------------|--------------|-------------------------------------------|\n| weaviate_url  | String | Input parameter. The default instance URL. |\n| api_key       | SecretString | Input parameter. The optional API key for authentication. |\n| index_name    | String | Input parameter. The optional index name. |\n| text_key      | String | Input parameter. The default text extraction key. |\n| input         | Data or DataFrame | Input parameter. The document or record.  |\n| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |\n| embedding     | Embeddings | Input parameter. Connect an [embedding model component](/components-embedding-models). |\n| number_of_results | Integer | Input parameter. The number of search results to return. Default: `4`. |\n| search_by_text | Boolean | Input parameter. Indicates whether to search by text. Default: Disabled (`false`). |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "bundles",
        "note",
        "parameters",
        "store",
        "vector",
        "weaviate"
      ]
    },
    "/bundles-wikipedia": {
      "title": "Wikipedia",
      "slug": "/bundles-wikipedia",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Wikipedia** bundle.\n\n**Note:** Wikipedia components are useful [tools for agents](/agents-tools) that need reliable sources for information retrieval, such as tutoring chatbots or generic research assistants.\n\nRather than broad internet searches or recall from internal model data, these components provide specific data returned from Wikipedia API requests.\n\n## Wikidata API\n\nThis component performs a similarity search using the [Wikidata API](https://www.wikidata.org/wiki/Wikidata:REST_API).\n\nIt accepts a string text query, and it outputs a [`DataFrame`](/data-types#dataframe) containing the search results.\n\n## Wikipedia API\n\nThis component searches and retrieves information from Wikipedia with the [WikiMedia API](https://www.mediawiki.org/wiki/API:Main_page) search endpoints, and then outputs the response as a [`DataFrame`](/data-types#dataframe).\n\n### Wikipedia API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input_value | String | Input parameter. The search query input. |\n| lang | String | Input parameter. The language code for Wikipedia. Default: `en`. |\n| k | Integer | Input parameter. The number of results to return. |\n| load_all_available_meta | Boolean | Input parameter. Whether to load all available metadata. |\n| doc_content_chars_max | Integer | Input parameter. The maximum number of characters for document content. |\n\n## See also\n\n* [**API Request** component](/components-data#api-request)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "note",
        "parameters",
        "request",
        "wikidata",
        "wikipedia"
      ]
    },
    "bundles-wikipedia": {
      "title": "Wikipedia",
      "slug": "/bundles-wikipedia",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **Wikipedia** bundle.\n\n**Note:** Wikipedia components are useful [tools for agents](/agents-tools) that need reliable sources for information retrieval, such as tutoring chatbots or generic research assistants.\n\nRather than broad internet searches or recall from internal model data, these components provide specific data returned from Wikipedia API requests.\n\n## Wikidata API\n\nThis component performs a similarity search using the [Wikidata API](https://www.wikidata.org/wiki/Wikidata:REST_API).\n\nIt accepts a string text query, and it outputs a [`DataFrame`](/data-types#dataframe) containing the search results.\n\n## Wikipedia API\n\nThis component searches and retrieves information from Wikipedia with the [WikiMedia API](https://www.mediawiki.org/wiki/API:Main_page) search endpoints, and then outputs the response as a [`DataFrame`](/data-types#dataframe).\n\n### Wikipedia API parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| input_value | String | Input parameter. The search query input. |\n| lang | String | Input parameter. The language code for Wikipedia. Default: `en`. |\n| k | Integer | Input parameter. The number of results to return. |\n| load_all_available_meta | Boolean | Input parameter. Whether to load all available metadata. |\n| doc_content_chars_max | Integer | Input parameter. The maximum number of characters for document content. |\n\n## See also\n\n* [**API Request** component](/components-data#api-request)",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "api",
        "bundles",
        "note",
        "parameters",
        "request",
        "wikidata",
        "wikipedia"
      ]
    },
    "/bundles-xai": {
      "title": "xAI",
      "slug": "/bundles-xai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **xAI** bundle.\n\nFor more information about xAI features and functionality used by xAI components, see the [xAI documentation](https://x.ai/).\n\n## xAI text generation\n\nThe **xAI** component generates text using xAI models like [Grok](https://x.ai/grok).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an xAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### xAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |\n| model_name | String | Input parameter. The xAI model to use. Default: `grok-2-latest`. |\n| base_url | String | Input parameter. Base URL for API requests. Default: `https://api.x.ai/v1`. |\n| api_key | SecretString | Input parameter. Your xAI API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.1`. |\n| seed | Integer | Input parameter. Controls reproducibility of the job. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text",
        "xai"
      ]
    },
    "bundles-xai": {
      "title": "xAI",
      "slug": "/bundles-xai",
      "category": "components",
      "content": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.\n\nThis page describes the components that are available in the **xAI** bundle.\n\nFor more information about xAI features and functionality used by xAI components, see the [xAI documentation](https://x.ai/).\n\n## xAI text generation\n\nThe **xAI** component generates text using xAI models like [Grok](https://x.ai/grok).\n\nIt can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).\n\nUse the **Language Model** output when you want to use an xAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.\n\nFor more information, see [Language model components](/components-models).\n\n### xAI text generation parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |\n| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |\n| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |\n| model_name | String | Input parameter. The xAI model to use. Default: `grok-2-latest`. |\n| base_url | String | Input parameter. Base URL for API requests. Default: `https://api.x.ai/v1`. |\n| api_key | SecretString | Input parameter. Your xAI API key for authentication. |\n| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.1`. |\n| seed | Integer | Input parameter. Controls reproducibility of the job. |",
      "summary": "[**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.",
      "keywords": [
        "agent",
        "bundles",
        "function",
        "generation",
        "language",
        "model",
        "parameters",
        "response",
        "smart",
        "text",
        "xai"
      ]
    },
    "/components-agents": {
      "title": "Agents",
      "slug": "/components-agents",
      "category": "components",
      "content": "Langflow's **Agent** and **MCP Tools** components are critical for building agent flows.\nThese components define the behavior and capabilities of AI agents in your flows.\n\n## Examples of agent flows\n\nFor examples of flows using the **Agent** and **MCP Tools** components, see the following:\n\n* [Langflow quickstart](/get-started-quickstart): Start with the **Simple Agent** template, modify its tools, and then learn how to use an agent flow in an application.\n\n    The **Simple Agent** template creates a basic agent flow with an **Agent** component that can use two other Langflow components as tools.\n    The LLM specified in the **Agent** component's settings can use its own built-in functionality as well as the functionality provided by the connected tools when generating responses.\n\n* [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool): Create a multi-agent flow.\n\n* [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server): Use the **Agent** and **MCP Tools** components to implement the Model Context Protocol (MCP) in your flows.\n\n## Agent component {#agent-component}\n\nThe **Agent** component is the primary agent actor in your agent flows.\nThis component uses an LLM integration to respond to input, such as a chat message or file upload.\n\nThe agent can use the tools already available in the base LLM as well as additional tools that you connect to the **Agent** component's **Tools** port.\nYou can connect any Langflow component as a tool, including other **Agent** components and MCP servers through the [**MCP Tools** component](#mcp-connection).\n\nFor more information about using this component, see [Use Langflow agents](/agents).\n\n## MCP Tools component {#mcp-connection}\n\nThe **MCP Tools** component connects to a Model Context Protocol (MCP) server and exposes the MCP server's functions as tools for Langflow agents to use to respond to input.\n\nIn addition to publicly available MCP servers and your own custom-built MCP servers, you can connect Langflow MCP servers, which allow your agent to use your Langflow flows as tools.\nTo do this, use the **MCP Tools** component's [SSE mode](/mcp-client#mcp-sse-mode) to connect to your Langflow project's MCP server at the `/api/v1/mcp/sse` endpoint.\n\nFor more information, see [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server).\n\n<details>\n<summary>Earlier versions of the MCP Tools component</summary>\n\n* In Langflow version 1.5, the **MCP Connection** component was renamed to the **MCP Tools** component.\n* In Langflow version 1.3, the **MCP Tools (stdio)** and **MCP Tools (SSE)** components were removed and replaced by the unified **MCP Connection** component, which was later renamed to **MCP Tools**.\n\n</details>\n\n## See also\n\n* [**Message History** component](/components-helpers#message-history)\n* [Store chat memory](/memory#store-chat-memory)\n* [Bundles](/components-bundle-components)\n* [Legacy LangChain components](/bundles-langchain#legacy-langchain-components)",
      "summary": "Langflow's **Agent** and **MCP Tools** components are critical for building agent flows.\nThese components define the behavior and capabilities of AI agents in your flows.",
      "keywords": [
        "agent",
        "agents",
        "component",
        "connection",
        "examples",
        "flows",
        "history",
        "mcp",
        "message",
        "simple",
        "sse",
        "stdio",
        "tools"
      ]
    },
    "components-agents": {
      "title": "Agents",
      "slug": "/components-agents",
      "category": "components",
      "content": "Langflow's **Agent** and **MCP Tools** components are critical for building agent flows.\nThese components define the behavior and capabilities of AI agents in your flows.\n\n## Examples of agent flows\n\nFor examples of flows using the **Agent** and **MCP Tools** components, see the following:\n\n* [Langflow quickstart](/get-started-quickstart): Start with the **Simple Agent** template, modify its tools, and then learn how to use an agent flow in an application.\n\n    The **Simple Agent** template creates a basic agent flow with an **Agent** component that can use two other Langflow components as tools.\n    The LLM specified in the **Agent** component's settings can use its own built-in functionality as well as the functionality provided by the connected tools when generating responses.\n\n* [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool): Create a multi-agent flow.\n\n* [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server): Use the **Agent** and **MCP Tools** components to implement the Model Context Protocol (MCP) in your flows.\n\n## Agent component {#agent-component}\n\nThe **Agent** component is the primary agent actor in your agent flows.\nThis component uses an LLM integration to respond to input, such as a chat message or file upload.\n\nThe agent can use the tools already available in the base LLM as well as additional tools that you connect to the **Agent** component's **Tools** port.\nYou can connect any Langflow component as a tool, including other **Agent** components and MCP servers through the [**MCP Tools** component](#mcp-connection).\n\nFor more information about using this component, see [Use Langflow agents](/agents).\n\n## MCP Tools component {#mcp-connection}\n\nThe **MCP Tools** component connects to a Model Context Protocol (MCP) server and exposes the MCP server's functions as tools for Langflow agents to use to respond to input.\n\nIn addition to publicly available MCP servers and your own custom-built MCP servers, you can connect Langflow MCP servers, which allow your agent to use your Langflow flows as tools.\nTo do this, use the **MCP Tools** component's [SSE mode](/mcp-client#mcp-sse-mode) to connect to your Langflow project's MCP server at the `/api/v1/mcp/sse` endpoint.\n\nFor more information, see [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server).\n\n<details>\n<summary>Earlier versions of the MCP Tools component</summary>\n\n* In Langflow version 1.5, the **MCP Connection** component was renamed to the **MCP Tools** component.\n* In Langflow version 1.3, the **MCP Tools (stdio)** and **MCP Tools (SSE)** components were removed and replaced by the unified **MCP Connection** component, which was later renamed to **MCP Tools**.\n\n</details>\n\n## See also\n\n* [**Message History** component](/components-helpers#message-history)\n* [Store chat memory](/memory#store-chat-memory)\n* [Bundles](/components-bundle-components)\n* [Legacy LangChain components](/bundles-langchain#legacy-langchain-components)",
      "summary": "Langflow's **Agent** and **MCP Tools** components are critical for building agent flows.\nThese components define the behavior and capabilities of AI agents in your flows.",
      "keywords": [
        "agent",
        "agents",
        "component",
        "connection",
        "examples",
        "flows",
        "history",
        "mcp",
        "message",
        "simple",
        "sse",
        "stdio",
        "tools"
      ]
    },
    "/components-bundle-components": {
      "title": "About bundles",
      "slug": "/components-bundle-components",
      "category": "components",
      "content": "Bundles contain custom components that support specific third-party integrations with Langflow.\nYou add them to your flows and configure them in the same way as Langflow's core components.\n\nTo browse bundles, click  **Bundles** in the visual editor.\n\n## Bundle maintenance and documentation\n\nMany bundled components are developed by third-party contributors to the Langflow codebase.\n\nSome providers contribute documentation with their bundles, whereas others document their bundles in their own documentation.\nSome bundles have no documentation.\n\nTo find documentation for a specific bundled component, browse the Langflow docs and your provider's documentation.\nIf available, you can also find links to relevant documentation, such as API endpoints, through the component itself:\n\n1. Click the component to expose the [component's header menu](/concepts-components#component-menus).\n2. Click  **More**.\n3. Select **Docs**.\n\nThe Langflow documentation focuses on using bundles within flows.\nFor that reason, it focuses on the Langflow-specific configuration steps for bundled components.\nFor information about provider-specific features or APIs, see the provider's documentation.\n\n## Component parameters\n\n## Core components and bundles\n\n**Note:** The Langflow documentation doesn't list all bundles or components in bundles.\nFor the most accurate and up-to-date list of bundles and components for your version of Langflow, check  **Bundles** in the visual editor.\n\nIf you can't find a component that you used in an earlier version of Langflow, it may have been removed or marked as a [legacy component](#legacy-bundles).\n\nLangflow offers generic  **Core components** in addition to third-party, provider-specific bundles.\n\nIf you are looking for a specific service or integration, you can  **Search** components in the visual editor.\n\nIf all else fails, you can always create your own [custom components](/components-custom-components).\n\n## Legacy bundles\n\nThe following bundles include only legacy components.\n\n### CrewAI bundle\n\nReplace the following legacy CrewAI components with other agentic components, such as the [**Agent** component](/components-agents).\n\n<details>\n<summary>CrewAI Agent</summary>\n\nThis component represents CrewAI agents, allowing for the creation of specialized AI agents with defined roles goals and capabilities within a crew.\nFor more information, see the [CrewAI agents documentation](https://docs.crewai.com/core-concepts/Agents/).\n\nThis component accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| role | Role | Input parameter. The role of the agent. |\n| goal | Goal | Input parameter. The objective of the agent. |\n| backstory | Backstory | Input parameter. The backstory of the agent. |\n| tools | Tools | Input parameter. The tools at the agent's disposal. |\n| llm | Language Model | Input parameter. The language model that runs the agent. |\n| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |\n| verbose | Verbose | Input parameter. This enables verbose output. |\n| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |\n| kwargs | kwargs | Input parameter. Additional keyword arguments for the agent. |\n| output | Agent | Output parameter. The constructed CrewAI Agent object. |\n\n</details>\n\n<details>\n<summary>CrewAI Hierarchical Crew, CrewAI Hierarchical Task</summary>\n\nThe **CrewAI Hierarchical Crew** component represents a group of agents managing how they should collaborate and the tasks they should perform in a hierarchical structure. This component allows for the creation of a crew with a manager overseeing the task execution.\nFor more information, see the [CrewAI hierarchical crew documentation](https://docs.crewai.com/how-to/Hierarchical/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| agents | Agents | Input parameter. The list of Agent objects representing the crew members. |\n| tasks | Tasks | Input parameter. The list of HierarchicalTask objects representing the tasks to be executed. |\n| manager_llm | Manager LLM | Input parameter. The language model for the manager agent. |\n| manager_agent | Manager Agent | Input parameter. The specific agent to act as the manager. |\n| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |\n| memory | Memory | Input parameter. The memory configuration for the crew. |\n| use_cache | Use Cache | Input parameter. This enables caching of results. |\n| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |\n| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |\n| crew | Crew | Output parameter. The constructed Crew object with hierarchical task execution. |\n\n</details>\n\n<details>\n<summary>CrewAI Sequential Crew, CrewAI Sequential Task</summary>\n\nThe **CrewAI Sequential Crew** component represents a group of agents with tasks that are executed sequentially. This component allows for the creation of a crew that performs tasks in a specific order.\nFor more information, see the [CrewAI sequential crew documentation](https://docs.crewai.com/how-to/Sequential/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| tasks | Tasks | Input parameter. The list of SequentialTask objects representing the tasks to be executed. |\n| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |\n| memory | Memory | Input parameter. The memory configuration for the crew. |\n| use_cache | Use Cache | Input parameter. This enables caching of results. |\n| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |\n| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |\n| crew | Crew | Output parameter. The constructed Crew object with sequential task execution. |\n\n</details>\n\n<details>\n<summary>CrewAI Sequential Task Agent</summary>\n\nThis component creates a CrewAI Task and its associated agent allowing for the definition of sequential tasks with specific agent roles and capabilities.\nFor more information, see the [CrewAI sequential agents documentation](https://docs.crewai.com/how-to/Sequential/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| role | Role | Input parameter. The role of the agent. |\n| goal | Goal | Input parameter. The objective of the agent. |\n| backstory | Backstory | Input parameter. The backstory of the agent. |\n| tools | Tools | Input parameter. The tools at the agent's disposal. |\n| llm | Language Model | Input parameter. The language model that runs the agent. |\n| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |\n| verbose | Verbose | Input parameter. This enables verbose output. |\n| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |\n| agent_kwargs | Agent kwargs | Input parameter. The additional kwargs for the agent. |\n| task_description | Task Description | Input parameter. The descriptive text detailing the task's purpose and execution. |\n| expected_output | Expected Task Output | Input parameter. The clear definition of the expected task outcome. |\n| async_execution | Async Execution | Input parameter. Boolean flag indicating asynchronous task execution. |\n| previous_task | Previous Task | Input parameter. The previous task in the sequence for chaining. |\n| task_output | Sequential Task | Output parameter. The list of SequentialTask objects representing the created tasks. |\n\n</details>\n\n### Embeddings bundle\n\n* **Embedding Similarity**: Replaced by built-in similarity search functionality in vector store components.\n* **Text Embedder**: Replaced by the embedding model components.\n\n### Vector Stores bundle\n\nThis bundle contains only the legacy **Local DB** component.\nAll other vector store components can be found within their respective provider-specific bundles, such as the [**DataStax** bundle](/bundles-datastax).\n\n<details>\n<summary>Local DB</summary>\n\nReplace the **Local DB** component with the **Chroma DB** vector store component (in the **Chroma** bundle) or another vector store component.\n\nThe **Local DB** component reads and writes to a persistent, in-memory Chroma DB instance intended for use with Langflow.\nIt has separate modes for reads and writes, automatic collection management, and default persistence in your Langflow cache directory.\n\nSet the **Mode** parameter to reflect the operation you want the component to perform, and then configure the other parameters accordingly.\nSome parameters are only available for one mode.\n\n</Tabs>\n\n</details>\n\n### Zep bundle\n\n<details>\n<summary>Zep Chat Memory</summary>\n\nThe **Zep Chat Memory** component is a legacy component.\nReplace this component with the [**Message History** component](/components-helpers#message-history).\n\nThis component creates a `ZepChatMessageHistory` instance, enabling storage and retrieval of chat messages using Zep, a memory server for LLMs.\n\nIt accepts the following parameters:\n\n| Name          | Type          | Description                                               |\n|---------------|---------------|-----------------------------------------------------------|\n| url           | MessageText   | Input parameter. The URL of the Zep instance. Required. |\n| api_key       | SecretString  | Input parameter. The API Key for authentication with the Zep instance. |\n| api_base_path | Dropdown      | Input parameter. The API version to use. Options include api/v1 or api/v2. |\n| session_id    | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |\n| message_history | BaseChatMessageHistory  | Output parameter. An instance of ZepChatMessageHistory for the session. |\n\n</details>\n\n## See also\n\n* [LangWatch observability and evaluation](/integrations-langwatch)",
      "summary": "Bundles contain custom components that support specific third-party integrations with Langflow.\nYou add them to your flows and configure them in the same way as Langflow's core components.",
      "keywords": [
        "agent",
        "bundle",
        "bundles",
        "chat",
        "chroma",
        "component",
        "components",
        "core",
        "crew",
        "crewai",
        "datastax",
        "docs",
        "documentation",
        "embedder",
        "embedding",
        "embeddings",
        "hierarchical",
        "history",
        "legacy",
        "local"
      ]
    },
    "components-bundles": {
      "title": "About bundles",
      "slug": "/components-bundle-components",
      "category": "components",
      "content": "Bundles contain custom components that support specific third-party integrations with Langflow.\nYou add them to your flows and configure them in the same way as Langflow's core components.\n\nTo browse bundles, click  **Bundles** in the visual editor.\n\n## Bundle maintenance and documentation\n\nMany bundled components are developed by third-party contributors to the Langflow codebase.\n\nSome providers contribute documentation with their bundles, whereas others document their bundles in their own documentation.\nSome bundles have no documentation.\n\nTo find documentation for a specific bundled component, browse the Langflow docs and your provider's documentation.\nIf available, you can also find links to relevant documentation, such as API endpoints, through the component itself:\n\n1. Click the component to expose the [component's header menu](/concepts-components#component-menus).\n2. Click  **More**.\n3. Select **Docs**.\n\nThe Langflow documentation focuses on using bundles within flows.\nFor that reason, it focuses on the Langflow-specific configuration steps for bundled components.\nFor information about provider-specific features or APIs, see the provider's documentation.\n\n## Component parameters\n\n## Core components and bundles\n\n**Note:** The Langflow documentation doesn't list all bundles or components in bundles.\nFor the most accurate and up-to-date list of bundles and components for your version of Langflow, check  **Bundles** in the visual editor.\n\nIf you can't find a component that you used in an earlier version of Langflow, it may have been removed or marked as a [legacy component](#legacy-bundles).\n\nLangflow offers generic  **Core components** in addition to third-party, provider-specific bundles.\n\nIf you are looking for a specific service or integration, you can  **Search** components in the visual editor.\n\nIf all else fails, you can always create your own [custom components](/components-custom-components).\n\n## Legacy bundles\n\nThe following bundles include only legacy components.\n\n### CrewAI bundle\n\nReplace the following legacy CrewAI components with other agentic components, such as the [**Agent** component](/components-agents).\n\n<details>\n<summary>CrewAI Agent</summary>\n\nThis component represents CrewAI agents, allowing for the creation of specialized AI agents with defined roles goals and capabilities within a crew.\nFor more information, see the [CrewAI agents documentation](https://docs.crewai.com/core-concepts/Agents/).\n\nThis component accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| role | Role | Input parameter. The role of the agent. |\n| goal | Goal | Input parameter. The objective of the agent. |\n| backstory | Backstory | Input parameter. The backstory of the agent. |\n| tools | Tools | Input parameter. The tools at the agent's disposal. |\n| llm | Language Model | Input parameter. The language model that runs the agent. |\n| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |\n| verbose | Verbose | Input parameter. This enables verbose output. |\n| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |\n| kwargs | kwargs | Input parameter. Additional keyword arguments for the agent. |\n| output | Agent | Output parameter. The constructed CrewAI Agent object. |\n\n</details>\n\n<details>\n<summary>CrewAI Hierarchical Crew, CrewAI Hierarchical Task</summary>\n\nThe **CrewAI Hierarchical Crew** component represents a group of agents managing how they should collaborate and the tasks they should perform in a hierarchical structure. This component allows for the creation of a crew with a manager overseeing the task execution.\nFor more information, see the [CrewAI hierarchical crew documentation](https://docs.crewai.com/how-to/Hierarchical/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| agents | Agents | Input parameter. The list of Agent objects representing the crew members. |\n| tasks | Tasks | Input parameter. The list of HierarchicalTask objects representing the tasks to be executed. |\n| manager_llm | Manager LLM | Input parameter. The language model for the manager agent. |\n| manager_agent | Manager Agent | Input parameter. The specific agent to act as the manager. |\n| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |\n| memory | Memory | Input parameter. The memory configuration for the crew. |\n| use_cache | Use Cache | Input parameter. This enables caching of results. |\n| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |\n| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |\n| crew | Crew | Output parameter. The constructed Crew object with hierarchical task execution. |\n\n</details>\n\n<details>\n<summary>CrewAI Sequential Crew, CrewAI Sequential Task</summary>\n\nThe **CrewAI Sequential Crew** component represents a group of agents with tasks that are executed sequentially. This component allows for the creation of a crew that performs tasks in a specific order.\nFor more information, see the [CrewAI sequential crew documentation](https://docs.crewai.com/how-to/Sequential/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| tasks | Tasks | Input parameter. The list of SequentialTask objects representing the tasks to be executed. |\n| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |\n| memory | Memory | Input parameter. The memory configuration for the crew. |\n| use_cache | Use Cache | Input parameter. This enables caching of results. |\n| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |\n| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |\n| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |\n| crew | Crew | Output parameter. The constructed Crew object with sequential task execution. |\n\n</details>\n\n<details>\n<summary>CrewAI Sequential Task Agent</summary>\n\nThis component creates a CrewAI Task and its associated agent allowing for the definition of sequential tasks with specific agent roles and capabilities.\nFor more information, see the [CrewAI sequential agents documentation](https://docs.crewai.com/how-to/Sequential/).\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| role | Role | Input parameter. The role of the agent. |\n| goal | Goal | Input parameter. The objective of the agent. |\n| backstory | Backstory | Input parameter. The backstory of the agent. |\n| tools | Tools | Input parameter. The tools at the agent's disposal. |\n| llm | Language Model | Input parameter. The language model that runs the agent. |\n| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |\n| verbose | Verbose | Input parameter. This enables verbose output. |\n| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |\n| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |\n| agent_kwargs | Agent kwargs | Input parameter. The additional kwargs for the agent. |\n| task_description | Task Description | Input parameter. The descriptive text detailing the task's purpose and execution. |\n| expected_output | Expected Task Output | Input parameter. The clear definition of the expected task outcome. |\n| async_execution | Async Execution | Input parameter. Boolean flag indicating asynchronous task execution. |\n| previous_task | Previous Task | Input parameter. The previous task in the sequence for chaining. |\n| task_output | Sequential Task | Output parameter. The list of SequentialTask objects representing the created tasks. |\n\n</details>\n\n### Embeddings bundle\n\n* **Embedding Similarity**: Replaced by built-in similarity search functionality in vector store components.\n* **Text Embedder**: Replaced by the embedding model components.\n\n### Vector Stores bundle\n\nThis bundle contains only the legacy **Local DB** component.\nAll other vector store components can be found within their respective provider-specific bundles, such as the [**DataStax** bundle](/bundles-datastax).\n\n<details>\n<summary>Local DB</summary>\n\nReplace the **Local DB** component with the **Chroma DB** vector store component (in the **Chroma** bundle) or another vector store component.\n\nThe **Local DB** component reads and writes to a persistent, in-memory Chroma DB instance intended for use with Langflow.\nIt has separate modes for reads and writes, automatic collection management, and default persistence in your Langflow cache directory.\n\nSet the **Mode** parameter to reflect the operation you want the component to perform, and then configure the other parameters accordingly.\nSome parameters are only available for one mode.\n\n</Tabs>\n\n</details>\n\n### Zep bundle\n\n<details>\n<summary>Zep Chat Memory</summary>\n\nThe **Zep Chat Memory** component is a legacy component.\nReplace this component with the [**Message History** component](/components-helpers#message-history).\n\nThis component creates a `ZepChatMessageHistory` instance, enabling storage and retrieval of chat messages using Zep, a memory server for LLMs.\n\nIt accepts the following parameters:\n\n| Name          | Type          | Description                                               |\n|---------------|---------------|-----------------------------------------------------------|\n| url           | MessageText   | Input parameter. The URL of the Zep instance. Required. |\n| api_key       | SecretString  | Input parameter. The API Key for authentication with the Zep instance. |\n| api_base_path | Dropdown      | Input parameter. The API version to use. Options include api/v1 or api/v2. |\n| session_id    | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |\n| message_history | BaseChatMessageHistory  | Output parameter. An instance of ZepChatMessageHistory for the session. |\n\n</details>\n\n## See also\n\n* [LangWatch observability and evaluation](/integrations-langwatch)",
      "summary": "Bundles contain custom components that support specific third-party integrations with Langflow.\nYou add them to your flows and configure them in the same way as Langflow's core components.",
      "keywords": [
        "agent",
        "bundle",
        "bundles",
        "chat",
        "chroma",
        "component",
        "components",
        "core",
        "crew",
        "crewai",
        "datastax",
        "docs",
        "documentation",
        "embedder",
        "embedding",
        "embeddings",
        "hierarchical",
        "history",
        "legacy",
        "local"
      ]
    },
    "/components-custom-components": {
      "title": "Create custom Python components",
      "slug": "/components-custom-components",
      "category": "components",
      "content": "Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.\n\nIn Langflow's node-based environment, each node is a \"component\" that performs discrete functions. Custom components are Python classes which define:\n\n* **Inputs**  Data or parameters your component requires.\n* **Outputs**  Data your component provides to downstream nodes.\n* **Logic**  How you process inputs to produce outputs.\n\nThe benefits of creating custom components include unlimited extensibility, reusability, automatic field generation in the visual editor based on inputs, and type-safe connections between nodes.\n\nCreate custom components for performing specialized tasks, calling APIs, or adding advanced logic.\n\nCustom components in Langflow are built upon:\n\n* The Python class that inherits from `Component`.\n* Class-level attributes that identify and describe the component.\n* Input and output lists that determine data flow.\n* Internal variables for logging and advanced logic.\n\n## Class-level attributes\n\nDefine these attributes to control a custom component's appearance and behavior:\n\n```python\nclass MyCsvReader(Component):\n    display_name = \"CSV Reader\"\n    description = \"Reads CSV files\"\n    icon = \"file-text\"\n    name = \"CSVReader\"\n    documentation = \"http://docs.example.com/csv_reader\"\n```\n\n* `display_name`: A user-friendly label shown in the visual editor.\n* `description`: A brief summary shown in tooltips and printed below the component name when added to a flow.\n* `icon`: A decorative icon from Langflow's icon library, printed next to the name.\n\n    Langflow uses [Lucide](https://lucide.dev/icons) for icons. To assign an icon to your component, set the icon attribute to the name of a Lucide icon as a string, such as `icon = \"file-text\"`. Langflow renders icons from the Lucide library automatically.\n\n* `name`: A unique internal identifier, typically the same name as the folder containing your component code.\n* `documentation`: An optional link to external documentation, such as API or product documentation.\n\n### Structure of a custom component\n\nA Langflow custom component is more than a class with inputs and outputs. It includes an internal structure with optional lifecycle steps, output generation, front-end interaction, and logic organization.\n\nA basic component:\n\n* Inherits from `langflow.custom.Component`.\n* Declares metadata like `display_name`, `description`, `icon`, and more.\n* Defines `inputs` and `outputs` lists.\n* Implements methods matching output specifications.\n\nA minimal custom component skeleton contains the following:\n\n```python\nfrom langflow.custom import Component\nfrom langflow.template import Output\n\nclass MyComponent(Component):\n    display_name = \"My Component\"\n    description = \"A short summary.\"\n    icon = \"sparkles\"\n    name = \"MyComponent\"\n\n    inputs = []\n    outputs = []\n\n    def some_output_method(self):\n        return ...\n```\n### Internal Lifecycle and Execution Flow\n\nLangflow's engine manages:\n\n* **Instantiation**:  A component is created and internal structures are initialized.\n* **Assigning Inputs**: Values from the visual editor or connections are assigned to component fields.\n* **Validation and Setup**: Optional hooks like `_pre_run_setup`.\n* **Outputs Generation**: `run()` or `build_results()` triggers output methods.\n\n**Optional Hooks**:\n\n* `initialize_data` or `_pre_run_setup` can run setup logic before the component's main execution.\n* `__call__`, `run()`, or `_run()` can be overridden to customize how the component is called or to define custom execution logic.\n\n### Inputs and outputs\n\nCustom component inputs are defined with properties like:\n\n* `name`, `display_name`\n* Optional: `info`, `value`, `advanced`, `is_list`, `tool_mode`, `real_time_refresh`\n\nFor example:\n\n* `StrInput`: simple text input.\n* `DropdownInput`: selectable options.\n* `HandleInput`: specialized connections.\n\nCustom component `Output` properties define:\n\n* `name`, `display_name`, `method`\n* Optional: `info`\n\nFor more information, see [Custom component inputs and outputs](/components-custom-components#custom-component-inputs-and-outputs).\n\n### Associated Methods\n\nEach output is linked to a method:\n\n* The output method name must match the method name.\n* The method typically returns objects like Message, Data, or DataFrame.\n* The method can use inputs with `self.<input_name>`.\n\nFor example:\n\n```python\nOutput(\n    display_name=\"File Contents\",\n    name=\"file_contents\",\n    method=\"read_file\"\n)\n#...\ndef read_file(self) -> Data:\n    path = self.filename\n    with open(path, \"r\") as f:\n        content = f.read()\n    self.status = f\"Read {len(content)} chars from {path}\"\n    return Data(data={\"content\": content})\n```\n\n### Components with multiple outputs\n\nA component can define multiple outputs.\nEach output can have a different corresponding method.\nFor example:\n\n```python\noutputs = [\n    Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\n    Output(display_name=\"Debug Info\", name=\"debug_info\", method=\"provide_debug_info\"),\n]\n```\n\n#### Output Grouping Behavior with `group_outputs`\n\nBy default, components in Langflow that produce multiple outputs only allow one output selection in the visual editor.\nThe component will have only one output port where the user can select the preferred output type.\n\nThis behavior is controlled by the `group_outputs` parameter:\n\n- **`group_outputs=False` (default)**: When a component has more than one output and `group_outputs` is `false` or not set, the outputs are grouped in the visual editor, and the user must select one.\n\n    Use this option when the component is expected to return only one type of output when used in a flow.\n\n- **`group_outputs=True`**: All outputs are available simultaneously in the visual editor. The component has one output port for each output, and the user can connect zero or more outputs to other components.\n\n    Use this option when the component is expected to return multiple values that are used in parallel by downstream components or processes.\n\n</Tabs>\n\n### Common internal patterns\n\n#### `_pre_run_setup()`\n\nTo initialize a custom component with counters set:\n\n```python\ndef _pre_run_setup(self):\n    if not hasattr(self, \"_initialized\"):\n        self._initialized = True\n        self.iteration = 0\n```\n\n#### Override `run` or `_run`\nYou can override `async def _run(self): ...` to define custom execution logic, although the default behavior from the base class usually covers most cases.\n\n#### Store data in `self.ctx`\nUse `self.ctx` as a shared storage for data or counters across the component's execution flow:\n\n```python\ndef some_method(self):\n    count = self.ctx.get(\"my_count\", 0)\n    self.ctx[\"my_count\"] = count + 1\n```\n\n## Directory structure requirements\n\nBy default, Langflow looks for custom components in the `/components` directory.\n\nIf you're creating custom components in a different location using the `LANGFLOW_COMPONENTS_PATH` [environment variable](/environment-variables), components must be organized in a specific directory structure to be properly loaded and displayed in the visual editor:\n\nEach category directory **must** contain an `__init__.py` file for Langflow to properly recognize and load the components.\nThis is a Python package requirement that ensures the directory is treated as a module.\n\n```\n/your/custom/components/path/    # Base directory set by LANGFLOW_COMPONENTS_PATH\n     category_name/          # Required category subfolder that determines menu name\n         __init__.py         # Required\n         custom_component.py # Component file\n```\n\nComponents must be placed inside category folders, not directly in the base directory.\n\nThe category folder name determines where the component appears in the Langflow  **Core components** menu.\nFor example, to add a component to the **Helpers** category, place it in the `helpers` subfolder:\n\n```\n/app/custom_components/          # LANGFLOW_COMPONENTS_PATH\n     helpers/                 # Displayed within the \"Helpers\" category\n         __init__.py          # Required\n         custom_component.py  # Your component\n```\n\nYou can have multiple category folders to organize components into different categories:\n```\n/app/custom_components/\n     helpers/\n        __init__.py\n        helper_component.py\n     tools/\n         __init__.py\n         tool_component.py\n```\n\nThis folder structure is required for Langflow to properly discover and load your custom components. Components placed directly in the base directory aren't loaded.\n\n```\n/app/custom_components/          # LANGFLOW_COMPONENTS_PATH\n     custom_component.py      # Won't be loaded - missing category folder!\n```\n\n## Custom component inputs and outputs\n\nInputs and outputs define how data flows through the component, how it appears in the visual editor, and how connections to other components are validated.\n\n### Inputs\n\nInputs are defined in a class-level `inputs` list. When Langflow loads the component, it uses this list to render component fields and [ports](/concepts-components#component-ports) in the visual editor. Users or other components provide values or connections to fill these inputs.\n\nAn input is usually an instance of a class from `langflow.io` (such as `StrInput`, `DataInput`, or `MessageTextInput`). The most common constructor parameters are:\n\n* **`name`**: The internal variable name, accessed with `self.<name>`.\n* **`display_name`**: The label shown to users in the visual editor.\n* **`info`** *(optional)*: A tooltip or short description.\n* **`value`** *(optional)*: The default value.\n* **`advanced`** *(optional)*: If `true`, moves the field into the \"Advanced\" section.\n* **`required`** *(optional)*: If `true`, forces the user to provide a value.\n* **`is_list`** *(optional)*: If `true`, allows multiple values.\n* **`input_types`** *(optional)*: Restricts allowed connection types (e.g., `[\"Data\"]`, `[\"LanguageModel\"]`).\n\nHere are the most commonly used input classes and their typical usage.\n\n**Text Inputs**: For simple text entries.\n* **`StrInput`** creates a single-line text field.\n* **`MultilineInput`** creates a multi-line text area.\n\n**Numeric and Boolean Inputs**: Ensures users can only enter valid numeric or Boolean data.\n* **`BoolInput`**, **`IntInput`**, and **`FloatInput`** provide fields for Boolean, integer, and float values, ensuring type consistency.\n\n**Dropdowns**: For selecting from predefined options, useful for modes or levels.\n* **`DropdownInput`**\n\n**Secrets**: A specialized input for sensitive data, ensuring input is hidden in the visual editor.\n* **`SecretStrInput`** for API keys and passwords.\n\n**Specialized Data Inputs**: Ensures type-checking and color-coded connections in the visual editor.\n* **`DataInput`** expects a `Data` object (typically with `.data` and optional `.text`).\n* **`MessageInput`** expects a `Message` object, used in chat or agent flows.\n* **`MessageTextInput`** simplifies access to the `.text` field of a `Message`.\n\n**Handle-Based Inputs**: Used to connect outputs of specific types, ensuring correct pipeline connections.\n- **`HandleInput`**\n\n**File Uploads**: Allows users to upload files directly through the visual editor or receive file paths from other components.\n- **`FileInput`**\n\n**Lists**: Set `is_list=True` to accept multiple values, ideal for batch or grouped operations.\n\nThis example defines three inputs: a text field (`StrInput`), a Boolean toggle (`BoolInput`), and a dropdown selection (`DropdownInput`).\n\n```python\nfrom langflow.io import StrInput, BoolInput, DropdownInput\n\ninputs = [\n    StrInput(name=\"title\", display_name=\"Title\"),\n    BoolInput(name=\"enabled\", display_name=\"Enabled\", value=True),\n    DropdownInput(name=\"mode\", display_name=\"Mode\", options=[\"Fast\", \"Safe\", \"Experimental\"], value=\"Safe\")\n]\n```\n\n### Outputs\n\nOutputs are defined in a class-level `outputs` list. When Langflow renders a component, each output becomes a connector point in the visual editor. When you connect something to an output, Langflow automatically calls the corresponding method and passes the returned object to the next component.\n\nAn output is usually an instance of `Output` from `langflow.io`, with common parameters:\n\n* **`name`**: The internal variable name.\n* **`display_name`**: The label shown in the visual editor.\n* **`method`**: The name of the method called to produce the output.\n* **`info`** *(optional)*: Help text shown on hover.\n\nThe method must exist in the class, and it is recommended to annotate its return type for better type checking.\nYou can also set a `self.status` message inside the method to show progress or logs.\n\n**Common Return Types**:\n- **`Message`**: Structured chat messages.\n- **`Data`**: Flexible object with `.data` and optional `.text`.\n- **`DataFrame`**: Pandas-based tables (`langflow.schema.DataFrame`).\n- **Primitive types**: `str`, `int`, `bool` (not recommended if you need type/color consistency).\n\nIn this example, the `DataToDataFrame` component defines its output using the outputs list. The `df_out` output is linked to the `build_df` method, so when connected to another component (node), Langflow calls this method and passes its returned `DataFrame` to the next node. This demonstrates how each output maps to a method that generates the actual output data.\n\n```python\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass DataToDataFrame(Component):\n    display_name = \"Data to DataFrame\"\n    description = \"Convert multiple Data objects into a DataFrame\"\n    icon = \"table\"\n    name = \"DataToDataFrame\"\n\n    inputs = [\n        DataInput(\n            name=\"items\",\n            display_name=\"Data Items\",\n            info=\"List of Data objects to convert\",\n            is_list=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"df_out\",\n            display_name=\"DataFrame Output\",\n            method=\"build_df\"\n        )\n    ]\n\n    def build_df(self) -> DataFrame:\n        rows = []\n        for item in self.items:\n            row_dict = item.data.copy() if item.data else {}\n            row_dict[\"text\"] = item.get_text() or \"\"\n            rows.append(row_dict)\n\n        df = DataFrame(rows)\n        self.status = f\"Built DataFrame with {len(rows)} rows.\"\n        return df\n```\n\n### Tool Mode\n\nComponents that support **Tool Mode** can be used as standalone components (when _not_ in **Tool Mode**) or as tools for other components with a **Tools** input, such as **Agent** components.\n\nYou can allow a custom component to support **Tool Mode** by setting `tool_mode=True`:\n\n```python\ninputs = [\n    MessageTextInput(\n        name=\"message\",\n        display_name=\"Mensage\",\n        info=\"Enter the message that will be processed directly by the tool\",\n        tool_mode=True,\n    ),\n]\n```\n\nLangflow currently supports the following input types for **Tool Mode**:\n\n* `DataInput`\n* `DataFrameInput`\n* `PromptInput`\n* `MessageTextInput`\n* `MultilineInput`\n* `DropdownInput`\n\n## Typed annotations\n\nIn Langflow, **typed annotations** allow Langflow to visually guide users and maintain flow consistency.\n\nTyped annotations provide:\n\n* **Color-coding**: Outputs like `-> Data` or `-> Message` get distinct colors.\n* **Validation**: Langflow blocks incompatible connections automatically.\n* **Readability**: Developers can quickly understand data flow.\n* **Development tools**: Better code suggestions and error checking in your code editor.\n\n### Common Return Types\n\n* `Message`: For chat-style outputs. Connects to any of several `Message`-compatible inputs.\n\n    ```python\n    def produce_message(self) -> Message:\n        return Message(text=\"Hello! from typed method!\", sender=\"System\")\n    ```\n\n* `Data`: For structured data like dicts or partial texts. Connects only to `DataInput` (ports that accept `Data`).\n\n    ```python\n    def get_processed_data(self) -> Data:\n        processed = {\"key1\": \"value1\", \"key2\": 123}\n        return Data(data=processed)\n    ```\n\n* `DataFrame`: For tabular data. Connects only to `DataFrameInput` (ports that accept `DataFrame`).\n\n    ```python\n    def build_df(self) -> DataFrame:\n        pdf = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        return DataFrame(pdf)\n    ```\n\n* Primitive Types (`str`, `int`, `bool`): Returning primitives is allowed but wrapping in `Data` or `Message` is recommended for better consistency in the visual editor.\n\n    ```python\n    def compute_sum(self) -> int:\n        return sum(self.numbers)\n    ```\n\n### Tips for typed annotations\n\nWhen using typed annotations, consider the following best practices:\n\n* **Always Annotate Outputs**: Specify return types like `-> Data`, `-> Message`, or `-> DataFrame` to enable proper visual editor color-coding and validation.\n* **Wrap Raw Data**: Use `Data`, `Message`, or `DataFrame` wrappers instead of returning plain structures.\n* **Use Primitives Carefully**: Direct `str` or `int` returns are fine for simple flows, but wrapping improves flexibility.\n* **Annotate Helpers Too**: Even if internal, typing improves maintainability and clarity.\n* **Handle Edge Cases**: Prefer returning structured `Data` with error fields when needed.\n* **Stay Consistent**: Use the same types across your components to make flows predictable and easier to build.\n\n## Enable dynamic fields\n\nIn **Langflow**, dynamic fields allow inputs to change or appear based on user interactions. You can make an input dynamic by setting `dynamic=True`.\nOptionally, setting `real_time_refresh=True` triggers the `update_build_config` method to adjust the input's visibility or properties in real time, creating a contextual visual editor experience that only exposes relevant fields based on the user's choices.\n\nIn this example, the operator field triggers updates with `real_time_refresh=True`.\nThe `regex_pattern` field is initially hidden and controlled with `dynamic=True`.\n\n```python\nfrom langflow.io import DropdownInput, StrInput\n\nclass RegexRouter(Component):\n    display_name = \"Regex Router\"\n    description = \"Demonstrates dynamic fields for regex input.\"\n\n    inputs = [\n        DropdownInput(\n            name=\"operator\",\n            display_name=\"Operator\",\n            options=[\"equals\", \"contains\", \"regex\"],\n            value=\"equals\",\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"regex_pattern\",\n            display_name=\"Regex Pattern\",\n            info=\"Used if operator='regex'\",\n            dynamic=True,\n            show=False,\n        ),\n    ]\n```\n\n### Implement `update_build_config`\n\nWhen a field with `real_time_refresh=True` is modified, Langflow calls the `update_build_config` method, passing the updated field name, value, and the component's configuration to dynamically adjust the visibility or properties of other fields based on user input.\n\nThis example will show or hide the `regex_pattern` field when the user selects a different operator.\n\n```python\ndef update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n    if field_name == \"operator\":\n        if field_value == \"regex\":\n            build_config[\"regex_pattern\"][\"show\"] = True\n        else:\n            build_config[\"regex_pattern\"][\"show\"] = False\n    return build_config\n```\n\n### Additional Dynamic Field Controls\n\nYou can also modify other properties within `update_build_config`, such as:\n* `required`: Set `build_config[\"some_field\"][\"required\"] = True/False`\n\n* `advanced`: Set `build_config[\"some_field\"][\"advanced\"] = True`\n\n* `options`: Modify dynamic dropdown options.\n\n### Tips for Managing Dynamic Fields\n\nWhen working with dynamic fields, consider the following best practices to ensure a smooth user experience:\n\n* **Minimize field changes**: Hide only fields that are truly irrelevant to avoid confusing users.\n* **Test behavior**: Ensure that adding or removing fields doesn't accidentally erase user input.\n* **Preserve data**: Use `build_config[\"some_field\"][\"show\"] = False` to hide fields without losing their values.\n* **Clarify logic**: Add `info` notes to explain why fields appear or disappear based on conditions.\n* **Keep it manageable**: If the dynamic logic becomes too complex, consider breaking it into smaller components, unless it serves a clear purpose in a single node.\n\n## Error handling and logging\n\nIn Langflow, robust error handling ensures that your components behave predictably, even when unexpected situations occur, such as invalid inputs, external API failures, or internal logic errors.\n\n### Error handling techniques\n\n* **Raise Exceptions**: If a critical error occurs, you can raise standard Python exceptions such as `ValueError`, or specialized exceptions like `ToolException`. Langflow will automatically catch these and display appropriate error messages in the visual editor, helping users quickly identify what went wrong.\n\n    ```python\n    def compute_result(self) -> str:\n        if not self.user_input:\n            raise ValueError(\"No input provided.\")\n        # ...\n    ```\n\n* **Return Structured Error Data**: Instead of stopping a flow abruptly, you can return a Data object containing an \"error\" field. This approach allows the flow to continue operating and enables downstream components to detect and handle the error gracefully.\n\n    ```python\n    def run_model(self) -> Data:\n        try:\n            # ...\n        except Exception as e:\n            return Data(data={\"error\": str(e)})\n    ```\n\n### Improve debugging and flow management\n\n* **Use `self.status`**: Each component has a status field where you can store short messages about the execution resultsuch as success summaries, partial progress, or error notifications. These appear directly in the visual editor, making troubleshooting easier for users.\n\n    ```python\n    def parse_data(self) -> Data:\n    # ...\n    self.status = f\"Parsed {len(rows)} rows successfully.\"\n    return Data(data={\"rows\": rows})\n    ```\n\n* **Stop specific outputs with `self.stop(...)`**: You can halt individual output paths when certain conditions fail, without affecting the entire component. This is especially useful when working with components that have multiple output branches.\n\n    ```python\n    def some_output(self) -> Data:\n    if <some condition>:\n        self.stop(\"some_output\")  # Tells Langflow no data flows\n        return Data(data={\"error\": \"Condition not met\"})\n    ```\n\n* **Log events**: You can log key execution details inside components. Logs are displayed in the \"Logs\" or \"Events\" section of the component's detail view and can be accessed later through the flow's debug panel or exported files, providing a clear trace of the component's behavior for easier debugging.\n\n    ```python\n    def process_file(self, file_path: str):\n    self.log(f\"Processing file {file_path}\")\n    # ...\n    ```\n\n### Tips for error handling and logging\n\nTo build more reliable components, consider the following best practices:\n\n* **Validate inputs early**: Catch missing or invalid inputs at the start to prevent broken logic.\n* **Summarize with `self.status`**: Use short success or error summaries to help users understand results quickly.\n* **Keep logs concise**: Focus on meaningful messages to avoid cluttering the visual editor.\n* **Return structured errors**: When appropriate, return `Data(data={\"error\": ...})` instead of raising exceptions to allow downstream handling.\n* **Stop outputs selectively**: Only halt specific outputs with `self.stop(...)` if necessary, to preserve correct flow behavior elsewhere.\n\n## Contribute custom components to Langflow\n\nSee [How to Contribute](/contributing-components) to contribute your custom component to Langflow.",
      "summary": "Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.",
      "keywords": [
        "additional",
        "advanced",
        "agent",
        "always",
        "annotate",
        "annotations",
        "assigning",
        "associated",
        "attributes",
        "based",
        "behavior",
        "boolean",
        "boolinput",
        "carefully",
        "cases",
        "changes",
        "clarify",
        "class",
        "coding",
        "color"
      ]
    },
    "components-custom-components": {
      "title": "Create custom Python components",
      "slug": "/components-custom-components",
      "category": "components",
      "content": "Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.\n\nIn Langflow's node-based environment, each node is a \"component\" that performs discrete functions. Custom components are Python classes which define:\n\n* **Inputs**  Data or parameters your component requires.\n* **Outputs**  Data your component provides to downstream nodes.\n* **Logic**  How you process inputs to produce outputs.\n\nThe benefits of creating custom components include unlimited extensibility, reusability, automatic field generation in the visual editor based on inputs, and type-safe connections between nodes.\n\nCreate custom components for performing specialized tasks, calling APIs, or adding advanced logic.\n\nCustom components in Langflow are built upon:\n\n* The Python class that inherits from `Component`.\n* Class-level attributes that identify and describe the component.\n* Input and output lists that determine data flow.\n* Internal variables for logging and advanced logic.\n\n## Class-level attributes\n\nDefine these attributes to control a custom component's appearance and behavior:\n\n```python\nclass MyCsvReader(Component):\n    display_name = \"CSV Reader\"\n    description = \"Reads CSV files\"\n    icon = \"file-text\"\n    name = \"CSVReader\"\n    documentation = \"http://docs.example.com/csv_reader\"\n```\n\n* `display_name`: A user-friendly label shown in the visual editor.\n* `description`: A brief summary shown in tooltips and printed below the component name when added to a flow.\n* `icon`: A decorative icon from Langflow's icon library, printed next to the name.\n\n    Langflow uses [Lucide](https://lucide.dev/icons) for icons. To assign an icon to your component, set the icon attribute to the name of a Lucide icon as a string, such as `icon = \"file-text\"`. Langflow renders icons from the Lucide library automatically.\n\n* `name`: A unique internal identifier, typically the same name as the folder containing your component code.\n* `documentation`: An optional link to external documentation, such as API or product documentation.\n\n### Structure of a custom component\n\nA Langflow custom component is more than a class with inputs and outputs. It includes an internal structure with optional lifecycle steps, output generation, front-end interaction, and logic organization.\n\nA basic component:\n\n* Inherits from `langflow.custom.Component`.\n* Declares metadata like `display_name`, `description`, `icon`, and more.\n* Defines `inputs` and `outputs` lists.\n* Implements methods matching output specifications.\n\nA minimal custom component skeleton contains the following:\n\n```python\nfrom langflow.custom import Component\nfrom langflow.template import Output\n\nclass MyComponent(Component):\n    display_name = \"My Component\"\n    description = \"A short summary.\"\n    icon = \"sparkles\"\n    name = \"MyComponent\"\n\n    inputs = []\n    outputs = []\n\n    def some_output_method(self):\n        return ...\n```\n### Internal Lifecycle and Execution Flow\n\nLangflow's engine manages:\n\n* **Instantiation**:  A component is created and internal structures are initialized.\n* **Assigning Inputs**: Values from the visual editor or connections are assigned to component fields.\n* **Validation and Setup**: Optional hooks like `_pre_run_setup`.\n* **Outputs Generation**: `run()` or `build_results()` triggers output methods.\n\n**Optional Hooks**:\n\n* `initialize_data` or `_pre_run_setup` can run setup logic before the component's main execution.\n* `__call__`, `run()`, or `_run()` can be overridden to customize how the component is called or to define custom execution logic.\n\n### Inputs and outputs\n\nCustom component inputs are defined with properties like:\n\n* `name`, `display_name`\n* Optional: `info`, `value`, `advanced`, `is_list`, `tool_mode`, `real_time_refresh`\n\nFor example:\n\n* `StrInput`: simple text input.\n* `DropdownInput`: selectable options.\n* `HandleInput`: specialized connections.\n\nCustom component `Output` properties define:\n\n* `name`, `display_name`, `method`\n* Optional: `info`\n\nFor more information, see [Custom component inputs and outputs](/components-custom-components#custom-component-inputs-and-outputs).\n\n### Associated Methods\n\nEach output is linked to a method:\n\n* The output method name must match the method name.\n* The method typically returns objects like Message, Data, or DataFrame.\n* The method can use inputs with `self.<input_name>`.\n\nFor example:\n\n```python\nOutput(\n    display_name=\"File Contents\",\n    name=\"file_contents\",\n    method=\"read_file\"\n)\n#...\ndef read_file(self) -> Data:\n    path = self.filename\n    with open(path, \"r\") as f:\n        content = f.read()\n    self.status = f\"Read {len(content)} chars from {path}\"\n    return Data(data={\"content\": content})\n```\n\n### Components with multiple outputs\n\nA component can define multiple outputs.\nEach output can have a different corresponding method.\nFor example:\n\n```python\noutputs = [\n    Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\n    Output(display_name=\"Debug Info\", name=\"debug_info\", method=\"provide_debug_info\"),\n]\n```\n\n#### Output Grouping Behavior with `group_outputs`\n\nBy default, components in Langflow that produce multiple outputs only allow one output selection in the visual editor.\nThe component will have only one output port where the user can select the preferred output type.\n\nThis behavior is controlled by the `group_outputs` parameter:\n\n- **`group_outputs=False` (default)**: When a component has more than one output and `group_outputs` is `false` or not set, the outputs are grouped in the visual editor, and the user must select one.\n\n    Use this option when the component is expected to return only one type of output when used in a flow.\n\n- **`group_outputs=True`**: All outputs are available simultaneously in the visual editor. The component has one output port for each output, and the user can connect zero or more outputs to other components.\n\n    Use this option when the component is expected to return multiple values that are used in parallel by downstream components or processes.\n\n</Tabs>\n\n### Common internal patterns\n\n#### `_pre_run_setup()`\n\nTo initialize a custom component with counters set:\n\n```python\ndef _pre_run_setup(self):\n    if not hasattr(self, \"_initialized\"):\n        self._initialized = True\n        self.iteration = 0\n```\n\n#### Override `run` or `_run`\nYou can override `async def _run(self): ...` to define custom execution logic, although the default behavior from the base class usually covers most cases.\n\n#### Store data in `self.ctx`\nUse `self.ctx` as a shared storage for data or counters across the component's execution flow:\n\n```python\ndef some_method(self):\n    count = self.ctx.get(\"my_count\", 0)\n    self.ctx[\"my_count\"] = count + 1\n```\n\n## Directory structure requirements\n\nBy default, Langflow looks for custom components in the `/components` directory.\n\nIf you're creating custom components in a different location using the `LANGFLOW_COMPONENTS_PATH` [environment variable](/environment-variables), components must be organized in a specific directory structure to be properly loaded and displayed in the visual editor:\n\nEach category directory **must** contain an `__init__.py` file for Langflow to properly recognize and load the components.\nThis is a Python package requirement that ensures the directory is treated as a module.\n\n```\n/your/custom/components/path/    # Base directory set by LANGFLOW_COMPONENTS_PATH\n     category_name/          # Required category subfolder that determines menu name\n         __init__.py         # Required\n         custom_component.py # Component file\n```\n\nComponents must be placed inside category folders, not directly in the base directory.\n\nThe category folder name determines where the component appears in the Langflow  **Core components** menu.\nFor example, to add a component to the **Helpers** category, place it in the `helpers` subfolder:\n\n```\n/app/custom_components/          # LANGFLOW_COMPONENTS_PATH\n     helpers/                 # Displayed within the \"Helpers\" category\n         __init__.py          # Required\n         custom_component.py  # Your component\n```\n\nYou can have multiple category folders to organize components into different categories:\n```\n/app/custom_components/\n     helpers/\n        __init__.py\n        helper_component.py\n     tools/\n         __init__.py\n         tool_component.py\n```\n\nThis folder structure is required for Langflow to properly discover and load your custom components. Components placed directly in the base directory aren't loaded.\n\n```\n/app/custom_components/          # LANGFLOW_COMPONENTS_PATH\n     custom_component.py      # Won't be loaded - missing category folder!\n```\n\n## Custom component inputs and outputs\n\nInputs and outputs define how data flows through the component, how it appears in the visual editor, and how connections to other components are validated.\n\n### Inputs\n\nInputs are defined in a class-level `inputs` list. When Langflow loads the component, it uses this list to render component fields and [ports](/concepts-components#component-ports) in the visual editor. Users or other components provide values or connections to fill these inputs.\n\nAn input is usually an instance of a class from `langflow.io` (such as `StrInput`, `DataInput`, or `MessageTextInput`). The most common constructor parameters are:\n\n* **`name`**: The internal variable name, accessed with `self.<name>`.\n* **`display_name`**: The label shown to users in the visual editor.\n* **`info`** *(optional)*: A tooltip or short description.\n* **`value`** *(optional)*: The default value.\n* **`advanced`** *(optional)*: If `true`, moves the field into the \"Advanced\" section.\n* **`required`** *(optional)*: If `true`, forces the user to provide a value.\n* **`is_list`** *(optional)*: If `true`, allows multiple values.\n* **`input_types`** *(optional)*: Restricts allowed connection types (e.g., `[\"Data\"]`, `[\"LanguageModel\"]`).\n\nHere are the most commonly used input classes and their typical usage.\n\n**Text Inputs**: For simple text entries.\n* **`StrInput`** creates a single-line text field.\n* **`MultilineInput`** creates a multi-line text area.\n\n**Numeric and Boolean Inputs**: Ensures users can only enter valid numeric or Boolean data.\n* **`BoolInput`**, **`IntInput`**, and **`FloatInput`** provide fields for Boolean, integer, and float values, ensuring type consistency.\n\n**Dropdowns**: For selecting from predefined options, useful for modes or levels.\n* **`DropdownInput`**\n\n**Secrets**: A specialized input for sensitive data, ensuring input is hidden in the visual editor.\n* **`SecretStrInput`** for API keys and passwords.\n\n**Specialized Data Inputs**: Ensures type-checking and color-coded connections in the visual editor.\n* **`DataInput`** expects a `Data` object (typically with `.data` and optional `.text`).\n* **`MessageInput`** expects a `Message` object, used in chat or agent flows.\n* **`MessageTextInput`** simplifies access to the `.text` field of a `Message`.\n\n**Handle-Based Inputs**: Used to connect outputs of specific types, ensuring correct pipeline connections.\n- **`HandleInput`**\n\n**File Uploads**: Allows users to upload files directly through the visual editor or receive file paths from other components.\n- **`FileInput`**\n\n**Lists**: Set `is_list=True` to accept multiple values, ideal for batch or grouped operations.\n\nThis example defines three inputs: a text field (`StrInput`), a Boolean toggle (`BoolInput`), and a dropdown selection (`DropdownInput`).\n\n```python\nfrom langflow.io import StrInput, BoolInput, DropdownInput\n\ninputs = [\n    StrInput(name=\"title\", display_name=\"Title\"),\n    BoolInput(name=\"enabled\", display_name=\"Enabled\", value=True),\n    DropdownInput(name=\"mode\", display_name=\"Mode\", options=[\"Fast\", \"Safe\", \"Experimental\"], value=\"Safe\")\n]\n```\n\n### Outputs\n\nOutputs are defined in a class-level `outputs` list. When Langflow renders a component, each output becomes a connector point in the visual editor. When you connect something to an output, Langflow automatically calls the corresponding method and passes the returned object to the next component.\n\nAn output is usually an instance of `Output` from `langflow.io`, with common parameters:\n\n* **`name`**: The internal variable name.\n* **`display_name`**: The label shown in the visual editor.\n* **`method`**: The name of the method called to produce the output.\n* **`info`** *(optional)*: Help text shown on hover.\n\nThe method must exist in the class, and it is recommended to annotate its return type for better type checking.\nYou can also set a `self.status` message inside the method to show progress or logs.\n\n**Common Return Types**:\n- **`Message`**: Structured chat messages.\n- **`Data`**: Flexible object with `.data` and optional `.text`.\n- **`DataFrame`**: Pandas-based tables (`langflow.schema.DataFrame`).\n- **Primitive types**: `str`, `int`, `bool` (not recommended if you need type/color consistency).\n\nIn this example, the `DataToDataFrame` component defines its output using the outputs list. The `df_out` output is linked to the `build_df` method, so when connected to another component (node), Langflow calls this method and passes its returned `DataFrame` to the next node. This demonstrates how each output maps to a method that generates the actual output data.\n\n```python\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, Output\nfrom langflow.schema import Data, DataFrame\n\nclass DataToDataFrame(Component):\n    display_name = \"Data to DataFrame\"\n    description = \"Convert multiple Data objects into a DataFrame\"\n    icon = \"table\"\n    name = \"DataToDataFrame\"\n\n    inputs = [\n        DataInput(\n            name=\"items\",\n            display_name=\"Data Items\",\n            info=\"List of Data objects to convert\",\n            is_list=True\n        )\n    ]\n\n    outputs = [\n        Output(\n            name=\"df_out\",\n            display_name=\"DataFrame Output\",\n            method=\"build_df\"\n        )\n    ]\n\n    def build_df(self) -> DataFrame:\n        rows = []\n        for item in self.items:\n            row_dict = item.data.copy() if item.data else {}\n            row_dict[\"text\"] = item.get_text() or \"\"\n            rows.append(row_dict)\n\n        df = DataFrame(rows)\n        self.status = f\"Built DataFrame with {len(rows)} rows.\"\n        return df\n```\n\n### Tool Mode\n\nComponents that support **Tool Mode** can be used as standalone components (when _not_ in **Tool Mode**) or as tools for other components with a **Tools** input, such as **Agent** components.\n\nYou can allow a custom component to support **Tool Mode** by setting `tool_mode=True`:\n\n```python\ninputs = [\n    MessageTextInput(\n        name=\"message\",\n        display_name=\"Mensage\",\n        info=\"Enter the message that will be processed directly by the tool\",\n        tool_mode=True,\n    ),\n]\n```\n\nLangflow currently supports the following input types for **Tool Mode**:\n\n* `DataInput`\n* `DataFrameInput`\n* `PromptInput`\n* `MessageTextInput`\n* `MultilineInput`\n* `DropdownInput`\n\n## Typed annotations\n\nIn Langflow, **typed annotations** allow Langflow to visually guide users and maintain flow consistency.\n\nTyped annotations provide:\n\n* **Color-coding**: Outputs like `-> Data` or `-> Message` get distinct colors.\n* **Validation**: Langflow blocks incompatible connections automatically.\n* **Readability**: Developers can quickly understand data flow.\n* **Development tools**: Better code suggestions and error checking in your code editor.\n\n### Common Return Types\n\n* `Message`: For chat-style outputs. Connects to any of several `Message`-compatible inputs.\n\n    ```python\n    def produce_message(self) -> Message:\n        return Message(text=\"Hello! from typed method!\", sender=\"System\")\n    ```\n\n* `Data`: For structured data like dicts or partial texts. Connects only to `DataInput` (ports that accept `Data`).\n\n    ```python\n    def get_processed_data(self) -> Data:\n        processed = {\"key1\": \"value1\", \"key2\": 123}\n        return Data(data=processed)\n    ```\n\n* `DataFrame`: For tabular data. Connects only to `DataFrameInput` (ports that accept `DataFrame`).\n\n    ```python\n    def build_df(self) -> DataFrame:\n        pdf = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n        return DataFrame(pdf)\n    ```\n\n* Primitive Types (`str`, `int`, `bool`): Returning primitives is allowed but wrapping in `Data` or `Message` is recommended for better consistency in the visual editor.\n\n    ```python\n    def compute_sum(self) -> int:\n        return sum(self.numbers)\n    ```\n\n### Tips for typed annotations\n\nWhen using typed annotations, consider the following best practices:\n\n* **Always Annotate Outputs**: Specify return types like `-> Data`, `-> Message`, or `-> DataFrame` to enable proper visual editor color-coding and validation.\n* **Wrap Raw Data**: Use `Data`, `Message`, or `DataFrame` wrappers instead of returning plain structures.\n* **Use Primitives Carefully**: Direct `str` or `int` returns are fine for simple flows, but wrapping improves flexibility.\n* **Annotate Helpers Too**: Even if internal, typing improves maintainability and clarity.\n* **Handle Edge Cases**: Prefer returning structured `Data` with error fields when needed.\n* **Stay Consistent**: Use the same types across your components to make flows predictable and easier to build.\n\n## Enable dynamic fields\n\nIn **Langflow**, dynamic fields allow inputs to change or appear based on user interactions. You can make an input dynamic by setting `dynamic=True`.\nOptionally, setting `real_time_refresh=True` triggers the `update_build_config` method to adjust the input's visibility or properties in real time, creating a contextual visual editor experience that only exposes relevant fields based on the user's choices.\n\nIn this example, the operator field triggers updates with `real_time_refresh=True`.\nThe `regex_pattern` field is initially hidden and controlled with `dynamic=True`.\n\n```python\nfrom langflow.io import DropdownInput, StrInput\n\nclass RegexRouter(Component):\n    display_name = \"Regex Router\"\n    description = \"Demonstrates dynamic fields for regex input.\"\n\n    inputs = [\n        DropdownInput(\n            name=\"operator\",\n            display_name=\"Operator\",\n            options=[\"equals\", \"contains\", \"regex\"],\n            value=\"equals\",\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"regex_pattern\",\n            display_name=\"Regex Pattern\",\n            info=\"Used if operator='regex'\",\n            dynamic=True,\n            show=False,\n        ),\n    ]\n```\n\n### Implement `update_build_config`\n\nWhen a field with `real_time_refresh=True` is modified, Langflow calls the `update_build_config` method, passing the updated field name, value, and the component's configuration to dynamically adjust the visibility or properties of other fields based on user input.\n\nThis example will show or hide the `regex_pattern` field when the user selects a different operator.\n\n```python\ndef update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:\n    if field_name == \"operator\":\n        if field_value == \"regex\":\n            build_config[\"regex_pattern\"][\"show\"] = True\n        else:\n            build_config[\"regex_pattern\"][\"show\"] = False\n    return build_config\n```\n\n### Additional Dynamic Field Controls\n\nYou can also modify other properties within `update_build_config`, such as:\n* `required`: Set `build_config[\"some_field\"][\"required\"] = True/False`\n\n* `advanced`: Set `build_config[\"some_field\"][\"advanced\"] = True`\n\n* `options`: Modify dynamic dropdown options.\n\n### Tips for Managing Dynamic Fields\n\nWhen working with dynamic fields, consider the following best practices to ensure a smooth user experience:\n\n* **Minimize field changes**: Hide only fields that are truly irrelevant to avoid confusing users.\n* **Test behavior**: Ensure that adding or removing fields doesn't accidentally erase user input.\n* **Preserve data**: Use `build_config[\"some_field\"][\"show\"] = False` to hide fields without losing their values.\n* **Clarify logic**: Add `info` notes to explain why fields appear or disappear based on conditions.\n* **Keep it manageable**: If the dynamic logic becomes too complex, consider breaking it into smaller components, unless it serves a clear purpose in a single node.\n\n## Error handling and logging\n\nIn Langflow, robust error handling ensures that your components behave predictably, even when unexpected situations occur, such as invalid inputs, external API failures, or internal logic errors.\n\n### Error handling techniques\n\n* **Raise Exceptions**: If a critical error occurs, you can raise standard Python exceptions such as `ValueError`, or specialized exceptions like `ToolException`. Langflow will automatically catch these and display appropriate error messages in the visual editor, helping users quickly identify what went wrong.\n\n    ```python\n    def compute_result(self) -> str:\n        if not self.user_input:\n            raise ValueError(\"No input provided.\")\n        # ...\n    ```\n\n* **Return Structured Error Data**: Instead of stopping a flow abruptly, you can return a Data object containing an \"error\" field. This approach allows the flow to continue operating and enables downstream components to detect and handle the error gracefully.\n\n    ```python\n    def run_model(self) -> Data:\n        try:\n            # ...\n        except Exception as e:\n            return Data(data={\"error\": str(e)})\n    ```\n\n### Improve debugging and flow management\n\n* **Use `self.status`**: Each component has a status field where you can store short messages about the execution resultsuch as success summaries, partial progress, or error notifications. These appear directly in the visual editor, making troubleshooting easier for users.\n\n    ```python\n    def parse_data(self) -> Data:\n    # ...\n    self.status = f\"Parsed {len(rows)} rows successfully.\"\n    return Data(data={\"rows\": rows})\n    ```\n\n* **Stop specific outputs with `self.stop(...)`**: You can halt individual output paths when certain conditions fail, without affecting the entire component. This is especially useful when working with components that have multiple output branches.\n\n    ```python\n    def some_output(self) -> Data:\n    if <some condition>:\n        self.stop(\"some_output\")  # Tells Langflow no data flows\n        return Data(data={\"error\": \"Condition not met\"})\n    ```\n\n* **Log events**: You can log key execution details inside components. Logs are displayed in the \"Logs\" or \"Events\" section of the component's detail view and can be accessed later through the flow's debug panel or exported files, providing a clear trace of the component's behavior for easier debugging.\n\n    ```python\n    def process_file(self, file_path: str):\n    self.log(f\"Processing file {file_path}\")\n    # ...\n    ```\n\n### Tips for error handling and logging\n\nTo build more reliable components, consider the following best practices:\n\n* **Validate inputs early**: Catch missing or invalid inputs at the start to prevent broken logic.\n* **Summarize with `self.status`**: Use short success or error summaries to help users understand results quickly.\n* **Keep logs concise**: Focus on meaningful messages to avoid cluttering the visual editor.\n* **Return structured errors**: When appropriate, return `Data(data={\"error\": ...})` instead of raising exceptions to allow downstream handling.\n* **Stop outputs selectively**: Only halt specific outputs with `self.stop(...)` if necessary, to preserve correct flow behavior elsewhere.\n\n## Contribute custom components to Langflow\n\nSee [How to Contribute](/contributing-components) to contribute your custom component to Langflow.",
      "summary": "Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.",
      "keywords": [
        "additional",
        "advanced",
        "agent",
        "always",
        "annotate",
        "annotations",
        "assigning",
        "associated",
        "attributes",
        "based",
        "behavior",
        "boolean",
        "boolinput",
        "carefully",
        "cases",
        "changes",
        "clarify",
        "class",
        "coding",
        "color"
      ]
    },
    "/components-data": {
      "title": "Data",
      "slug": "/components-data",
      "category": "components",
      "content": "Data components bring data into your flows from various sources like files, API endpoints, and URLs.\nFor example:\n\n* **Load files**: Import data from a file or directory with the [**File** component](#file) and [**Directory** component](#directory).\n\n* **Search the web**: Fetch data from the web with components like the [**News Search** component](#news-search), [**RSS Reader** component](#rss-reader), [**Web Search** component](#web-search), and [**URL** component](#url).\n\n* **Make API calls**: Use APIs to trigger flows or perform actions with the [**API Request** component](#api-request) and [**Webhook** component](#webhook).\n\n* **Run SQL queries**: Query an SQL database with the [**SQL Database** component](#sql-database).\n\nEach component runs different commands for retrieval, processing, and type checking.\nSome components are a minimal wrapper for a command that you provide, and others include built-in scripts to fetch and process data based on variable inputs.\nAdditionally, some components return raw data, whereas others can convert, restructure, or validate the data before outputting it.\nThis means that some similar components might produce different results.\n\n**Note:** Data components pair well with [Processing components](/components-processing) that can perform additional parsing, transformation, and validation after retrieving the data.\n\nThis can include basic operations, like saving a file in a specific format, or more complex tasks, like using a **Text Splitter** component to break down a large document into smaller chunks before generating embeddings for vector search.\n\n## Use Data components in flows\n\nData components are used often in flows because they offer a versatile way to perform common functions.\n\nYou can use these components to perform their base functions as isolated steps in your flow, or you can connect them to an **Agent** component as tools.\n\n![An agent flow with three Data components connected to the agent as tools](/img/connect-data-components-to-agent.png)\n\nFor example flows, see the following:\n\n* [Create a chatbot that can ingest files](/chat-with-files): Learn how to use a **File** component to load a file as context for a chatbot.\nThe file and user input are both passed to the LLM so you can ask questions about the file you uploaded.\n\n* [Create a vector RAG chatbot](/chat-with-rag): Learn how to ingest files for use in Retrieval-Augmented Generation (RAG), and then set up a chatbot that can use the ingested files as context.\nThe two flows in this tutorial prepare files for RAG, and then let your LLM use vector search to retrieve contextually relevant data during a chat session.\n\n* [Configure tools for agents](/agents-tools): Learn how to use any component as a tool for an agent.\nWhen used as tools, the agent autonomously decides when to call a component based on the user's query.\n\n* [Trigger flows with webhooks](/webhook): Learn how to use the **Webhook** component to trigger a flow run in response to an external event.\n\n## API Request\n\nThe **API Request** component constructs and sends HTTP requests using URLs or curl commands:\n\n* **URL mode**: Enter one or more comma-separated URLs, and then select the method for the request to each URL.\n* **curl mode**: Enter the curl command to execute.\n\nYou can enable additional request options and fields in the component's parameters.\n\nReturns a [`Data` object](/data-types#data) containing the response.\n\nFor provider-specific API components, see  [**Bundles**](/components-bundle-components).\n\n### API Request parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| mode | Mode | Input parameter. Set the mode to either **URL** or **curl**. |\n| urls | URL | Input parameter. Enter one or more comma-separated URLs for the request. |\n| curl | curl | Input parameter. **curl mode** only. Enter a complete curl command. Other component parameters are populated from the command arguments. |\n| method | Method | Input parameter. The HTTP method to use. |\n| query_params | Query Parameters | Input parameter. The query parameters to append to the URL. |\n| body | Body | Input parameter. The body to send with POST, PATCH, and PUT requests as a dictionary. |\n| headers | Headers | Input parameter. The headers to send with the request as a dictionary. |\n| timeout | Timeout | Input parameter. The timeout to use for the request. |\n| follow_redirects | Follow Redirects | Input parameter. Whether to follow HTTP redirects. The default is enabled (`true`). If disabled (`false`), HTTP redirects aren't followed. |\n| save_to_file | Save to File | Input parameter. Whether to save the API response to a temporary file. Default: Disabled (`false`) |\n| include_httpx_metadata | Include HTTPx Metadata | Input parameter. Whether to include properties such as `headers`, `status_code`, `response_headers`, and `redirection_history` in the output. Default: Disabled (`false`) |\n\n## Directory\n\nThe **Directory** component recursively loads files from a directory, with options for file types, depth, and concurrency.\n\nFiles must be of a [supported type and size](#file-type-and-size-limits) to be loaded.\n\nOutputs either a [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) object, depending on the directory contents.\n\n### Directory parameters\n\n| Name               | Type             | Description                                        |\n| ------------------ | ---------------- | -------------------------------------------------- |\n| path               | MessageTextInput | Input parameter. The path to the directory to load files from. Default: Current directory (`.`) |\n| types              | MessageTextInput | Input parameter. The file types to load. Select one or more, or leave empty to attempt to load all files. |\n| depth              | IntInput         | Input parameter. The depth to search for files. |\n| max_concurrency    | IntInput         | Input parameter. The maximum concurrency for loading multiple files. |\n| load_hidden        | BoolInput        | Input parameter. If `true`, hidden files are loaded.                  |\n| recursive          | BoolInput        | Input parameter. If `true`, the search is recursive.                  |\n| silent_errors      | BoolInput        | Input parameter. If `true`, errors don't raise an exception.         |\n| use_multithreading | BoolInput        | Input parameter. If `true`, multithreading is used.                   |\n\n## File\n\nThe **File** component loads and parses files, converts the content into a `Data`, `DataFrame`, or `Message` object.\nIt supports multiple file types, provides parameters for parallel processing and error handling, and supports advanced parsing with the Docling library.\n\nYou can add files to the **File** component in the visual editor or at runtime, and you can upload multiple files at once.\nFor more information about uploading files and working with files in flows, see [File management](/concepts-file-management) and [Create a chatbot that can ingest files](/chat-with-files).\n\n### File type and size limits\n\nBy default, the maximum file size is 1024 MB.\nTo modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n<details>\n<summary>Supported file types</summary>\n\nThe following file types are supported by the **File** component.\nUse archive and compressed formats to bundle multiple files together, or use the [**Directory** component](#directory) to load all files in a directory.\n\n- `.bz2`\n- `.csv`\n- `.docx`\n- `.gz`\n- `.htm`\n- `.html`\n- `.json`\n- `.js`\n- `.md`\n- `.mdx`\n- `.pdf`\n- `.py`\n- `.sh`\n- `.sql`\n- `.tar`\n- `.tgz`\n- `.ts`\n- `.tsx`\n- `.txt`\n- `.xml`\n- `.yaml`\n- `.yml`\n- `.zip`\n\n</details>\n\nIf you need to load an unsupported file type, you must use a different component that supports that file type and, potentially, parses it outside Langflow, or you must convert it to a supported type before uploading it.\n\nFor images, see [Upload images](/concepts-file-management#upload-images).\n\nFor videos, see the **Twelve Labs** and **YouTube**  [**Bundles**](/components-bundle-components).\n\n### File parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| path | Files | Input parameter. The path to files to load. Can be local or in [Langflow file management](/concepts-file-management). Supports individual files and bundled archives. |\n| file_path | Server File Path | Input parameter. A `Data` object with a `file_path` property pointing to a file in [Langflow file management](/concepts-file-management) or a `Message` object with a path to the file. Supersedes **Files** (`path`) but supports the same file types. |\n| separator | Separator | Input parameter. The separator to use between multiple outputs in `Message` format. |\n| silent_errors | Silent Errors | Input parameter. If `true`, errors in the component don't raise an exception. Default: Disabled (`false`). |\n| delete_server_file_after_processing | Delete Server File After Processing | Input parameter. If `true` (default), the **Server File Path** (`file_path`) is deleted after processing. |\n| ignore_unsupported_extensions | Ignore Unsupported Extensions | Input parameter. If enabled (`true`), files with unsupported extensions are accepted but not processed. If disabled (`false`), the **File** component either can throw an error if an unsupported file type is provided. The default is `true`. |\n| ignore_unspecified_files | Ignore Unspecified Files | Input parameter. If `true`, `Data` with no `file_path` property is ignored. If `false` (default), the component errors when a file isn't specified. |\n| concurrency_multithreading | Processing Concurrency | Input parameter. The number of files to process concurrently if multiple files are uploaded. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. Ignored for single-file uploads and advanced parsing. |\n| advanced_parser | Advanced Parser | Input parameter. If `true`, enables [advanced parsing](#advanced-parsing). Only available for single-file uploads of compatible file types. Default: Disabled (`false`). |\n\n### Advanced parsing\n\nStarting in Langflow version 1.6, the **File** component supports advanced document parsing using the [Docling](https://docling-project.github.io/docling/) library for supported file types.\n\nTo use advanced parsing, do the following:\n\n1. Complete the following prerequisites, if applicable:\n\n    * **Install Langflow version 1.6 or later**: Earlier versions don't support advanced parsing with the **File** component. For upgrade guidance, see the [Release notes](/release-notes).\n\n    * **Install Docling dependency on macOS Intel (x86_64)**: The Docling dependency isn't installed by default for macOS Intel (x86_64). Use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.\n\n        For all other operating systems, the Docling dependency is installed by default.\n\n    * **Enable Developer Mode for Windows**:\n    \n\n        Developer Mode isn't required for Langflow OSS on Windows.\n\n2. Add one valid file to the **File** component.\n\n    \n**Note:** Advanced parsing limitations\n    * Advanced parsing processes only one file.\n    If you select multiple files, the **File** component processes the first file only, ignoring any additional files.\n    To process multiple files with advanced parsing, pass each file to a separate **File** components, or use the dedicated [**Docling** components](/bundles-docling).\n\n    * Advanced parsing can process any of the **File** component's supported file types except `.csv`, `.xlsx`, and `.parquet` files because it is designed for document processing, such as extracting text from PDFs.\n    For structured data analysis, use the [**Parser** component](/components-processing#parser).\n    \n\n3. Enable **Advanced Parsing**.\n\n4. Click  **Controls** in the [component's header menu](/concepts-components#component-menus) to configure advanced parsing parameters, which are hidden by default:\n\n    | Name | Display Name | Info |\n    |------|--------------|------|\n    | pipeline | Pipeline | Input parameter, advanced parsing. The Docling pipeline to use, either `standard` (default, recommended) or `vlm` (may produce inconsistent results). |\n    | ocr_engine | OCR Engine | Input parameter, advanced parsing. The OCR parser to use if `pipeline` is `standard`. Options are `None` (default) or [`EasyOCR`](https://pypi.org/project/easyocr/). `None` means that no OCR engine is used, and this can produce inconsistent or broken results for some documents. This setting has no effect with the `vlm` pipeline. |\n    | md_image_placeholder | Markdown Image Placeholder | Input parameter, advanced parsing. Defines the placeholder for image files if the output type is **Markdown**. Default: ``. |\n    | md_page_break_placeholder | Markdown Page Break Placeholder | Input parameter, advanced parsing. Defines the placeholder for page breaks if the output type is **Markdown**. Default: `\"\"` (empty string). |\n    | doc_key | Document Key | Input parameter, advanced parsing. The key to use for the `DoclingDocument` column, which holds the structured information extracted from the source document. See [Docling Document](https://docling-project.github.io/docling/concepts/docling_document/) for details. Default: `doc`. |\n\n    \n**Note:** For additional Docling features, including other components and OCR parsers, use the [**Docling** bundle](/bundles-docling).\n    \n\n### File output\n\nThe output of the **File** component depends on the number of files loaded and whether advanced parsing is enabled.\nIf multiple options are available, you can set the output type near the component's output port.\n\n</Tabs>\n\n## News Search\n\nThe **News Search** component searches Google News through RSS, and then returns clean article data as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.\nThe component's `clean_html` method parses the HTML content with the BeautifulSoup library, removes HTML markup, and strips whitespace to output clean data.\n\nFor other RSS feeds, use the [**RSS Reader** component](#rss-reader), and for other searches use the [**Web Search** component](#web-search) or provider-specific  [**Bundles**](/components-bundle-components).\n\nWhen used as a standard component in a flow, the **News Search** component must be connected to a component that accepts `DataFrame` input.\nYou can connect the **News Search** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **News Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **News Search** component based on the user's query, and it can process the `DataFrame` output directly.\n\n### News Search parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| query | Search Query | Input parameter. Search keywords for news articles. |\n| hl | Language (hl) | Input parameter. Language code, such as en-US, fr, de. Default: `en-US`. |\n| gl | Country (gl) | Input parameter. Country code, such as US, FR, DE. Default: `US`. |\n| ceid | Country:Language (ceid) | Input parameter. Language, such as US:en, FR:fr. Default: `US:en`. |\n| topic | Topic | Input parameter. One of: `WORLD`, `NATION`, `BUSINESS`, `TECHNOLOGY`, `ENTERTAINMENT`, `SCIENCE`, `SPORTS`, `HEALTH`. |\n| location | Location (Geo) | Input parameter. City, state, or country for location-based news. Leave blank for keyword search. |\n| timeout | Timeout | Input parameter. Timeout for the request in seconds. |\n| articles | News Articles | Output parameter. A `DataFrame` with the key columns `title`, `link`, `published` and `summary`. |\n\n## RSS Reader\n\nThe **RSS Reader** component fetches and parses RSS feeds from any valid RSS feed URL, and then returns the feed content as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.\n\nWhen used as a standard component in a flow, the **RSS Reader** component must be connected to a component that accepts `DataFrame` input.\nYou can connect the **RSS Reader** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **RSS Reader** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **RSS Reader** component based on the user's query, and it can process the `DataFrame` output directly.\n\n### RSS Reader parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| rss_url | RSS Feed URL | Input parameter. URL of the RSS feed to parse, such as `https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml`. |\n| timeout | Timeout | Input parameter. Timeout for the RSS feed request in seconds. Default: `5`. |\n| articles | Articles | Output parameter. A `DataFrame` containing the key columns `title`, `link`, `published` and `summary`. |\n\n## SQL Database\n\nThe **SQL Database** component executes SQL queries on [SQLAlchemy-compatible databases](https://docs.sqlalchemy.org/en/20/).\nIt supports any SQLAlchemy-compatible database, such as PostgreSQL, MySQL, and SQLite.\n\nFor CQL queries, see the [**DataStax** bundle](/bundles-datastax).\n\n### Query an SQL database with natural language prompts\n\nThe following example demonstrates how to use the **SQL Database** component in a flow, and then modify the component to support natural language queries through an **Agent** component.\n\nThis allows you to use the same **SQL Database** component for any query, rather than limiting it to a single manually entered query or requiring the user, application, or another component to provide valid SQL syntax as input.\nUsers don't need to master SQL syntax because the **Agent** component translates the users' natural language prompts into SQL queries, passes the query to the **SQL Database** component, and then returns the results to the user.\n\nAdditionally, input from applications and other components doesn't have to be extracted and transformed to exact SQL queries.\nInstead, you only need to provide enough context for the agent to understand that it should create and run a SQL query according to the incoming data.\n\n1. Use your own sample database or create a test database.\n\n    <details>\n    <summary>Create a test SQL database</summary>\n\n    1. Create a database called `test.db`:\n\n        ```shell\n        sqlite3 test.db\n        ```\n\n    2. Add some values to the database:\n\n        ```shell\n        sqlite3 test.db \"\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            email TEXT,\n            age INTEGER\n        );\n\n        INSERT INTO users (name, email, age) VALUES\n            ('John Doe', 'john@example.com', 30),\n            ('Jane Smith', 'jane@example.com', 25),\n            ('Bob Johnson', 'bob@example.com', 35);\n        \"\n        ```\n\n    3. Verify that the database has been created and contains your data:\n\n        ```shell\n        sqlite3 test.db \"SELECT * FROM users;\"\n        ```\n\n        The result should list the text data you entered in the previous step:\n\n        ```shell\n        1|John Doe|john@example.com\n        2|Jane Smith|jane@example.com\n        3|John Doe|john@example.com\n        4|Jane Smith|jane@example.com\n        ```\n\n    </details>\n\n2. Add an **SQL Database** component to your flow.\n\n3. In the **Database URL** field, add the connection string for your database, such as `sqlite:///test.db`.\n\n    At this point, you can enter an SQL query in the **SQL Query** field or use the [port](/concepts-components#component-ports) to pass a query from another component, such as a **Chat Input** component.\n    If you need more space, click  **Expand** to open a full-screen text field.\n\n    However, to make this component more dynamic in an agentic context, use an **Agent** component to transform natural language input to SQL queries, as explained in the following steps.\n\n4. Click the **SQL Database** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.\n\n    You can now use this component as a tool for an agent.\n    In **Tool Mode**, no query is set in the **SQL Database** component because the agent will generate and send one if it determines that the tool is required to complete the user's request.\n    For more information, see [Configure tools for agents](/agents-tools).\n\n5. Add an **Agent** component to your flow, and then enter your OpenAI API key.\n\n    The default model is an OpenAI model.\n    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n    If you need to execute highly specialized queries, consider selecting a model that is trained for tasks like advanced SQL queries.\n    If your preferred model isn't in the **Agent** component's built-in model list, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).\n\n6. Connect the **SQL Database** component's **Toolset** output to the **Agent** component's **Tools** input.\n\n    ![SQL Database component connected to an Agent component](/img/component-sql-database.png)\n\n7. Click **Playground**, and then ask the agent a question about the data in your database, such as `Which users are in my database?`\n\n    The agent determines that it needs to query the database to answer the question, uses the LLM to generate an SQL query, and then uses the **SQL Database** component's `RUN_SQL_QUERY` action to run the query on your database.\n    Finally, it returns the results in a conversational format, unless you provide instructions to return raw results or a different format.\n\n    The following example queried a test database with little data, but with a more robust dataset you could ask more detailed or complex questions.\n\n    ```text\n    Here are the users in your database:\n\n    1. **John Doe** - Email: john@example.com\n    2. **Jane Smith** - Email: jane@example.com\n    3. **John Doe** - Email: john@example.com\n    4. **Jane Smith** - Email: jane@example.com\n\n    It seems there are duplicate entries for the users.\n    ```\n\n### SQL Database parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| database_url | Database URL | Input parameter. The SQLAlchemy-compatible database connection URL. |\n| query | SQL Query | Input parameter. The SQL query to execute, which can be entered directly, passed in from another component, or, in **Tool Mode**, automatically provided by an **Agent** component. |\n| include_columns | Include Columns | Input parameter. Whether to include column names in the result. The default is enabled (`true`). |\n| add_error | Add Error | Input parameter. If enabled, adds any error messages to the result, if any are returned. The default is disabled (`false`). |\n| run_sql_query | Result Table | Output parameter. The query results as a [`DataFrame`](/data-types#dataframe). |\n\n## URL\n\nThe **URL** component fetches content from one or more URLs, processes the content, and returns it in various formats.\nIt follows links recursively to a given depth, and it supports output in plain text or raw HTML.\n\n### URL parameters\n\nSome of the available parameters include the following:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| urls | URLs | Input parameter. One or more URLs to crawl recursively. In the visual editor, click  **Add URL** to add multiple URLs. |\n| max_depth | Depth | Input parameter. Controls link traversal: how many \"clicks\" away from the initial page the crawler will go. A depth of 1 limits the crawl to the first page at the given URL only. A depth of 2 means the crawler crawls the first page plus each page directly linked from the first page, then stops. This setting exclusively controls link traversal; it doesn't limit the number of URL path segments or the domain. |\n| prevent_outside | Prevent Outside | Input parameter. If enabled, only crawls URLs within the same domain as the root URL. This prevents the crawler from accessing sites outside the given URL's domain, even if they are linked from one of the crawled pages. |\n| use_async | Use Async | Input parameter. If enabled, uses asynchronous loading which can be significantly faster but might use more system resources. |\n| format | Output Format | Input parameter. Sets the desired output format as **Text** or **HTML**. The default is **Text**. For more information, see [URL output](#url-output).|\n| timeout | Timeout | Input parameter. Timeout for the request in seconds. |\n| headers | Headers | Input parameter. The headers to send with the request if needed for authentication or otherwise. |\n\nAdditional input parameters are available for error handling and encoding.\n\n### URL output\n\nThere are two settings that control the output of the **URL** component at different stages:\n\n* **Output Format**: This optional parameter controls the content extracted from the crawled pages:\n\n    * **Text (default)**: The component extracts only the text from the HTML of the crawled pages.\n    * **HTML**: The component extracts the entire raw HTML content of the crawled pages.\n\n* **Output data type**: In the component's output field (near the output port) you can select the structure of the outgoing data when it is passed to other components:\n\n    * **Extracted Pages**: Outputs a [`DataFrame`](/data-types#dataframe) that breaks the crawled pages into columns for the entire page content (`text`) and metadata like `url` and `title`.\n    * **Raw Content**: Outputs a [`Message`](/data-types#message) containing the entire text or HTML from the crawled pages, including metadata, in a single block of text.\n\nWhen used as a standard component in a flow, the **URL** component must be connected to a component that accepts the selected output data type (`DataFrame` or `Message`).\nYou can connect the **URL** component directly to a compatible component, or you can use a [**Type Convert** component](/components-processing#type-convert) to convert the output to another type before passing the data to other components if the data types aren't directly compatible.\n\nProcessing components like the **Type Convert** component are useful with the **URL** component because it can extract a large amount of data from the crawled pages.\nFor example, if you only want to pass specific fields to other components, you can use a [**Parser** component](/components-processing#parser) to extract only that data from the crawled pages before passing the data to other components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **URL** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **URL** component based on the user's query, and it can process the `DataFrame` or `Message` output directly.\n\n## Web Search\n\nThe **Web Search** component performs a basic web search using DuckDuckGo's HTML scraping interface.\nFor other search APIs, see  [**Bundles**](/components-bundle-components).\n\n**Note:** The **Web Search** component uses web scraping that can be subject to rate limits.\n\nFor production use, consider using another search component with more robust API support, such as provider-specific bundles.\n\n### Use the Web Search component in a flow\n\nThe following steps demonstrate one way that you can use a **Web Search** component in a flow:\n\n1. Create a flow based on the **Basic Prompting** template.\n\n2. Add a **Web Search** component, and then enter a search query, such as `environmental news`.\n\n3. Add a [**Type Convert** component](/components-processing#type-convert), set the **Output Type** to **Message**, and then connect the **Web Search** component's output to the **Type Convert** component's input.\n\n    By default, the **Web Search** component outputs a `DataFrame`.\n    Because the **Prompt Template** component only accepts `Message` data, this conversion is required so that the flow can pass the search results to the **Prompt Template** component.\n    For more information, see [Web Search output](#web-search-output).\n\n5. In the **Prompt Template** component's **Template** field, add a variable like `{searchresults}` or `{context}`.\n\n    This adds a field to the **Prompt Template** component that you can use to pass the converted search results to the prompt.\n    For more information, see [Define variables in prompts](/components-prompts#define-variables-in-prompts).\n\n6. Connect the **Type Convert** component's output to the new variable field on the **Prompt Template** component.\n\n    ![Type convert web search output to chat](/img/component-type-convert-and-web-search.png)\n\n7. In the **Language Model** component, add your OpenAI API key, or select a different provider and model.\n\n8. Click **Playground**, and then enter `latest news`.\n\n    The LLM processes the request, including the context passed through the **Prompt Template** component, and then prints the response in the **Playground** chat interface.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example of a possible response.\n    Your response may vary based on the current state of the web, your specific query, the model, and other factors.\n\n    ```text\n    Here are some of the latest news articles related to the environment:\n    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more\n    ...\n    ```\n\n    </details>\n\n### Web Search parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| query | Search Query | Input parameter. Keywords to search for. |\n| timeout | Timeout | Input parameter. Timeout for the web search request in seconds. Default: `5`. |\n| results | Search Results | Output parameter. Returns a `DataFrame` containing `title`, `links`, and `snippets`. For more information, see [Web Search output](#web-search-output). |\n\n### Web Search output\n\nThe **Web Search** component outputs a [`DataFrame`](/data-types#dataframe) containing the key columns `title`, `links`, and `snippets`.\n\nWhen used as a standard component in a flow, the **Web Search** component must be connected to a component that accepts `DataFrame` input, or you must use a [**Type Convert** component](/components-processing#type-convert) to convert the output to `Data` or `Message` type before passing the data to other components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **Web Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **Web Search** component based on the user's query, and it can process the `DataFrame` output directly.\n\n## Webhook\n\nThe **Webhook** component defines a webhook trigger that runs a flow when it receives an HTTP POST request.\n\n### Trigger the webhook\n\nWhen you add a **Webhook** component to your flow, a **Webhook curl** tab is added to the flow's [**API Access** pane](/concepts-publish#api-access).\nThis tab automatically generates an HTTP POST request code snippet that you can use to trigger your flow through the **Webhook** component.\nFor example:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/webhook/$FLOW_ID\" \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: $LANGFLOW_API_KEY' \\\n  -d '{\"any\": \"data\"}'\n```\n\nFor more information, see [Trigger flows with webhooks](/webhook).\n\n### Webhook parameters\n\n| Name | Display Name | Description |\n|------|--------------|-------------|\n| data | Payload | Input parameter. Receives a payload from external systems through HTTP POST requests. |\n| curl | curl | Input parameter. The curl command template for making requests to this webhook. |\n| endpoint | Endpoint | Input parameter. The endpoint URL where this webhook receives requests. |\n| output_data | Data | Output parameter. The processed data from the webhook input. Returns an empty [`Data`](/data-types#data) object if no input is provided. If the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow. |\n\n## Additional Data components\n\nLangflow's core components are meant to be generic and support a range of use cases.\nCore components typically aren't limited to a single provider.\n\nIf the core components don't meet your needs, you can find provider-specific components in  [**Bundles**](/components-bundle-components).\n\nFor example, the [**DataStax** bundle](/bundles-datastax) includes components for CQL queries, and the [**Google** bundle](/bundles-google) includes components for Google Search APIs.\n\n## Legacy Data components\n\nThe following Data components are in legacy status:\n\n* **Load CSV**\n* **Load JSON**\n\nReplace these components with the **File** component, which supports loading CSV and JSON files, as well as many other file types.\n\n## See also\n\n- [**Google** bundle](/bundles-google)\n- [**Composio** bundle](/bundles-composio)\n- [File management](/concepts-file-management)",
      "summary": "Data components bring data into your flows from various sources like files, API endpoints, and URLs.\nFor example:",
      "keywords": [
        "access",
        "add",
        "additional",
        "advanced",
        "agent",
        "api",
        "basic",
        "bundles",
        "calls",
        "chat",
        "component",
        "components",
        "composio",
        "connect",
        "content",
        "controls",
        "convert",
        "csv",
        "curl",
        "data"
      ]
    },
    "components-data": {
      "title": "Data",
      "slug": "/components-data",
      "category": "components",
      "content": "Data components bring data into your flows from various sources like files, API endpoints, and URLs.\nFor example:\n\n* **Load files**: Import data from a file or directory with the [**File** component](#file) and [**Directory** component](#directory).\n\n* **Search the web**: Fetch data from the web with components like the [**News Search** component](#news-search), [**RSS Reader** component](#rss-reader), [**Web Search** component](#web-search), and [**URL** component](#url).\n\n* **Make API calls**: Use APIs to trigger flows or perform actions with the [**API Request** component](#api-request) and [**Webhook** component](#webhook).\n\n* **Run SQL queries**: Query an SQL database with the [**SQL Database** component](#sql-database).\n\nEach component runs different commands for retrieval, processing, and type checking.\nSome components are a minimal wrapper for a command that you provide, and others include built-in scripts to fetch and process data based on variable inputs.\nAdditionally, some components return raw data, whereas others can convert, restructure, or validate the data before outputting it.\nThis means that some similar components might produce different results.\n\n**Note:** Data components pair well with [Processing components](/components-processing) that can perform additional parsing, transformation, and validation after retrieving the data.\n\nThis can include basic operations, like saving a file in a specific format, or more complex tasks, like using a **Text Splitter** component to break down a large document into smaller chunks before generating embeddings for vector search.\n\n## Use Data components in flows\n\nData components are used often in flows because they offer a versatile way to perform common functions.\n\nYou can use these components to perform their base functions as isolated steps in your flow, or you can connect them to an **Agent** component as tools.\n\n![An agent flow with three Data components connected to the agent as tools](/img/connect-data-components-to-agent.png)\n\nFor example flows, see the following:\n\n* [Create a chatbot that can ingest files](/chat-with-files): Learn how to use a **File** component to load a file as context for a chatbot.\nThe file and user input are both passed to the LLM so you can ask questions about the file you uploaded.\n\n* [Create a vector RAG chatbot](/chat-with-rag): Learn how to ingest files for use in Retrieval-Augmented Generation (RAG), and then set up a chatbot that can use the ingested files as context.\nThe two flows in this tutorial prepare files for RAG, and then let your LLM use vector search to retrieve contextually relevant data during a chat session.\n\n* [Configure tools for agents](/agents-tools): Learn how to use any component as a tool for an agent.\nWhen used as tools, the agent autonomously decides when to call a component based on the user's query.\n\n* [Trigger flows with webhooks](/webhook): Learn how to use the **Webhook** component to trigger a flow run in response to an external event.\n\n## API Request\n\nThe **API Request** component constructs and sends HTTP requests using URLs or curl commands:\n\n* **URL mode**: Enter one or more comma-separated URLs, and then select the method for the request to each URL.\n* **curl mode**: Enter the curl command to execute.\n\nYou can enable additional request options and fields in the component's parameters.\n\nReturns a [`Data` object](/data-types#data) containing the response.\n\nFor provider-specific API components, see  [**Bundles**](/components-bundle-components).\n\n### API Request parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| mode | Mode | Input parameter. Set the mode to either **URL** or **curl**. |\n| urls | URL | Input parameter. Enter one or more comma-separated URLs for the request. |\n| curl | curl | Input parameter. **curl mode** only. Enter a complete curl command. Other component parameters are populated from the command arguments. |\n| method | Method | Input parameter. The HTTP method to use. |\n| query_params | Query Parameters | Input parameter. The query parameters to append to the URL. |\n| body | Body | Input parameter. The body to send with POST, PATCH, and PUT requests as a dictionary. |\n| headers | Headers | Input parameter. The headers to send with the request as a dictionary. |\n| timeout | Timeout | Input parameter. The timeout to use for the request. |\n| follow_redirects | Follow Redirects | Input parameter. Whether to follow HTTP redirects. The default is enabled (`true`). If disabled (`false`), HTTP redirects aren't followed. |\n| save_to_file | Save to File | Input parameter. Whether to save the API response to a temporary file. Default: Disabled (`false`) |\n| include_httpx_metadata | Include HTTPx Metadata | Input parameter. Whether to include properties such as `headers`, `status_code`, `response_headers`, and `redirection_history` in the output. Default: Disabled (`false`) |\n\n## Directory\n\nThe **Directory** component recursively loads files from a directory, with options for file types, depth, and concurrency.\n\nFiles must be of a [supported type and size](#file-type-and-size-limits) to be loaded.\n\nOutputs either a [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) object, depending on the directory contents.\n\n### Directory parameters\n\n| Name               | Type             | Description                                        |\n| ------------------ | ---------------- | -------------------------------------------------- |\n| path               | MessageTextInput | Input parameter. The path to the directory to load files from. Default: Current directory (`.`) |\n| types              | MessageTextInput | Input parameter. The file types to load. Select one or more, or leave empty to attempt to load all files. |\n| depth              | IntInput         | Input parameter. The depth to search for files. |\n| max_concurrency    | IntInput         | Input parameter. The maximum concurrency for loading multiple files. |\n| load_hidden        | BoolInput        | Input parameter. If `true`, hidden files are loaded.                  |\n| recursive          | BoolInput        | Input parameter. If `true`, the search is recursive.                  |\n| silent_errors      | BoolInput        | Input parameter. If `true`, errors don't raise an exception.         |\n| use_multithreading | BoolInput        | Input parameter. If `true`, multithreading is used.                   |\n\n## File\n\nThe **File** component loads and parses files, converts the content into a `Data`, `DataFrame`, or `Message` object.\nIt supports multiple file types, provides parameters for parallel processing and error handling, and supports advanced parsing with the Docling library.\n\nYou can add files to the **File** component in the visual editor or at runtime, and you can upload multiple files at once.\nFor more information about uploading files and working with files in flows, see [File management](/concepts-file-management) and [Create a chatbot that can ingest files](/chat-with-files).\n\n### File type and size limits\n\nBy default, the maximum file size is 1024 MB.\nTo modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n<details>\n<summary>Supported file types</summary>\n\nThe following file types are supported by the **File** component.\nUse archive and compressed formats to bundle multiple files together, or use the [**Directory** component](#directory) to load all files in a directory.\n\n- `.bz2`\n- `.csv`\n- `.docx`\n- `.gz`\n- `.htm`\n- `.html`\n- `.json`\n- `.js`\n- `.md`\n- `.mdx`\n- `.pdf`\n- `.py`\n- `.sh`\n- `.sql`\n- `.tar`\n- `.tgz`\n- `.ts`\n- `.tsx`\n- `.txt`\n- `.xml`\n- `.yaml`\n- `.yml`\n- `.zip`\n\n</details>\n\nIf you need to load an unsupported file type, you must use a different component that supports that file type and, potentially, parses it outside Langflow, or you must convert it to a supported type before uploading it.\n\nFor images, see [Upload images](/concepts-file-management#upload-images).\n\nFor videos, see the **Twelve Labs** and **YouTube**  [**Bundles**](/components-bundle-components).\n\n### File parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| path | Files | Input parameter. The path to files to load. Can be local or in [Langflow file management](/concepts-file-management). Supports individual files and bundled archives. |\n| file_path | Server File Path | Input parameter. A `Data` object with a `file_path` property pointing to a file in [Langflow file management](/concepts-file-management) or a `Message` object with a path to the file. Supersedes **Files** (`path`) but supports the same file types. |\n| separator | Separator | Input parameter. The separator to use between multiple outputs in `Message` format. |\n| silent_errors | Silent Errors | Input parameter. If `true`, errors in the component don't raise an exception. Default: Disabled (`false`). |\n| delete_server_file_after_processing | Delete Server File After Processing | Input parameter. If `true` (default), the **Server File Path** (`file_path`) is deleted after processing. |\n| ignore_unsupported_extensions | Ignore Unsupported Extensions | Input parameter. If enabled (`true`), files with unsupported extensions are accepted but not processed. If disabled (`false`), the **File** component either can throw an error if an unsupported file type is provided. The default is `true`. |\n| ignore_unspecified_files | Ignore Unspecified Files | Input parameter. If `true`, `Data` with no `file_path` property is ignored. If `false` (default), the component errors when a file isn't specified. |\n| concurrency_multithreading | Processing Concurrency | Input parameter. The number of files to process concurrently if multiple files are uploaded. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. Ignored for single-file uploads and advanced parsing. |\n| advanced_parser | Advanced Parser | Input parameter. If `true`, enables [advanced parsing](#advanced-parsing). Only available for single-file uploads of compatible file types. Default: Disabled (`false`). |\n\n### Advanced parsing\n\nStarting in Langflow version 1.6, the **File** component supports advanced document parsing using the [Docling](https://docling-project.github.io/docling/) library for supported file types.\n\nTo use advanced parsing, do the following:\n\n1. Complete the following prerequisites, if applicable:\n\n    * **Install Langflow version 1.6 or later**: Earlier versions don't support advanced parsing with the **File** component. For upgrade guidance, see the [Release notes](/release-notes).\n\n    * **Install Docling dependency on macOS Intel (x86_64)**: The Docling dependency isn't installed by default for macOS Intel (x86_64). Use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.\n\n        For all other operating systems, the Docling dependency is installed by default.\n\n    * **Enable Developer Mode for Windows**:\n    \n\n        Developer Mode isn't required for Langflow OSS on Windows.\n\n2. Add one valid file to the **File** component.\n\n    \n**Note:** Advanced parsing limitations\n    * Advanced parsing processes only one file.\n    If you select multiple files, the **File** component processes the first file only, ignoring any additional files.\n    To process multiple files with advanced parsing, pass each file to a separate **File** components, or use the dedicated [**Docling** components](/bundles-docling).\n\n    * Advanced parsing can process any of the **File** component's supported file types except `.csv`, `.xlsx`, and `.parquet` files because it is designed for document processing, such as extracting text from PDFs.\n    For structured data analysis, use the [**Parser** component](/components-processing#parser).\n    \n\n3. Enable **Advanced Parsing**.\n\n4. Click  **Controls** in the [component's header menu](/concepts-components#component-menus) to configure advanced parsing parameters, which are hidden by default:\n\n    | Name | Display Name | Info |\n    |------|--------------|------|\n    | pipeline | Pipeline | Input parameter, advanced parsing. The Docling pipeline to use, either `standard` (default, recommended) or `vlm` (may produce inconsistent results). |\n    | ocr_engine | OCR Engine | Input parameter, advanced parsing. The OCR parser to use if `pipeline` is `standard`. Options are `None` (default) or [`EasyOCR`](https://pypi.org/project/easyocr/). `None` means that no OCR engine is used, and this can produce inconsistent or broken results for some documents. This setting has no effect with the `vlm` pipeline. |\n    | md_image_placeholder | Markdown Image Placeholder | Input parameter, advanced parsing. Defines the placeholder for image files if the output type is **Markdown**. Default: ``. |\n    | md_page_break_placeholder | Markdown Page Break Placeholder | Input parameter, advanced parsing. Defines the placeholder for page breaks if the output type is **Markdown**. Default: `\"\"` (empty string). |\n    | doc_key | Document Key | Input parameter, advanced parsing. The key to use for the `DoclingDocument` column, which holds the structured information extracted from the source document. See [Docling Document](https://docling-project.github.io/docling/concepts/docling_document/) for details. Default: `doc`. |\n\n    \n**Note:** For additional Docling features, including other components and OCR parsers, use the [**Docling** bundle](/bundles-docling).\n    \n\n### File output\n\nThe output of the **File** component depends on the number of files loaded and whether advanced parsing is enabled.\nIf multiple options are available, you can set the output type near the component's output port.\n\n</Tabs>\n\n## News Search\n\nThe **News Search** component searches Google News through RSS, and then returns clean article data as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.\nThe component's `clean_html` method parses the HTML content with the BeautifulSoup library, removes HTML markup, and strips whitespace to output clean data.\n\nFor other RSS feeds, use the [**RSS Reader** component](#rss-reader), and for other searches use the [**Web Search** component](#web-search) or provider-specific  [**Bundles**](/components-bundle-components).\n\nWhen used as a standard component in a flow, the **News Search** component must be connected to a component that accepts `DataFrame` input.\nYou can connect the **News Search** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **News Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **News Search** component based on the user's query, and it can process the `DataFrame` output directly.\n\n### News Search parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| query | Search Query | Input parameter. Search keywords for news articles. |\n| hl | Language (hl) | Input parameter. Language code, such as en-US, fr, de. Default: `en-US`. |\n| gl | Country (gl) | Input parameter. Country code, such as US, FR, DE. Default: `US`. |\n| ceid | Country:Language (ceid) | Input parameter. Language, such as US:en, FR:fr. Default: `US:en`. |\n| topic | Topic | Input parameter. One of: `WORLD`, `NATION`, `BUSINESS`, `TECHNOLOGY`, `ENTERTAINMENT`, `SCIENCE`, `SPORTS`, `HEALTH`. |\n| location | Location (Geo) | Input parameter. City, state, or country for location-based news. Leave blank for keyword search. |\n| timeout | Timeout | Input parameter. Timeout for the request in seconds. |\n| articles | News Articles | Output parameter. A `DataFrame` with the key columns `title`, `link`, `published` and `summary`. |\n\n## RSS Reader\n\nThe **RSS Reader** component fetches and parses RSS feeds from any valid RSS feed URL, and then returns the feed content as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.\n\nWhen used as a standard component in a flow, the **RSS Reader** component must be connected to a component that accepts `DataFrame` input.\nYou can connect the **RSS Reader** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **RSS Reader** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **RSS Reader** component based on the user's query, and it can process the `DataFrame` output directly.\n\n### RSS Reader parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| rss_url | RSS Feed URL | Input parameter. URL of the RSS feed to parse, such as `https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml`. |\n| timeout | Timeout | Input parameter. Timeout for the RSS feed request in seconds. Default: `5`. |\n| articles | Articles | Output parameter. A `DataFrame` containing the key columns `title`, `link`, `published` and `summary`. |\n\n## SQL Database\n\nThe **SQL Database** component executes SQL queries on [SQLAlchemy-compatible databases](https://docs.sqlalchemy.org/en/20/).\nIt supports any SQLAlchemy-compatible database, such as PostgreSQL, MySQL, and SQLite.\n\nFor CQL queries, see the [**DataStax** bundle](/bundles-datastax).\n\n### Query an SQL database with natural language prompts\n\nThe following example demonstrates how to use the **SQL Database** component in a flow, and then modify the component to support natural language queries through an **Agent** component.\n\nThis allows you to use the same **SQL Database** component for any query, rather than limiting it to a single manually entered query or requiring the user, application, or another component to provide valid SQL syntax as input.\nUsers don't need to master SQL syntax because the **Agent** component translates the users' natural language prompts into SQL queries, passes the query to the **SQL Database** component, and then returns the results to the user.\n\nAdditionally, input from applications and other components doesn't have to be extracted and transformed to exact SQL queries.\nInstead, you only need to provide enough context for the agent to understand that it should create and run a SQL query according to the incoming data.\n\n1. Use your own sample database or create a test database.\n\n    <details>\n    <summary>Create a test SQL database</summary>\n\n    1. Create a database called `test.db`:\n\n        ```shell\n        sqlite3 test.db\n        ```\n\n    2. Add some values to the database:\n\n        ```shell\n        sqlite3 test.db \"\n        CREATE TABLE users (\n            id INTEGER PRIMARY KEY,\n            name TEXT,\n            email TEXT,\n            age INTEGER\n        );\n\n        INSERT INTO users (name, email, age) VALUES\n            ('John Doe', 'john@example.com', 30),\n            ('Jane Smith', 'jane@example.com', 25),\n            ('Bob Johnson', 'bob@example.com', 35);\n        \"\n        ```\n\n    3. Verify that the database has been created and contains your data:\n\n        ```shell\n        sqlite3 test.db \"SELECT * FROM users;\"\n        ```\n\n        The result should list the text data you entered in the previous step:\n\n        ```shell\n        1|John Doe|john@example.com\n        2|Jane Smith|jane@example.com\n        3|John Doe|john@example.com\n        4|Jane Smith|jane@example.com\n        ```\n\n    </details>\n\n2. Add an **SQL Database** component to your flow.\n\n3. In the **Database URL** field, add the connection string for your database, such as `sqlite:///test.db`.\n\n    At this point, you can enter an SQL query in the **SQL Query** field or use the [port](/concepts-components#component-ports) to pass a query from another component, such as a **Chat Input** component.\n    If you need more space, click  **Expand** to open a full-screen text field.\n\n    However, to make this component more dynamic in an agentic context, use an **Agent** component to transform natural language input to SQL queries, as explained in the following steps.\n\n4. Click the **SQL Database** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.\n\n    You can now use this component as a tool for an agent.\n    In **Tool Mode**, no query is set in the **SQL Database** component because the agent will generate and send one if it determines that the tool is required to complete the user's request.\n    For more information, see [Configure tools for agents](/agents-tools).\n\n5. Add an **Agent** component to your flow, and then enter your OpenAI API key.\n\n    The default model is an OpenAI model.\n    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n    If you need to execute highly specialized queries, consider selecting a model that is trained for tasks like advanced SQL queries.\n    If your preferred model isn't in the **Agent** component's built-in model list, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).\n\n6. Connect the **SQL Database** component's **Toolset** output to the **Agent** component's **Tools** input.\n\n    ![SQL Database component connected to an Agent component](/img/component-sql-database.png)\n\n7. Click **Playground**, and then ask the agent a question about the data in your database, such as `Which users are in my database?`\n\n    The agent determines that it needs to query the database to answer the question, uses the LLM to generate an SQL query, and then uses the **SQL Database** component's `RUN_SQL_QUERY` action to run the query on your database.\n    Finally, it returns the results in a conversational format, unless you provide instructions to return raw results or a different format.\n\n    The following example queried a test database with little data, but with a more robust dataset you could ask more detailed or complex questions.\n\n    ```text\n    Here are the users in your database:\n\n    1. **John Doe** - Email: john@example.com\n    2. **Jane Smith** - Email: jane@example.com\n    3. **John Doe** - Email: john@example.com\n    4. **Jane Smith** - Email: jane@example.com\n\n    It seems there are duplicate entries for the users.\n    ```\n\n### SQL Database parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| database_url | Database URL | Input parameter. The SQLAlchemy-compatible database connection URL. |\n| query | SQL Query | Input parameter. The SQL query to execute, which can be entered directly, passed in from another component, or, in **Tool Mode**, automatically provided by an **Agent** component. |\n| include_columns | Include Columns | Input parameter. Whether to include column names in the result. The default is enabled (`true`). |\n| add_error | Add Error | Input parameter. If enabled, adds any error messages to the result, if any are returned. The default is disabled (`false`). |\n| run_sql_query | Result Table | Output parameter. The query results as a [`DataFrame`](/data-types#dataframe). |\n\n## URL\n\nThe **URL** component fetches content from one or more URLs, processes the content, and returns it in various formats.\nIt follows links recursively to a given depth, and it supports output in plain text or raw HTML.\n\n### URL parameters\n\nSome of the available parameters include the following:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| urls | URLs | Input parameter. One or more URLs to crawl recursively. In the visual editor, click  **Add URL** to add multiple URLs. |\n| max_depth | Depth | Input parameter. Controls link traversal: how many \"clicks\" away from the initial page the crawler will go. A depth of 1 limits the crawl to the first page at the given URL only. A depth of 2 means the crawler crawls the first page plus each page directly linked from the first page, then stops. This setting exclusively controls link traversal; it doesn't limit the number of URL path segments or the domain. |\n| prevent_outside | Prevent Outside | Input parameter. If enabled, only crawls URLs within the same domain as the root URL. This prevents the crawler from accessing sites outside the given URL's domain, even if they are linked from one of the crawled pages. |\n| use_async | Use Async | Input parameter. If enabled, uses asynchronous loading which can be significantly faster but might use more system resources. |\n| format | Output Format | Input parameter. Sets the desired output format as **Text** or **HTML**. The default is **Text**. For more information, see [URL output](#url-output).|\n| timeout | Timeout | Input parameter. Timeout for the request in seconds. |\n| headers | Headers | Input parameter. The headers to send with the request if needed for authentication or otherwise. |\n\nAdditional input parameters are available for error handling and encoding.\n\n### URL output\n\nThere are two settings that control the output of the **URL** component at different stages:\n\n* **Output Format**: This optional parameter controls the content extracted from the crawled pages:\n\n    * **Text (default)**: The component extracts only the text from the HTML of the crawled pages.\n    * **HTML**: The component extracts the entire raw HTML content of the crawled pages.\n\n* **Output data type**: In the component's output field (near the output port) you can select the structure of the outgoing data when it is passed to other components:\n\n    * **Extracted Pages**: Outputs a [`DataFrame`](/data-types#dataframe) that breaks the crawled pages into columns for the entire page content (`text`) and metadata like `url` and `title`.\n    * **Raw Content**: Outputs a [`Message`](/data-types#message) containing the entire text or HTML from the crawled pages, including metadata, in a single block of text.\n\nWhen used as a standard component in a flow, the **URL** component must be connected to a component that accepts the selected output data type (`DataFrame` or `Message`).\nYou can connect the **URL** component directly to a compatible component, or you can use a [**Type Convert** component](/components-processing#type-convert) to convert the output to another type before passing the data to other components if the data types aren't directly compatible.\n\nProcessing components like the **Type Convert** component are useful with the **URL** component because it can extract a large amount of data from the crawled pages.\nFor example, if you only want to pass specific fields to other components, you can use a [**Parser** component](/components-processing#parser) to extract only that data from the crawled pages before passing the data to other components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **URL** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **URL** component based on the user's query, and it can process the `DataFrame` or `Message` output directly.\n\n## Web Search\n\nThe **Web Search** component performs a basic web search using DuckDuckGo's HTML scraping interface.\nFor other search APIs, see  [**Bundles**](/components-bundle-components).\n\n**Note:** The **Web Search** component uses web scraping that can be subject to rate limits.\n\nFor production use, consider using another search component with more robust API support, such as provider-specific bundles.\n\n### Use the Web Search component in a flow\n\nThe following steps demonstrate one way that you can use a **Web Search** component in a flow:\n\n1. Create a flow based on the **Basic Prompting** template.\n\n2. Add a **Web Search** component, and then enter a search query, such as `environmental news`.\n\n3. Add a [**Type Convert** component](/components-processing#type-convert), set the **Output Type** to **Message**, and then connect the **Web Search** component's output to the **Type Convert** component's input.\n\n    By default, the **Web Search** component outputs a `DataFrame`.\n    Because the **Prompt Template** component only accepts `Message` data, this conversion is required so that the flow can pass the search results to the **Prompt Template** component.\n    For more information, see [Web Search output](#web-search-output).\n\n5. In the **Prompt Template** component's **Template** field, add a variable like `{searchresults}` or `{context}`.\n\n    This adds a field to the **Prompt Template** component that you can use to pass the converted search results to the prompt.\n    For more information, see [Define variables in prompts](/components-prompts#define-variables-in-prompts).\n\n6. Connect the **Type Convert** component's output to the new variable field on the **Prompt Template** component.\n\n    ![Type convert web search output to chat](/img/component-type-convert-and-web-search.png)\n\n7. In the **Language Model** component, add your OpenAI API key, or select a different provider and model.\n\n8. Click **Playground**, and then enter `latest news`.\n\n    The LLM processes the request, including the context passed through the **Prompt Template** component, and then prints the response in the **Playground** chat interface.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example of a possible response.\n    Your response may vary based on the current state of the web, your specific query, the model, and other factors.\n\n    ```text\n    Here are some of the latest news articles related to the environment:\n    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more\n    ...\n    ```\n\n    </details>\n\n### Web Search parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| query | Search Query | Input parameter. Keywords to search for. |\n| timeout | Timeout | Input parameter. Timeout for the web search request in seconds. Default: `5`. |\n| results | Search Results | Output parameter. Returns a `DataFrame` containing `title`, `links`, and `snippets`. For more information, see [Web Search output](#web-search-output). |\n\n### Web Search output\n\nThe **Web Search** component outputs a [`DataFrame`](/data-types#dataframe) containing the key columns `title`, `links`, and `snippets`.\n\nWhen used as a standard component in a flow, the **Web Search** component must be connected to a component that accepts `DataFrame` input, or you must use a [**Type Convert** component](/components-processing#type-convert) to convert the output to `Data` or `Message` type before passing the data to other components.\n\nWhen used in **Tool Mode** with an **Agent** component, the **Web Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.\nThe agent decides whether to use the **Web Search** component based on the user's query, and it can process the `DataFrame` output directly.\n\n## Webhook\n\nThe **Webhook** component defines a webhook trigger that runs a flow when it receives an HTTP POST request.\n\n### Trigger the webhook\n\nWhen you add a **Webhook** component to your flow, a **Webhook curl** tab is added to the flow's [**API Access** pane](/concepts-publish#api-access).\nThis tab automatically generates an HTTP POST request code snippet that you can use to trigger your flow through the **Webhook** component.\nFor example:\n\n```bash\ncurl -X POST \\\n  \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/webhook/$FLOW_ID\" \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: $LANGFLOW_API_KEY' \\\n  -d '{\"any\": \"data\"}'\n```\n\nFor more information, see [Trigger flows with webhooks](/webhook).\n\n### Webhook parameters\n\n| Name | Display Name | Description |\n|------|--------------|-------------|\n| data | Payload | Input parameter. Receives a payload from external systems through HTTP POST requests. |\n| curl | curl | Input parameter. The curl command template for making requests to this webhook. |\n| endpoint | Endpoint | Input parameter. The endpoint URL where this webhook receives requests. |\n| output_data | Data | Output parameter. The processed data from the webhook input. Returns an empty [`Data`](/data-types#data) object if no input is provided. If the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow. |\n\n## Additional Data components\n\nLangflow's core components are meant to be generic and support a range of use cases.\nCore components typically aren't limited to a single provider.\n\nIf the core components don't meet your needs, you can find provider-specific components in  [**Bundles**](/components-bundle-components).\n\nFor example, the [**DataStax** bundle](/bundles-datastax) includes components for CQL queries, and the [**Google** bundle](/bundles-google) includes components for Google Search APIs.\n\n## Legacy Data components\n\nThe following Data components are in legacy status:\n\n* **Load CSV**\n* **Load JSON**\n\nReplace these components with the **File** component, which supports loading CSV and JSON files, as well as many other file types.\n\n## See also\n\n- [**Google** bundle](/bundles-google)\n- [**Composio** bundle](/bundles-composio)\n- [File management](/concepts-file-management)",
      "summary": "Data components bring data into your flows from various sources like files, API endpoints, and URLs.\nFor example:",
      "keywords": [
        "access",
        "add",
        "additional",
        "advanced",
        "agent",
        "api",
        "basic",
        "bundles",
        "calls",
        "chat",
        "component",
        "components",
        "composio",
        "connect",
        "content",
        "controls",
        "convert",
        "csv",
        "curl",
        "data"
      ]
    },
    "/components-embedding-models": {
      "title": "Embedding Model",
      "slug": "/components-embedding-models",
      "category": "components",
      "content": "Embedding model components in Langflow generate text embeddings using a specified Large Language Model (LLM).\n\nLangflow includes an **Embedding Model** core component that has built-in support for some LLMs.\nAlternatively, you can use any [additional embedding model](#additional-embedding-models) in place of the **Embedding Model** core component.\n\n## Use embedding model components in a flow\n\nUse embedding model components anywhere you need to generate embeddings in a flow.\n\nThis example shows how to use an embedding model component in a flow to create a semantic search system.\nThis flow loads a text file, splits the text into chunks, generates embeddings for each chunk, and then loads the chunks and embeddings into a vector store. The input and output components allow a user to query the vector store through a chat interface.\n\n![A semantic search flow that uses Embedding Model, File, Split Text, Chroma DB, Chat Input, and Chat Output components](/img/component-embedding-models-add-chat.png)\n\n1. Create a flow, add a **File** component, and then select a file containing text data, such as a PDF, that you can use to test the flow.\n\n2. Add the **Embedding Model** core component, and then provide a valid OpenAI API key.\nYou can enter the API key directly or use a  [global variable](/configuration-global-variables).\n\n    \n**Note:** My preferred provider or model isn't listed\n    If your preferred embedding model provider or model isn't supported by the **Embedding Model** core component, you can use any [additional embedding models](#additional-embedding-models) in place of the core component.\n\n    Browse  [**Bundles**](/components-bundle-components) or  **Search** for your preferred provider to find additional embedding models, such as the [**Hugging Face Embeddings Inference** component](/bundles-huggingface#hugging-face-embeddings-inference).\n    \n\n3. Add a [**Split Text** component](/components-processing#split-text) to your flow.\nThis component splits text input into smaller chunks to be processed into embeddings.\n\n4. Add a vector store component, such as the **Chroma DB** component, to your flow, and then configure the component to connect to your vector database.\nThis component stores the generated embeddings so they can be used for similarity search.\n\n5. Connect the components:\n\n    * Connect the **File** component's **Loaded Files** output to the **Split Text** component's **Data or DataFrame** input.\n    * Connect the **Split Text** component's **Chunks** output to the vector store component's **Ingest Data** input.\n    * Connect the **Embedding Model** component's **Embeddings** output to the vector store component's **Embedding** input.\n\n6. To query the vector store, add [**Chat Input and Output** components](/components-io#chat-io):\n\n    * Connect the **Chat Input** component to the vector store component's **Search Query** input.\n    * Connect the vector store component's **Search Results** output to the **Chat Output** component.\n\n7. Click **Playground**, and then enter a search query to retrieve text chunks that are most semantically similar to your query.\n\n## Embedding Model parameters\n\nThe following parameters are for the **Embedding Model** core component.\nOther embedding model components can have additional or different parameters.\n\n| Name | Display Name | Type | Description |\n|------|--------------|------|-------------|\n| provider | Model Provider | List | Input parameter. Select the embedding model provider. |\n| model | Model Name | List | Input parameter. Select the embedding model to use.|\n| api_key | OpenAI API Key | Secret[String] | Input parameter. The API key required for authenticating with the provider. |\n| api_base | API Base URL | String | Input parameter. Base URL for the API. Leave empty for default. |\n| dimensions | Dimensions | Integer | Input parameter. The number of dimensions for the output embeddings. |\n| chunk_size | Chunk Size | Integer | Input parameter. The size of text chunks to process. Default: `1000`. |\n| request_timeout | Request Timeout | Float | Input parameter. Timeout for API requests. |\n| max_retries | Max Retries | Integer | Input parameter. Maximum number of retry attempts. Default: `3`. |\n| show_progress_bar | Show Progress Bar | Boolean | Input parameter. Whether to display a progress bar during embedding generation. |\n| model_kwargs | Model Kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n| embeddings | Embeddings | Embeddings | Output parameter. An instance for generating embeddings using the selected provider. |\n\n## Additional embedding models\n\nIf your provider or model isn't supported by the **Embedding Model** core component, you can replace this component with any other component that generates embeddings.\n\nTo find additional embedding model components, browse  [**Bundles**](/components-bundle-components) or  **Search** for your preferred provider.\n\n## Pair models with vector stores\n\n<details>\n<summary>Example: Vector search flow</summary>\n\n</details>",
      "summary": "Embedding model components in Langflow generate text embeddings using a specified Large Language Model (LLM).",
      "keywords": [
        "additional",
        "bundles",
        "chat",
        "chroma",
        "chunks",
        "components",
        "data",
        "dataframe",
        "embedding",
        "embeddings",
        "face",
        "file",
        "files",
        "flow",
        "hugging",
        "inference",
        "ingest",
        "input",
        "loaded",
        "model"
      ]
    },
    "components-embedding-models": {
      "title": "Embedding Model",
      "slug": "/components-embedding-models",
      "category": "components",
      "content": "Embedding model components in Langflow generate text embeddings using a specified Large Language Model (LLM).\n\nLangflow includes an **Embedding Model** core component that has built-in support for some LLMs.\nAlternatively, you can use any [additional embedding model](#additional-embedding-models) in place of the **Embedding Model** core component.\n\n## Use embedding model components in a flow\n\nUse embedding model components anywhere you need to generate embeddings in a flow.\n\nThis example shows how to use an embedding model component in a flow to create a semantic search system.\nThis flow loads a text file, splits the text into chunks, generates embeddings for each chunk, and then loads the chunks and embeddings into a vector store. The input and output components allow a user to query the vector store through a chat interface.\n\n![A semantic search flow that uses Embedding Model, File, Split Text, Chroma DB, Chat Input, and Chat Output components](/img/component-embedding-models-add-chat.png)\n\n1. Create a flow, add a **File** component, and then select a file containing text data, such as a PDF, that you can use to test the flow.\n\n2. Add the **Embedding Model** core component, and then provide a valid OpenAI API key.\nYou can enter the API key directly or use a  [global variable](/configuration-global-variables).\n\n    \n**Note:** My preferred provider or model isn't listed\n    If your preferred embedding model provider or model isn't supported by the **Embedding Model** core component, you can use any [additional embedding models](#additional-embedding-models) in place of the core component.\n\n    Browse  [**Bundles**](/components-bundle-components) or  **Search** for your preferred provider to find additional embedding models, such as the [**Hugging Face Embeddings Inference** component](/bundles-huggingface#hugging-face-embeddings-inference).\n    \n\n3. Add a [**Split Text** component](/components-processing#split-text) to your flow.\nThis component splits text input into smaller chunks to be processed into embeddings.\n\n4. Add a vector store component, such as the **Chroma DB** component, to your flow, and then configure the component to connect to your vector database.\nThis component stores the generated embeddings so they can be used for similarity search.\n\n5. Connect the components:\n\n    * Connect the **File** component's **Loaded Files** output to the **Split Text** component's **Data or DataFrame** input.\n    * Connect the **Split Text** component's **Chunks** output to the vector store component's **Ingest Data** input.\n    * Connect the **Embedding Model** component's **Embeddings** output to the vector store component's **Embedding** input.\n\n6. To query the vector store, add [**Chat Input and Output** components](/components-io#chat-io):\n\n    * Connect the **Chat Input** component to the vector store component's **Search Query** input.\n    * Connect the vector store component's **Search Results** output to the **Chat Output** component.\n\n7. Click **Playground**, and then enter a search query to retrieve text chunks that are most semantically similar to your query.\n\n## Embedding Model parameters\n\nThe following parameters are for the **Embedding Model** core component.\nOther embedding model components can have additional or different parameters.\n\n| Name | Display Name | Type | Description |\n|------|--------------|------|-------------|\n| provider | Model Provider | List | Input parameter. Select the embedding model provider. |\n| model | Model Name | List | Input parameter. Select the embedding model to use.|\n| api_key | OpenAI API Key | Secret[String] | Input parameter. The API key required for authenticating with the provider. |\n| api_base | API Base URL | String | Input parameter. Base URL for the API. Leave empty for default. |\n| dimensions | Dimensions | Integer | Input parameter. The number of dimensions for the output embeddings. |\n| chunk_size | Chunk Size | Integer | Input parameter. The size of text chunks to process. Default: `1000`. |\n| request_timeout | Request Timeout | Float | Input parameter. Timeout for API requests. |\n| max_retries | Max Retries | Integer | Input parameter. Maximum number of retry attempts. Default: `3`. |\n| show_progress_bar | Show Progress Bar | Boolean | Input parameter. Whether to display a progress bar during embedding generation. |\n| model_kwargs | Model Kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |\n| embeddings | Embeddings | Embeddings | Output parameter. An instance for generating embeddings using the selected provider. |\n\n## Additional embedding models\n\nIf your provider or model isn't supported by the **Embedding Model** core component, you can replace this component with any other component that generates embeddings.\n\nTo find additional embedding model components, browse  [**Bundles**](/components-bundle-components) or  **Search** for your preferred provider.\n\n## Pair models with vector stores\n\n<details>\n<summary>Example: Vector search flow</summary>\n\n</details>",
      "summary": "Embedding model components in Langflow generate text embeddings using a specified Large Language Model (LLM).",
      "keywords": [
        "additional",
        "bundles",
        "chat",
        "chroma",
        "chunks",
        "components",
        "data",
        "dataframe",
        "embedding",
        "embeddings",
        "face",
        "file",
        "files",
        "flow",
        "hugging",
        "inference",
        "ingest",
        "input",
        "loaded",
        "model"
      ]
    },
    "/components-helpers": {
      "title": "Helpers",
      "slug": "/components-helpers",
      "category": "components",
      "content": "Helper components provide utility functions to help manage data and perform simple tasks in your flow.\n\n## Calculator\n\nThe **Calculator** component performs basic arithmetic operations on mathematical expressions.\nIt supports addition, subtraction, multiplication, division, and exponentiation operations.\n\nFor an example of using this component in a flow, see the [**Python Interpreter** component](/components-processing#python-interpreter).\n\n### Calculator parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| expression | String | Input parameter. The arithmetic expression to evaluate, such as `4*4*(33/22)+12-20`. |\n| result | Data | Output parameter. The calculation result as a [`Data` object](/data-types) containing the evaluated expression. |\n\n## Current Date\n\nThe **Current Date** component returns the current date and time in a selected timezone. This component provides a flexible way to obtain timezone-specific date and time information within a Langflow pipeline.\n\n### Current Date parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| timezone | String | Input parameter. The timezone for the current date and time. |\n| current_date | String | Output parameter. The resulting current date and time in the selected timezone. |\n\n## Message History\n\nThe **Message History** component provides combined chat history and message storage functionality.\nIt can store and retrieve chat messages from either [Langflow storage](/memory) _or_ a dedicated chat memory database like Mem0 or Redis.\n\n**Note:** The **Agent** component has built-in chat memory that is enabled by default and uses Langflow storage.\nThis built-in chat memory functionality is sufficient for most use cases.\n\nUse the **Message History** component for the following use cases:\n\n* You need to store and retrieve chat memory for a language model component (not an agent).\n* You need to retrieve chat memories outside the chat context, such as a sentiment analysis flow that retrieves and analyzes recently stored memories.\n* You want to store memories in a specific database that is separate from Langflow storage.\n\nFor more information, see [Store chat memory](/memory#store-chat-memory).\n\n### Use the Message History component in a flow\n\nThe **Message History** component has two modes, depending on where you want to use it in your flow:\n\n* **Retrieve mode**: The component retrieves chat messages from your Langflow database or external memory.\n* **Store mode**: The component stores chat messages in your Langflow database or external memory.\n\nThis means that you need multiple **Message History** components in your flow if you want to both store and retrieve chat messages.\n\n</Tabs>\n\n### Message History parameters\n\nThe available parameters depend on whether the component is in **Retrieve** or **Store** mode.\n\n</Tabs>\n\n### Message History output\n\nMemories can be retrieved in one of two formats:\n\n* **Message**: Retrieve memories as `Message` objects, including `messages_text` containing retrieved chat message text.\nThis is the typical output format used to pass memories _as chat messages_ to another component.\n\n* **DataFrame**: Returns memories as a `DataFrame` containing the message data.\nUseful for cases where you need to retrieve memories in a tabular format rather than as chat messages.\n\nYou can set the output type near the component's output port.\n\n## Legacy Helper components\n\nThe following Helper components are in legacy status:\n\n* **Message Store**: Replaced by the [**Message History** component](#message-history)\n* **Create List**: Replace with [Processing components](/components-processing)\n* **ID Generator**: Replace with a component that executes arbitrary code to generate an ID or embed an ID generator script your application code (external to your Langflow flows).\n* **Output Parser**: Replace with the [**Structured Output** component](/components-processing#structured-output) and [**Parser** component](/components-processing#parser).\nThe components you need depend on the data types and complexity of the parsing task.\n\n    The **Output Parser** component transformed the output of a language model into comma-separated values (CSV) format, such as `[\"item1\", \"item2\", \"item3\"]`, using LangChain's `CommaSeparatedListOutputParser`.\n    The **Structured Output** component is a good alternative for this component because it also formats LLM responses with support for custom schemas and more complex parsing.\n\n    **Parsing** components only provide formatting instructions and parsing functionality.\n    _They don't include prompts._\n    You must connect parsers to **Prompt Template** components to create prompts that LLMs can use.",
      "summary": "Helper components provide utility functions to help manage data and perform simple tasks in your flow.",
      "keywords": [
        "agent",
        "calculator",
        "component",
        "components",
        "create",
        "current",
        "dataframe",
        "date",
        "flow",
        "generator",
        "helper",
        "helpers",
        "history",
        "interpreter",
        "legacy",
        "list",
        "message",
        "mode",
        "note",
        "output"
      ]
    },
    "components-helpers": {
      "title": "Helpers",
      "slug": "/components-helpers",
      "category": "components",
      "content": "Helper components provide utility functions to help manage data and perform simple tasks in your flow.\n\n## Calculator\n\nThe **Calculator** component performs basic arithmetic operations on mathematical expressions.\nIt supports addition, subtraction, multiplication, division, and exponentiation operations.\n\nFor an example of using this component in a flow, see the [**Python Interpreter** component](/components-processing#python-interpreter).\n\n### Calculator parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| expression | String | Input parameter. The arithmetic expression to evaluate, such as `4*4*(33/22)+12-20`. |\n| result | Data | Output parameter. The calculation result as a [`Data` object](/data-types) containing the evaluated expression. |\n\n## Current Date\n\nThe **Current Date** component returns the current date and time in a selected timezone. This component provides a flexible way to obtain timezone-specific date and time information within a Langflow pipeline.\n\n### Current Date parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| timezone | String | Input parameter. The timezone for the current date and time. |\n| current_date | String | Output parameter. The resulting current date and time in the selected timezone. |\n\n## Message History\n\nThe **Message History** component provides combined chat history and message storage functionality.\nIt can store and retrieve chat messages from either [Langflow storage](/memory) _or_ a dedicated chat memory database like Mem0 or Redis.\n\n**Note:** The **Agent** component has built-in chat memory that is enabled by default and uses Langflow storage.\nThis built-in chat memory functionality is sufficient for most use cases.\n\nUse the **Message History** component for the following use cases:\n\n* You need to store and retrieve chat memory for a language model component (not an agent).\n* You need to retrieve chat memories outside the chat context, such as a sentiment analysis flow that retrieves and analyzes recently stored memories.\n* You want to store memories in a specific database that is separate from Langflow storage.\n\nFor more information, see [Store chat memory](/memory#store-chat-memory).\n\n### Use the Message History component in a flow\n\nThe **Message History** component has two modes, depending on where you want to use it in your flow:\n\n* **Retrieve mode**: The component retrieves chat messages from your Langflow database or external memory.\n* **Store mode**: The component stores chat messages in your Langflow database or external memory.\n\nThis means that you need multiple **Message History** components in your flow if you want to both store and retrieve chat messages.\n\n</Tabs>\n\n### Message History parameters\n\nThe available parameters depend on whether the component is in **Retrieve** or **Store** mode.\n\n</Tabs>\n\n### Message History output\n\nMemories can be retrieved in one of two formats:\n\n* **Message**: Retrieve memories as `Message` objects, including `messages_text` containing retrieved chat message text.\nThis is the typical output format used to pass memories _as chat messages_ to another component.\n\n* **DataFrame**: Returns memories as a `DataFrame` containing the message data.\nUseful for cases where you need to retrieve memories in a tabular format rather than as chat messages.\n\nYou can set the output type near the component's output port.\n\n## Legacy Helper components\n\nThe following Helper components are in legacy status:\n\n* **Message Store**: Replaced by the [**Message History** component](#message-history)\n* **Create List**: Replace with [Processing components](/components-processing)\n* **ID Generator**: Replace with a component that executes arbitrary code to generate an ID or embed an ID generator script your application code (external to your Langflow flows).\n* **Output Parser**: Replace with the [**Structured Output** component](/components-processing#structured-output) and [**Parser** component](/components-processing#parser).\nThe components you need depend on the data types and complexity of the parsing task.\n\n    The **Output Parser** component transformed the output of a language model into comma-separated values (CSV) format, such as `[\"item1\", \"item2\", \"item3\"]`, using LangChain's `CommaSeparatedListOutputParser`.\n    The **Structured Output** component is a good alternative for this component because it also formats LLM responses with support for custom schemas and more complex parsing.\n\n    **Parsing** components only provide formatting instructions and parsing functionality.\n    _They don't include prompts._\n    You must connect parsers to **Prompt Template** components to create prompts that LLMs can use.",
      "summary": "Helper components provide utility functions to help manage data and perform simple tasks in your flow.",
      "keywords": [
        "agent",
        "calculator",
        "component",
        "components",
        "create",
        "current",
        "dataframe",
        "date",
        "flow",
        "generator",
        "helper",
        "helpers",
        "history",
        "interpreter",
        "legacy",
        "list",
        "message",
        "mode",
        "note",
        "output"
      ]
    },
    "/components-io": {
      "title": "Input / Output",
      "slug": "/components-io",
      "category": "components",
      "content": "Input and output components define where data enters and exits your flow, but they don't have identical functionality.\n\nSpecifically, **Chat Input and Output** components are designed to facilitate conversational interactions where messages are exchanged in a cumulative dialogue.\nThe data handled by these components includes the message text plus additional metadata like senders, session IDs, and timestamps.\n\nIn contrast, **Text Input and Output** components are designed for simple string input and output that doesn't require the additional context and metadata associated with chat messages.\nThe data handled by these components is pared down to basic text strings.\n\n## Chat Input and Output {#chat-io}\n\n**Note:** **Chat Input and Output** components are required to chat with your flow in the **Playground**.\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n**Chat Input and Output** components are designed to handle conversational interactions in Langflow.\n\n### Chat Input\n\nThe **Chat Input** component accepts text and file input, such as a chat message or a file.\nThis data is passed to other components as [`Message` data](/data-types) containing the provided input as well as associated chat metadata, such as the sender, session ID, timestamp, and file attachments.\n\nInitial input should _not_ be provided as a complete `Message` object because the **Chat Input** component constructs the `Message` object that is then passed to other components in the flow.\n\n#### Chat Input parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n|input_value|Input Text| Input parameter. The message text string to be passed as input. |\n|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|\n|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |\n|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|\n|files|Files| Input parameter. The files to be sent with the message.|\n|background_color|Background Color| Input parameter. The background color of the icon.|\n|chat_icon|Icon| Input parameter. The icon of the message.|\n|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|\n|text_color|Text Color| Input parameter. The text color of the name.|\n\nFor information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).\n\n<details>\n<summary>Message method for Chat Input</summary>\n\nThe `ChatInput` class provides an asynchronous method to create and store a `Message` object based on the input parameters.\nThe `Message` object is created in the `message_response` method of the `ChatInput` class using the `Message.create()` factory method.\n\n```python\nmessage = await Message.create(\n    text=self.input_value,\n    sender=self.sender,\n    sender_name=self.sender_name,\n    session_id=self.session_id,\n    files=self.files,\n    properties={\n        \"background_color\": background_color,\n        \"text_color\": text_color,\n        \"icon\": icon,\n    },\n)\n```\n\n</details>\n\n### Chat Output\n\nThe **Chat Output** component ingests `Message`, `Data`, or `DataFrame` data from other components, transforms it into `Message` data if needed, and then emits the final output as a chat message.\nFor information about these data types, see [Use Langflow data types](/data-types).\n\nIn the **Playground**, chat output is limited to the parts of the `Message` object that are relevant to the chat interface, such as the text response, sender name, and file attachments.\nTo see the metadata associated with a chat message, inspect the message logs in the **Playground**.\n\nWhen using the Langflow API, the API response includes the **Chat Output** `Message` object along with other response data from the flow run.\nLangflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.\nFor an example, see the [Langflow quickstart](/get-started-quickstart).\n\n#### Chat Output parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n|input_value|Inputs| Input parameter. The message text string to be passed as output. |\n|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|\n|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|\n|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |\n|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|\n|data_template|Data Template| Input parameter. The template to convert [`Data` input](/data-types#data) to `text`. If empty, it is dynamically set to the `Data` object's `text` key.|\n|background_color|Background Color| Input parameter. The background color of the icon.|\n|chat_icon|Icon| Input parameter. The icon of the message.|\n|text_color|Text Color| Input parameter. The text color of the name.|\n|clean_data|Basic Clean Data| Input parameter. When enabled, [`DataFrame` input](/data-types#dataframe) is cleaned when converted to text. Cleaning removes empty rows, empty lines in cells, and multiple newlines.|\n\nFor information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).\n\n### Use Chat Input and Output components in a flow\n\nTo use the **Chat Input** and **Chat Output** components in a flow, connect them to components that accept or emit [`Message` data](/data-types#message).\n\nFor example, the following flow connects **Chat Input** and **Chat Output** to a **Language Model** component, creating a simple LLM-based chat flow.\n\n![Chat Input and Output components connected to an OpenAI component](/img/component-chat-io.png)\n\n**Note:** For detailed examples of **Chat Input and Output** components in flows, see the following:\n\n* [Langflow quickstart](/get-started-quickstart): Create and run a basic agent flow.\n* **Basic Prompting** template: Create an LLM-based chat flow that accepts chat input as well as a prompt with additional instructions for the LLM. Many other Langflow templates also use **Chat Input and Output** components.\n* [Connect applications to agents](/agent-tutorial): Explore more advanced concepts around agent flows and prompting, including triggering agent flows from external applications.\n\n### Send chat input with the Langflow API\n\nYou can use the Langflow API to run a flow by sending input to a **Chat Input** component:\n\n```bash\ncurl --request POST \\\n  --url \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"What's the recommended way to install Docker on Mac M1?\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\"\n}'\n```\n\nWhen triggering flows with the Langflow API, the payload must contain the values for the **Chat Input** component's input parameters, such as `input_value`.\n\nNot all parameters need to be specified in the request.\nFor example, `session_id` uses the flow's default session ID if omitted.\nIf you want to use a custom session ID, include `session_id` in your request:\n\n```bash\ncurl --request POST \\\n  --url \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Whats the recommended way to install Docker on Mac M1\",\n  \"session_id\": \"$USER_ID\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\"\n}'\n```\n\nFor more information, see [Trigger flows with the Langflow API](/concepts-publish).\n\n## Text Input and Output {#text-io}\n\n**Note:** **Text Input and Output** components aren't supported in the **Playground**.\nBecause the data isn't formatted as a chat message, the data doesn't appear in the **Playground**, and you can't chat with your flow in the **Playground**.\n\nIf you want to chat with a flow in the **Playground**, you must use the [**Chat Input and Output** components](#chat-io).\n\n**Text Input and Output** components are designed for flows that ingest or emit simple text strings.\nThese components don't support full conversational interactions.\n\nPassing chat-like metadata to a **Text Input and Output** component doesn't change the component's behavior; the result is still a simple text string.\n\n### Text Input\n\nThe **Text Input** component accepts a text string input that is passed to other components as [`Message` data](/data-types) containing only the provided input text string in the `text` attribute.\n\nIt accepts only **Text** (`input_value`), which is the text supplied as input to the component.\nThis can be entered directly into the component or passed as `Message` data from other components.\n\nInitial input _shouldn't_ be provided as a complete `Message` object because the **Text Input** component constructs the `Message` object that is then passed to other components in the flow.\n\n### Text Output\n\nThe **Text Output** component ingests [`Message` data](/data-types#message) from other components, emitting only the `text` attribute in a simplified `Message` object.\n\nIt accepts only **Text** (`input_value`), which is the text to be ingested and output as a string.\nThis can be entered directly into the component or passed as `Message` data from other components.",
      "summary": "Input and output components define where data enters and exits your flow, but they don't have identical functionality.",
      "keywords": [
        "api",
        "basic",
        "chat",
        "components",
        "flow",
        "input",
        "langflow",
        "language",
        "model",
        "note",
        "output",
        "parameters",
        "playground",
        "prompting",
        "send",
        "text"
      ]
    },
    "components-io": {
      "title": "Input / Output",
      "slug": "/components-io",
      "category": "components",
      "content": "Input and output components define where data enters and exits your flow, but they don't have identical functionality.\n\nSpecifically, **Chat Input and Output** components are designed to facilitate conversational interactions where messages are exchanged in a cumulative dialogue.\nThe data handled by these components includes the message text plus additional metadata like senders, session IDs, and timestamps.\n\nIn contrast, **Text Input and Output** components are designed for simple string input and output that doesn't require the additional context and metadata associated with chat messages.\nThe data handled by these components is pared down to basic text strings.\n\n## Chat Input and Output {#chat-io}\n\n**Note:** **Chat Input and Output** components are required to chat with your flow in the **Playground**.\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n**Chat Input and Output** components are designed to handle conversational interactions in Langflow.\n\n### Chat Input\n\nThe **Chat Input** component accepts text and file input, such as a chat message or a file.\nThis data is passed to other components as [`Message` data](/data-types) containing the provided input as well as associated chat metadata, such as the sender, session ID, timestamp, and file attachments.\n\nInitial input should _not_ be provided as a complete `Message` object because the **Chat Input** component constructs the `Message` object that is then passed to other components in the flow.\n\n#### Chat Input parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n|input_value|Input Text| Input parameter. The message text string to be passed as input. |\n|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|\n|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |\n|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|\n|files|Files| Input parameter. The files to be sent with the message.|\n|background_color|Background Color| Input parameter. The background color of the icon.|\n|chat_icon|Icon| Input parameter. The icon of the message.|\n|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|\n|text_color|Text Color| Input parameter. The text color of the name.|\n\nFor information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).\n\n<details>\n<summary>Message method for Chat Input</summary>\n\nThe `ChatInput` class provides an asynchronous method to create and store a `Message` object based on the input parameters.\nThe `Message` object is created in the `message_response` method of the `ChatInput` class using the `Message.create()` factory method.\n\n```python\nmessage = await Message.create(\n    text=self.input_value,\n    sender=self.sender,\n    sender_name=self.sender_name,\n    session_id=self.session_id,\n    files=self.files,\n    properties={\n        \"background_color\": background_color,\n        \"text_color\": text_color,\n        \"icon\": icon,\n    },\n)\n```\n\n</details>\n\n### Chat Output\n\nThe **Chat Output** component ingests `Message`, `Data`, or `DataFrame` data from other components, transforms it into `Message` data if needed, and then emits the final output as a chat message.\nFor information about these data types, see [Use Langflow data types](/data-types).\n\nIn the **Playground**, chat output is limited to the parts of the `Message` object that are relevant to the chat interface, such as the text response, sender name, and file attachments.\nTo see the metadata associated with a chat message, inspect the message logs in the **Playground**.\n\nWhen using the Langflow API, the API response includes the **Chat Output** `Message` object along with other response data from the flow run.\nLangflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.\nFor an example, see the [Langflow quickstart](/get-started-quickstart).\n\n#### Chat Output parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n|input_value|Inputs| Input parameter. The message text string to be passed as output. |\n|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|\n|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|\n|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |\n|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|\n|data_template|Data Template| Input parameter. The template to convert [`Data` input](/data-types#data) to `text`. If empty, it is dynamically set to the `Data` object's `text` key.|\n|background_color|Background Color| Input parameter. The background color of the icon.|\n|chat_icon|Icon| Input parameter. The icon of the message.|\n|text_color|Text Color| Input parameter. The text color of the name.|\n|clean_data|Basic Clean Data| Input parameter. When enabled, [`DataFrame` input](/data-types#dataframe) is cleaned when converted to text. Cleaning removes empty rows, empty lines in cells, and multiple newlines.|\n\nFor information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).\n\n### Use Chat Input and Output components in a flow\n\nTo use the **Chat Input** and **Chat Output** components in a flow, connect them to components that accept or emit [`Message` data](/data-types#message).\n\nFor example, the following flow connects **Chat Input** and **Chat Output** to a **Language Model** component, creating a simple LLM-based chat flow.\n\n![Chat Input and Output components connected to an OpenAI component](/img/component-chat-io.png)\n\n**Note:** For detailed examples of **Chat Input and Output** components in flows, see the following:\n\n* [Langflow quickstart](/get-started-quickstart): Create and run a basic agent flow.\n* **Basic Prompting** template: Create an LLM-based chat flow that accepts chat input as well as a prompt with additional instructions for the LLM. Many other Langflow templates also use **Chat Input and Output** components.\n* [Connect applications to agents](/agent-tutorial): Explore more advanced concepts around agent flows and prompting, including triggering agent flows from external applications.\n\n### Send chat input with the Langflow API\n\nYou can use the Langflow API to run a flow by sending input to a **Chat Input** component:\n\n```bash\ncurl --request POST \\\n  --url \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"What's the recommended way to install Docker on Mac M1?\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\"\n}'\n```\n\nWhen triggering flows with the Langflow API, the payload must contain the values for the **Chat Input** component's input parameters, such as `input_value`.\n\nNot all parameters need to be specified in the request.\nFor example, `session_id` uses the flow's default session ID if omitted.\nIf you want to use a custom session ID, include `session_id` in your request:\n\n```bash\ncurl --request POST \\\n  --url \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Whats the recommended way to install Docker on Mac M1\",\n  \"session_id\": \"$USER_ID\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\"\n}'\n```\n\nFor more information, see [Trigger flows with the Langflow API](/concepts-publish).\n\n## Text Input and Output {#text-io}\n\n**Note:** **Text Input and Output** components aren't supported in the **Playground**.\nBecause the data isn't formatted as a chat message, the data doesn't appear in the **Playground**, and you can't chat with your flow in the **Playground**.\n\nIf you want to chat with a flow in the **Playground**, you must use the [**Chat Input and Output** components](#chat-io).\n\n**Text Input and Output** components are designed for flows that ingest or emit simple text strings.\nThese components don't support full conversational interactions.\n\nPassing chat-like metadata to a **Text Input and Output** component doesn't change the component's behavior; the result is still a simple text string.\n\n### Text Input\n\nThe **Text Input** component accepts a text string input that is passed to other components as [`Message` data](/data-types) containing only the provided input text string in the `text` attribute.\n\nIt accepts only **Text** (`input_value`), which is the text supplied as input to the component.\nThis can be entered directly into the component or passed as `Message` data from other components.\n\nInitial input _shouldn't_ be provided as a complete `Message` object because the **Text Input** component constructs the `Message` object that is then passed to other components in the flow.\n\n### Text Output\n\nThe **Text Output** component ingests [`Message` data](/data-types#message) from other components, emitting only the `text` attribute in a simplified `Message` object.\n\nIt accepts only **Text** (`input_value`), which is the text to be ingested and output as a string.\nThis can be entered directly into the component or passed as `Message` data from other components.",
      "summary": "Input and output components define where data enters and exits your flow, but they don't have identical functionality.",
      "keywords": [
        "api",
        "basic",
        "chat",
        "components",
        "flow",
        "input",
        "langflow",
        "language",
        "model",
        "note",
        "output",
        "parameters",
        "playground",
        "prompting",
        "send",
        "text"
      ]
    },
    "/components-logic": {
      "title": "Logic",
      "slug": "/components-logic",
      "category": "components",
      "content": "Logic components provide functionalities for routing, conditional processing, and flow management.\n\n## If-Else (conditional router) {#if-else}\n\nThe **If-Else** component is a conditional router that routes messages by comparing two strings.\nIt evaluates a condition by comparing two text inputs using the specified operator, and then routes the message to `true_result` or `false_result` depending on the evaluation.\n\nThe operator looks for single strings in the input (`input_text`) based on an operator and match text (`match_text`), but it can also search for multiple words by matching a regex.\nAvailable operators include:\n\n- **equals**: Exact match comparison\n- **not equals**: Inverse of exact match\n- **contains**: Checks if the `match_text` is found within `input_text`\n- **starts with**: Checks if `input_text` begins with `match_text`\n- **ends with**: Checks if `input_text` ends with `match_text`\n- **regex**: Matches on a case-sensitive pattern\n\nBy default, all operators are case insensitive except **regex**.\n**regex** is always case sensitive, and you can enable case sensitivity for all other operators in the [If-Else parameters](#if-else-parameters).\n\n### Use the If-Else component in a flow\n\nThe following example uses the **If-Else** component to check incoming chat messages with regex matching, and then output a different response depending on whether the match evaluated to true or false.\n\n![A conditional router connected to two OpenAI components](/img/component-conditional-router.png)\n\n1. Add an **If-Else** component to your flow, and then configure it as follows:\n\n    * **Text Input**: Connect the **Text Input** port to a **Chat Input** component or another `Message` input.\n\n        If your input isn't in `Message` format, you can use another component to transform it, such as the [**Type Convert** component](/components-processing#type-convert) or [**Parser** component](/components-processing#parser).\n        If your input isn't appropriate for `Message` format, consider using another component for conditional routing, such as the [**Data Operations** component](/components-processing#data-operations).\n\n    * **Match Text**: Enter `.*(urgent|warning|caution).*` so the component looks for these values in incoming input. The regex match is case sensitive, so if you need to look for all permutations of `warning`, enter `warning|Warning|WARNING`.\n\n    * **Operator**: Select **regex**.\n\n    * **Case True**: In the [component's header menu](/concepts-components#component-menus), click  **Controls**, enable the **Case True** parameter, click **Close**, and then enter `New Message Detected` in the field.\n\n        The **Case True** message is sent from the **True** output port when the match condition evaluates to true.\n\n        No message is set for **Case False** so the component doesn't emit a message when the condition evaluates to false.\n\n3. Depending on what you want to happen when the outcome is **True**, add components to your flow to execute that logic:\n\n    1. Add a **Language Model**, **Prompt Template**, and **Chat Output** component to your flow.\n\n    2. In the **Language Model** component, enter your OpenAI API key or select a different provider and model.\n\n    3. Connect the **If-Else** component's **True** output port to the **Language Model** component's **Input** port.\n\n    4. In the **Prompt Template** component, enter instructions for the model when the evaluation is true, such as `Send a message that a new warning, caution, or urgent message was received`.\n\n    5. Connect the **Prompt Template** component to the **Language Model** component's **System Message** port.\n\n    6. Connect the **Language Model** component's output to the **Chat Output** component.\n\n4. Repeat the same process with another set of **Language Model**, **Prompt Template**, and **Chat Output** components for the **False** outcome.\n\n    Connect the **If-Else** component's **False** output port to the second **Language Model** component's **Input** port.\n    In the second **Prompt Template**, enter instructions for the model when the evaluation is false, such as `Send a message that a new low-priority message was received`.\n\n5. To test the flow, open the **Playground**, and then send the flow some messages with and without your regex strings.\nThe chat output should reflect the instructions in your prompts based on the regex evaluation.\n\n    ```text\n    User: A new user was created.\n\n    AI: A new low-priority message was received.\n\n    User: Sign-in warning: new user locked out.\n\n    AI: A new warning, caution, or urgent message was received. Please review it at your earliest convenience.\n    ```\n\n### If-Else parameters\n\n| Name           | Type     | Description                                                       |\n|----------------|----------|-------------------------------------------------------------------|\n| input_text     | String   | Input parameter. The primary text input for the operation. |\n| match_text     | String   | Input parameter. The text to compare against. |\n| operator       | Dropdown | Input parameter. The operator used to compare texts. Options include `equals`, `not equals`, `contains`, `starts with`, `ends with`, and `regex`. The default is `equals`. |\n| case_sensitive | Boolean  | Input parameter. When `true`, the comparison is case sensitive. The default is `false`. This setting doesn't apply to regex comparisons. |\n| max_iterations | Integer  | Input parameter. The maximum number of iterations allowed for the conditional router. The default is 10. |\n| default_route  | Dropdown | Input parameter. The route to take when max iterations are reached. Options include `true_result` or `false_result`. The default is `false_result`. |\n| true_result  | Message | Output parameter. The output produced when the condition is true. |\n| false_result | Message | Output parameter. The output produced when the condition is false. |\n\n## Loop\n\nThe **Loop** component iterates over a list of input by passing individual items to other components attached at the **Item** output port until there are no items left to process.\nThen, the **Loop** component passes the aggregated result of all looping to the component connected to the **Done** port.\n\n### The looping process\n\nThe **Loop** component is like a miniature flow within your flow.\nHere's a breakdown of the looping process:\n\n1. Accepts a list of [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) objects, such as a CSV file, through the **Loop** component's **Inputs** port.\n\n2. Splits the input into individual items. For example, a CSV file is broken down by rows.\n\n    Specifically, the **Loop** component repeatedly extracts items by `text` key in the `Data` or `DataFrame` objects until there are no more items to extract.\n    Each `item` output is a `Data` objects.\n\n3. Iterates over each `item` by passing them to the **Item** output port.\n\n    This port connects to one or more components that perform actions on each item.\n    The final component in the **Item** loop connects back to the **Loop** component's **Looping** port to process the next item.\n\n    Only one component connects to the **Item** port, but you can pass the data through as many components as you need, as long as the last component in the chain connects back to the **Looping** port.\n\n    The **If-Else** component isn't compatible with the **Loop** component.\n    For more information, see [Conditional looping](#conditional-looping).\n\n4. After processing all items, the results are aggregated into a single `Data` object that is passed from the **Loop** component's **Done** port to the next component in the flow.\n\nThe following simplified Python code summarizes how the **Loop** component works.\nThis _isn't_ the actual component code; it is only meant to help you understand the general process.\n\n```python\nfor i in input:             # Receive input data as a list\n    process_item(i)         # Process each item through components connected at the Item port\n    if has_more_items():\n        continue            # Loop back to Looping port to process the next item\n    else:\n        break               # Exit the loop when no more items are left\n\ndone = aggregate_results()  # Compile all returned items\n\nprint(done)                 # Send the aggregated results from the Done port to another component\n```\n\n### Loop example\n\nIn the follow example, the **Loop** component iterates over a CSV file until there are no rows left to process.\nIn this case, the **Item** port passes each row to a **Type Convert** component to convert the row into a `Message` object, passes the `Message` to a **Structured Output** component to be processed into structured data that is then passed back to the **Loop** component's **Looping** port.\nAfter processing all rows, the **Loop** component loads the aggregated list of structured data into a Chroma DB database through the **Chroma DB** component connected to the **Done** port.\n\n![Loop CSV parser](/img/component-loop-csv.png)\n\n**Note:** For more examples of the **Loop** component, try the **Research Translation Loop** template in Langflow, or see the video tutorial [Mastering the Loop Component & Agentic RAG in Langflow](https://www.youtube.com/watch?v=9Wx7WODSKTo).\n\n### Conditional looping\n\nThe **If-Else** component isn't compatible with the **Loop** component.\nIf you need conditional loop events, redesign your flow to process conditions before the loop.\nFor example, if you are looping over a `DataFrame`, you could use multiple [**DataFrame Operations** components](/components-processing#dataframe-operations) to conditionally filter data, and then run separate loops on each set of filtered data.\n\n![A flow with conditional looping.](/img/conditional-looping.png)\n\n## Notify and Listen\n\nThe **Notify** and **Listen** components are used together.\n\nThe **Notify** component builds a notification from the current flow's context, including specific data content and a status identifier.\n\nThe resulting notification is sent to the **Listen** component.\nThe notification data can then be passed to other components in the flow, such as the **If-Else** component.\n\n## Run flow\n\nThe **Run Flow** component runs another Langflow flow as a subprocess of the current flow.\n\nYou can use this component to chain flows together, run flows conditionally, and attach flows to [**Agent** components](/components-agents) as [tools for agents](/agents-tools) to run as needed.\n\nWhen used with an agent, the `name` and `description` metadata that the agent uses to register the tool are created automatically.\n\nWhen you select a flow for the **Run Flow** component, it uses the target flow's graph structure to dynamically generate input and output fields on the **Run Flow** component.\n\n### Run Flow parameters\n\n| Name              | Type     | Description                                                    |\n|-------------------|----------|----------------------------------------------------------------|\n| flow_name_selected| Dropdown | Input parameter. The name of the flow to run.                  |\n| session_id        | String   | Input parameter. The session ID for the flow run, if you want to pass a custom session ID for the subflow. |\n| flow_tweak_data   | Dict     | Input parameter. Dictionary of tweaks to customize the flow's behavior. Available tweaks depend on the selected flow. |\n| dynamic inputs    | Various  | Input parameter. Additional inputs are generated based on the selected flow. |\n| run_outputs  | A `List` of types (`Data`, `Message`, or `DataFrame`)  | Output parameter. All outputs are generated from running the flow.  |\n\n## Legacy Logic components\n\nThe following Logic components are in legacy status:\n\n<details>\n<summary>Condition</summary>\n\nAs an alternative to this legacy component, see the [**If-Else** component](#if-else).\n\nThe **Condition** component routes `Data` objects based on a condition applied to a specified key, including Boolean validation.\nIt supports `true_output` and `false_output` for routing the results based on the condition evaluation.\n\nThis component is useful in workflows that require conditional routing of complex data structures, enabling dynamic decision-making based on data content.\n\nIt can process either a single `Data` object or a list of `Data` objects.\nThe following actions occur when processing a list of `Data` objects:\n\n- Each object in the list is evaluated individually.\n- Objects meeting the condition go to `true_output`.\n- Objects not meeting the condition go to `false_output`.\n- If all objects go to one output, the other output is empty.\n\nThe **Condition** component accepts the following parameters:\n\n| Name          | Type     | Description                                 |\n|---------------|----------|---------------------------------------------|\n| data_input    | Data     | Input parameter. The Data object or list of Data objects to process. This input can handle both single items and lists. |\n| key_name      | String   | Input parameter. The name of the key in the Data object to check.     |\n| operator      | Dropdown | Input parameter. The operator to apply. Options: `equals`, `not equals`, `contains`, `starts with`, `ends with`, `boolean validator`. Default: `equals`. |\n| compare_value | String   | Input parameter. The value to compare against. Not shown/used when operator is `boolean validator`. |\n\nThe `operator` options have the following behaviors:\n\n- `equals`: Exact match comparison between the key's value and compare_value.\n- `not equals`: Inverse of exact match.\n- `contains`: Checks if compare_value is found within the key's value.\n- `starts with`: Checks if the key's value begins with compare_value.\n- `ends with`: Checks if the key's value ends with compare_value.\n- `boolean validator`: Treats the key's value as a Boolean. The following values are considered true:\n  - Boolean `true`.\n  - Strings: `true`, `1`, `yes`, `y`, `on` (case-insensitive)\n  - Any other value is converted using Python's `bool()` function\n\n</details>\n\n<details>\n<summary>Pass</summary>\n\nAs an alternative to this legacy component, use the [**If-Else** component](#if-else) to pass a message without modification.\n\nThe **Pass** component forwards the input message without modification.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_message | Input Message | Input parameter. The message to forward. |\n| ignored_message | Ignored Message | Input parameter. A second message that is ignored. Used as a workaround for continuity. |\n| output_message | Output Message | Output parameter. The forwarded message from the input. |\n\n</details>\n\n<details>\n<summary>Flow As Tool</summary>\n\nThis component constructed a tool from a function that ran a loaded flow.\n\nIt was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).\n\n</details>\n\n<details>\n<summary>Sub Flow</summary>\n\nThis component integrated entire flows as components within a larger workflow.\nIt dynamically generated inputs based on the selected flow and executed the flow with provided parameters.\n\nIt was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).\n\n</details>",
      "summary": "Logic components provide functionalities for routing, conditional processing, and flow management.",
      "keywords": [
        "agent",
        "case",
        "chat",
        "chroma",
        "close",
        "component",
        "components",
        "condition",
        "conditional",
        "contains",
        "controls",
        "convert",
        "data",
        "dataframe",
        "done",
        "else",
        "ends",
        "equals",
        "false",
        "flow"
      ]
    },
    "components-logic": {
      "title": "Logic",
      "slug": "/components-logic",
      "category": "components",
      "content": "Logic components provide functionalities for routing, conditional processing, and flow management.\n\n## If-Else (conditional router) {#if-else}\n\nThe **If-Else** component is a conditional router that routes messages by comparing two strings.\nIt evaluates a condition by comparing two text inputs using the specified operator, and then routes the message to `true_result` or `false_result` depending on the evaluation.\n\nThe operator looks for single strings in the input (`input_text`) based on an operator and match text (`match_text`), but it can also search for multiple words by matching a regex.\nAvailable operators include:\n\n- **equals**: Exact match comparison\n- **not equals**: Inverse of exact match\n- **contains**: Checks if the `match_text` is found within `input_text`\n- **starts with**: Checks if `input_text` begins with `match_text`\n- **ends with**: Checks if `input_text` ends with `match_text`\n- **regex**: Matches on a case-sensitive pattern\n\nBy default, all operators are case insensitive except **regex**.\n**regex** is always case sensitive, and you can enable case sensitivity for all other operators in the [If-Else parameters](#if-else-parameters).\n\n### Use the If-Else component in a flow\n\nThe following example uses the **If-Else** component to check incoming chat messages with regex matching, and then output a different response depending on whether the match evaluated to true or false.\n\n![A conditional router connected to two OpenAI components](/img/component-conditional-router.png)\n\n1. Add an **If-Else** component to your flow, and then configure it as follows:\n\n    * **Text Input**: Connect the **Text Input** port to a **Chat Input** component or another `Message` input.\n\n        If your input isn't in `Message` format, you can use another component to transform it, such as the [**Type Convert** component](/components-processing#type-convert) or [**Parser** component](/components-processing#parser).\n        If your input isn't appropriate for `Message` format, consider using another component for conditional routing, such as the [**Data Operations** component](/components-processing#data-operations).\n\n    * **Match Text**: Enter `.*(urgent|warning|caution).*` so the component looks for these values in incoming input. The regex match is case sensitive, so if you need to look for all permutations of `warning`, enter `warning|Warning|WARNING`.\n\n    * **Operator**: Select **regex**.\n\n    * **Case True**: In the [component's header menu](/concepts-components#component-menus), click  **Controls**, enable the **Case True** parameter, click **Close**, and then enter `New Message Detected` in the field.\n\n        The **Case True** message is sent from the **True** output port when the match condition evaluates to true.\n\n        No message is set for **Case False** so the component doesn't emit a message when the condition evaluates to false.\n\n3. Depending on what you want to happen when the outcome is **True**, add components to your flow to execute that logic:\n\n    1. Add a **Language Model**, **Prompt Template**, and **Chat Output** component to your flow.\n\n    2. In the **Language Model** component, enter your OpenAI API key or select a different provider and model.\n\n    3. Connect the **If-Else** component's **True** output port to the **Language Model** component's **Input** port.\n\n    4. In the **Prompt Template** component, enter instructions for the model when the evaluation is true, such as `Send a message that a new warning, caution, or urgent message was received`.\n\n    5. Connect the **Prompt Template** component to the **Language Model** component's **System Message** port.\n\n    6. Connect the **Language Model** component's output to the **Chat Output** component.\n\n4. Repeat the same process with another set of **Language Model**, **Prompt Template**, and **Chat Output** components for the **False** outcome.\n\n    Connect the **If-Else** component's **False** output port to the second **Language Model** component's **Input** port.\n    In the second **Prompt Template**, enter instructions for the model when the evaluation is false, such as `Send a message that a new low-priority message was received`.\n\n5. To test the flow, open the **Playground**, and then send the flow some messages with and without your regex strings.\nThe chat output should reflect the instructions in your prompts based on the regex evaluation.\n\n    ```text\n    User: A new user was created.\n\n    AI: A new low-priority message was received.\n\n    User: Sign-in warning: new user locked out.\n\n    AI: A new warning, caution, or urgent message was received. Please review it at your earliest convenience.\n    ```\n\n### If-Else parameters\n\n| Name           | Type     | Description                                                       |\n|----------------|----------|-------------------------------------------------------------------|\n| input_text     | String   | Input parameter. The primary text input for the operation. |\n| match_text     | String   | Input parameter. The text to compare against. |\n| operator       | Dropdown | Input parameter. The operator used to compare texts. Options include `equals`, `not equals`, `contains`, `starts with`, `ends with`, and `regex`. The default is `equals`. |\n| case_sensitive | Boolean  | Input parameter. When `true`, the comparison is case sensitive. The default is `false`. This setting doesn't apply to regex comparisons. |\n| max_iterations | Integer  | Input parameter. The maximum number of iterations allowed for the conditional router. The default is 10. |\n| default_route  | Dropdown | Input parameter. The route to take when max iterations are reached. Options include `true_result` or `false_result`. The default is `false_result`. |\n| true_result  | Message | Output parameter. The output produced when the condition is true. |\n| false_result | Message | Output parameter. The output produced when the condition is false. |\n\n## Loop\n\nThe **Loop** component iterates over a list of input by passing individual items to other components attached at the **Item** output port until there are no items left to process.\nThen, the **Loop** component passes the aggregated result of all looping to the component connected to the **Done** port.\n\n### The looping process\n\nThe **Loop** component is like a miniature flow within your flow.\nHere's a breakdown of the looping process:\n\n1. Accepts a list of [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) objects, such as a CSV file, through the **Loop** component's **Inputs** port.\n\n2. Splits the input into individual items. For example, a CSV file is broken down by rows.\n\n    Specifically, the **Loop** component repeatedly extracts items by `text` key in the `Data` or `DataFrame` objects until there are no more items to extract.\n    Each `item` output is a `Data` objects.\n\n3. Iterates over each `item` by passing them to the **Item** output port.\n\n    This port connects to one or more components that perform actions on each item.\n    The final component in the **Item** loop connects back to the **Loop** component's **Looping** port to process the next item.\n\n    Only one component connects to the **Item** port, but you can pass the data through as many components as you need, as long as the last component in the chain connects back to the **Looping** port.\n\n    The **If-Else** component isn't compatible with the **Loop** component.\n    For more information, see [Conditional looping](#conditional-looping).\n\n4. After processing all items, the results are aggregated into a single `Data` object that is passed from the **Loop** component's **Done** port to the next component in the flow.\n\nThe following simplified Python code summarizes how the **Loop** component works.\nThis _isn't_ the actual component code; it is only meant to help you understand the general process.\n\n```python\nfor i in input:             # Receive input data as a list\n    process_item(i)         # Process each item through components connected at the Item port\n    if has_more_items():\n        continue            # Loop back to Looping port to process the next item\n    else:\n        break               # Exit the loop when no more items are left\n\ndone = aggregate_results()  # Compile all returned items\n\nprint(done)                 # Send the aggregated results from the Done port to another component\n```\n\n### Loop example\n\nIn the follow example, the **Loop** component iterates over a CSV file until there are no rows left to process.\nIn this case, the **Item** port passes each row to a **Type Convert** component to convert the row into a `Message` object, passes the `Message` to a **Structured Output** component to be processed into structured data that is then passed back to the **Loop** component's **Looping** port.\nAfter processing all rows, the **Loop** component loads the aggregated list of structured data into a Chroma DB database through the **Chroma DB** component connected to the **Done** port.\n\n![Loop CSV parser](/img/component-loop-csv.png)\n\n**Note:** For more examples of the **Loop** component, try the **Research Translation Loop** template in Langflow, or see the video tutorial [Mastering the Loop Component & Agentic RAG in Langflow](https://www.youtube.com/watch?v=9Wx7WODSKTo).\n\n### Conditional looping\n\nThe **If-Else** component isn't compatible with the **Loop** component.\nIf you need conditional loop events, redesign your flow to process conditions before the loop.\nFor example, if you are looping over a `DataFrame`, you could use multiple [**DataFrame Operations** components](/components-processing#dataframe-operations) to conditionally filter data, and then run separate loops on each set of filtered data.\n\n![A flow with conditional looping.](/img/conditional-looping.png)\n\n## Notify and Listen\n\nThe **Notify** and **Listen** components are used together.\n\nThe **Notify** component builds a notification from the current flow's context, including specific data content and a status identifier.\n\nThe resulting notification is sent to the **Listen** component.\nThe notification data can then be passed to other components in the flow, such as the **If-Else** component.\n\n## Run flow\n\nThe **Run Flow** component runs another Langflow flow as a subprocess of the current flow.\n\nYou can use this component to chain flows together, run flows conditionally, and attach flows to [**Agent** components](/components-agents) as [tools for agents](/agents-tools) to run as needed.\n\nWhen used with an agent, the `name` and `description` metadata that the agent uses to register the tool are created automatically.\n\nWhen you select a flow for the **Run Flow** component, it uses the target flow's graph structure to dynamically generate input and output fields on the **Run Flow** component.\n\n### Run Flow parameters\n\n| Name              | Type     | Description                                                    |\n|-------------------|----------|----------------------------------------------------------------|\n| flow_name_selected| Dropdown | Input parameter. The name of the flow to run.                  |\n| session_id        | String   | Input parameter. The session ID for the flow run, if you want to pass a custom session ID for the subflow. |\n| flow_tweak_data   | Dict     | Input parameter. Dictionary of tweaks to customize the flow's behavior. Available tweaks depend on the selected flow. |\n| dynamic inputs    | Various  | Input parameter. Additional inputs are generated based on the selected flow. |\n| run_outputs  | A `List` of types (`Data`, `Message`, or `DataFrame`)  | Output parameter. All outputs are generated from running the flow.  |\n\n## Legacy Logic components\n\nThe following Logic components are in legacy status:\n\n<details>\n<summary>Condition</summary>\n\nAs an alternative to this legacy component, see the [**If-Else** component](#if-else).\n\nThe **Condition** component routes `Data` objects based on a condition applied to a specified key, including Boolean validation.\nIt supports `true_output` and `false_output` for routing the results based on the condition evaluation.\n\nThis component is useful in workflows that require conditional routing of complex data structures, enabling dynamic decision-making based on data content.\n\nIt can process either a single `Data` object or a list of `Data` objects.\nThe following actions occur when processing a list of `Data` objects:\n\n- Each object in the list is evaluated individually.\n- Objects meeting the condition go to `true_output`.\n- Objects not meeting the condition go to `false_output`.\n- If all objects go to one output, the other output is empty.\n\nThe **Condition** component accepts the following parameters:\n\n| Name          | Type     | Description                                 |\n|---------------|----------|---------------------------------------------|\n| data_input    | Data     | Input parameter. The Data object or list of Data objects to process. This input can handle both single items and lists. |\n| key_name      | String   | Input parameter. The name of the key in the Data object to check.     |\n| operator      | Dropdown | Input parameter. The operator to apply. Options: `equals`, `not equals`, `contains`, `starts with`, `ends with`, `boolean validator`. Default: `equals`. |\n| compare_value | String   | Input parameter. The value to compare against. Not shown/used when operator is `boolean validator`. |\n\nThe `operator` options have the following behaviors:\n\n- `equals`: Exact match comparison between the key's value and compare_value.\n- `not equals`: Inverse of exact match.\n- `contains`: Checks if compare_value is found within the key's value.\n- `starts with`: Checks if the key's value begins with compare_value.\n- `ends with`: Checks if the key's value ends with compare_value.\n- `boolean validator`: Treats the key's value as a Boolean. The following values are considered true:\n  - Boolean `true`.\n  - Strings: `true`, `1`, `yes`, `y`, `on` (case-insensitive)\n  - Any other value is converted using Python's `bool()` function\n\n</details>\n\n<details>\n<summary>Pass</summary>\n\nAs an alternative to this legacy component, use the [**If-Else** component](#if-else) to pass a message without modification.\n\nThe **Pass** component forwards the input message without modification.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_message | Input Message | Input parameter. The message to forward. |\n| ignored_message | Ignored Message | Input parameter. A second message that is ignored. Used as a workaround for continuity. |\n| output_message | Output Message | Output parameter. The forwarded message from the input. |\n\n</details>\n\n<details>\n<summary>Flow As Tool</summary>\n\nThis component constructed a tool from a function that ran a loaded flow.\n\nIt was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).\n\n</details>\n\n<details>\n<summary>Sub Flow</summary>\n\nThis component integrated entire flows as components within a larger workflow.\nIt dynamically generated inputs based on the selected flow and executed the flow with provided parameters.\n\nIt was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).\n\n</details>",
      "summary": "Logic components provide functionalities for routing, conditional processing, and flow management.",
      "keywords": [
        "agent",
        "case",
        "chat",
        "chroma",
        "close",
        "component",
        "components",
        "condition",
        "conditional",
        "contains",
        "controls",
        "convert",
        "data",
        "dataframe",
        "done",
        "else",
        "ends",
        "equals",
        "false",
        "flow"
      ]
    },
    "/components-models": {
      "title": "Language Model",
      "slug": "/components-models",
      "category": "components",
      "content": "Language model components in Langflow generate text using a specified Large Language Model (LLM).\nThese components accept inputs like chat messages, files, and instructions in order to generate a text response.\n\nLangflow includes a **Language Model** core component that has built-in support for many LLMs.\nAlternatively, you can use any [additional language model](#additional-language-models) in place of the **Language Model** core component.\n\n## Use language model components in flows\n\nUse language model components anywhere you would use an LLM in a flow.\n\n</Tabs>\n\n## Language model parameters\n\nThe following parameters are for the **Language Model** core component.\nOther language model components can have additional or different parameters.\n\n| Name | Type | Description |\n|------|------|-------------|\n| provider | String | Input parameter. The model provider to use. |\n| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |\n| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |\n| input_value | String | Input parameter. The input text to send to the model. |\n| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |\n| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |\n| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |\n\n## Language model output types\n\nLanguage model components, including the core component and bundled components, can produce two types of output:\n\n* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).\nUse this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.\n\n* **Language Model**: Change the language model component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow, such as an **Agent** or **Smart Function** component.\n\n    With this configuration, the language model component supports an action completed by another component, rather than a direct chat interaction.\n    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.\n\n## Additional language models\n\nIf your provider or model isn't supported by the **Language Model** core component, additional language model components are available in  [**Bundles**](/components-bundle-components).\n\nYou can use these components in the same way that you use the core **Language Model** component, as explained in [Use language model components in flows](#use-language-model-components-in-flows).\n\n## Pair models with vector stores\n\n<details>\n<summary>Example: Vector search flow</summary>\n\n</details>",
      "summary": "Language model components in Langflow generate text using a specified Large Language Model (LLM).\nThese components accept inputs like chat messages, files, and instructions in order to generate a text response.",
      "keywords": [
        "additional",
        "agent",
        "bundles",
        "components",
        "flows",
        "function",
        "language",
        "model",
        "models",
        "output",
        "pair",
        "parameters",
        "response",
        "smart",
        "stores",
        "types",
        "vector"
      ]
    },
    "components-models": {
      "title": "Language Model",
      "slug": "/components-models",
      "category": "components",
      "content": "Language model components in Langflow generate text using a specified Large Language Model (LLM).\nThese components accept inputs like chat messages, files, and instructions in order to generate a text response.\n\nLangflow includes a **Language Model** core component that has built-in support for many LLMs.\nAlternatively, you can use any [additional language model](#additional-language-models) in place of the **Language Model** core component.\n\n## Use language model components in flows\n\nUse language model components anywhere you would use an LLM in a flow.\n\n</Tabs>\n\n## Language model parameters\n\nThe following parameters are for the **Language Model** core component.\nOther language model components can have additional or different parameters.\n\n| Name | Type | Description |\n|------|------|-------------|\n| provider | String | Input parameter. The model provider to use. |\n| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |\n| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |\n| input_value | String | Input parameter. The input text to send to the model. |\n| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |\n| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |\n| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |\n| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |\n\n## Language model output types\n\nLanguage model components, including the core component and bundled components, can produce two types of output:\n\n* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).\nUse this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.\n\n* **Language Model**: Change the language model component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow, such as an **Agent** or **Smart Function** component.\n\n    With this configuration, the language model component supports an action completed by another component, rather than a direct chat interaction.\n    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.\n\n## Additional language models\n\nIf your provider or model isn't supported by the **Language Model** core component, additional language model components are available in  [**Bundles**](/components-bundle-components).\n\nYou can use these components in the same way that you use the core **Language Model** component, as explained in [Use language model components in flows](#use-language-model-components-in-flows).\n\n## Pair models with vector stores\n\n<details>\n<summary>Example: Vector search flow</summary>\n\n</details>",
      "summary": "Language model components in Langflow generate text using a specified Large Language Model (LLM).\nThese components accept inputs like chat messages, files, and instructions in order to generate a text response.",
      "keywords": [
        "additional",
        "agent",
        "bundles",
        "components",
        "flows",
        "function",
        "language",
        "model",
        "models",
        "output",
        "pair",
        "parameters",
        "response",
        "smart",
        "stores",
        "types",
        "vector"
      ]
    },
    "/components-processing": {
      "title": "Processing components",
      "slug": "/components-processing",
      "category": "components",
      "content": "Processing components process and transform data within a flow.\nThey have many uses, including:\n\n* Feed instructions and context to your LLMs and agents with the [**Prompt Template** component](#prompt-template).\n* Extract content from larger chunks of data with a [**Parser** component](#parser).\n* Filter data with natural language with the [**Smart Function** component](#smart-transform).\n* Save data to your local machine with the [**Save File** component](#save-file).\n* Transform data into a different data type with the [**Type Convert** component](#type-convert) to pass it between incompatible components.\n\n## Prompt Template\n\nSee [**Prompt Template** component](/components-prompts).\n\n## Batch Run\n\nThe **Batch Run** component runs a language model over _each row of one text column_ in a [`DataFrame`](/data-types#dataframe), and then returns a new `DataFrame` with the original text and an LLM response.\nThe output contains the following columns:\n\n* `text_input`: The original text from the input `DataFrame`\n* `model_response`: The model's response for each input\n* `batch_index`: The 0-indexed processing order for all rows in the `DataFrame`\n* `metadata` (optional): Additional information about the processing\n\n### Use the Batch Run component in a flow\n\nIf you pass the **Batch Run** output to a [**Parser** component](/components-processing#parser), you can use variables in the parsing template to reference these keys, such as `{text_input}` and `{model_response}`.\nThis is demonstrated in the following example.\n\n![A batch run component connected to OpenAI and a Parser](/img/component-batch-run.png)\n\n1. Connect any language model component to a **Batch Run** component's **Language model** port.\n\n2. Connect `DataFrame` output from another component to the **Batch Run** component's **DataFrame** input.\nFor example, you could connect a **File** component with a CSV file.\n\n3. In the **Batch Run** component's **Column Name** field, enter the name of the column in the incoming `DataFrame` that contains the text to process.\nFor example, if you want to extract text from a `name` column in a CSV file, enter `name` in the **Column Name** field.\n\n4. Connect the **Batch Run** component's **Batch Results** output to a **Parser** component's **DataFrame** input.\n\n5. Optional: In the **Batch Run** [component's header menu](/concepts-components#component-menus), click  **Controls**, enable the **System Message** parameter, click **Close**, and then enter an instruction for how you want the LLM to process each cell extracted from the file.\nFor example, `Create a business card for each name.`\n\n6. In the **Parser** component's **Template** field, enter a template for processing the **Batch Run** component's new `DataFrame` columns (`text_input`, `model_response`, and `batch_index`):\n\n    For example, this template uses three columns from the resulting, post-batch `DataFrame`:\n\n    ```text\n    record_number: {batch_index}, name: {text_input}, summary: {model_response}\n    ```\n\n7. To test the processing, click the **Parser** component, click  **Run component**, and then click  **Inspect output** to view the final `DataFrame`.\n\n    You can also connect a **Chat Output** component to the **Parser** component if you want to see the output in the **Playground**.\n\n### Batch Run parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | HandleInput | Input parameter. Connect the 'Language Model' output from a language model component. Required. |\n| system_message | MultilineInput | Input parameter. A multi-line system instruction for all rows in the DataFrame. |\n| df | DataFrameInput | Input parameter. The DataFrame whose column is treated as text messages, as specified by 'column_name'. Required. |\n| column_name | MessageTextInput | Input parameter. The name of the DataFrame column to treat as text messages. If empty, all columns are formatted in TOML. |\n| output_column_name | MessageTextInput | Input parameter. Name of the column where the model's response is stored. Default=`model_response`. |\n| enable_metadata | BoolInput | Input parameter. If `True`, add metadata to the output DataFrame. |\n| batch_results | DataFrame | Output parameter. A DataFrame with all original columns plus the model's response column. |\n\n## Data Operations\n\nThe **Data Operations** component performs operations on [`Data`](/data-types#data) objects, including extracting, filtering, and editing keys and values in the `Data`.\nFor all options, see [Available data operations](#available-data-operations).\nThe output is a new `Data` object containing the modified data after running the selected operation.\n\n### Use the Data Operations component in a flow\n\nThe following example demonstrates how to use a **Data Operations** component in a flow using data from a webhook payload:\n\n1. Create a flow with a **Webhook** component and a **Data Operations** component, and then connect the **Webhook** component's output to the **Data Operations** component's **Data** input.\n\n    All operations in the **Data Operations** component require at least one `Data` input from another component.\n    If the preceding component doesn't produce `Data` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **Data Operations** component.\n    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **DataFrame Operations** components.\n\n2. In the **Operations** field, select the operation you want to perform on the incoming `Data`.\nFor this example, select the **Select Keys** operation.\n\n    \n**Note:** You can select only one operation.\n    If you need to perform multiple operations on the data, you can chain multiple **Data Operations** components together to execute each operation in sequence.\n    For more complex multi-step operations, consider using a component like the **Smart Function** component.\n    \n\n3. Under **Select Keys**, add keys for `name`, `username`, and `email`.\nClick  **Add more** to add a field for each key.\n\n    For this example, assume that the webhook will receive consistent payloads that always contain `name`, `username`, and `email` keys.\n    The **Select Keys** operation extracts the value of these keys from each incoming payload.\n\n4. Optional: If you want to view the output in the **Playground**, connect the **Data Operations** component's output to a **Chat Output** component.\n\n    ![A flow with Webhook, Data Operations, and Chat Output components](/img/component-data-operations-select-key.png)\n\n5. To test the flow, send the following request to your flow's webhook endpoint.\nFor more information about the webhook endpoint, see [Trigger flows with webhooks](/webhook).\n\n    ```bash\n    curl -X POST \"http://$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n    -d '{\n      \"id\": 1,\n      \"name\": \"Leanne Graham\",\n      \"username\": \"Bret\",\n      \"email\": \"Sincere@april.biz\",\n      \"address\": {\n        \"street\": \"Main Street\",\n        \"suite\": \"Apt. 556\",\n        \"city\": \"Springfield\",\n        \"zipcode\": \"92998-3874\",\n        \"geo\": {\n          \"lat\": \"-37.3159\",\n          \"lng\": \"81.1496\"\n        }\n      },\n      \"phone\": \"1-770-736-8031 x56442\",\n      \"website\": \"hildegard.org\",\n      \"company\": {\n        \"name\": \"Acme-Corp\",\n        \"catchPhrase\": \"Multi-layered client-server neural-net\",\n        \"bs\": \"harness real-time e-markets\"\n      }\n    }'\n    ```\n\n6. To view the `Data` resulting from the **Select Keys** operation, do one of the following:\n\n   * If you attached a **Chat Output** component, open the **Playground** to see the result as a chat message.\n   * Click  **Inspect output** on the **Data Operations** component.\n\n### Data Operations parameters\n\nMany parameters are conditional based on the selected **Operation** (`operation`).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data | Data | Input parameter. The `Data` object to operate on. |\n| operation | Operation | Input parameter. The operation to perform on the data. See [Available data operations](#available-data-operations) |\n| select_keys_input | Select Keys | Input parameter. A list of keys to select from the data. |\n| filter_key | Filter Key | Input parameter. The key to filter by. |\n| operator | Comparison Operator | Input parameter. The operator to apply for comparing values. |\n| filter_values | Filter Values | Input parameter. A list of values to filter by. |\n| append_update_data | Append or Update | Input parameter. The data to append or update the existing data with. |\n| remove_keys_input | Remove Keys | Input parameter. A list of keys to remove from the data. |\n| rename_keys_input | Rename Keys | Input parameter. A list of keys to rename in the data. |\n\n#### Available data operations\n\nOptions for the `operations` input parameter are as follows.\nAll operations act on an incoming `Data` object.\n\n| Name | Required Inputs | Process |\n|-----------|----------------|-------------|\n| Select Keys | `select_keys_input` | Selects specific keys from the data. |\n| Literal Eval | None | Evaluates string values as Python literals. |\n| Combine | None | Combines multiple data objects into one. |\n| Filter Values | `filter_key`, `filter_values`, `operator` | Filters data based on key-value pair. |\n| Append or Update | `append_update_data` | Adds or updates key-value pairs. |\n| Remove Keys | `remove_keys_input` | Removes specified keys from the data. |\n| Rename Keys | `rename_keys_input` | Renames keys in the data. |\n\n## DataFrame Operations\n\nThe **DataFrame Operations** component performs operations on [`DataFrame`](/data-types#dataframe) (table) rows and columns, including schema changes, record changes, sorting, and filtering.\nFor all options, see [DataFrame Operations parameters](#dataframe-operations-parameters).\n\nThe output is a new `DataFrame` containing the modified data after running the selected operation.\n\n### Use the DataFrame Operations component in a flow\n\nThe following steps explain how to configure a **DataFrame Operations** component in a flow.\nYou can follow along with an example or use your own flow.\nThe only requirement is that the preceding component must create `DataFrame` output that you can pass to the **DataFrame Operations** component.\n\n1. Create a new flow or use an existing flow.\n\n    <details>\n    <summary>Example: API response extraction flow</summary>\n\n    The following example flow uses five components to extract `Data` from an API response, transform it to a `DataFrame`, and then perform further processing on the tabular data using a **DataFrame Operations** component.\n    The sixth component, **Chat Output**, is optional in this example.\n    It only serves as a convenient way for you to view the final output in the **Playground**, rather than inspecting the component logs.\n\n    ![A flow that ingests an API response, extracts it to a DataFrame with a Smart Function component, and the processes it through a DataFrame Operations component](/img/component-dataframe-operations.png)\n\n    If you want to use this example to test the **DataFrame Operations** component, do the following:\n\n    1. Create a flow with the following components:\n\n        * **API Request**\n        * **Language Model**\n        * **Smart Function**\n        * **Type Convert**\n\n    2. Configure the [**Smart Function** component](#smart-transform) and its dependencies:\n\n        * **API Request**: Configure the [**API Request** component](/components-data#api-request) to get JSON data from an endpoint of your choice, and then connect the **API Response** output to the **Smart Function** component's **Data** input.\n        * **Language Model**: Select your preferred provider and model, and then enter a valid API key.\n        Change the output to **Language Model**, and then connect the `LanguageModel` output to the **Smart Function** component's **Language Model** input.\n        * **Smart Function**: In the **Instructions** field, enter natural language instructions to extract data from the API response.\n        Your instructions depend on the response content and desired outcome.\n        For example, if the response contains a large `result` field, you might provide instructions like `explode the result field out into a Data object`.\n\n    3. Convert the **Smart Function** component's `Data` output to `DataFrame`:\n\n        1. Connect the **Filtered Data** output to the **Type Convert** component's **Data** input.\n        2. Set the **Type Convert** component's **Output Type** to **DataFrame**.\n\n    Now the flow is ready for you to add the **DataFrame Operations** component.\n\n    </details>\n\n2. Add a **DataFrame Operations** component to the flow, and then connect `DataFrame` output from another component to the **DataFrame** input.\n\n    All operations in the **DataFrame Operations** component require at least one `DataFrame` input from another component.\n    If a component doesn't produce `DataFrame` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **DataFrame Operations** component.\n    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **Data Operations** components.\n\n    If you are following along with the example flow, connect the **Type Convert** component's **DataFrame Output** port to the **DataFrame** input.\n\n3. In the **Operations** field, select the operation you want to perform on the incoming `DataFrame`.\nFor example, the **Filter** operation filters the rows based on a specified column and value.\n\n    \n**Note:** You can select only one operation.\n    If you need to perform multiple operations on the data, you can chain multiple **DataFrame Operations** components together to execute each operation in sequence.\n    For more complex multi-step operations, like dramatic schema changes or pivots, consider using an LLM-powered component, like the **Structured Output** or **Smart Function** component, as a replacement or preparation for the **DataFrame Operations** component.\n    \n\n    If you're following along with the example flow, select any operation that you want to apply to the data that was extracted by the **Smart Function** component.\n    To view the contents of the incoming `DataFrame`, click  **Run component** on the **Type Convert** component, and then  **Inspect output**.\n    If the `DataFrame` seems malformed, click  **Inspect output** on each upstream component to determine where the error occurs, and then modify your flow's configuration as needed.\n    For example, if the **Smart Function** component didn't extract the expected fields, modify your instructions or verify that the given fields are present in the **API Response** output.\n\n4. Configure the operation's parameters.\nThe specific parameters depend on the selected operation.\nFor example, if you select the **Filter** operation, you must define a filter condition using the **Column Name**, **Filter Value**, and **Filter Operator** parameters.\nFor more information, see [DataFrame Operations parameters](#dataframe-operations-parameters)\n\n5. To test the flow, click  **Run component** on the **DataFrame Operations** component, and then click  **Inspect output** to view the new `DataFrame` created from the **Filter** operation.\n\n   If you want to view the output in the **Playground**, connect the **DataFrame Operations** component's output to a **Chat Output** component, rerun the **DataFrame Operations** component, and then click **Playground**.\n\nFor another example, see [Conditional looping](/components-logic#conditional-looping).\n\n### DataFrame Operations parameters\n\nMost **DataFrame Operations** parameters are conditional because they only apply to specific operations.\n\nThe only permanent parameters are **DataFrame** (`df`), which is the `DataFrame` input, and **Operation** (`operation`), which is the operation to perform on the `DataFrame`.\nOnce you select an operation, the conditional parameters for that operation appear on the **DataFrame Operations** component.\n\n</Tabs>\n\n## LLM Router\n\nThe **LLM Router** component routes requests to the most appropriate LLM based on [OpenRouter](https://openrouter.ai/docs/quickstart) model specifications.\n\nTo use the component in a flow, you connect multiple language model components to the **LLM Router** components.\nOne model is the judge LLM that analyzes input messages to understand the evaluation context, selects the most appropriate model from the other attached LLMs, and then routes the input to the selected model.\nThe selected model processes the input, and then returns the generated response.\n\nThe following example flow has three language model components.\nOne is the judge LLM, and the other two are in the LLM pool for request routing.\nThe input and output components create a seamless chat interaction where you send a message and receive a response without any user awareness of the underlying routing.\n\n![LLM Router component](/img/component-llm-router.png)\n\n### LLM Router parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| `models` | **Language Models** | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from multiple [language model components](/components-models) to create a pool of models. The `judge_llm` selects models from this pool when routing requests. The first model you connect is the default model if there is a problem with model selection or routing. |\n| `input_value` | **Input** | Input parameter. The incoming query to be routed to the model selected by the judge LLM. |\n| `judge_llm` | **Judge LLM** | Input parameter. Connect `LanguageModel` output from _one_ **Language Model** component to serve as the judge LLM for request routing. |\n| `optimization` | **Optimization** | Input parameter. Set a preferred characteristic for model selection by the judge LLM. The options are `quality` (highest response quality), `speed` (fastest response time), `cost` (most cost-effective model), or `balanced` (equal weight for quality, speed, and cost). Default: `balanced` |\n| `use_openrouter_specs` | **Use OpenRouter Specs** | Input parameter. Whether to fetch model specifications from the OpenRouter API.\nIf `false`, only the model name is provided to the judge LLM. Default: Enabled (`true`) |\n| `timeout` | **API Timeout** | Input parameter. Set a timeout duration in seconds for API requests made by the router. Default: `10` |\n| `fallback_to_first` | **Fallback to First Model** | Input parameter. Whether to use the first LLM in `models` as a backup if routing fails to reach the selected model. Default: Enabled (`true`) |\n\n### LLM Router outputs\n\nThe **LLM Router** component provides three output options.\nYou can set the desired output type near the component's output port.\n\n* **Output**: A `Message` containing the response to the original query as generated by the selected LLM.\nUse this output for regular chat interactions.\n\n* **Selected Model Info**: A `Data` object containing information about the selected model, such as its name and version.\n\n* **Routing Decision**: A `Message` containing the judge model's reasoning for selecting a particular model, including input query length and number of models considered.\nFor example:\n\n    ```text\n    Model Selection Decision:\n    - Selected Model Index: 0\n    - Selected Langflow Model Name: gpt-4o-mini\n    - Selected API Model ID (if resolved): openai/gpt-4o-mini\n    - Optimization Preference: cost\n    - Input Query Length: 27 characters (~5 tokens)\n    - Number of Models Considered: 2\n    - Specifications Source: OpenRouter API\n    ```\n\n    This is useful for debugging if you feel the judge model isn't selecting the best model.\n\n## Parser {#parser}\n\nThe **Parser** component extracts text from structured data (`DataFrame` or `Data`) using a template or direct stringification.\nThe output is a `Message` containing the parsed text.\n\nThis is a versatile component for data extraction and manipulation in your flows.\nFor examples of **Parser** components in flows, see the following:\n\n* [**Batch Run** component example](#batch-run)\n* [**Structured Output** component example](#structured-output)\n* **Financial Report Parser** template\n* [Trigger flows with webhooks](/webhook)\n* [Create a vector RAG chatbot](/chat-with-rag)\n\n![A flow that uses a Parser component to extract text from a Structured Output component.](/img/component-parser.png)\n\n### Parsing modes\n\nThe **Parser** component has two modes: **Parser** and **Stringify**.\n\n</Tabs>\n\n### Test and troubleshoot parsed text\n\nTo test the **Parser** component, click  **Run component**, and then click  **Inspect output** to see the `Message` output with the parsed text.\nYou can also connect a **Chat Output** component if you want to view the output in the **Playground**.\n\nIf the `Message` output from the **Parser** component has empty or unexpected values, there might be a mapping error between the input and the parsing mode, the input has empty values, or the input isn't suitable for plaintext extraction.\n\nFor example, assume you use the following template to parse a `DataFrame`:\n\n```text\n{employee_first_name} {employee_last_name} is a {job_title} ({grade}).\n```\n\nThe following `Message` could result from parsing a row where `employee_first_name` was empty and `grade` was `null`:\n\n```text\n Smith is a Software Engineer (null).\n```\n\nTo troubleshoot missing or unexpected values, you can do the following:\n\n* Make sure the variables in your template map to keys in the incoming `Data` or `DataFrame`.\nTo see the data being passed directly to the **Parser** component, click  **Inspect output** on the component that is sending data to the **Parser** component.\n\n* Check the source data for missing or incorrect values.\nThere are several ways you can address these inconsistencies:\n\n    * Rectify the source data directly.\n    * Use other components to amend or filter anomalies before passing the data to the **Parser** component.\n    There are many components you can use for this depending on your goal, such as the **Data Operations**, **Structured Output**, and **Smart Function** components.\n    * Enable the **Parser** component's **Clean Data** parameter to skip empty rows or lines.\n\n## Python Interpreter\n\nThis component allows you to execute Python code with imported packages.\n\nThe **Python Interpreter** component can only import packages that are already installed in your Langflow environment.\nIf you encounter an `ImportError` when trying to use a package, you need to install it first.\n\nTo install custom packages, see [Install custom dependencies](/install-custom-dependencies).\n\n### Use the Python Interpreter in a flow\n\n1. To use this component in a flow, in the **Global Imports** field, add the packages you want to import as a comma-separated list, such as `math,pandas`.\nAt least one import is required.\n2. In the **Python Code** field, enter the Python code you want to execute. Use `print()` to see the output.\n3. Optional: Enable **Tool Mode**, and then connect the **Python Interpreter** component to an **Agent** component as a tool.\nFor example, connect a **Python Interpreter** component and a [**Calculator** component](/components-helpers#calculator) as tools for an **Agent** component, and then test how it chooses different tools to solve math problems.\n![Python Interpreter and Calculator components connected to an Agent component](/img/component-python-interpreter.png)\n4. Ask the agent an easier math question.\nThe **Calculator** tool can add, subtract, multiple, divide, or perform exponentiation.\nThe agent executes the `evaluate_expression` tool to correctly answer the question.\n\nResult:\n```text\nExecuted evaluate_expression\nInput:\n{\n  \"expression\": \"2+5\"\n}\nOutput:\n{\n  \"result\": \"7\"\n}\n```\n\n5. Give the agent complete Python code.\nThis example creates a Pandas DataFrame table with the imported `pandas` packages, and returns the square root of the mean squares.\n\n```python\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'numbers': [1, 2, 3, 4, 5],\n    'squares': [x**2 for x in range(1, 6)]\n})\n\n# Calculate the square root of the mean\nresult = math.sqrt(df['squares'].mean())\nprint(f\"Square root of mean squares: {result}\")\n```\n\nThe agent correctly chooses the `run_python_repl` tool to solve the problem.\n\nResult:\n```text\nExecuted run_python_repl\n\nInput:\n\n{\n  \"python_code\": \"import pandas as pd\\nimport math\\n\\n# Create a simple DataFrame\\ndf = pd.DataFrame({\\n    'numbers': [1, 2, 3, 4, 5],\\n    'squares': [x**2 for x in range(1, 6)]\\n})\\n\\n# Calculate the square root of the mean\\nresult = math.sqrt(df['squares'].mean())\\nprint(f\\\"Square root of mean squares: {result}\\\")\"\n}\nOutput:\n\n{\n  \"result\": \"Square root of mean squares: 3.3166247903554\"\n}\n```\n\nIf you don't include the package imports in the chat, the agent can still create the table using `pd.DataFrame`, because the `pandas` package is imported globally by the **Python Interpreter** component in the **Global Imports** field.\n\n### Python Interpreter parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| global_imports | String | Input parameter. A comma-separated list of modules to import globally, such as `math,pandas,numpy`. |\n| python_code | Code | Input parameter. The Python code to execute. Only modules specified in Global Imports can be used. |\n| results | Data | Output parameter. The output of the executed Python code, including any printed results or errors. |\n\n## Save File\n\nThe **Save File** component creates a file containing data produced by another component.\nSeveral file formats are supported, and you can store files in [Langflow storage](/memory) or the local file system.\n\nTo configure the **Save File** component and use it in a flow, do the following:\n\n1. Connect [`DataFrame`](/data-types#dataframe), [`Data`](/data-types#data), or [`Message`](/data-types#message) output from another component to the **Save File** component's **Input** port.\n\n    You can connect the same output to multiple **Save File** components if you want to create multiple files, save the data in different file formats, or save files to multiple locations.\n\n2. In **File Name**, enter a file name and an optional path.\n\n    The **File Name** parameter controls where the file is saved.\n    It can contain a file name or an entire file path:\n\n    * **Default location**: If you only provide a file name, then the file is stored in the Langflow data directory. For example,`~/Library/Caches/langflow/data` on macOS.\n\n    * **Subdirectory**: To store files in subdirectories, add the path to the **File Name** parameter.\n    If a given subdirectory doesn't already exist, Langflow automatically creates it.\n    For example, `files/my_file` creates `my_file` in `/data/files`, and it creates the `files` subdirectory if it doesn't already exist.\n\n    * **Absolute or relative path**: To store files elsewhere in your environment or local file storage, provide the absolute or relative path to the desired location.\n    For example, `~/Desktop/my_file` saves `my_file` to the desktop.\n\n    Don't include an extension in the file name.\n    If you do, the extension is treated as part of the file name; it has no impact on the **File Format** parameter.\n\n3. In the [component's header menu](/concepts-components#component-menus), click  **Controls**, select the desired file format, and then click **Close**.\n\n    The available **File Format** options depend on the input data type:\n\n    * `DataFrame` can be saved to CSV (default), Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (fallback default), or Markdown.\n\n    * `Data` can be saved to CSV, Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (default), or Markdown.\n\n    * `Message` can be saved to TXT, JSON (default), or Markdown.\n\n    \n**Note:** Overwrites allowed\n    If you have multiple **Save File** components, in one or more flows, with the same file name, path, and extension, the file contains the data from the most recent run only.\n    Langflow doesn't block overwrites if a matching file already exists.\n    To avoid unintended overwrites, use unique file names and paths.\n    \n\n4. To test the **Save File** component, click  **Run component**, and then click  **Inspect output** to get the filepath where the file was saved.\n\n    The component's literal output is a `Message` containing the original data type, the file name and extension, and the absolute filepath to the file based on the **File Name** parameter.\n    For example:\n\n    ```text\n    DataFrame saved successfully as 'my_file.csv' at /Users/user.name/Library/Caches/langflow/data/my_file.csv\n    ```\n\n    If the **File Name** contains a subdirectory or other non-default path, this is reflected in the `Message` output.\n    For example, a CSV file with the file name `~/Desktop/my_file` could produce the following output:\n\n    ```text\n    DataFrame saved successfully as '/Users/user.name/Desktop/my_file.csv' at /Users/user.name/Desktop/my_file.csv\n    ```\n\n5. Optional: If you want to use the saved file in a flow, you must use an API call or another component to retrieve the file from the given filepath.\n\n## Smart Function {#smart-transform}\n\nIn Langflow version 1.5, this component was renamed from **Lambda Filter** to **Smart Function**.\n\nThe **Smart Function** component uses an LLM to generate a Lambda function to filter or transform structured data based on natural language instructions.\nYou must connect this component to a [language model component](/components-models), which is used to generate a function based on the natural language instructions you provide in the **Instructions** parameter.\nThe LLM runs the function against the data input, and then outputs the results as [`Data`](/data-types#data).\n\n**Note:** Provide brief, clear instructions, focusing on the desired outcome or specific actions, such as `Filter the data to only include items where the 'status' is 'active'`.\nOne sentence or less is preferred because end punctuation, like periods, can cause errors or unexpected behavior.\n\nIf you need to provide more details instructions that aren't directly relevant to the Lambda function, you can input them in the **Language Model** component's **Input** field or through a **Prompt Template** component.\n\nThe following example uses the **API Request** endpoint to pass JSON data from the `https://jsonplaceholder.typicode.com/users` endpoint to the **Smart Function** component.\nThen, the **Smart Function** component passes the data and the instruction `extract emails` to the attached **Language Model** component.\nFrom there, the LLM generates a filter function that extracts email addresses from the JSON data, returning the filtered data as chat output.\n\n![A small flow using a Smart Function component to extract data from an API response.](/img/component-lambda-filter.png)\n\n### Smart Function parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data | Data | Input parameter. The structured data to filter or transform using a Lambda function. |\n| llm | Language Model | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component. |\n| filter_instruction | Instructions | Input parameter. The natural language instructions for how to filter or transform the data. The LLM uses these instructions to create a Lambda function. |\n| sample_size | Sample Size | Input parameter. For large datasets, the number of characters to sample from the dataset head and tail. Only applied if the dataset meets or exceeds `max_size`. Default: `1000`. |\n| max_size | Max Size | Input parameter. The number of characters for the dataset to be considered large, which triggers sampling by the `sample_size` value. Default: `30000`. |\n\n## Split Text\n\nThe **Split Text** component splits data into chunks based on parameters like chunk size and separator.\nIt is often used to chunk data to be tokenized and embedded into vector databases.\nFor examples, see [Use embedding model components in a flow](/components-embedding-models#use-embedding-model-components-in-a-flow) and [Create a Vector RAG chatbot](/chat-with-rag).\n\n![An embedding generation flow that uses a Split Text component to chunk data.](/img/component-split-text.png)\n\nThe component accepts `Message`, `Data`, or `DataFrame`, and then outputs either **Chunks** or **DataFrame**.\nThe **Chunks** output returns a list of [`Data`](/data-types#data) objects containing individual text chunks.\nThe **DataFrame** output returns the list of chunks as a structured [`DataFrame`](/data-types#dataframe) with additional `text` and `metadata` columns.\n\n### Split Text parameters\n\nThe **Split Text** component's parameters control how the text is split into chunks, specifically the `chunk_size`, `chunk_overlap`, and `separator` parameters.\n\nTo test the chunking behavior, add a **Text Input** or **File** component with some sample data to chunk, click  **Run component** on the **Split Text** component, and then click  **Inspect output** to view the list of chunks and their metadata. The **text** column contains the actual text chunks created from your chunking settings.\nIf the chunks aren't split as you expect, adjust the parameters, rerun the component, and then inspect the new output.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n| chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n| separator | Separator | Input parameter. A string defining a character to split on, such as `\\n` to split on new line characters, `\\n\\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n| text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n| keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If `False`, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n\n### Tokenization errors due to chunk size {#chunk-size}\n\nWhen using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\nThe **Split Text** component doesn't always enforce the exact chunk size you set, and individual chunks may exceed your specified limit.\nIf you encounter tokenization errors, modify your text splitting strategy by reducing the chunk size, changing the overlap length, or using a more common separator.\nThen, test your configuration by running the flow and inspecting the component's output.\n\n### Other text splitters\n\nSee [LangChain text splitter components](/bundles-langchain#text-splitters).\n\n## Structured Output\n\nThe **Structured Output** component uses an LLM to transform any input into structured data (`Data` or `DataFrame`) using natural language formatting instructions and an output schema definition.\nFor example, you can extract specific details from documents, like email messages or scientific papers.\n\n### Use the Structured Output component in a flow\n\nTo use the **Structured Output** component in a flow, do the following:\n\n1. Provide an **Input Message**, which is the source material from which you want to extract structured data.\nThis can come from practically any component, but it is typically a **Chat Input**, **File**, or other component that provides some unstructured or semi-structured input.\n\n    \n**Note:** Not all source material has to become structured output.\n    The power of the **Structured Output** component is that you can specify the information you want to extract, even if that data isn't explicitly labeled or an exact keyword match.\n    Then, the LLM can use your instructions to analyze the source material, extract the relevant data, and format it according to your specifications.\n    Any irrelevant source material isn't included in the structured output.\n    \n\n2. Define **Format Instructions** and an **Output Schema** to specify the data to extract from the source material and how to structure it in the final `Data` or `DataFrame` output.\n\n    The instructions are a prompt that tell the LLM what data to extract, how to format it, how to handle exceptions, and any other instructions relevant to preparing the structured data.\n\n    The schema is a table that defines the fields (keys) and data types to organize the data extracted by the LLM into a structured `Data` or `DataFrame` object.\n    For more information, see [Output Schema options](#output-schema-options)\n\n3. Attach a [language model component](/components-models) that is set to emit [`LanguageModel`](/data-types#languagemodel) output.\n\n    The LLM uses the **Input Message** and **Format Instructions** from the **Structured Output** component to extract specific pieces of data from the input text.\n    The output schema is applied to the model's response to produce the final `Data` or `DataFrame` structured object.\n\n4. Optional: Typically, the structured output is passed to downstream components that use the extracted data for other processes, such as the **Parser** or **Data Operations** components.\n\n![A basic flow with Structured Output, Language Model, Type Convert, and Chat Input and Output components.](/img/component-structured-output.png)\n\n<details>\n<summary>Structured Output example: Financial Report Parser template</summary>\n\nThe **Financial Report Parser** template provides an example of how the **Structured Output** component can be used to extract structured data from unstructured text.\n\nThe template's **Structured Output** component has the following configuration:\n\n* The **Input Message** comes from a **Chat Input** component that is preloaded with quotes from sample financial reports\n\n* The **Format Instructions** are as follows:\n\n    ```text\n    You are an AI that extracts structured JSON objects from unstructured text.\n    Use a predefined schema with expected types (str, int, float, bool, dict).\n    Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all.\n    Fill missing or ambiguous values with defaults: null for missing values.\n    Remove exact duplicates but keep variations that have different field values.\n    Always return valid JSON in the expected format, never throw errors.\n    If multiple objects can be extracted, return them all in the structured format.\n    ```\n\n* The **Output Schema** includes keys for `EBITDA`, `NET_INCOME`, and `GROSS_PROFIT`.\n\nThe structured `Data` object is passed to a **Parser** component that produces a text string by mapping the schema keys to variables in the parsing template:\n\n```text\nEBITDA: {EBITDA}  ,  Net Income: {NET_INCOME} , GROSS_PROFIT: {GROSS_PROFIT}\n```\n\nWhen printed to the **Playground**, the resulting `Message` replaces the variables with the actual values extracted by the **Structured Output** component. For example:\n\n```text\nEBITDA: 900 million , Net Income: 500 million , GROSS_PROFIT: 1.2 billion\n```\n\n</details>\n\n### Structured Output parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Language Model (`llm`) | `LanguageModel` | Input parameter. The [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component that defines the LLM to use to analyze, extract, and prepare the structured output. |\n| Input Message (`input_value`) | String | Input parameter. The input message containing source material for extraction. |\n| Format Instructions (`system_prompt`) | String | Input parameter. The instructions to the language model for extracting and formatting the output. |\n| Schema Name (`schema_name`) | String | Input parameter. An optional title for the **Output Schema**. |\n| Output Schema (`output_schema`)| Table | Input parameter. A table describing the schema of the desired structured output, ultimately determining the content of the `Data` or `DataFrame` output. See [Output Schema options](#output-schema-options). |\n| Structured Output (`structured_output`) | `Data` or `DataFrame` | Output parameter. The final structured output produced by the component. Near the component's output port, you can select the output data type as either **Structured Output Data** or **Structured Output DataFrame**. The specific content and structure of the output depends on the input parameters. |\n\n#### Output Schema options {#output-schema-options}\n\nAfter the LLM extracts the relevant data from the **Input Message** and **Format Instructions**, the data is organized according to the **Output Schema**.\n\nThe schema is a table that defines the fields (keys) and data types for the final `Data` or `DataFrame` output from the **Structured Output** component.\n\nThe default schema is a single `field` string.\n\nTo add a key to the schema, click  **Add a new row**, and then edit each column to define the schema:\n\n* **Name**: The name of the output field. Typically a specific key for which you want to extract a value.\n\n    You can reference these keys as variables in downstream components, such as a **Parser** component's template.\n    For example, the schema key `NET_INCOME` could be referenced by the variable `{NET_INCOME}`.\n\n* **Description**: An optional metadata description of the field's contents and purpose.\n\n* **Type**: The data type of the value stored in the field.\nSupported types are `str` (default), `int`, `float`, `bool`, and `dict`.\n\n* **As List**: Enable this setting if you want the field to contain a list of values rather than a single value.\n\nFor simple schemas, you might only extract a few `string` or `int` fields.\nFor more complex schemas with lists and dictionaries, it might help to refer to the `Data` and `DataFrame` structures and attributes, as described in [Langflow data types](/data-types).\nYou can also emit a rough `Data` or `DataFrame`, and then use downstream components for further refinement, such as a **Data Operations** component.\n\n## Type Convert\n\nThe **Type Convert** component converts data from one type to another.\nIt supports `Data`, `DataFrame`, and `Message` data types.\n\n</Tabs>\n\nFor more information, see [Langflow data types](/data-types).\n\n### Use the Type Convert component in a flow\n\nThe **Type Convert** component is typically used to transform data into a format required by a downstream component.\nFor example, if a component outputs a `Message`, but the following component requires `Data`, then you can use the **Type Convert** component to reformat the `Message` as `Data` before passing it to the downstream component.\n\nThe following example uses the **Type Convert** component to convert the `DataFrame` output from a **Web Search** component into `Message` data that is passed as text input for an LLM:\n\n1. Create a flow based on the **Basic prompting** template.\n\n2. Add a **Web Search** component to the flow, and then enter a search query, such as `environmental news`.\n\n3. In the **Prompt Template** component, replace the contents of the **Template** field with the following text:\n\n    ```text\n    Answer the user's question using the {context}\n    ```\n\n    The curly braces define a [prompt variable](/components-prompts#define-variables-in-prompts) that becomes an input field on the **Prompt Template** component.\n    In this example, you will use the **context** field to pass the search results into the template, as explained in the next steps.\n\n3. Add a **Type Convert** component to the flow, and then set the **Output Type** to **Message**.\n\n    Because the **Web Search** component's `DataFrame` output is incompatible with the **context** variable's `Message` input, you must use the **Type Convert** component to change the `DataFrame` to a `Message` in order to pass the search results to the **Prompt Template** component.\n\n4. Connect the additional components to the rest of the flow:\n\n    * Connect the **Web Search** component's output to the **Type Convert** component's input.\n    * Connect the **Type Convert** component's output to the **Prompt Template** component's **context** input.\n\n    ![Convert web search output to text input](/img/component-type-convert-and-web-search.png)\n\n5. In the **Language Model** component, add your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n6. Click **Playground**, and then ask something relevant to your search query, such as `latest news` or `what's the latest research on the environment?`.\n\n    <details>\n    <summary>Result</summary>\n\n    The LLM uses the search results context, your chat message, and it's built-in training data to respond to your question.\n    For example:\n\n    ```text\n    Here are some of the latest news articles related to the environment:\n    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more\n    ...\n    ```\n\n    </details>\n\n### Type Convert parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_data | Input Data | Input parameter. The data to convert. Accepts `Data`, `DataFrame`, or `Message` input. |\n| output_type | Output Type | Input parameter. The desired output type, as one of **Data**, **DataFrame** or **Message**. |\n| output | Output | Output parameter. The converted data in the specified format. The output port changes depending on the selected **Output Type**. |\n\n## Legacy Processing components\n\nThe following Processing components are in legacy status:\n\n<details>\n<summary>Alter Metadata</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component modifies metadata of input objects. It can add new metadata, update existing metadata, and remove specified metadata fields. The component works with both `Message` and `Data` objects, and can also create a new `Data` object from user-provided text.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_value | Input | Input parameter. Objects to which Metadata should be added. |\n| text_in | User Text | Input parameter. Text input; the value is contained in the 'text' attribute of the `Data` object. Empty text entries are ignored. |\n| metadata | Metadata | Input parameter. Metadata to add to each object. |\n| remove_fields | Fields to Remove | Input parameter. Metadata fields to remove. |\n| data | Data | Output parameter. List of Input objects, each with added metadata. |\n\n</details>\n\n<details>\n<summary>Combine Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations) or the [**Loop** component](/components-logic#loop).\n\nThis component combines multiple data sources into a single unified `Data` object.\n\nThe component iterates through a list of `Data` objects, merging them into a single `Data` object (`merged_data`).\nIf the input list is empty, it returns an empty data object.\nIf there's only one input data object, it returns that object unchanged.\n\nThe merging process uses the addition operator to combine data objects.\n\n</details>\n\n<details>\n<summary>Combine Text</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component concatenates two text inputs into a single text chunk using a specified delimiter, outputting a `Message` object with the combined text.\n\n</details>\n\n<details>\n<summary>Create Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component dynamically creates a `Data` object with a specified number of fields and a text key.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| number_of_fields | Number of Fields | Input parameter. The number of fields to be added to the record. |\n| text_key | Text Key | Input parameter. Key that identifies the field to be used as the text content. |\n| text_key_validator | Text Key Validator | Input parameter. If enabled, checks if the given `Text Key` is present in the given `Data`. |\n\n</details>\n\n<details>\n<summary>Extract Key</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component extracts a specific key from a `Data` object and returns the value associated with that key.\n\n</details>\n\n<details>\n<summary>Data to DataFrame/Data to Message</summary>\n\nReplace these legacy components with newer Processing components, such as the [**Data Operations** component](#data-operations) and [**Type Convert** component](#type-convert).\n\nThese components converted one or more `Data` objects into a `DataFrame` or `Message` object.\n\nFor the **Data to DataFrame** component, each `Data` object corresponds to one row in the resulting `DataFrame`.\nFields from the `.data` attribute become columns, and the `.text` field (if present) is placed in a `text` column.\n\n</details>\n\n<details>\n<summary>Filter Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component filters a `Data` object based on a list of keys (`filter_criteria`), returning a new `Data` object (`filtered_data`) that contains only the key-value pairs that match the filter criteria.\n\n</details>\n\n<details>\n<summary>Filter Values</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThe Filter values component filters a list of data items based on a specified key, filter value, and comparison operator.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_data | Input data | Input parameter. The list of data items to filter. |\n| filter_key | Filter Key | Input parameter. The key to filter on. |\n| filter_value | Filter Value | Input parameter. The value to filter by. |\n| operator | Comparison Operator | Input parameter. The operator to apply for comparing the values. |\n| filtered_data | Filtered data | Output parameter. The resulting list of filtered data items. |\n\n</details>\n\n<details>\n<summary>JSON Cleaner</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component cleans JSON strings to ensure they are fully compliant with the JSON specification.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| json_str | JSON String | Input parameter. The JSON string to be cleaned. This can be a raw, potentially malformed JSON string produced by language models or other sources that may not fully comply with JSON specifications. |\n| remove_control_chars | Remove Control Characters | Input parameter. If set to `True`, this option removes control characters (ASCII characters 0-31 and 127) from the JSON string. This can help eliminate invisible characters that might cause parsing issues or make the JSON invalid. |\n| normalize_unicode | Normalize Unicode | Input parameter. When enabled, this option normalizes Unicode characters in the JSON string to their canonical composition form (NFC). This ensures consistent representation of Unicode characters across different systems and prevents potential issues with character encoding. |\n| validate_json | Validate JSON | Input parameter. If set to `True`, this option attempts to parse the JSON string to ensure it is well-formed before applying the final repair operation. It raises a ValueError if the JSON is invalid, allowing for early detection of major structural issues in the JSON. |\n| output | Cleaned JSON String | Output parameter. The resulting cleaned, repaired, and validated JSON string that fully complies with the JSON specification. |\n\n</details>\n\n<details>\n<summary>Message to Data</summary>\n\nReplace this legacy component with the [**Type Convert** component](#type-convert).\n\nThis component converts `Message` objects to `Data` objects.\n\n</details>\n\n<details>\n<summary>Parse DataFrame</summary>\n\nReplace this legacy component with the [**DataFrame Operations** component](#dataframe-operations) or [**Parser** component](#parser).\n\nThis component converts `DataFrame` objects into plain text using templates.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| df | DataFrame | Input parameter. The DataFrame to convert to text rows. |\n| template | Template | Input parameter. Template for formatting (use `{column_name}` placeholders). |\n| sep | Separator | Input parameter. String to join rows in output. |\n| text | Text | Output parameter. All rows combined into single text. |\n\n</details>\n\n<details>\n<summary>Parse JSON</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component converts and extracts JSON fields in `Message` and `Data` objects using JQ queries, then returns `filtered_data`, which is a list of `Data` objects.\n\n</details>\n\n<details>\n<summary>Regex Extractor</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component extracts patterns in text using regular expressions. It can be used to find and extract specific patterns or information in text.\n\n</details>\n\n<details>\n<summary>Select Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component selects a single `Data` object from a list.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data_list | Data List | Input parameter. List of data to select from |\n| data_index | Data Index | Input parameter. Index of the data to select |\n| selected_data | Selected Data | Output parameter. The selected `Data` object. |\n\n</details>\n\n<details>\n<summary>Update Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component dynamically updates or appends data with specified fields.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| old_data | Data | Input parameter. The records to update. |\n| number_of_fields | Number of Fields | Input parameter. The number of fields to add. The maximum is 15. |\n| text_key | Text Key | Input parameter. The key for text content. |\n| text_key_validator | Text Key Validator | Input parameter. Validates the text key presence. |\n| data | Data | Output parameter. The updated Data objects. |\n\n</details>",
      "summary": "Processing components process and transform data within a flow.\nThey have many uses, including:",
      "keywords": [
        "absolute",
        "add",
        "agent",
        "api",
        "available",
        "basic",
        "batch",
        "calculate",
        "calculator",
        "chat",
        "chooses",
        "chunk",
        "chunks",
        "clean",
        "close",
        "code",
        "column",
        "component",
        "components",
        "context"
      ]
    },
    "components-processing": {
      "title": "Processing components",
      "slug": "/components-processing",
      "category": "components",
      "content": "Processing components process and transform data within a flow.\nThey have many uses, including:\n\n* Feed instructions and context to your LLMs and agents with the [**Prompt Template** component](#prompt-template).\n* Extract content from larger chunks of data with a [**Parser** component](#parser).\n* Filter data with natural language with the [**Smart Function** component](#smart-transform).\n* Save data to your local machine with the [**Save File** component](#save-file).\n* Transform data into a different data type with the [**Type Convert** component](#type-convert) to pass it between incompatible components.\n\n## Prompt Template\n\nSee [**Prompt Template** component](/components-prompts).\n\n## Batch Run\n\nThe **Batch Run** component runs a language model over _each row of one text column_ in a [`DataFrame`](/data-types#dataframe), and then returns a new `DataFrame` with the original text and an LLM response.\nThe output contains the following columns:\n\n* `text_input`: The original text from the input `DataFrame`\n* `model_response`: The model's response for each input\n* `batch_index`: The 0-indexed processing order for all rows in the `DataFrame`\n* `metadata` (optional): Additional information about the processing\n\n### Use the Batch Run component in a flow\n\nIf you pass the **Batch Run** output to a [**Parser** component](/components-processing#parser), you can use variables in the parsing template to reference these keys, such as `{text_input}` and `{model_response}`.\nThis is demonstrated in the following example.\n\n![A batch run component connected to OpenAI and a Parser](/img/component-batch-run.png)\n\n1. Connect any language model component to a **Batch Run** component's **Language model** port.\n\n2. Connect `DataFrame` output from another component to the **Batch Run** component's **DataFrame** input.\nFor example, you could connect a **File** component with a CSV file.\n\n3. In the **Batch Run** component's **Column Name** field, enter the name of the column in the incoming `DataFrame` that contains the text to process.\nFor example, if you want to extract text from a `name` column in a CSV file, enter `name` in the **Column Name** field.\n\n4. Connect the **Batch Run** component's **Batch Results** output to a **Parser** component's **DataFrame** input.\n\n5. Optional: In the **Batch Run** [component's header menu](/concepts-components#component-menus), click  **Controls**, enable the **System Message** parameter, click **Close**, and then enter an instruction for how you want the LLM to process each cell extracted from the file.\nFor example, `Create a business card for each name.`\n\n6. In the **Parser** component's **Template** field, enter a template for processing the **Batch Run** component's new `DataFrame` columns (`text_input`, `model_response`, and `batch_index`):\n\n    For example, this template uses three columns from the resulting, post-batch `DataFrame`:\n\n    ```text\n    record_number: {batch_index}, name: {text_input}, summary: {model_response}\n    ```\n\n7. To test the processing, click the **Parser** component, click  **Run component**, and then click  **Inspect output** to view the final `DataFrame`.\n\n    You can also connect a **Chat Output** component to the **Parser** component if you want to see the output in the **Playground**.\n\n### Batch Run parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| model | HandleInput | Input parameter. Connect the 'Language Model' output from a language model component. Required. |\n| system_message | MultilineInput | Input parameter. A multi-line system instruction for all rows in the DataFrame. |\n| df | DataFrameInput | Input parameter. The DataFrame whose column is treated as text messages, as specified by 'column_name'. Required. |\n| column_name | MessageTextInput | Input parameter. The name of the DataFrame column to treat as text messages. If empty, all columns are formatted in TOML. |\n| output_column_name | MessageTextInput | Input parameter. Name of the column where the model's response is stored. Default=`model_response`. |\n| enable_metadata | BoolInput | Input parameter. If `True`, add metadata to the output DataFrame. |\n| batch_results | DataFrame | Output parameter. A DataFrame with all original columns plus the model's response column. |\n\n## Data Operations\n\nThe **Data Operations** component performs operations on [`Data`](/data-types#data) objects, including extracting, filtering, and editing keys and values in the `Data`.\nFor all options, see [Available data operations](#available-data-operations).\nThe output is a new `Data` object containing the modified data after running the selected operation.\n\n### Use the Data Operations component in a flow\n\nThe following example demonstrates how to use a **Data Operations** component in a flow using data from a webhook payload:\n\n1. Create a flow with a **Webhook** component and a **Data Operations** component, and then connect the **Webhook** component's output to the **Data Operations** component's **Data** input.\n\n    All operations in the **Data Operations** component require at least one `Data` input from another component.\n    If the preceding component doesn't produce `Data` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **Data Operations** component.\n    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **DataFrame Operations** components.\n\n2. In the **Operations** field, select the operation you want to perform on the incoming `Data`.\nFor this example, select the **Select Keys** operation.\n\n    \n**Note:** You can select only one operation.\n    If you need to perform multiple operations on the data, you can chain multiple **Data Operations** components together to execute each operation in sequence.\n    For more complex multi-step operations, consider using a component like the **Smart Function** component.\n    \n\n3. Under **Select Keys**, add keys for `name`, `username`, and `email`.\nClick  **Add more** to add a field for each key.\n\n    For this example, assume that the webhook will receive consistent payloads that always contain `name`, `username`, and `email` keys.\n    The **Select Keys** operation extracts the value of these keys from each incoming payload.\n\n4. Optional: If you want to view the output in the **Playground**, connect the **Data Operations** component's output to a **Chat Output** component.\n\n    ![A flow with Webhook, Data Operations, and Chat Output components](/img/component-data-operations-select-key.png)\n\n5. To test the flow, send the following request to your flow's webhook endpoint.\nFor more information about the webhook endpoint, see [Trigger flows with webhooks](/webhook).\n\n    ```bash\n    curl -X POST \"http://$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n    -d '{\n      \"id\": 1,\n      \"name\": \"Leanne Graham\",\n      \"username\": \"Bret\",\n      \"email\": \"Sincere@april.biz\",\n      \"address\": {\n        \"street\": \"Main Street\",\n        \"suite\": \"Apt. 556\",\n        \"city\": \"Springfield\",\n        \"zipcode\": \"92998-3874\",\n        \"geo\": {\n          \"lat\": \"-37.3159\",\n          \"lng\": \"81.1496\"\n        }\n      },\n      \"phone\": \"1-770-736-8031 x56442\",\n      \"website\": \"hildegard.org\",\n      \"company\": {\n        \"name\": \"Acme-Corp\",\n        \"catchPhrase\": \"Multi-layered client-server neural-net\",\n        \"bs\": \"harness real-time e-markets\"\n      }\n    }'\n    ```\n\n6. To view the `Data` resulting from the **Select Keys** operation, do one of the following:\n\n   * If you attached a **Chat Output** component, open the **Playground** to see the result as a chat message.\n   * Click  **Inspect output** on the **Data Operations** component.\n\n### Data Operations parameters\n\nMany parameters are conditional based on the selected **Operation** (`operation`).\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data | Data | Input parameter. The `Data` object to operate on. |\n| operation | Operation | Input parameter. The operation to perform on the data. See [Available data operations](#available-data-operations) |\n| select_keys_input | Select Keys | Input parameter. A list of keys to select from the data. |\n| filter_key | Filter Key | Input parameter. The key to filter by. |\n| operator | Comparison Operator | Input parameter. The operator to apply for comparing values. |\n| filter_values | Filter Values | Input parameter. A list of values to filter by. |\n| append_update_data | Append or Update | Input parameter. The data to append or update the existing data with. |\n| remove_keys_input | Remove Keys | Input parameter. A list of keys to remove from the data. |\n| rename_keys_input | Rename Keys | Input parameter. A list of keys to rename in the data. |\n\n#### Available data operations\n\nOptions for the `operations` input parameter are as follows.\nAll operations act on an incoming `Data` object.\n\n| Name | Required Inputs | Process |\n|-----------|----------------|-------------|\n| Select Keys | `select_keys_input` | Selects specific keys from the data. |\n| Literal Eval | None | Evaluates string values as Python literals. |\n| Combine | None | Combines multiple data objects into one. |\n| Filter Values | `filter_key`, `filter_values`, `operator` | Filters data based on key-value pair. |\n| Append or Update | `append_update_data` | Adds or updates key-value pairs. |\n| Remove Keys | `remove_keys_input` | Removes specified keys from the data. |\n| Rename Keys | `rename_keys_input` | Renames keys in the data. |\n\n## DataFrame Operations\n\nThe **DataFrame Operations** component performs operations on [`DataFrame`](/data-types#dataframe) (table) rows and columns, including schema changes, record changes, sorting, and filtering.\nFor all options, see [DataFrame Operations parameters](#dataframe-operations-parameters).\n\nThe output is a new `DataFrame` containing the modified data after running the selected operation.\n\n### Use the DataFrame Operations component in a flow\n\nThe following steps explain how to configure a **DataFrame Operations** component in a flow.\nYou can follow along with an example or use your own flow.\nThe only requirement is that the preceding component must create `DataFrame` output that you can pass to the **DataFrame Operations** component.\n\n1. Create a new flow or use an existing flow.\n\n    <details>\n    <summary>Example: API response extraction flow</summary>\n\n    The following example flow uses five components to extract `Data` from an API response, transform it to a `DataFrame`, and then perform further processing on the tabular data using a **DataFrame Operations** component.\n    The sixth component, **Chat Output**, is optional in this example.\n    It only serves as a convenient way for you to view the final output in the **Playground**, rather than inspecting the component logs.\n\n    ![A flow that ingests an API response, extracts it to a DataFrame with a Smart Function component, and the processes it through a DataFrame Operations component](/img/component-dataframe-operations.png)\n\n    If you want to use this example to test the **DataFrame Operations** component, do the following:\n\n    1. Create a flow with the following components:\n\n        * **API Request**\n        * **Language Model**\n        * **Smart Function**\n        * **Type Convert**\n\n    2. Configure the [**Smart Function** component](#smart-transform) and its dependencies:\n\n        * **API Request**: Configure the [**API Request** component](/components-data#api-request) to get JSON data from an endpoint of your choice, and then connect the **API Response** output to the **Smart Function** component's **Data** input.\n        * **Language Model**: Select your preferred provider and model, and then enter a valid API key.\n        Change the output to **Language Model**, and then connect the `LanguageModel` output to the **Smart Function** component's **Language Model** input.\n        * **Smart Function**: In the **Instructions** field, enter natural language instructions to extract data from the API response.\n        Your instructions depend on the response content and desired outcome.\n        For example, if the response contains a large `result` field, you might provide instructions like `explode the result field out into a Data object`.\n\n    3. Convert the **Smart Function** component's `Data` output to `DataFrame`:\n\n        1. Connect the **Filtered Data** output to the **Type Convert** component's **Data** input.\n        2. Set the **Type Convert** component's **Output Type** to **DataFrame**.\n\n    Now the flow is ready for you to add the **DataFrame Operations** component.\n\n    </details>\n\n2. Add a **DataFrame Operations** component to the flow, and then connect `DataFrame` output from another component to the **DataFrame** input.\n\n    All operations in the **DataFrame Operations** component require at least one `DataFrame` input from another component.\n    If a component doesn't produce `DataFrame` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **DataFrame Operations** component.\n    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **Data Operations** components.\n\n    If you are following along with the example flow, connect the **Type Convert** component's **DataFrame Output** port to the **DataFrame** input.\n\n3. In the **Operations** field, select the operation you want to perform on the incoming `DataFrame`.\nFor example, the **Filter** operation filters the rows based on a specified column and value.\n\n    \n**Note:** You can select only one operation.\n    If you need to perform multiple operations on the data, you can chain multiple **DataFrame Operations** components together to execute each operation in sequence.\n    For more complex multi-step operations, like dramatic schema changes or pivots, consider using an LLM-powered component, like the **Structured Output** or **Smart Function** component, as a replacement or preparation for the **DataFrame Operations** component.\n    \n\n    If you're following along with the example flow, select any operation that you want to apply to the data that was extracted by the **Smart Function** component.\n    To view the contents of the incoming `DataFrame`, click  **Run component** on the **Type Convert** component, and then  **Inspect output**.\n    If the `DataFrame` seems malformed, click  **Inspect output** on each upstream component to determine where the error occurs, and then modify your flow's configuration as needed.\n    For example, if the **Smart Function** component didn't extract the expected fields, modify your instructions or verify that the given fields are present in the **API Response** output.\n\n4. Configure the operation's parameters.\nThe specific parameters depend on the selected operation.\nFor example, if you select the **Filter** operation, you must define a filter condition using the **Column Name**, **Filter Value**, and **Filter Operator** parameters.\nFor more information, see [DataFrame Operations parameters](#dataframe-operations-parameters)\n\n5. To test the flow, click  **Run component** on the **DataFrame Operations** component, and then click  **Inspect output** to view the new `DataFrame` created from the **Filter** operation.\n\n   If you want to view the output in the **Playground**, connect the **DataFrame Operations** component's output to a **Chat Output** component, rerun the **DataFrame Operations** component, and then click **Playground**.\n\nFor another example, see [Conditional looping](/components-logic#conditional-looping).\n\n### DataFrame Operations parameters\n\nMost **DataFrame Operations** parameters are conditional because they only apply to specific operations.\n\nThe only permanent parameters are **DataFrame** (`df`), which is the `DataFrame` input, and **Operation** (`operation`), which is the operation to perform on the `DataFrame`.\nOnce you select an operation, the conditional parameters for that operation appear on the **DataFrame Operations** component.\n\n</Tabs>\n\n## LLM Router\n\nThe **LLM Router** component routes requests to the most appropriate LLM based on [OpenRouter](https://openrouter.ai/docs/quickstart) model specifications.\n\nTo use the component in a flow, you connect multiple language model components to the **LLM Router** components.\nOne model is the judge LLM that analyzes input messages to understand the evaluation context, selects the most appropriate model from the other attached LLMs, and then routes the input to the selected model.\nThe selected model processes the input, and then returns the generated response.\n\nThe following example flow has three language model components.\nOne is the judge LLM, and the other two are in the LLM pool for request routing.\nThe input and output components create a seamless chat interaction where you send a message and receive a response without any user awareness of the underlying routing.\n\n![LLM Router component](/img/component-llm-router.png)\n\n### LLM Router parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| `models` | **Language Models** | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from multiple [language model components](/components-models) to create a pool of models. The `judge_llm` selects models from this pool when routing requests. The first model you connect is the default model if there is a problem with model selection or routing. |\n| `input_value` | **Input** | Input parameter. The incoming query to be routed to the model selected by the judge LLM. |\n| `judge_llm` | **Judge LLM** | Input parameter. Connect `LanguageModel` output from _one_ **Language Model** component to serve as the judge LLM for request routing. |\n| `optimization` | **Optimization** | Input parameter. Set a preferred characteristic for model selection by the judge LLM. The options are `quality` (highest response quality), `speed` (fastest response time), `cost` (most cost-effective model), or `balanced` (equal weight for quality, speed, and cost). Default: `balanced` |\n| `use_openrouter_specs` | **Use OpenRouter Specs** | Input parameter. Whether to fetch model specifications from the OpenRouter API.\nIf `false`, only the model name is provided to the judge LLM. Default: Enabled (`true`) |\n| `timeout` | **API Timeout** | Input parameter. Set a timeout duration in seconds for API requests made by the router. Default: `10` |\n| `fallback_to_first` | **Fallback to First Model** | Input parameter. Whether to use the first LLM in `models` as a backup if routing fails to reach the selected model. Default: Enabled (`true`) |\n\n### LLM Router outputs\n\nThe **LLM Router** component provides three output options.\nYou can set the desired output type near the component's output port.\n\n* **Output**: A `Message` containing the response to the original query as generated by the selected LLM.\nUse this output for regular chat interactions.\n\n* **Selected Model Info**: A `Data` object containing information about the selected model, such as its name and version.\n\n* **Routing Decision**: A `Message` containing the judge model's reasoning for selecting a particular model, including input query length and number of models considered.\nFor example:\n\n    ```text\n    Model Selection Decision:\n    - Selected Model Index: 0\n    - Selected Langflow Model Name: gpt-4o-mini\n    - Selected API Model ID (if resolved): openai/gpt-4o-mini\n    - Optimization Preference: cost\n    - Input Query Length: 27 characters (~5 tokens)\n    - Number of Models Considered: 2\n    - Specifications Source: OpenRouter API\n    ```\n\n    This is useful for debugging if you feel the judge model isn't selecting the best model.\n\n## Parser {#parser}\n\nThe **Parser** component extracts text from structured data (`DataFrame` or `Data`) using a template or direct stringification.\nThe output is a `Message` containing the parsed text.\n\nThis is a versatile component for data extraction and manipulation in your flows.\nFor examples of **Parser** components in flows, see the following:\n\n* [**Batch Run** component example](#batch-run)\n* [**Structured Output** component example](#structured-output)\n* **Financial Report Parser** template\n* [Trigger flows with webhooks](/webhook)\n* [Create a vector RAG chatbot](/chat-with-rag)\n\n![A flow that uses a Parser component to extract text from a Structured Output component.](/img/component-parser.png)\n\n### Parsing modes\n\nThe **Parser** component has two modes: **Parser** and **Stringify**.\n\n</Tabs>\n\n### Test and troubleshoot parsed text\n\nTo test the **Parser** component, click  **Run component**, and then click  **Inspect output** to see the `Message` output with the parsed text.\nYou can also connect a **Chat Output** component if you want to view the output in the **Playground**.\n\nIf the `Message` output from the **Parser** component has empty or unexpected values, there might be a mapping error between the input and the parsing mode, the input has empty values, or the input isn't suitable for plaintext extraction.\n\nFor example, assume you use the following template to parse a `DataFrame`:\n\n```text\n{employee_first_name} {employee_last_name} is a {job_title} ({grade}).\n```\n\nThe following `Message` could result from parsing a row where `employee_first_name` was empty and `grade` was `null`:\n\n```text\n Smith is a Software Engineer (null).\n```\n\nTo troubleshoot missing or unexpected values, you can do the following:\n\n* Make sure the variables in your template map to keys in the incoming `Data` or `DataFrame`.\nTo see the data being passed directly to the **Parser** component, click  **Inspect output** on the component that is sending data to the **Parser** component.\n\n* Check the source data for missing or incorrect values.\nThere are several ways you can address these inconsistencies:\n\n    * Rectify the source data directly.\n    * Use other components to amend or filter anomalies before passing the data to the **Parser** component.\n    There are many components you can use for this depending on your goal, such as the **Data Operations**, **Structured Output**, and **Smart Function** components.\n    * Enable the **Parser** component's **Clean Data** parameter to skip empty rows or lines.\n\n## Python Interpreter\n\nThis component allows you to execute Python code with imported packages.\n\nThe **Python Interpreter** component can only import packages that are already installed in your Langflow environment.\nIf you encounter an `ImportError` when trying to use a package, you need to install it first.\n\nTo install custom packages, see [Install custom dependencies](/install-custom-dependencies).\n\n### Use the Python Interpreter in a flow\n\n1. To use this component in a flow, in the **Global Imports** field, add the packages you want to import as a comma-separated list, such as `math,pandas`.\nAt least one import is required.\n2. In the **Python Code** field, enter the Python code you want to execute. Use `print()` to see the output.\n3. Optional: Enable **Tool Mode**, and then connect the **Python Interpreter** component to an **Agent** component as a tool.\nFor example, connect a **Python Interpreter** component and a [**Calculator** component](/components-helpers#calculator) as tools for an **Agent** component, and then test how it chooses different tools to solve math problems.\n![Python Interpreter and Calculator components connected to an Agent component](/img/component-python-interpreter.png)\n4. Ask the agent an easier math question.\nThe **Calculator** tool can add, subtract, multiple, divide, or perform exponentiation.\nThe agent executes the `evaluate_expression` tool to correctly answer the question.\n\nResult:\n```text\nExecuted evaluate_expression\nInput:\n{\n  \"expression\": \"2+5\"\n}\nOutput:\n{\n  \"result\": \"7\"\n}\n```\n\n5. Give the agent complete Python code.\nThis example creates a Pandas DataFrame table with the imported `pandas` packages, and returns the square root of the mean squares.\n\n```python\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'numbers': [1, 2, 3, 4, 5],\n    'squares': [x**2 for x in range(1, 6)]\n})\n\n# Calculate the square root of the mean\nresult = math.sqrt(df['squares'].mean())\nprint(f\"Square root of mean squares: {result}\")\n```\n\nThe agent correctly chooses the `run_python_repl` tool to solve the problem.\n\nResult:\n```text\nExecuted run_python_repl\n\nInput:\n\n{\n  \"python_code\": \"import pandas as pd\\nimport math\\n\\n# Create a simple DataFrame\\ndf = pd.DataFrame({\\n    'numbers': [1, 2, 3, 4, 5],\\n    'squares': [x**2 for x in range(1, 6)]\\n})\\n\\n# Calculate the square root of the mean\\nresult = math.sqrt(df['squares'].mean())\\nprint(f\\\"Square root of mean squares: {result}\\\")\"\n}\nOutput:\n\n{\n  \"result\": \"Square root of mean squares: 3.3166247903554\"\n}\n```\n\nIf you don't include the package imports in the chat, the agent can still create the table using `pd.DataFrame`, because the `pandas` package is imported globally by the **Python Interpreter** component in the **Global Imports** field.\n\n### Python Interpreter parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| global_imports | String | Input parameter. A comma-separated list of modules to import globally, such as `math,pandas,numpy`. |\n| python_code | Code | Input parameter. The Python code to execute. Only modules specified in Global Imports can be used. |\n| results | Data | Output parameter. The output of the executed Python code, including any printed results or errors. |\n\n## Save File\n\nThe **Save File** component creates a file containing data produced by another component.\nSeveral file formats are supported, and you can store files in [Langflow storage](/memory) or the local file system.\n\nTo configure the **Save File** component and use it in a flow, do the following:\n\n1. Connect [`DataFrame`](/data-types#dataframe), [`Data`](/data-types#data), or [`Message`](/data-types#message) output from another component to the **Save File** component's **Input** port.\n\n    You can connect the same output to multiple **Save File** components if you want to create multiple files, save the data in different file formats, or save files to multiple locations.\n\n2. In **File Name**, enter a file name and an optional path.\n\n    The **File Name** parameter controls where the file is saved.\n    It can contain a file name or an entire file path:\n\n    * **Default location**: If you only provide a file name, then the file is stored in the Langflow data directory. For example,`~/Library/Caches/langflow/data` on macOS.\n\n    * **Subdirectory**: To store files in subdirectories, add the path to the **File Name** parameter.\n    If a given subdirectory doesn't already exist, Langflow automatically creates it.\n    For example, `files/my_file` creates `my_file` in `/data/files`, and it creates the `files` subdirectory if it doesn't already exist.\n\n    * **Absolute or relative path**: To store files elsewhere in your environment or local file storage, provide the absolute or relative path to the desired location.\n    For example, `~/Desktop/my_file` saves `my_file` to the desktop.\n\n    Don't include an extension in the file name.\n    If you do, the extension is treated as part of the file name; it has no impact on the **File Format** parameter.\n\n3. In the [component's header menu](/concepts-components#component-menus), click  **Controls**, select the desired file format, and then click **Close**.\n\n    The available **File Format** options depend on the input data type:\n\n    * `DataFrame` can be saved to CSV (default), Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (fallback default), or Markdown.\n\n    * `Data` can be saved to CSV, Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (default), or Markdown.\n\n    * `Message` can be saved to TXT, JSON (default), or Markdown.\n\n    \n**Note:** Overwrites allowed\n    If you have multiple **Save File** components, in one or more flows, with the same file name, path, and extension, the file contains the data from the most recent run only.\n    Langflow doesn't block overwrites if a matching file already exists.\n    To avoid unintended overwrites, use unique file names and paths.\n    \n\n4. To test the **Save File** component, click  **Run component**, and then click  **Inspect output** to get the filepath where the file was saved.\n\n    The component's literal output is a `Message` containing the original data type, the file name and extension, and the absolute filepath to the file based on the **File Name** parameter.\n    For example:\n\n    ```text\n    DataFrame saved successfully as 'my_file.csv' at /Users/user.name/Library/Caches/langflow/data/my_file.csv\n    ```\n\n    If the **File Name** contains a subdirectory or other non-default path, this is reflected in the `Message` output.\n    For example, a CSV file with the file name `~/Desktop/my_file` could produce the following output:\n\n    ```text\n    DataFrame saved successfully as '/Users/user.name/Desktop/my_file.csv' at /Users/user.name/Desktop/my_file.csv\n    ```\n\n5. Optional: If you want to use the saved file in a flow, you must use an API call or another component to retrieve the file from the given filepath.\n\n## Smart Function {#smart-transform}\n\nIn Langflow version 1.5, this component was renamed from **Lambda Filter** to **Smart Function**.\n\nThe **Smart Function** component uses an LLM to generate a Lambda function to filter or transform structured data based on natural language instructions.\nYou must connect this component to a [language model component](/components-models), which is used to generate a function based on the natural language instructions you provide in the **Instructions** parameter.\nThe LLM runs the function against the data input, and then outputs the results as [`Data`](/data-types#data).\n\n**Note:** Provide brief, clear instructions, focusing on the desired outcome or specific actions, such as `Filter the data to only include items where the 'status' is 'active'`.\nOne sentence or less is preferred because end punctuation, like periods, can cause errors or unexpected behavior.\n\nIf you need to provide more details instructions that aren't directly relevant to the Lambda function, you can input them in the **Language Model** component's **Input** field or through a **Prompt Template** component.\n\nThe following example uses the **API Request** endpoint to pass JSON data from the `https://jsonplaceholder.typicode.com/users` endpoint to the **Smart Function** component.\nThen, the **Smart Function** component passes the data and the instruction `extract emails` to the attached **Language Model** component.\nFrom there, the LLM generates a filter function that extracts email addresses from the JSON data, returning the filtered data as chat output.\n\n![A small flow using a Smart Function component to extract data from an API response.](/img/component-lambda-filter.png)\n\n### Smart Function parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data | Data | Input parameter. The structured data to filter or transform using a Lambda function. |\n| llm | Language Model | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component. |\n| filter_instruction | Instructions | Input parameter. The natural language instructions for how to filter or transform the data. The LLM uses these instructions to create a Lambda function. |\n| sample_size | Sample Size | Input parameter. For large datasets, the number of characters to sample from the dataset head and tail. Only applied if the dataset meets or exceeds `max_size`. Default: `1000`. |\n| max_size | Max Size | Input parameter. The number of characters for the dataset to be considered large, which triggers sampling by the `sample_size` value. Default: `30000`. |\n\n## Split Text\n\nThe **Split Text** component splits data into chunks based on parameters like chunk size and separator.\nIt is often used to chunk data to be tokenized and embedded into vector databases.\nFor examples, see [Use embedding model components in a flow](/components-embedding-models#use-embedding-model-components-in-a-flow) and [Create a Vector RAG chatbot](/chat-with-rag).\n\n![An embedding generation flow that uses a Split Text component to chunk data.](/img/component-split-text.png)\n\nThe component accepts `Message`, `Data`, or `DataFrame`, and then outputs either **Chunks** or **DataFrame**.\nThe **Chunks** output returns a list of [`Data`](/data-types#data) objects containing individual text chunks.\nThe **DataFrame** output returns the list of chunks as a structured [`DataFrame`](/data-types#dataframe) with additional `text` and `metadata` columns.\n\n### Split Text parameters\n\nThe **Split Text** component's parameters control how the text is split into chunks, specifically the `chunk_size`, `chunk_overlap`, and `separator` parameters.\n\nTo test the chunking behavior, add a **Text Input** or **File** component with some sample data to chunk, click  **Run component** on the **Split Text** component, and then click  **Inspect output** to view the list of chunks and their metadata. The **text** column contains the actual text chunks created from your chunking settings.\nIf the chunks aren't split as you expect, adjust the parameters, rerun the component, and then inspect the new output.\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |\n| chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |\n| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |\n| separator | Separator | Input parameter. A string defining a character to split on, such as `\\n` to split on new line characters, `\\n\\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |\n| text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |\n| keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If `False`, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |\n\n### Tokenization errors due to chunk size {#chunk-size}\n\nWhen using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.\nThe **Split Text** component doesn't always enforce the exact chunk size you set, and individual chunks may exceed your specified limit.\nIf you encounter tokenization errors, modify your text splitting strategy by reducing the chunk size, changing the overlap length, or using a more common separator.\nThen, test your configuration by running the flow and inspecting the component's output.\n\n### Other text splitters\n\nSee [LangChain text splitter components](/bundles-langchain#text-splitters).\n\n## Structured Output\n\nThe **Structured Output** component uses an LLM to transform any input into structured data (`Data` or `DataFrame`) using natural language formatting instructions and an output schema definition.\nFor example, you can extract specific details from documents, like email messages or scientific papers.\n\n### Use the Structured Output component in a flow\n\nTo use the **Structured Output** component in a flow, do the following:\n\n1. Provide an **Input Message**, which is the source material from which you want to extract structured data.\nThis can come from practically any component, but it is typically a **Chat Input**, **File**, or other component that provides some unstructured or semi-structured input.\n\n    \n**Note:** Not all source material has to become structured output.\n    The power of the **Structured Output** component is that you can specify the information you want to extract, even if that data isn't explicitly labeled or an exact keyword match.\n    Then, the LLM can use your instructions to analyze the source material, extract the relevant data, and format it according to your specifications.\n    Any irrelevant source material isn't included in the structured output.\n    \n\n2. Define **Format Instructions** and an **Output Schema** to specify the data to extract from the source material and how to structure it in the final `Data` or `DataFrame` output.\n\n    The instructions are a prompt that tell the LLM what data to extract, how to format it, how to handle exceptions, and any other instructions relevant to preparing the structured data.\n\n    The schema is a table that defines the fields (keys) and data types to organize the data extracted by the LLM into a structured `Data` or `DataFrame` object.\n    For more information, see [Output Schema options](#output-schema-options)\n\n3. Attach a [language model component](/components-models) that is set to emit [`LanguageModel`](/data-types#languagemodel) output.\n\n    The LLM uses the **Input Message** and **Format Instructions** from the **Structured Output** component to extract specific pieces of data from the input text.\n    The output schema is applied to the model's response to produce the final `Data` or `DataFrame` structured object.\n\n4. Optional: Typically, the structured output is passed to downstream components that use the extracted data for other processes, such as the **Parser** or **Data Operations** components.\n\n![A basic flow with Structured Output, Language Model, Type Convert, and Chat Input and Output components.](/img/component-structured-output.png)\n\n<details>\n<summary>Structured Output example: Financial Report Parser template</summary>\n\nThe **Financial Report Parser** template provides an example of how the **Structured Output** component can be used to extract structured data from unstructured text.\n\nThe template's **Structured Output** component has the following configuration:\n\n* The **Input Message** comes from a **Chat Input** component that is preloaded with quotes from sample financial reports\n\n* The **Format Instructions** are as follows:\n\n    ```text\n    You are an AI that extracts structured JSON objects from unstructured text.\n    Use a predefined schema with expected types (str, int, float, bool, dict).\n    Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all.\n    Fill missing or ambiguous values with defaults: null for missing values.\n    Remove exact duplicates but keep variations that have different field values.\n    Always return valid JSON in the expected format, never throw errors.\n    If multiple objects can be extracted, return them all in the structured format.\n    ```\n\n* The **Output Schema** includes keys for `EBITDA`, `NET_INCOME`, and `GROSS_PROFIT`.\n\nThe structured `Data` object is passed to a **Parser** component that produces a text string by mapping the schema keys to variables in the parsing template:\n\n```text\nEBITDA: {EBITDA}  ,  Net Income: {NET_INCOME} , GROSS_PROFIT: {GROSS_PROFIT}\n```\n\nWhen printed to the **Playground**, the resulting `Message` replaces the variables with the actual values extracted by the **Structured Output** component. For example:\n\n```text\nEBITDA: 900 million , Net Income: 500 million , GROSS_PROFIT: 1.2 billion\n```\n\n</details>\n\n### Structured Output parameters\n\n| Name | Type | Description |\n|------|------|-------------|\n| Language Model (`llm`) | `LanguageModel` | Input parameter. The [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component that defines the LLM to use to analyze, extract, and prepare the structured output. |\n| Input Message (`input_value`) | String | Input parameter. The input message containing source material for extraction. |\n| Format Instructions (`system_prompt`) | String | Input parameter. The instructions to the language model for extracting and formatting the output. |\n| Schema Name (`schema_name`) | String | Input parameter. An optional title for the **Output Schema**. |\n| Output Schema (`output_schema`)| Table | Input parameter. A table describing the schema of the desired structured output, ultimately determining the content of the `Data` or `DataFrame` output. See [Output Schema options](#output-schema-options). |\n| Structured Output (`structured_output`) | `Data` or `DataFrame` | Output parameter. The final structured output produced by the component. Near the component's output port, you can select the output data type as either **Structured Output Data** or **Structured Output DataFrame**. The specific content and structure of the output depends on the input parameters. |\n\n#### Output Schema options {#output-schema-options}\n\nAfter the LLM extracts the relevant data from the **Input Message** and **Format Instructions**, the data is organized according to the **Output Schema**.\n\nThe schema is a table that defines the fields (keys) and data types for the final `Data` or `DataFrame` output from the **Structured Output** component.\n\nThe default schema is a single `field` string.\n\nTo add a key to the schema, click  **Add a new row**, and then edit each column to define the schema:\n\n* **Name**: The name of the output field. Typically a specific key for which you want to extract a value.\n\n    You can reference these keys as variables in downstream components, such as a **Parser** component's template.\n    For example, the schema key `NET_INCOME` could be referenced by the variable `{NET_INCOME}`.\n\n* **Description**: An optional metadata description of the field's contents and purpose.\n\n* **Type**: The data type of the value stored in the field.\nSupported types are `str` (default), `int`, `float`, `bool`, and `dict`.\n\n* **As List**: Enable this setting if you want the field to contain a list of values rather than a single value.\n\nFor simple schemas, you might only extract a few `string` or `int` fields.\nFor more complex schemas with lists and dictionaries, it might help to refer to the `Data` and `DataFrame` structures and attributes, as described in [Langflow data types](/data-types).\nYou can also emit a rough `Data` or `DataFrame`, and then use downstream components for further refinement, such as a **Data Operations** component.\n\n## Type Convert\n\nThe **Type Convert** component converts data from one type to another.\nIt supports `Data`, `DataFrame`, and `Message` data types.\n\n</Tabs>\n\nFor more information, see [Langflow data types](/data-types).\n\n### Use the Type Convert component in a flow\n\nThe **Type Convert** component is typically used to transform data into a format required by a downstream component.\nFor example, if a component outputs a `Message`, but the following component requires `Data`, then you can use the **Type Convert** component to reformat the `Message` as `Data` before passing it to the downstream component.\n\nThe following example uses the **Type Convert** component to convert the `DataFrame` output from a **Web Search** component into `Message` data that is passed as text input for an LLM:\n\n1. Create a flow based on the **Basic prompting** template.\n\n2. Add a **Web Search** component to the flow, and then enter a search query, such as `environmental news`.\n\n3. In the **Prompt Template** component, replace the contents of the **Template** field with the following text:\n\n    ```text\n    Answer the user's question using the {context}\n    ```\n\n    The curly braces define a [prompt variable](/components-prompts#define-variables-in-prompts) that becomes an input field on the **Prompt Template** component.\n    In this example, you will use the **context** field to pass the search results into the template, as explained in the next steps.\n\n3. Add a **Type Convert** component to the flow, and then set the **Output Type** to **Message**.\n\n    Because the **Web Search** component's `DataFrame` output is incompatible with the **context** variable's `Message` input, you must use the **Type Convert** component to change the `DataFrame` to a `Message` in order to pass the search results to the **Prompt Template** component.\n\n4. Connect the additional components to the rest of the flow:\n\n    * Connect the **Web Search** component's output to the **Type Convert** component's input.\n    * Connect the **Type Convert** component's output to the **Prompt Template** component's **context** input.\n\n    ![Convert web search output to text input](/img/component-type-convert-and-web-search.png)\n\n5. In the **Language Model** component, add your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n6. Click **Playground**, and then ask something relevant to your search query, such as `latest news` or `what's the latest research on the environment?`.\n\n    <details>\n    <summary>Result</summary>\n\n    The LLM uses the search results context, your chat message, and it's built-in training data to respond to your question.\n    For example:\n\n    ```text\n    Here are some of the latest news articles related to the environment:\n    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more\n    ...\n    ```\n\n    </details>\n\n### Type Convert parameters\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_data | Input Data | Input parameter. The data to convert. Accepts `Data`, `DataFrame`, or `Message` input. |\n| output_type | Output Type | Input parameter. The desired output type, as one of **Data**, **DataFrame** or **Message**. |\n| output | Output | Output parameter. The converted data in the specified format. The output port changes depending on the selected **Output Type**. |\n\n## Legacy Processing components\n\nThe following Processing components are in legacy status:\n\n<details>\n<summary>Alter Metadata</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component modifies metadata of input objects. It can add new metadata, update existing metadata, and remove specified metadata fields. The component works with both `Message` and `Data` objects, and can also create a new `Data` object from user-provided text.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_value | Input | Input parameter. Objects to which Metadata should be added. |\n| text_in | User Text | Input parameter. Text input; the value is contained in the 'text' attribute of the `Data` object. Empty text entries are ignored. |\n| metadata | Metadata | Input parameter. Metadata to add to each object. |\n| remove_fields | Fields to Remove | Input parameter. Metadata fields to remove. |\n| data | Data | Output parameter. List of Input objects, each with added metadata. |\n\n</details>\n\n<details>\n<summary>Combine Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations) or the [**Loop** component](/components-logic#loop).\n\nThis component combines multiple data sources into a single unified `Data` object.\n\nThe component iterates through a list of `Data` objects, merging them into a single `Data` object (`merged_data`).\nIf the input list is empty, it returns an empty data object.\nIf there's only one input data object, it returns that object unchanged.\n\nThe merging process uses the addition operator to combine data objects.\n\n</details>\n\n<details>\n<summary>Combine Text</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component concatenates two text inputs into a single text chunk using a specified delimiter, outputting a `Message` object with the combined text.\n\n</details>\n\n<details>\n<summary>Create Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component dynamically creates a `Data` object with a specified number of fields and a text key.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| number_of_fields | Number of Fields | Input parameter. The number of fields to be added to the record. |\n| text_key | Text Key | Input parameter. Key that identifies the field to be used as the text content. |\n| text_key_validator | Text Key Validator | Input parameter. If enabled, checks if the given `Text Key` is present in the given `Data`. |\n\n</details>\n\n<details>\n<summary>Extract Key</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component extracts a specific key from a `Data` object and returns the value associated with that key.\n\n</details>\n\n<details>\n<summary>Data to DataFrame/Data to Message</summary>\n\nReplace these legacy components with newer Processing components, such as the [**Data Operations** component](#data-operations) and [**Type Convert** component](#type-convert).\n\nThese components converted one or more `Data` objects into a `DataFrame` or `Message` object.\n\nFor the **Data to DataFrame** component, each `Data` object corresponds to one row in the resulting `DataFrame`.\nFields from the `.data` attribute become columns, and the `.text` field (if present) is placed in a `text` column.\n\n</details>\n\n<details>\n<summary>Filter Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component filters a `Data` object based on a list of keys (`filter_criteria`), returning a new `Data` object (`filtered_data`) that contains only the key-value pairs that match the filter criteria.\n\n</details>\n\n<details>\n<summary>Filter Values</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThe Filter values component filters a list of data items based on a specified key, filter value, and comparison operator.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| input_data | Input data | Input parameter. The list of data items to filter. |\n| filter_key | Filter Key | Input parameter. The key to filter on. |\n| filter_value | Filter Value | Input parameter. The value to filter by. |\n| operator | Comparison Operator | Input parameter. The operator to apply for comparing the values. |\n| filtered_data | Filtered data | Output parameter. The resulting list of filtered data items. |\n\n</details>\n\n<details>\n<summary>JSON Cleaner</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component cleans JSON strings to ensure they are fully compliant with the JSON specification.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| json_str | JSON String | Input parameter. The JSON string to be cleaned. This can be a raw, potentially malformed JSON string produced by language models or other sources that may not fully comply with JSON specifications. |\n| remove_control_chars | Remove Control Characters | Input parameter. If set to `True`, this option removes control characters (ASCII characters 0-31 and 127) from the JSON string. This can help eliminate invisible characters that might cause parsing issues or make the JSON invalid. |\n| normalize_unicode | Normalize Unicode | Input parameter. When enabled, this option normalizes Unicode characters in the JSON string to their canonical composition form (NFC). This ensures consistent representation of Unicode characters across different systems and prevents potential issues with character encoding. |\n| validate_json | Validate JSON | Input parameter. If set to `True`, this option attempts to parse the JSON string to ensure it is well-formed before applying the final repair operation. It raises a ValueError if the JSON is invalid, allowing for early detection of major structural issues in the JSON. |\n| output | Cleaned JSON String | Output parameter. The resulting cleaned, repaired, and validated JSON string that fully complies with the JSON specification. |\n\n</details>\n\n<details>\n<summary>Message to Data</summary>\n\nReplace this legacy component with the [**Type Convert** component](#type-convert).\n\nThis component converts `Message` objects to `Data` objects.\n\n</details>\n\n<details>\n<summary>Parse DataFrame</summary>\n\nReplace this legacy component with the [**DataFrame Operations** component](#dataframe-operations) or [**Parser** component](#parser).\n\nThis component converts `DataFrame` objects into plain text using templates.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| df | DataFrame | Input parameter. The DataFrame to convert to text rows. |\n| template | Template | Input parameter. Template for formatting (use `{column_name}` placeholders). |\n| sep | Separator | Input parameter. String to join rows in output. |\n| text | Text | Output parameter. All rows combined into single text. |\n\n</details>\n\n<details>\n<summary>Parse JSON</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component converts and extracts JSON fields in `Message` and `Data` objects using JQ queries, then returns `filtered_data`, which is a list of `Data` objects.\n\n</details>\n\n<details>\n<summary>Regex Extractor</summary>\n\nReplace this legacy component with the [**Parser** component](#parser).\n\nThis component extracts patterns in text using regular expressions. It can be used to find and extract specific patterns or information in text.\n\n</details>\n\n<details>\n<summary>Select Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component selects a single `Data` object from a list.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| data_list | Data List | Input parameter. List of data to select from |\n| data_index | Data Index | Input parameter. Index of the data to select |\n| selected_data | Selected Data | Output parameter. The selected `Data` object. |\n\n</details>\n\n<details>\n<summary>Update Data</summary>\n\nReplace this legacy component with the [**Data Operations** component](#data-operations).\n\nThis component dynamically updates or appends data with specified fields.\n\nIt accepts the following parameters:\n\n| Name | Display Name | Info |\n|------|--------------|------|\n| old_data | Data | Input parameter. The records to update. |\n| number_of_fields | Number of Fields | Input parameter. The number of fields to add. The maximum is 15. |\n| text_key | Text Key | Input parameter. The key for text content. |\n| text_key_validator | Text Key Validator | Input parameter. Validates the text key presence. |\n| data | Data | Output parameter. The updated Data objects. |\n\n</details>",
      "summary": "Processing components process and transform data within a flow.\nThey have many uses, including:",
      "keywords": [
        "absolute",
        "add",
        "agent",
        "api",
        "available",
        "basic",
        "batch",
        "calculate",
        "calculator",
        "chat",
        "chooses",
        "chunk",
        "chunks",
        "clean",
        "close",
        "code",
        "column",
        "component",
        "components",
        "context"
      ]
    },
    "/components-prompts": {
      "title": "Prompt Template",
      "slug": "/components-prompts",
      "category": "components",
      "content": "Use the **Prompt Template** core component to create a _prompt_ that supplies instructions and context to an LLM or agent, separate from other input like chat messages and file uploads.\n\nPrompts are structured input that use natural language, fixed values, and dynamic variables to provide baseline context for the LLM.\nFor example:\n\n* Define a consistent structure for user queries, making it easier for the LLM to understand and respond appropriately.\n* Define a specific output format for the LLM, such as JSON or structured text.\n* Define a role for the LLM, such as `You are a helpful assistant` or `You are an expert in microbiology`.\n* Allow the LLM to reference chat memory.\n\nThe **Prompt Template** component can also output variable instructions to other components later in the flow.\n\n## Prompt Template parameters\n\n| Name     | Display Name   | Description                                                       |\n|----------|----------------|-------------------------------------------------------------------|\n| template | Template       | Input parameter. Create a prompt template with dynamic variables in curly braces, such as `{VARIABLE_NAME}`.  |\n| prompt   | Prompt Message | Output parameter. The built prompt message returned by the `build_prompt` method. |\n\n## Define variables in prompts\n\nVariables in a **Prompt Template** component dynamically add fields to the **Prompt Template** component so that your flow can receive definitions for those values from other components, Langflow global variables, or fixed input.\n\nFor example, with the [**Message History** component](/components-helpers#message-history), you can use a `{memory}` variable to pass chat history to the prompt.\nHowever, the **Agent** component includes built-in chat memory that is enabled by default.\nFor more information, see [Memory management options](/memory).\n\nThe following steps demonstrate how to add variables to a **Prompt Template** component:\n\n1. Create a flow based on the **Basic prompting** template.\n\n    This template already has a **Prompt Template** component, but the template only contains natural language instructions: `Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.`\n\n    This prompt defines a role for the LLM's chat interactions, but it doesn't include variables that help you create prompts that adapt dynamically to changing contexts, such as different users and environments.\n\n2. Click the **Prompt Template** component, and then add some variables to the **Template** field.\n\n    Variables are declared by wrapping the variable name in curly braces, like `{variable_name}`.\n    For example, the following template creates `context` and `user_question` variables:\n\n    ```text\n    Given the context:\n\n    {context}\n\n    Answer the question:\n\n    {user_question}\n    ```\n\n    \n\n4. Click **Check & Save** to save the template.\n\n    After adding the variables to the template, new fields are added to the **Prompt Template** component for each variable.\n\n5. Provide input for the variable fields:\n\n   * Connect the fields to other components to pass the output from those components to the variables.\n   * Use Langflow global variables.\n   * Enter fixed values directly into the fields.\n\nYou can add as many variables as you like in your template.\nFor example, you could add variables for `{references}` and `{instructions}`, and then feed that information in from other components, such as **Text Input**, **URL**, or **File** components.\n\n## See also\n\n* [**LangChain Prompt Hub** component](/bundles-langchain#prompt-hub)\n* [Processing components](/components-processing)",
      "summary": "Use the **Prompt Template** core component to create a _prompt_ that supplies instructions and context to an LLM or agent, separate from other input like chat messages and file uploads.",
      "keywords": [
        "agent",
        "basic",
        "check",
        "define",
        "file",
        "history",
        "hub",
        "input",
        "langchain",
        "message",
        "parameters",
        "prompt",
        "prompting",
        "prompts",
        "save",
        "template",
        "text",
        "url",
        "variables"
      ]
    },
    "components-prompts": {
      "title": "Prompt Template",
      "slug": "/components-prompts",
      "category": "components",
      "content": "Use the **Prompt Template** core component to create a _prompt_ that supplies instructions and context to an LLM or agent, separate from other input like chat messages and file uploads.\n\nPrompts are structured input that use natural language, fixed values, and dynamic variables to provide baseline context for the LLM.\nFor example:\n\n* Define a consistent structure for user queries, making it easier for the LLM to understand and respond appropriately.\n* Define a specific output format for the LLM, such as JSON or structured text.\n* Define a role for the LLM, such as `You are a helpful assistant` or `You are an expert in microbiology`.\n* Allow the LLM to reference chat memory.\n\nThe **Prompt Template** component can also output variable instructions to other components later in the flow.\n\n## Prompt Template parameters\n\n| Name     | Display Name   | Description                                                       |\n|----------|----------------|-------------------------------------------------------------------|\n| template | Template       | Input parameter. Create a prompt template with dynamic variables in curly braces, such as `{VARIABLE_NAME}`.  |\n| prompt   | Prompt Message | Output parameter. The built prompt message returned by the `build_prompt` method. |\n\n## Define variables in prompts\n\nVariables in a **Prompt Template** component dynamically add fields to the **Prompt Template** component so that your flow can receive definitions for those values from other components, Langflow global variables, or fixed input.\n\nFor example, with the [**Message History** component](/components-helpers#message-history), you can use a `{memory}` variable to pass chat history to the prompt.\nHowever, the **Agent** component includes built-in chat memory that is enabled by default.\nFor more information, see [Memory management options](/memory).\n\nThe following steps demonstrate how to add variables to a **Prompt Template** component:\n\n1. Create a flow based on the **Basic prompting** template.\n\n    This template already has a **Prompt Template** component, but the template only contains natural language instructions: `Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.`\n\n    This prompt defines a role for the LLM's chat interactions, but it doesn't include variables that help you create prompts that adapt dynamically to changing contexts, such as different users and environments.\n\n2. Click the **Prompt Template** component, and then add some variables to the **Template** field.\n\n    Variables are declared by wrapping the variable name in curly braces, like `{variable_name}`.\n    For example, the following template creates `context` and `user_question` variables:\n\n    ```text\n    Given the context:\n\n    {context}\n\n    Answer the question:\n\n    {user_question}\n    ```\n\n    \n\n4. Click **Check & Save** to save the template.\n\n    After adding the variables to the template, new fields are added to the **Prompt Template** component for each variable.\n\n5. Provide input for the variable fields:\n\n   * Connect the fields to other components to pass the output from those components to the variables.\n   * Use Langflow global variables.\n   * Enter fixed values directly into the fields.\n\nYou can add as many variables as you like in your template.\nFor example, you could add variables for `{references}` and `{instructions}`, and then feed that information in from other components, such as **Text Input**, **URL**, or **File** components.\n\n## See also\n\n* [**LangChain Prompt Hub** component](/bundles-langchain#prompt-hub)\n* [Processing components](/components-processing)",
      "summary": "Use the **Prompt Template** core component to create a _prompt_ that supplies instructions and context to an LLM or agent, separate from other input like chat messages and file uploads.",
      "keywords": [
        "agent",
        "basic",
        "check",
        "define",
        "file",
        "history",
        "hub",
        "input",
        "langchain",
        "message",
        "parameters",
        "prompt",
        "prompting",
        "prompts",
        "save",
        "template",
        "text",
        "url",
        "variables"
      ]
    },
    "/components-tools": {
      "title": "Tools",
      "slug": "/components-tools",
      "category": "components",
      "content": "In Langflow version 1.5, the **Tools** category was deprecated.\nAll components that were in this category were replaced by other components or moved to other component categories.\n\n## MCP Connection component\n\nThis component was moved to the **Agents** category and renamed to the [**MCP Tools** component](/components-agents#mcp-connection)\n\n## Legacy Tools components\n\nThe following Tools components are in legacy status:\n\n* **Calculator Tool**: Replaced by the [**Calculator** component](/components-helpers#calculator).\n* **Python Code Structured**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).\n* **Python REPL**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).\n* **Search API**: Replaced by the [**SearchApi** bundle](/bundles-searchapi).\n* **SearXNG Search**: No direct replacement. Use another provider's search component, create a custom component, or use a core component like the [**API Request** component](/components-data#api-request).\n* **Serp Search API**: Replace by the **SerpApi** bundle.\n* **Tavily Search API**: Replaced by the **Tavily** bundle.\n* **Wikidata API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).\n* **Wikipedia API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).\n* **Yahoo! Finance**: Replaced by the **Yahoo! Search** bundle.\n\n## See also\n\n* [**API Request** component](/components-data#api-request)\n* [**News Search** component](/components-data#news-search)\n* [**Web Search** component](/components-data#web-search)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)\n* [**Exa** bundle](/bundles-exa)\n* [**Google** bundle](/bundles-google)\n* [**Serper** bundle](/bundles-serper)",
      "summary": "In Langflow version 1.5, the **Tools** category was deprecated.\nAll components that were in this category were replaced by other components or moved to other component categories.",
      "keywords": [
        "agents",
        "api",
        "bing",
        "calculator",
        "code",
        "component",
        "components",
        "connection",
        "duckduckgo",
        "exa",
        "finance",
        "google",
        "interpreter",
        "legacy",
        "mcp",
        "news",
        "python",
        "repl",
        "request",
        "search"
      ]
    },
    "components-tools": {
      "title": "Tools",
      "slug": "/components-tools",
      "category": "components",
      "content": "In Langflow version 1.5, the **Tools** category was deprecated.\nAll components that were in this category were replaced by other components or moved to other component categories.\n\n## MCP Connection component\n\nThis component was moved to the **Agents** category and renamed to the [**MCP Tools** component](/components-agents#mcp-connection)\n\n## Legacy Tools components\n\nThe following Tools components are in legacy status:\n\n* **Calculator Tool**: Replaced by the [**Calculator** component](/components-helpers#calculator).\n* **Python Code Structured**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).\n* **Python REPL**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).\n* **Search API**: Replaced by the [**SearchApi** bundle](/bundles-searchapi).\n* **SearXNG Search**: No direct replacement. Use another provider's search component, create a custom component, or use a core component like the [**API Request** component](/components-data#api-request).\n* **Serp Search API**: Replace by the **SerpApi** bundle.\n* **Tavily Search API**: Replaced by the **Tavily** bundle.\n* **Wikidata API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).\n* **Wikipedia API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).\n* **Yahoo! Finance**: Replaced by the **Yahoo! Search** bundle.\n\n## See also\n\n* [**API Request** component](/components-data#api-request)\n* [**News Search** component](/components-data#news-search)\n* [**Web Search** component](/components-data#web-search)\n* [**Bing** bundle](/bundles-bing)\n* [**DuckDuckGo** bundle](/bundles-duckduckgo)\n* [**Exa** bundle](/bundles-exa)\n* [**Google** bundle](/bundles-google)\n* [**Serper** bundle](/bundles-serper)",
      "summary": "In Langflow version 1.5, the **Tools** category was deprecated.\nAll components that were in this category were replaced by other components or moved to other component categories.",
      "keywords": [
        "agents",
        "api",
        "bing",
        "calculator",
        "code",
        "component",
        "components",
        "connection",
        "duckduckgo",
        "exa",
        "finance",
        "google",
        "interpreter",
        "legacy",
        "mcp",
        "news",
        "python",
        "repl",
        "request",
        "search"
      ]
    },
    "/concepts-components": {
      "title": "Components overview",
      "slug": "/concepts-components",
      "category": "components",
      "content": "Components are the building blocks of your flows.\nLike classes in an application, each component is designed for a specific use case or integration.\n\n**Note:** Langflow provides keyboard shortcuts for the workspace.\n\nIn the Langflow header, click your profile icon, select **Settings**, and then click **Shortcuts** to view the available shortcuts.\n\n## Add a component to a flow {#component-menus}\n\nTo add a component to a flow, drag the component from the  **Core components** or  **Bundles** menu into the [workspace](/concepts-overview#workspace).\n\nComponents are grouped by type or provider, and some components are hidden by default:\n\n*  **Core components**: Langflow's base components are grouped by purpose, such as **Inputs and Outputs** or **Data**.\nThese components either provide generic functionality, like loops and parsing, or they provide single components that support multiple third-party integrations.\n\n*  **Bundles**: Bundles contain one or more components that support specific third-party integrations, and they are grouped by service provider.\n\n* **Legacy**: These components are hidden by default.\nFor more information, see [Legacy components](#legacy-components).\n\n### Configure a component\n\nAfter adding a component to a flow, configure the component's parameters and connect it to the other components in your flows.\n\nEach component has inputs, outputs, parameters, and controls related to the component's purpose.\nBy default, components show only required and common options.\nTo access additional settings and controls, including meta settings, use the [component's header menu](#component-header-menus).\n\n### Component header menus\n\nTo access a component's header menu, click the component in your workspace.\n\n![Agent component](/img/agent-component.png)\n\nA few options are available directly on the header menu.\nFor example:\n\n- **Code**: Modify component settings by directly editing the component's Python code.\n- **Controls**: Adjust all component parameters, including optional settings that are hidden by default.\n- **Tool Mode**: Enable this option when combining a component with an **Agent** component.\n\nFor all other options, including **Delete** and **Duplicate** controls, click  **Show More**.\n\n### Rename a component\n\nTo modify a component's name or description, click the component in the workspace, and then click  **Edit**.\nComponent descriptions accept Markdown syntax.\n\n### Run a component\n\nTo run a single component, click  **Run component**.\nA **Last Run** value indicates that the component ran successfully.\n\nRunning a single component is different from running an entire flow. In a single component run, the `build_vertex` function is called, which builds and runs only the single component with direct inputs provided through the visual editor (the `inputs_dict` parameter). The `VertexBuildResult` data is passed to the `build_and_run` method that calls the component's `build` method and runs it. Unlike running an entire flow, running a single component doesn't automatically execute its upstream dependencies.\n\n### Inspect component output and logs\n\nTo view the output and logs for a single component, click  **Inspect**.\n\n### Freeze a component\n\n**Note:** Freezing a component also freezes all components upstream of the selected component.\n\nUse the freeze option if you expect consistent output from a component _and all upstream components_, and you only need to run those components once.\n\nFreezing a component prevents that component and all upstream components from re-running, and it preserves the last output state for those components.\nAny future flow runs use the preserved output.\n\nTo freeze a component, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Freeze**.\n\n## Component ports\n\nAround the border of each component, there are circular port icons like .\nThese indicate a component _connection point_ or _port_.\n\nPorts either accept input or produce output of a specific data type.\nYou can infer the data type from the field the port is attached to or from the [port's color](#port-colors).\nFor example, the **System Message** field accepts [message data](/data-types#message), as illustrated by the blue port icon: .\n\n![Prompt Template component with multiple inputs](/img/prompt-component.png)\n\nWhen building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.\nFor information about the programmatic representation of each data type, see [Langflow data types](/data-types).\n\n**Note:** * In the workspace, hover over a port to see connection details for that port.\nClick a port to  **Search** for compatible components.\n\n* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.\n\n### Dynamic ports\n\nSome components have ports that are dynamically added or removed.\nFor example, the **Prompt Template** component accepts [inputs wrapped in curly braces](/components-prompts#define-variables-in-prompts), and new ports are opened when a value wrapped in curly braces is detected in the **Template** field.\n\n![Prompt Template component with multiple inputs](/img/prompt-component-with-multiple-inputs.png)\n\n### Output type selection\n\nAll components produce output that is either sent to another component in the flow or returned as the final flow result.\n\nSome components can produce multiple types of output:\n\n* If the component emits all types at once, the component has multiple output ports in the visual editor. In component code, this is represented by `group_outputs=True`\n\n* If the component emits only one type, you must select the output type by clicking the output label near the output port, and then selecting the desired output type. In component code, this is represented by `group_outputs=False` or omitting the `group_outputs` parameter.\n\nFor example, a language model component can output _either_ a **Model Response** or **Language Model**.\nThe **Model Response** output produces [`Message`](/data-types#message) data that can be passed to another component's `Message` port.\nThe **Language Model** output must be connected to a component with a **Language Model** input, such as the [**Structured Output** component](/components-processing#structured-output), that uses the attached LLM to power the receiving component's reasoning.\n\n![Output type selection in the Language Model component](/img/select-output.png)\n\n### Port colors\n\nComponent port colors indicate the data type ingested or emitted by the port.\nFor example, a **Message** port either accepts or emits `Message` data.\n\nThe following table lists the component data types and their corresponding port colors:\n\n| Data type | Port color | Port icon example |\n|-----------|--------------|----------|\n| Data | Red |  |\n| DataFrame | Pink |  |\n| Embeddings | Emerald |  |\n| LanguageModel | Fuchsia |  |\n| Memory | Orange |  |\n| Message | Indigo |  |\n| Tool | Cyan |  |\n| Unknown or multiple types | Gray |  |\n\n## Component code\n\nYou can edit components in the [workspace](/concepts-overview#workspace) and in code. When editing a flow, select a component, and then click  **Code** to see and edit the component's underlying Python code.\n\nAll components have underlying code that determines how you configure them and what actions they can perform.\nIn the context of creating and running flows, component code does the following:\n\n* Determines what configuration options to show in the visual editor.\n* Validates inputs based on the component's defined input types.\n* Processes data using the configured parameters, methods, and functions.\n* Passes results to the next component in the flow.\n\nAll components inherit from a base `Component` class that defines the component's interface and behavior.\nFor example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the [`LCTextSplitterComponent`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/base/textsplitters/model.py) class.\n\nEach component's code includes definitions for inputs and outputs, which are represented in the workspace as [component ports](#component-ports).\nFor example, the `RecursiveCharacterTextSplitter` has four inputs. Each input definition specifies the input type, such as `IntInput`, as well as the encoded name, display name, description, and other parameters for that specific input.\nThese values determine the component settings, such as display names and tooltips in the visual editor.\n\n```python\n    inputs = [\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum length of each chunk.\",\n            value=1000,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"The amount of overlap between chunks.\",\n            value=200,\n        ),\n        DataInput(\n            name=\"data_input\",\n            display_name=\"Input\",\n            info=\"The texts to split.\",\n            input_types=[\"Document\", \"Data\"],\n        ),\n        MessageTextInput(\n            name=\"separators\",\n            display_name=\"Separators\",\n            info='The characters to split on.\\nIf left empty defaults to [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"].',\n            is_list=True,\n        ),\n    ]\n```\n\nAdditionally, components have methods or functions that handle their functionality.\nFor example, the `RecursiveCharacterTextSplitter` has two methods:\n\n```python\n    def get_data_input(self) -> Any:\n        return self.data_input\n\n    def build_text_splitter(self) -> TextSplitter:\n        if not self.separators:\n            separators: list[str] | None = None\n        else:\n            # check if the separators list has escaped characters\n            # if there are escaped characters, unescape them\n            separators = [unescape_string(x) for x in self.separators]\n\n        return RecursiveCharacterTextSplitter(\n            separators=separators,\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n```\n\nThe `get_data_input` method retrieves the text to be split from the component's input, which makes the data available to the class.\nThe `build_text_splitter` method creates a `RecursiveCharacterTextSplitter` object by calling its parent class's `build` method. Then, the text is split with the created splitter and passed to the next component.\n\n## Component versions\n\nComponent versions and states are stored in an internal Langflow database. When you add a component to a flow, you create a detached copy of the component based on the information in the Langflow database.\nThese copies are detached from the primary Langflow database, and they don't synchronize with any updates that can occur when you upgrade your Langflow version.\n\nIn other words, an individual instance of a component retains the version number and state from the moment you add it to a specific flow. For example, if a component is at version 1.0 when you add it to a flow, it remains at version 1.0 _in that flow_ unless you update it.\n\n### Update component versions\n\nWhen editing a flow in the workspace, Langflow notifies you if a component's workspace version is behind the database version so you can update the component's workspace version:\n\n* **Update ready**: This notification means the component update contains no breaking changes.\n* **Update available**: This notification means the component update might contain breaking changes.\n\n    Breaking changes modify component inputs and outputs, causing the components to be disconnected and break the flow. After updating the component, you might need to edit the component settings or reconnect component ports.\n\nThere are two ways to update components:\n\n* Click **Update** to update a single component. This is recommended for updates without breaking changes.\n* Click **Review** to view all available updates and create a snapshot before updating. This is recommended for updates with breaking changes.\n\n    To save a snapshot of your flow before updating the components, enable **Create backup flow before updating**. Backup flows are stored in the same project folder as the original flow with the suffix `(backup)`.\n\n    To update specific components, select the components you want to update, and then click **Update Components**.\n\nComponents are updated to the latest available version, based on the version of Langflow you are running.\n\n## Group components\n\nMultiple components can be grouped into a single component for reuse. This is useful for organizing large flows by combining related components together, such as a RAG **Agent** component and it's associated tools or vector store components.\n\n1. Hold <kbd>Shift</kbd>, and then click and drag to highlight all components you want to merge. Components must be completely within the selection area to be merged.\n\n    Alternatively, to select components for merging one by one, hold <kbd>Ctrl</kbd> on Windows or <kbd>Cmd</kbd> on Mac, and then click each component to add them to the group.\n\n2. Release the mouse and keyboard, and then click **Group** to merge the components into a single, group component.\n\nGrouped components are configured and managed as a single component, including the component name, code, and settings.\n\nTo ungroup the components, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Ungroup**.\n\nIf you want to reuse this grouping in other flows, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Save** to save the component to the  **Core components** menu as a custom component.\n\n## Legacy components",
      "summary": "Components are the building blocks of your flows.\nLike classes in an application, each component is designed for a specific use case or integration.",
      "keywords": [
        "add",
        "agent",
        "available",
        "backup",
        "before",
        "bundles",
        "character",
        "code",
        "colors",
        "component",
        "components",
        "configure",
        "controls",
        "convert",
        "core",
        "create",
        "data",
        "delete",
        "duplicate",
        "dynamic"
      ]
    },
    "concepts-components": {
      "title": "Components overview",
      "slug": "/concepts-components",
      "category": "components",
      "content": "Components are the building blocks of your flows.\nLike classes in an application, each component is designed for a specific use case or integration.\n\n**Note:** Langflow provides keyboard shortcuts for the workspace.\n\nIn the Langflow header, click your profile icon, select **Settings**, and then click **Shortcuts** to view the available shortcuts.\n\n## Add a component to a flow {#component-menus}\n\nTo add a component to a flow, drag the component from the  **Core components** or  **Bundles** menu into the [workspace](/concepts-overview#workspace).\n\nComponents are grouped by type or provider, and some components are hidden by default:\n\n*  **Core components**: Langflow's base components are grouped by purpose, such as **Inputs and Outputs** or **Data**.\nThese components either provide generic functionality, like loops and parsing, or they provide single components that support multiple third-party integrations.\n\n*  **Bundles**: Bundles contain one or more components that support specific third-party integrations, and they are grouped by service provider.\n\n* **Legacy**: These components are hidden by default.\nFor more information, see [Legacy components](#legacy-components).\n\n### Configure a component\n\nAfter adding a component to a flow, configure the component's parameters and connect it to the other components in your flows.\n\nEach component has inputs, outputs, parameters, and controls related to the component's purpose.\nBy default, components show only required and common options.\nTo access additional settings and controls, including meta settings, use the [component's header menu](#component-header-menus).\n\n### Component header menus\n\nTo access a component's header menu, click the component in your workspace.\n\n![Agent component](/img/agent-component.png)\n\nA few options are available directly on the header menu.\nFor example:\n\n- **Code**: Modify component settings by directly editing the component's Python code.\n- **Controls**: Adjust all component parameters, including optional settings that are hidden by default.\n- **Tool Mode**: Enable this option when combining a component with an **Agent** component.\n\nFor all other options, including **Delete** and **Duplicate** controls, click  **Show More**.\n\n### Rename a component\n\nTo modify a component's name or description, click the component in the workspace, and then click  **Edit**.\nComponent descriptions accept Markdown syntax.\n\n### Run a component\n\nTo run a single component, click  **Run component**.\nA **Last Run** value indicates that the component ran successfully.\n\nRunning a single component is different from running an entire flow. In a single component run, the `build_vertex` function is called, which builds and runs only the single component with direct inputs provided through the visual editor (the `inputs_dict` parameter). The `VertexBuildResult` data is passed to the `build_and_run` method that calls the component's `build` method and runs it. Unlike running an entire flow, running a single component doesn't automatically execute its upstream dependencies.\n\n### Inspect component output and logs\n\nTo view the output and logs for a single component, click  **Inspect**.\n\n### Freeze a component\n\n**Note:** Freezing a component also freezes all components upstream of the selected component.\n\nUse the freeze option if you expect consistent output from a component _and all upstream components_, and you only need to run those components once.\n\nFreezing a component prevents that component and all upstream components from re-running, and it preserves the last output state for those components.\nAny future flow runs use the preserved output.\n\nTo freeze a component, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Freeze**.\n\n## Component ports\n\nAround the border of each component, there are circular port icons like .\nThese indicate a component _connection point_ or _port_.\n\nPorts either accept input or produce output of a specific data type.\nYou can infer the data type from the field the port is attached to or from the [port's color](#port-colors).\nFor example, the **System Message** field accepts [message data](/data-types#message), as illustrated by the blue port icon: .\n\n![Prompt Template component with multiple inputs](/img/prompt-component.png)\n\nWhen building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.\nFor information about the programmatic representation of each data type, see [Langflow data types](/data-types).\n\n**Note:** * In the workspace, hover over a port to see connection details for that port.\nClick a port to  **Search** for compatible components.\n\n* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.\n\n### Dynamic ports\n\nSome components have ports that are dynamically added or removed.\nFor example, the **Prompt Template** component accepts [inputs wrapped in curly braces](/components-prompts#define-variables-in-prompts), and new ports are opened when a value wrapped in curly braces is detected in the **Template** field.\n\n![Prompt Template component with multiple inputs](/img/prompt-component-with-multiple-inputs.png)\n\n### Output type selection\n\nAll components produce output that is either sent to another component in the flow or returned as the final flow result.\n\nSome components can produce multiple types of output:\n\n* If the component emits all types at once, the component has multiple output ports in the visual editor. In component code, this is represented by `group_outputs=True`\n\n* If the component emits only one type, you must select the output type by clicking the output label near the output port, and then selecting the desired output type. In component code, this is represented by `group_outputs=False` or omitting the `group_outputs` parameter.\n\nFor example, a language model component can output _either_ a **Model Response** or **Language Model**.\nThe **Model Response** output produces [`Message`](/data-types#message) data that can be passed to another component's `Message` port.\nThe **Language Model** output must be connected to a component with a **Language Model** input, such as the [**Structured Output** component](/components-processing#structured-output), that uses the attached LLM to power the receiving component's reasoning.\n\n![Output type selection in the Language Model component](/img/select-output.png)\n\n### Port colors\n\nComponent port colors indicate the data type ingested or emitted by the port.\nFor example, a **Message** port either accepts or emits `Message` data.\n\nThe following table lists the component data types and their corresponding port colors:\n\n| Data type | Port color | Port icon example |\n|-----------|--------------|----------|\n| Data | Red |  |\n| DataFrame | Pink |  |\n| Embeddings | Emerald |  |\n| LanguageModel | Fuchsia |  |\n| Memory | Orange |  |\n| Message | Indigo |  |\n| Tool | Cyan |  |\n| Unknown or multiple types | Gray |  |\n\n## Component code\n\nYou can edit components in the [workspace](/concepts-overview#workspace) and in code. When editing a flow, select a component, and then click  **Code** to see and edit the component's underlying Python code.\n\nAll components have underlying code that determines how you configure them and what actions they can perform.\nIn the context of creating and running flows, component code does the following:\n\n* Determines what configuration options to show in the visual editor.\n* Validates inputs based on the component's defined input types.\n* Processes data using the configured parameters, methods, and functions.\n* Passes results to the next component in the flow.\n\nAll components inherit from a base `Component` class that defines the component's interface and behavior.\nFor example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the [`LCTextSplitterComponent`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/base/textsplitters/model.py) class.\n\nEach component's code includes definitions for inputs and outputs, which are represented in the workspace as [component ports](#component-ports).\nFor example, the `RecursiveCharacterTextSplitter` has four inputs. Each input definition specifies the input type, such as `IntInput`, as well as the encoded name, display name, description, and other parameters for that specific input.\nThese values determine the component settings, such as display names and tooltips in the visual editor.\n\n```python\n    inputs = [\n        IntInput(\n            name=\"chunk_size\",\n            display_name=\"Chunk Size\",\n            info=\"The maximum length of each chunk.\",\n            value=1000,\n        ),\n        IntInput(\n            name=\"chunk_overlap\",\n            display_name=\"Chunk Overlap\",\n            info=\"The amount of overlap between chunks.\",\n            value=200,\n        ),\n        DataInput(\n            name=\"data_input\",\n            display_name=\"Input\",\n            info=\"The texts to split.\",\n            input_types=[\"Document\", \"Data\"],\n        ),\n        MessageTextInput(\n            name=\"separators\",\n            display_name=\"Separators\",\n            info='The characters to split on.\\nIf left empty defaults to [\"\\\\n\\\\n\", \"\\\\n\", \" \", \"\"].',\n            is_list=True,\n        ),\n    ]\n```\n\nAdditionally, components have methods or functions that handle their functionality.\nFor example, the `RecursiveCharacterTextSplitter` has two methods:\n\n```python\n    def get_data_input(self) -> Any:\n        return self.data_input\n\n    def build_text_splitter(self) -> TextSplitter:\n        if not self.separators:\n            separators: list[str] | None = None\n        else:\n            # check if the separators list has escaped characters\n            # if there are escaped characters, unescape them\n            separators = [unescape_string(x) for x in self.separators]\n\n        return RecursiveCharacterTextSplitter(\n            separators=separators,\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n```\n\nThe `get_data_input` method retrieves the text to be split from the component's input, which makes the data available to the class.\nThe `build_text_splitter` method creates a `RecursiveCharacterTextSplitter` object by calling its parent class's `build` method. Then, the text is split with the created splitter and passed to the next component.\n\n## Component versions\n\nComponent versions and states are stored in an internal Langflow database. When you add a component to a flow, you create a detached copy of the component based on the information in the Langflow database.\nThese copies are detached from the primary Langflow database, and they don't synchronize with any updates that can occur when you upgrade your Langflow version.\n\nIn other words, an individual instance of a component retains the version number and state from the moment you add it to a specific flow. For example, if a component is at version 1.0 when you add it to a flow, it remains at version 1.0 _in that flow_ unless you update it.\n\n### Update component versions\n\nWhen editing a flow in the workspace, Langflow notifies you if a component's workspace version is behind the database version so you can update the component's workspace version:\n\n* **Update ready**: This notification means the component update contains no breaking changes.\n* **Update available**: This notification means the component update might contain breaking changes.\n\n    Breaking changes modify component inputs and outputs, causing the components to be disconnected and break the flow. After updating the component, you might need to edit the component settings or reconnect component ports.\n\nThere are two ways to update components:\n\n* Click **Update** to update a single component. This is recommended for updates without breaking changes.\n* Click **Review** to view all available updates and create a snapshot before updating. This is recommended for updates with breaking changes.\n\n    To save a snapshot of your flow before updating the components, enable **Create backup flow before updating**. Backup flows are stored in the same project folder as the original flow with the suffix `(backup)`.\n\n    To update specific components, select the components you want to update, and then click **Update Components**.\n\nComponents are updated to the latest available version, based on the version of Langflow you are running.\n\n## Group components\n\nMultiple components can be grouped into a single component for reuse. This is useful for organizing large flows by combining related components together, such as a RAG **Agent** component and it's associated tools or vector store components.\n\n1. Hold <kbd>Shift</kbd>, and then click and drag to highlight all components you want to merge. Components must be completely within the selection area to be merged.\n\n    Alternatively, to select components for merging one by one, hold <kbd>Ctrl</kbd> on Windows or <kbd>Cmd</kbd> on Mac, and then click each component to add them to the group.\n\n2. Release the mouse and keyboard, and then click **Group** to merge the components into a single, group component.\n\nGrouped components are configured and managed as a single component, including the component name, code, and settings.\n\nTo ungroup the components, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Ungroup**.\n\nIf you want to reuse this grouping in other flows, click the component in the workspace to expose the component's header menu, click  **Show More**, and then select **Save** to save the component to the  **Core components** menu as a custom component.\n\n## Legacy components",
      "summary": "Components are the building blocks of your flows.\nLike classes in an application, each component is designed for a specific use case or integration.",
      "keywords": [
        "add",
        "agent",
        "available",
        "backup",
        "before",
        "bundles",
        "character",
        "code",
        "colors",
        "component",
        "components",
        "configure",
        "controls",
        "convert",
        "core",
        "create",
        "data",
        "delete",
        "duplicate",
        "dynamic"
      ]
    },
    "/contributing-bundles": {
      "title": "Contribute bundles",
      "slug": "/contributing-bundles",
      "category": "contributing",
      "content": "[**Bundles**](/components-bundle-components) are groups of components that are related to a specific service provider.\nIf you want to contribute your custom components back to the Langflow project, you must put them into a bundle.\n\nFollow these steps to add components to  **Bundles** in the Langflow visual editor.\nThis example adds a bundle named `DarthVader`.\n\n## Add the bundle to the backend folder\n\n1. Navigate to the backend directory in the Langflow repository and create a new folder for your bundle.\nThe path for your new component is `src > backend > base > langflow > components > darth_vader`.\nYou can view the [components folder](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components) in the Langflow repository.\n\n2. Within the newly created `darth_vader` folder, add the following files:\n\n* `darth_vader_component.py`  This file contains the backend logic for the new bundle. Create multiple `.py` files for multiple components.\n* `__init__.py`  This file initializes the bundle components. You can use any existing `__init__.py` as an example to see how it should be structured.\n\nFor an example of adding multiple components in a bundle, see the [Notion](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components/Notion) bundle.\n\n## Add the bundle to the frontend folder\n\n1. Navigate to the frontend directory in the Langflow repository to add your bundle's icon.\nThe path for your new component icon is `src > frontend > src > icons > DarthVader`\nYou can view the [icons folder](https://github.com/langflow-ai/langflow/tree/main/src/frontend/src/icons) in the Langflow repository.\nTo add your icon, create **three** files inside the `icons/darth_vader` folder.\n\n2. In the `icons/darth_vader` folder, add the raw SVG file of your icon, such as `darth_vader-icon.svg`.\n\n    \n**Note:** To convert the SVG file to JSX format, you can use an online tool like SVG to JSX.\n    It's highly recommended to use the original, lighter version of the SVG.\n    \n\n3. In the `icons/darth_vader` folder, add the icon as a React component in JSX format, such as `DarthVaderIcon.jsx`.\n\n4. Update the JSX file to include the correct component name and structure.\nEnsure you include the `{...props}` spread operator in your JSX file.\nFor example, here is `DarthVaderIcon.jsx`:\n\n    ```javascript\n    const DarthVaderIcon = (props) => (\n      <svg\n        xmlns=\"http://www.w3.org/2000/svg\"\n        width={24}\n        height={24}\n        viewBox=\"0 0 32 32\"\n        fill=\"none\"\n        style={{ backgroundColor: \"#9100ff\", borderRadius: \"6px\" }}\n        {...props}\n      >\n        <g transform=\"translate(7, 7)\">\n          <path\n            fillRule=\"evenodd\"\n            clipRule=\"evenodd\"\n            d=\"M6.27406 0.685082C8.46664 -0.228361 10.9302 -0.228361 13.1229 0.685082C14.6773 1.33267 16.0054 2.40178 16.9702 3.75502C17.6126 4.65574 17.0835 5.84489 16.045 6.21613L13.5108 7.12189C12.9962 7.30585 12.4289 7.26812 11.9429 7.01756C11.8253 6.95701 11.7298 6.86089 11.6696 6.74266L10.2591 3.97469C10.0249 3.51519 9.37195 3.51519 9.13783 3.97469L7.72731 6.74274C7.66714 6.86089 7.57155 6.95701 7.454 7.01756L4.70187 8.43618C4.24501 8.67169 4.24501 9.3284 4.70187 9.56391L7.454 10.9825C7.57155 11.0431 7.66714 11.1392 7.72731 11.2574L9.13783 14.0254C9.37195 14.4849 10.0249 14.4849 10.2591 14.0254L11.6696 11.2574C11.7298 11.1392 11.8253 11.0431 11.9428 10.9825C12.429 10.7319 12.9965 10.6942 13.5112 10.8781L16.045 11.7838C17.0835 12.1551 17.6126 13.3442 16.9704 14.245C16.0054 15.5982\"\n            fill={props.isdark === \"true\" ? \"white\" : \"black\"}\n          />\n        </g>\n      </svg>\n    );\n\n    export default DarthVaderIcon;\n    ```\n\n5. In the `icons/darth_vader` folder, add the React component itself in TypeScript format, such as `index.tsx`.\nEnsure the icon's React component name corresponds to the JSX component you just created, such as `DarthVaderIcon`:\n\n    ```typescript\n    import { useDarkStore } from \"@/stores/darkStore\";\n    import React, { forwardRef } from \"react\";\n    import DarthVaderIconSVG from \"./DarthVaderIcon\";\n\n    export const DarthVaderIcon = forwardRef<\n      SVGSVGElement,\n      React.PropsWithChildren<{}>\n    >((props, ref) => {\n      const isdark = useDarkStore((state) => state.dark).toString();\n\n      return ;\n    });\n\n    export default DarthVaderIcon;\n    ```\n\n6. To link your new bundle to the frontend, open `/src/frontend/src/icons/lazyIconImports.ts`.\nYou can view the [lazyIconImports.ts](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/icons/lazyIconImports.ts) in the Langflow repository.\n\n7. Add the name of your icon, which should match the icon name you used in the `.tsx` file.\nFor example:\n\n    ```typescript\n      CrewAI: () =>\n        import(\"@/icons/CrewAI\").then((mod) => ({ default: mod.CrewAiIcon })),\n      DarthVader: () =>\n        import(\"@/icons/DarthVader\").then((mod) => ({ default: mod.DarthVaderIcon })),\n      DeepSeek: () =>\n        import(\"@/icons/DeepSeek\").then((mod) => ({ default: mod.DeepSeekIcon })),\n    ```\n\n8. To add your bundle to the  **Bundles** menu, edit the [`SIDEBAR_BUNDLES` array](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/utils/styleUtils.ts#L231) in `/src/frontend/src/utils/styleUtils.ts`.\n\n    Add an object to the array with the following keys:\n\n    * `display_name`: The text label shown in the Langflow visual editor\n    * `name`: The name of the folder you created within the `/src/backend/base/langflow/components` directory\n    * `icon`: The name of the bundle's icon that you defined in the previous steps\n\n    For example:\n\n    ```typescript\n    { display_name: \"AssemblyAI\", name: \"assemblyai\", icon: \"AssemblyAI\" },\n    { display_name: \"DarthVader\", name: \"darth_vader\", icon: \"DarthVader\" },\n    { display_name: \"DataStax\", name: \"astra_assistants\", icon: \"DarthVader\" },\n    ```\n\n## Update bundle components with icons\n\nIn your component bundle, associate the icon variable with your new bundle.\n\nIn your `darth_vader_component.py` file, in the component class, include the icon that you defined in the frontend.\nThe `icon` must point to the directory you created for your icons within the `src > frontend > src > icons` directory.\nFor example:\n```\nclass DarthVaderAPIComponent(LCToolComponent):\n    display_name: str = \"Darth Vader Tools\"\n    description: str = \"Use the force to run actions with your agent\"\n    name = \"DarthVaderAPI\"\n    icon = \"DarthVader\"\n```\n\n## Ensure the application builds your component bundle\n\n1. To rebuild the backend and frontend, run `make install_frontend && make build_frontend && make install_backend && uv run langflow run --port 7860`.\n\n2. Refresh the frontend application.\nYour new bundle called `DarthVader` is available in the  **Bundles** menu in the visual editor.",
      "summary": "[**Bundles**](/components-bundle-components) are groups of components that are related to a specific service provider.\nIf you want to contribute your custom components back to the Langflow project, you must put them into a bundle.",
      "keywords": [
        "add",
        "application",
        "backend",
        "builds",
        "bundle",
        "bundles",
        "component",
        "components",
        "contribute",
        "ensure",
        "folder",
        "frontend",
        "icons",
        "note",
        "three",
        "update"
      ]
    },
    "contributing-bundles": {
      "title": "Contribute bundles",
      "slug": "/contributing-bundles",
      "category": "contributing",
      "content": "[**Bundles**](/components-bundle-components) are groups of components that are related to a specific service provider.\nIf you want to contribute your custom components back to the Langflow project, you must put them into a bundle.\n\nFollow these steps to add components to  **Bundles** in the Langflow visual editor.\nThis example adds a bundle named `DarthVader`.\n\n## Add the bundle to the backend folder\n\n1. Navigate to the backend directory in the Langflow repository and create a new folder for your bundle.\nThe path for your new component is `src > backend > base > langflow > components > darth_vader`.\nYou can view the [components folder](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components) in the Langflow repository.\n\n2. Within the newly created `darth_vader` folder, add the following files:\n\n* `darth_vader_component.py`  This file contains the backend logic for the new bundle. Create multiple `.py` files for multiple components.\n* `__init__.py`  This file initializes the bundle components. You can use any existing `__init__.py` as an example to see how it should be structured.\n\nFor an example of adding multiple components in a bundle, see the [Notion](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components/Notion) bundle.\n\n## Add the bundle to the frontend folder\n\n1. Navigate to the frontend directory in the Langflow repository to add your bundle's icon.\nThe path for your new component icon is `src > frontend > src > icons > DarthVader`\nYou can view the [icons folder](https://github.com/langflow-ai/langflow/tree/main/src/frontend/src/icons) in the Langflow repository.\nTo add your icon, create **three** files inside the `icons/darth_vader` folder.\n\n2. In the `icons/darth_vader` folder, add the raw SVG file of your icon, such as `darth_vader-icon.svg`.\n\n    \n**Note:** To convert the SVG file to JSX format, you can use an online tool like SVG to JSX.\n    It's highly recommended to use the original, lighter version of the SVG.\n    \n\n3. In the `icons/darth_vader` folder, add the icon as a React component in JSX format, such as `DarthVaderIcon.jsx`.\n\n4. Update the JSX file to include the correct component name and structure.\nEnsure you include the `{...props}` spread operator in your JSX file.\nFor example, here is `DarthVaderIcon.jsx`:\n\n    ```javascript\n    const DarthVaderIcon = (props) => (\n      <svg\n        xmlns=\"http://www.w3.org/2000/svg\"\n        width={24}\n        height={24}\n        viewBox=\"0 0 32 32\"\n        fill=\"none\"\n        style={{ backgroundColor: \"#9100ff\", borderRadius: \"6px\" }}\n        {...props}\n      >\n        <g transform=\"translate(7, 7)\">\n          <path\n            fillRule=\"evenodd\"\n            clipRule=\"evenodd\"\n            d=\"M6.27406 0.685082C8.46664 -0.228361 10.9302 -0.228361 13.1229 0.685082C14.6773 1.33267 16.0054 2.40178 16.9702 3.75502C17.6126 4.65574 17.0835 5.84489 16.045 6.21613L13.5108 7.12189C12.9962 7.30585 12.4289 7.26812 11.9429 7.01756C11.8253 6.95701 11.7298 6.86089 11.6696 6.74266L10.2591 3.97469C10.0249 3.51519 9.37195 3.51519 9.13783 3.97469L7.72731 6.74274C7.66714 6.86089 7.57155 6.95701 7.454 7.01756L4.70187 8.43618C4.24501 8.67169 4.24501 9.3284 4.70187 9.56391L7.454 10.9825C7.57155 11.0431 7.66714 11.1392 7.72731 11.2574L9.13783 14.0254C9.37195 14.4849 10.0249 14.4849 10.2591 14.0254L11.6696 11.2574C11.7298 11.1392 11.8253 11.0431 11.9428 10.9825C12.429 10.7319 12.9965 10.6942 13.5112 10.8781L16.045 11.7838C17.0835 12.1551 17.6126 13.3442 16.9704 14.245C16.0054 15.5982\"\n            fill={props.isdark === \"true\" ? \"white\" : \"black\"}\n          />\n        </g>\n      </svg>\n    );\n\n    export default DarthVaderIcon;\n    ```\n\n5. In the `icons/darth_vader` folder, add the React component itself in TypeScript format, such as `index.tsx`.\nEnsure the icon's React component name corresponds to the JSX component you just created, such as `DarthVaderIcon`:\n\n    ```typescript\n    import { useDarkStore } from \"@/stores/darkStore\";\n    import React, { forwardRef } from \"react\";\n    import DarthVaderIconSVG from \"./DarthVaderIcon\";\n\n    export const DarthVaderIcon = forwardRef<\n      SVGSVGElement,\n      React.PropsWithChildren<{}>\n    >((props, ref) => {\n      const isdark = useDarkStore((state) => state.dark).toString();\n\n      return ;\n    });\n\n    export default DarthVaderIcon;\n    ```\n\n6. To link your new bundle to the frontend, open `/src/frontend/src/icons/lazyIconImports.ts`.\nYou can view the [lazyIconImports.ts](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/icons/lazyIconImports.ts) in the Langflow repository.\n\n7. Add the name of your icon, which should match the icon name you used in the `.tsx` file.\nFor example:\n\n    ```typescript\n      CrewAI: () =>\n        import(\"@/icons/CrewAI\").then((mod) => ({ default: mod.CrewAiIcon })),\n      DarthVader: () =>\n        import(\"@/icons/DarthVader\").then((mod) => ({ default: mod.DarthVaderIcon })),\n      DeepSeek: () =>\n        import(\"@/icons/DeepSeek\").then((mod) => ({ default: mod.DeepSeekIcon })),\n    ```\n\n8. To add your bundle to the  **Bundles** menu, edit the [`SIDEBAR_BUNDLES` array](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/utils/styleUtils.ts#L231) in `/src/frontend/src/utils/styleUtils.ts`.\n\n    Add an object to the array with the following keys:\n\n    * `display_name`: The text label shown in the Langflow visual editor\n    * `name`: The name of the folder you created within the `/src/backend/base/langflow/components` directory\n    * `icon`: The name of the bundle's icon that you defined in the previous steps\n\n    For example:\n\n    ```typescript\n    { display_name: \"AssemblyAI\", name: \"assemblyai\", icon: \"AssemblyAI\" },\n    { display_name: \"DarthVader\", name: \"darth_vader\", icon: \"DarthVader\" },\n    { display_name: \"DataStax\", name: \"astra_assistants\", icon: \"DarthVader\" },\n    ```\n\n## Update bundle components with icons\n\nIn your component bundle, associate the icon variable with your new bundle.\n\nIn your `darth_vader_component.py` file, in the component class, include the icon that you defined in the frontend.\nThe `icon` must point to the directory you created for your icons within the `src > frontend > src > icons` directory.\nFor example:\n```\nclass DarthVaderAPIComponent(LCToolComponent):\n    display_name: str = \"Darth Vader Tools\"\n    description: str = \"Use the force to run actions with your agent\"\n    name = \"DarthVaderAPI\"\n    icon = \"DarthVader\"\n```\n\n## Ensure the application builds your component bundle\n\n1. To rebuild the backend and frontend, run `make install_frontend && make build_frontend && make install_backend && uv run langflow run --port 7860`.\n\n2. Refresh the frontend application.\nYour new bundle called `DarthVader` is available in the  **Bundles** menu in the visual editor.",
      "summary": "[**Bundles**](/components-bundle-components) are groups of components that are related to a specific service provider.\nIf you want to contribute your custom components back to the Langflow project, you must put them into a bundle.",
      "keywords": [
        "add",
        "application",
        "backend",
        "builds",
        "bundle",
        "bundles",
        "component",
        "components",
        "contribute",
        "ensure",
        "folder",
        "frontend",
        "icons",
        "note",
        "three",
        "update"
      ]
    },
    "/contributing-community": {
      "title": "Join the Langflow community",
      "slug": "/contributing-community",
      "category": "contributing",
      "content": "There are several ways you can interact with the Langflow community and learn more about the Langflow codebase.\n\n## Join the Langflow Discord server\n\nJoin the [Langflow Discord Server](https://discord.gg/EqksyE2EX9) to ask questions and showcase your projects.\n\n## Follow Langflow on X\n\nFollow [@langflow_ai](https://twitter.com/langflow_ai) on X to get the latest news about Langflow.\n\n## Star Langflow on GitHub\n\nIf you like Langflow, you can star the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\nStars help other users find Langflow more easily, and quickly understand that other users have found it useful.\n\nBecause Langflow is open-source, the more visible the repository is, the more likely the codebase is to attract [contributors](/contributing-how-to-contribute).\n\n## Watch the GitHub repository\n\nYou can watch the [Langflow GitHub repository](https://github.com/langflow-ai/langflow) to get notified about new releases and other repository activity.\n\nTo get release notifications only, select **Releases only**.\n\nIf you select **Watching**, you will receive notifications about new releases as well as issues, discussions, and pull requests, if you are interested in that activity.\nFor information about customizing repository notifications, see the [GitHub documentation on repository subscriptions](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/managing-subscriptions-for-activity-on-github/viewing-your-subscriptions).\n\n## Request enhancements and get help through GitHub\n\nYou can also submit feature requests and get help with Langflow through the GitHub repository.\nFor more information, see [Get help and request enhancements](/contributing-github-issues).",
      "summary": "There are several ways you can interact with the Langflow community and learn more about the Langflow codebase.",
      "keywords": [
        "community",
        "discord",
        "enhancements",
        "follow",
        "get",
        "github",
        "help",
        "join",
        "langflow",
        "only",
        "releases",
        "repository",
        "request",
        "server",
        "star",
        "through",
        "watch",
        "watching"
      ]
    },
    "contributing-community": {
      "title": "Join the Langflow community",
      "slug": "/contributing-community",
      "category": "contributing",
      "content": "There are several ways you can interact with the Langflow community and learn more about the Langflow codebase.\n\n## Join the Langflow Discord server\n\nJoin the [Langflow Discord Server](https://discord.gg/EqksyE2EX9) to ask questions and showcase your projects.\n\n## Follow Langflow on X\n\nFollow [@langflow_ai](https://twitter.com/langflow_ai) on X to get the latest news about Langflow.\n\n## Star Langflow on GitHub\n\nIf you like Langflow, you can star the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\nStars help other users find Langflow more easily, and quickly understand that other users have found it useful.\n\nBecause Langflow is open-source, the more visible the repository is, the more likely the codebase is to attract [contributors](/contributing-how-to-contribute).\n\n## Watch the GitHub repository\n\nYou can watch the [Langflow GitHub repository](https://github.com/langflow-ai/langflow) to get notified about new releases and other repository activity.\n\nTo get release notifications only, select **Releases only**.\n\nIf you select **Watching**, you will receive notifications about new releases as well as issues, discussions, and pull requests, if you are interested in that activity.\nFor information about customizing repository notifications, see the [GitHub documentation on repository subscriptions](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/managing-subscriptions-for-activity-on-github/viewing-your-subscriptions).\n\n## Request enhancements and get help through GitHub\n\nYou can also submit feature requests and get help with Langflow through the GitHub repository.\nFor more information, see [Get help and request enhancements](/contributing-github-issues).",
      "summary": "There are several ways you can interact with the Langflow community and learn more about the Langflow codebase.",
      "keywords": [
        "community",
        "discord",
        "enhancements",
        "follow",
        "get",
        "github",
        "help",
        "join",
        "langflow",
        "only",
        "releases",
        "repository",
        "request",
        "server",
        "star",
        "through",
        "watch",
        "watching"
      ]
    },
    "/contributing-component-tests": {
      "title": "Contribute component tests",
      "slug": "/contributing-component-tests",
      "category": "contributing",
      "content": "This guide outlines how to structure and implement tests for application components to ensure consistency and adequate coverage.\n\n## File naming\n\n* The test file should follow the same directory structure as the component being tested, but should be placed in the corresponding unit tests folder.\n\n    For example, if the file path for the component is `src/backend/base/langflow/components/prompts/`, then the test file should be located at `src/backend/tests/unit/components/prompts`.\n\n* The test file name should use snake case and follow the pattern `test_<file_name>.py`.\n\n    For example, if the file to be tested is `PromptComponent.py`, then the test file should be named `test_prompt_component.py`.\n\n## File structure\n\n* Each test file should group tests into classes by component. There should be no standalone test functions in the file only test methods within classes.\n* Class names should follow the pattern `Test<ClassName>`.\nFor example, if the component being tested is `PromptComponent`, then the test class should be named `TestPromptComponent`.\n\n## Imports, inheritance, and mandatory methods\n\nTo standardize component tests, base test classes have been created and should be imported and inherited by all component test classes. These base classes are located in the file `src/backend/tests/unit/base.py`.\n\nTo import the base test classes:\n\n```python\nfrom tests.base import ComponentTestBaseWithClient\nfrom tests.base import ComponentTestBaseWithoutClient\n```\n\nThese base classes enforce mandatory methods that the component test classes must implement. The base classes ensure that components built in previous versions continues to work in the current version. By inheriting from one of these base classes, the developer must define the following methods decorated with `@pytest.fixture`:\n\n* `component_class:` Returns the class of the component to be tested. For example:\n\n    ```python\n    @pytest.fixture\n    def component_class(self):\n        return PromptComponent\n    ```\n\n* `default_kwargs:` Returns a dictionary with the default arguments required to instantiate the component. For example:\n\n    ```python\n    @pytest.fixture\n    def default_kwargs(self):\n        return {\"template\": \"Hello {name}!\", \"name\": \"John\", \"_session_id\": \"123\"}\n    ```\n\n* `file_names_mapping:` Returns a list of dictionaries representing the relationship between `version`, `module`, and `file_name` that the tested component has had over time. This can be left empty if it is an unreleased component. For example:\n\n    ```python\n    @pytest.fixture\n    def file_names_mapping(self):\n        return [\n            {\"version\": \"1.0.15\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.16\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.17\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.18\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.19\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n        ]\n    ```\n\n## Testing component functionalities\n\nOnce the basic structure of the test file is defined, implement test methods for the component's functionalities. The following guidelines must be followed:\n\n1. Test method names should be descriptive, use snake case, and follow the pattern `test_<case_name>`.\n2. Each test should follow the **Arrange, Act, Assert** pattern:\n    1. **Arrange**: Prepare the data.\n    2. **Act**: Execute the component.\n    3. **Assert**: Verify the result.\n\n### Example\n\n1. **Arrange**: Prepare the data.\n\n    It is recommended, but not mandatory, that you use the fixtures defined in the basic structure.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n    ```\n\n2. **Act**: Execute the component.\n\n    Call the `.to_frontend_node()` method of the component prepared during the **Arrange** step.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n\n        frontend_node = component.to_frontend_node()\n    ```\n\n3. **Assert**: Verify the result.\n\n    After executing the `.to_frontend_node()` method, the resulting data is available for verification in the dictionary `frontend_node[\"data\"][\"node\"]`. Assertions should be clear and cover the expected outcomes.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n\n        frontend_node = component.to_frontend_node()\n\n        node_data = frontend_node[\"data\"][\"node\"]\n        assert node_data[\"template\"][\"template\"][\"value\"] == \"Hello {name}!\"\n        assert \"name\" in node_data[\"custom_fields\"][\"template\"]\n        assert \"name\" in node_data[\"template\"]\n        assert node_data[\"template\"][\"name\"][\"value\"] == \"John\"\n    ```",
      "summary": "This guide outlines how to structure and implement tests for application components to ensure consistency and adequate coverage.",
      "keywords": [
        "act",
        "arrange",
        "assert",
        "call",
        "component",
        "contribute",
        "def",
        "during",
        "execute",
        "file",
        "functionalities",
        "imports",
        "inheritance",
        "mandatory",
        "method",
        "methods",
        "naming",
        "prepared",
        "python",
        "self"
      ]
    },
    "contributing-component-tests": {
      "title": "Contribute component tests",
      "slug": "/contributing-component-tests",
      "category": "contributing",
      "content": "This guide outlines how to structure and implement tests for application components to ensure consistency and adequate coverage.\n\n## File naming\n\n* The test file should follow the same directory structure as the component being tested, but should be placed in the corresponding unit tests folder.\n\n    For example, if the file path for the component is `src/backend/base/langflow/components/prompts/`, then the test file should be located at `src/backend/tests/unit/components/prompts`.\n\n* The test file name should use snake case and follow the pattern `test_<file_name>.py`.\n\n    For example, if the file to be tested is `PromptComponent.py`, then the test file should be named `test_prompt_component.py`.\n\n## File structure\n\n* Each test file should group tests into classes by component. There should be no standalone test functions in the file only test methods within classes.\n* Class names should follow the pattern `Test<ClassName>`.\nFor example, if the component being tested is `PromptComponent`, then the test class should be named `TestPromptComponent`.\n\n## Imports, inheritance, and mandatory methods\n\nTo standardize component tests, base test classes have been created and should be imported and inherited by all component test classes. These base classes are located in the file `src/backend/tests/unit/base.py`.\n\nTo import the base test classes:\n\n```python\nfrom tests.base import ComponentTestBaseWithClient\nfrom tests.base import ComponentTestBaseWithoutClient\n```\n\nThese base classes enforce mandatory methods that the component test classes must implement. The base classes ensure that components built in previous versions continues to work in the current version. By inheriting from one of these base classes, the developer must define the following methods decorated with `@pytest.fixture`:\n\n* `component_class:` Returns the class of the component to be tested. For example:\n\n    ```python\n    @pytest.fixture\n    def component_class(self):\n        return PromptComponent\n    ```\n\n* `default_kwargs:` Returns a dictionary with the default arguments required to instantiate the component. For example:\n\n    ```python\n    @pytest.fixture\n    def default_kwargs(self):\n        return {\"template\": \"Hello {name}!\", \"name\": \"John\", \"_session_id\": \"123\"}\n    ```\n\n* `file_names_mapping:` Returns a list of dictionaries representing the relationship between `version`, `module`, and `file_name` that the tested component has had over time. This can be left empty if it is an unreleased component. For example:\n\n    ```python\n    @pytest.fixture\n    def file_names_mapping(self):\n        return [\n            {\"version\": \"1.0.15\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.16\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.17\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.18\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n            {\"version\": \"1.0.19\", \"module\": \"prompts\", \"file_name\": \"Prompt\"},\n        ]\n    ```\n\n## Testing component functionalities\n\nOnce the basic structure of the test file is defined, implement test methods for the component's functionalities. The following guidelines must be followed:\n\n1. Test method names should be descriptive, use snake case, and follow the pattern `test_<case_name>`.\n2. Each test should follow the **Arrange, Act, Assert** pattern:\n    1. **Arrange**: Prepare the data.\n    2. **Act**: Execute the component.\n    3. **Assert**: Verify the result.\n\n### Example\n\n1. **Arrange**: Prepare the data.\n\n    It is recommended, but not mandatory, that you use the fixtures defined in the basic structure.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n    ```\n\n2. **Act**: Execute the component.\n\n    Call the `.to_frontend_node()` method of the component prepared during the **Arrange** step.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n\n        frontend_node = component.to_frontend_node()\n    ```\n\n3. **Assert**: Verify the result.\n\n    After executing the `.to_frontend_node()` method, the resulting data is available for verification in the dictionary `frontend_node[\"data\"][\"node\"]`. Assertions should be clear and cover the expected outcomes.\n\n    ```python\n    def test_post_code_processing(self, component_class, default_kwargs):\n        component = component_class(**default_kwargs)\n\n        frontend_node = component.to_frontend_node()\n\n        node_data = frontend_node[\"data\"][\"node\"]\n        assert node_data[\"template\"][\"template\"][\"value\"] == \"Hello {name}!\"\n        assert \"name\" in node_data[\"custom_fields\"][\"template\"]\n        assert \"name\" in node_data[\"template\"]\n        assert node_data[\"template\"][\"name\"][\"value\"] == \"John\"\n    ```",
      "summary": "This guide outlines how to structure and implement tests for application components to ensure consistency and adequate coverage.",
      "keywords": [
        "act",
        "arrange",
        "assert",
        "call",
        "component",
        "contribute",
        "def",
        "during",
        "execute",
        "file",
        "functionalities",
        "imports",
        "inheritance",
        "mandatory",
        "method",
        "methods",
        "naming",
        "prepared",
        "python",
        "self"
      ]
    },
    "/contributing-components": {
      "title": "Contribute components",
      "slug": "/contributing-components",
      "category": "contributing",
      "content": "New components are added as objects of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class.\n\nDependencies are added to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L148) file.\n\n## Contribute an example component to Langflow\n\nAnyone can contribute an example component. For example, to create a new data component called **DataFrame processor**, follow these steps to contribute it to Langflow.\n\n1. Create a Python file called `dataframe_processor.py`.\n2. Write your processor as an object of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class. You'll create a new class, `DataFrameProcessor`, that will inherit from `Component` and override the base class's methods.\n\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n```\n\n3. Define class attributes to provide information about your custom component:\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n\n    display_name: str = \"DataFrame Processor\"\n    description: str = \"Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation.\"\n    documentation: str = \"https://docs.langflow.org/components-dataframe-processor\"\n    icon: str = \"DataframeIcon\"\n    priority: int = 100\n    name: str = \"dataframe_processor\"\n```\n\n   * `display_name`: A user-friendly name shown in the visual editor.\n   * `description`: A brief description of what your component does.\n   * `documentation`: A link to detailed documentation.\n   * `icon`: An emoji or icon identifier for visual representation.\n    For more information, see [Contributing bundles](/contributing-bundles#add-the-bundle-to-the-frontend-folder).\n   * `priority`: An optional integer to control display order. Lower numbers appear first.\n   * `name`: An optional internal identifier that defaults to class name.\n\n4. Define the component's interface by specifying its inputs, outputs, and the method that will process them. The method name must match the `method` field in your outputs list, as this is how Langflow knows which method to call to generate each output.\nThis example creates a minimal custom component skeleton.\nFor more information on creating your custom component, see [Create custom Python components](/components-custom-components).\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n\n    display_name: str = \"DataFrame Processor\"\n    description: str = \"Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation.\"\n    documentation: str = \"https://docs.langflow.org/components-dataframe-processor\"\n    icon: str = \"DataframeIcon\"\n    priority: int = 100\n    name: str = \"dataframe_processor\"\n\n    # input and output lists\n    inputs = []\n    outputs = []\n\n    # method\n    def some_output_method(self):\n        return ...\n```\n\n5. Save the `dataframe_processor.py` to the `src > backend > base > langflow > components` directory.\nThis example adds a data component, so add it to the `/data` directory.\n\n6. Add the component dependency to `src > backend > base > langflow > components > data > __init__.py` as `from .DataFrameProcessor import DataFrameProcessor`.\nYou can view the [/data/__init__.py](https://github.com/langflow-ai/langflow/blob/dev/src/backend/base/langflow/components/data/__init__.py) in the Langflow repository.\n\n7. Add any new dependencies to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L20) file.\n\n8. Submit documentation for your component. For this example component, you would submit documentation to the [Data components page](https://github.com/langflow-ai/langflow/blob/main/docs/docs/Components/components-data.mdx).\n\n9. Submit your changes as a pull request. The Langflow team will review, suggest changes, and add your component to Langflow.\n\n## Best practices for modifying components\n\nWhen creating or updating components, follow these best practices to maintain backward compatibility and ensure a smooth experience for users.\n\n### Don't rename the class or `name` attribute\n\nChanging the class name or the `name` attribute breaks the component for all existing users. This happens because the frontend tests the `type` attribute, which is set to the class' name or the `name` attribute. If these names change, the component effectively becomes a new component, and the old component disappears.\n\nInstead, do the following:\n* Change only the display name if the old name is unclear.\n* Change only the display name if functionality changes but remains related.\n* If a new internal name is necessary, mark the old component as `legacy=true` and create a new component.\n\nFor example:\n```python\nclass MyCustomComponent(BaseComponent):\n    name = \"my_custom_component_internal\"\n    legacy = True\n```\n\n### Don't remove fields and outputs\n\nRemoving fields or outputs can cause edges to disconnect and change the behavior of components.\n\nInstead, mark fields as `deprecated` and keep them in the same location. If removal is absolutely necessary, you must define and document a migration plan. Always clearly communicate any changes in the field's information to users.\n\n### Maintain outdated components as legacy\n\nWhen updating components, create them as completely separate entities while maintaining the old component as a legacy version. Always ensure backward compatibility and never remove methods and attributes from base classes, such as `LCModelComponent`.\n\n### Favor asynchronous methods\n\nAlways favor asynchronous methods and functions in your components. When interacting with files, use `aiofile` and `anyio.Path` for better performance and compatibility.\n\n### Include tests with your component\n\nInclude tests for your changes using `ComponentTestBase` classes. For more information, see [Contribute component tests](/contributing-component-tests).\n\n### Documentation\n\nWhen documenting changes in pull requests, clearly explain *what* changed, such as display name updates or new fields, *why* it changed, such as improvements or bug fixes, and the *impact* on existing users.\n\nFor example:\n\n<details>\n<summary>Example PR</summary>\n\n```markdown\n# Pull request with changes to Notify component\n\nThis pull request updates the Notify component.\n\n## What changed\n- Added new `timeout` field to control how long the component waits for a response.\n- Renamed `message` field to `notification_text` for clarity.\n- Added support for async operations.\n- Deprecated the `retry_count` field in favor of `max_retries`.\n\n## Why it changed\n- `timeout` field addresses user requests for better control over wait times.\n- `message` to `notification_text` change makes the field's purpose clearer.\n- Async support improves performance in complex flows.\n- `retry_count` to `max_retries` aligns with common retry pattern terminology.\n\n## Impact on users\n- New `timeout` field is optional (defaults to 30 seconds).\n- Users will see a deprecation warning for `retry_count`.\n  - Migration: Replace `retry_count` with `max_retries` in existing flows.\n  - Both fields will work until version 2.0.\n- No action needed for async support - it's backward compatible.\n```\n\n</details>\n\n## Example pull request flow\n\n1. Create or update a component.\nMaintain the class name and `name` attribute if the purpose remains the same.\nOtherwise, create a new component and move the old component to `legacy`.\n2. Add tests.\nCreate tests using one of the `ComponentTestBase` classes.\nFor more information, see [Contribute component tests](/contributing-component-tests).\n3. Flag outdated fields and outputs as `deprecated` and keep them in the same location to ensure backward compatibility.\n4. Document your changes.\nInclude migration instructions if breaking changes occur.",
      "summary": "New components are added as objects of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class.",
      "keywords": [
        "asynchronous",
        "attribute",
        "best",
        "changed",
        "changes",
        "class",
        "component",
        "components",
        "contribute",
        "dataframe",
        "documentation",
        "don",
        "favor",
        "fields",
        "flow",
        "impact",
        "include",
        "langflow",
        "legacy",
        "maintain"
      ]
    },
    "contributing-components": {
      "title": "Contribute components",
      "slug": "/contributing-components",
      "category": "contributing",
      "content": "New components are added as objects of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class.\n\nDependencies are added to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L148) file.\n\n## Contribute an example component to Langflow\n\nAnyone can contribute an example component. For example, to create a new data component called **DataFrame processor**, follow these steps to contribute it to Langflow.\n\n1. Create a Python file called `dataframe_processor.py`.\n2. Write your processor as an object of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class. You'll create a new class, `DataFrameProcessor`, that will inherit from `Component` and override the base class's methods.\n\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n```\n\n3. Define class attributes to provide information about your custom component:\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n\n    display_name: str = \"DataFrame Processor\"\n    description: str = \"Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation.\"\n    documentation: str = \"https://docs.langflow.org/components-dataframe-processor\"\n    icon: str = \"DataframeIcon\"\n    priority: int = 100\n    name: str = \"dataframe_processor\"\n```\n\n   * `display_name`: A user-friendly name shown in the visual editor.\n   * `description`: A brief description of what your component does.\n   * `documentation`: A link to detailed documentation.\n   * `icon`: An emoji or icon identifier for visual representation.\n    For more information, see [Contributing bundles](/contributing-bundles#add-the-bundle-to-the-frontend-folder).\n   * `priority`: An optional integer to control display order. Lower numbers appear first.\n   * `name`: An optional internal identifier that defaults to class name.\n\n4. Define the component's interface by specifying its inputs, outputs, and the method that will process them. The method name must match the `method` field in your outputs list, as this is how Langflow knows which method to call to generate each output.\nThis example creates a minimal custom component skeleton.\nFor more information on creating your custom component, see [Create custom Python components](/components-custom-components).\n```python\nfrom typing import Any, Dict, Optional\n\nfrom langflow.custom import Component\n\nclass DataFrameProcessor(Component):\n    \"\"\"A component that processes pandas DataFrames with various operations.\"\"\"\n\n    display_name: str = \"DataFrame Processor\"\n    description: str = \"Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation.\"\n    documentation: str = \"https://docs.langflow.org/components-dataframe-processor\"\n    icon: str = \"DataframeIcon\"\n    priority: int = 100\n    name: str = \"dataframe_processor\"\n\n    # input and output lists\n    inputs = []\n    outputs = []\n\n    # method\n    def some_output_method(self):\n        return ...\n```\n\n5. Save the `dataframe_processor.py` to the `src > backend > base > langflow > components` directory.\nThis example adds a data component, so add it to the `/data` directory.\n\n6. Add the component dependency to `src > backend > base > langflow > components > data > __init__.py` as `from .DataFrameProcessor import DataFrameProcessor`.\nYou can view the [/data/__init__.py](https://github.com/langflow-ai/langflow/blob/dev/src/backend/base/langflow/components/data/__init__.py) in the Langflow repository.\n\n7. Add any new dependencies to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L20) file.\n\n8. Submit documentation for your component. For this example component, you would submit documentation to the [Data components page](https://github.com/langflow-ai/langflow/blob/main/docs/docs/Components/components-data.mdx).\n\n9. Submit your changes as a pull request. The Langflow team will review, suggest changes, and add your component to Langflow.\n\n## Best practices for modifying components\n\nWhen creating or updating components, follow these best practices to maintain backward compatibility and ensure a smooth experience for users.\n\n### Don't rename the class or `name` attribute\n\nChanging the class name or the `name` attribute breaks the component for all existing users. This happens because the frontend tests the `type` attribute, which is set to the class' name or the `name` attribute. If these names change, the component effectively becomes a new component, and the old component disappears.\n\nInstead, do the following:\n* Change only the display name if the old name is unclear.\n* Change only the display name if functionality changes but remains related.\n* If a new internal name is necessary, mark the old component as `legacy=true` and create a new component.\n\nFor example:\n```python\nclass MyCustomComponent(BaseComponent):\n    name = \"my_custom_component_internal\"\n    legacy = True\n```\n\n### Don't remove fields and outputs\n\nRemoving fields or outputs can cause edges to disconnect and change the behavior of components.\n\nInstead, mark fields as `deprecated` and keep them in the same location. If removal is absolutely necessary, you must define and document a migration plan. Always clearly communicate any changes in the field's information to users.\n\n### Maintain outdated components as legacy\n\nWhen updating components, create them as completely separate entities while maintaining the old component as a legacy version. Always ensure backward compatibility and never remove methods and attributes from base classes, such as `LCModelComponent`.\n\n### Favor asynchronous methods\n\nAlways favor asynchronous methods and functions in your components. When interacting with files, use `aiofile` and `anyio.Path` for better performance and compatibility.\n\n### Include tests with your component\n\nInclude tests for your changes using `ComponentTestBase` classes. For more information, see [Contribute component tests](/contributing-component-tests).\n\n### Documentation\n\nWhen documenting changes in pull requests, clearly explain *what* changed, such as display name updates or new fields, *why* it changed, such as improvements or bug fixes, and the *impact* on existing users.\n\nFor example:\n\n<details>\n<summary>Example PR</summary>\n\n```markdown\n# Pull request with changes to Notify component\n\nThis pull request updates the Notify component.\n\n## What changed\n- Added new `timeout` field to control how long the component waits for a response.\n- Renamed `message` field to `notification_text` for clarity.\n- Added support for async operations.\n- Deprecated the `retry_count` field in favor of `max_retries`.\n\n## Why it changed\n- `timeout` field addresses user requests for better control over wait times.\n- `message` to `notification_text` change makes the field's purpose clearer.\n- Async support improves performance in complex flows.\n- `retry_count` to `max_retries` aligns with common retry pattern terminology.\n\n## Impact on users\n- New `timeout` field is optional (defaults to 30 seconds).\n- Users will see a deprecation warning for `retry_count`.\n  - Migration: Replace `retry_count` with `max_retries` in existing flows.\n  - Both fields will work until version 2.0.\n- No action needed for async support - it's backward compatible.\n```\n\n</details>\n\n## Example pull request flow\n\n1. Create or update a component.\nMaintain the class name and `name` attribute if the purpose remains the same.\nOtherwise, create a new component and move the old component to `legacy`.\n2. Add tests.\nCreate tests using one of the `ComponentTestBase` classes.\nFor more information, see [Contribute component tests](/contributing-component-tests).\n3. Flag outdated fields and outputs as `deprecated` and keep them in the same location to ensure backward compatibility.\n4. Document your changes.\nInclude migration instructions if breaking changes occur.",
      "summary": "New components are added as objects of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class.",
      "keywords": [
        "asynchronous",
        "attribute",
        "best",
        "changed",
        "changes",
        "class",
        "component",
        "components",
        "contribute",
        "dataframe",
        "documentation",
        "don",
        "favor",
        "fields",
        "flow",
        "impact",
        "include",
        "langflow",
        "legacy",
        "maintain"
      ]
    },
    "/contributing-how-to-contribute": {
      "title": "Contribute to Langflow",
      "slug": "/contributing-how-to-contribute",
      "category": "contributing",
      "content": "This guide is intended to help you start contributing to Langflow.\nAs an open-source codebase in a rapidly developing field, Langflow welcomes contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\n\nTo contribute code or documentation to Langflow, follow the [pull request guide](#open-a-pull-request).\n\n## Langflow services overview\n\nThis overview will help you understand how to set up your development environment.\n\nLangflow consists of two main services:\n- Frontend: A React/TypeScript application that provides the user interface.\n- Backend: A Python/FastAPI service that handles API requests.\n\nDuring development, the frontend and backend services run separately on different ports:\n- Frontend development server: `http://localhost:3000` (with hot reload).\n- Backend API server: `http://localhost:7860`.\n\nWhen you install Langflow as an end user, the backend serves the frontend internally, making both services available on a single port (`7860` by default).\n\n## Install Langflow from source\n\nInstall Langflow from source by forking the repository and setting up your development environment.\n\n### Prerequisites\n\n* [uv](https://docs.astral.sh/uv/getting-started/installation/) version 0.4 or later\n* [Node.js](https://nodejs.org/en/download/package-manager)\n* [Make](https://www.gnu.org/software/make/#documentation) (Linux and macOS only)\n\n### Clone the Langflow repository\n\n1. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\n\n2. Add the new remote to your local repository on your local machine:\n```bash\ngit remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git\n```\nReplace the following:\n* `FORK_NAME`: A name for your fork of the repository\n* `GIT_USERNAME`: Your Git username\n\n### Run Langflow from source\n\nYou can run Langflow from source after cloning the repository, even if you aren't contributing to the codebase.\nThis builds the frontend and serves it through the backend on port `7860`.\n\nThe instructions below are for running Langflow from source. For development with hot reload, see [Set up development environment](#set-up-your-langflow-development-environment).\n\n<details>\n<summary>Run from source on macOS/Linux</summary>\n\nIn your terminal, navigate to the root of the Langflow directory, and then run `make run_cli`.\n\nThis command does the following:\n\n- Installs frontend and backend dependencies\n- Builds the frontend static files\n- Starts the application with default settings\n\nThe Langflow frontend is served at `http://localhost:7860`.\n</details>\n\n<details>\n<summary>Run from source with Windows CMD</summary>\n\nTo run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.\n\nDo one of the following:\n\n* To install and run Langflow with the included Windows Batch file, navigate to the `scripts/windows` directory, and then run the `build_and_run.bat` file.\n\n* To run Langflow from the Windows Command Line:\n\n    1. Build the frontend static files:\n\n        ```\n        cd src/frontend\n        npm install\n        npm run build\n        ```\n\n    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.\n\n    3. Start Langflow:\n\n        ```\n        uv run langflow run\n        ```\n\nThe Langflow frontend is served at `http://localhost:7860`.\n\n</details>\n\n<details>\n<summary>Run from source with PowerShell</summary>\n\nTo run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.\n\nDo one of the following:\n\n* To install and run Langflow with the included scripts, navigate to the `scripts/windows` directory, and then run the `build_and_run.ps1` file.\n\n* To run Langflow from a PowerShell terminal:\n\n    1. Build the frontend static files:\n\n        ```\n        cd src/frontend\n        npm install\n        npm run build\n        ```\n\n    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.\n\n    3. Start Langflow:\n\n        ```\n        uv run langflow run\n        ```\n\nThe Langflow frontend is served at `http://localhost:7860`.\n\n</details>\n\n### Set up your Langflow development environment\n\n**Note:** Run `make help` to display all available make commands.\n\nThis section is for contributors who want to develop and test code changes with hot reload enabled.\n\nIf you just want to run Langflow locally without making code changes, see [Run Langflow from source](#run-langflow-from-source).\n\n</Tabs>\n\n### Troubleshoot frontend build issues\n\nIf you encounter frontend build problems or are upgrading from an older version of Langflow, run `make run_clic` once.\n\n```bash\nmake run_clic\n```\n\nThis command cleans the build cache and rebuilds everything from scratch, which resolves most frontend-related issues when switching between versions.\n\n### Debug\n\nThe repo includes a `.vscode/launch.json` file for debugging the backend in VSCode, which is faster than debugging with Docker Compose.\n\nFor more information, see the [VSCode documentation](https://code.visualstudio.com/docs/debugtest/debugging#_start-a-debugging-session).\n\n### Additional contribution guides\n\n- [Contribute bundles](./contributing-bundles.mdx)\n- [Contribute components](./contributing-components.mdx)\n- [Contribute tests](./contributing-component-tests.mdx)\n- [Contribute templates](./contributing-templates.mdx)\n\n## Contribute documentation\n\nThe documentation is built using [Docusaurus](https://docusaurus.io/) and written in [Markdown](https://docusaurus.io/docs/markdown-features).\nFor style guidance, see the [Google Developer Documentation Style Guide](https://developers.google.com/style).\n\n1. Install [Node.js](https://nodejs.org/en/download/package-manager) and [Yarn](https://yarnpkg.com/)\n\n2. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\n\n3. Add the new remote to your local repository on your local machine:\n\n    ```bash\n    git remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git\n    ```\n\n    Replace the following:\n\n    * `FORK_NAME`: A name for your fork of the repository\n    * `GIT_USERNAME`: Your Git username\n\n4. From the root of your local Langflow fork, change to the `/docs` directory:\n\n    ```bash\n    cd docs\n    ```\n\n    If you're using a dev container, run the documentation build from outside the container on your host terminal, not from within the dev container workspace.\n    The documentation build may not work properly when run inside the dev container.\n\n5. Install dependencies and start a local Docusaurus static site with hot reload:\n\n    ```bash\n    yarn install\n    yarn start\n    ```\n\n    The documentation is served at `http://localhost:3000`.\n\n6. To edit and create content, work with the `.mdx` files in the `langflow/docs/docs` directory.\n\n    Create new files in `.mdx` format.\n\n    Navigation is defined in `langflow/docs/sidebars.js`.\n\n    Most pages use a `slug` for shorthand cross-referencing, rather than supplying the full or relative directory path.\n    For example, if a page has a `slug` of `/cool-page`, you can link to it with `[Cool page](/cool-page)` from any other `/docs` page.\n\n7. Recommended: After making some changes, run `yarn build` to build the site locally with more robust logging.\nThis can help you find broken links before creating a PR.\n\n## Open a pull request\n\nTo submit a pull request, do the following:\n\n1. Open a new GitHub pull request with your patch against the `main` branch.\n2. Ensure the PR title follows semantic commit conventions. For example, features are `feat: add new feature` and fixes are `fix: correct issue with X`.\n\nSome additional guidance on pull request titles:\n* Ensure the pull request description clearly describes the problem and solution. If the PR fixes an issue, include a link to the fixed issue in the PR description with `Fixes #1234`.\n* Pull request titles appear in Langflow's release notes, so they should explain what the PR does as explicitly as possible.\n* Pull requests should strive to fix one thing **only**, and should contain a good description of what is being fixed.\n\n3. A Langflow maintainer will review your pull request and may request changes, so ensure you pay attention to your PRs. Thanks for your contribution!\n\nFor more information, see the [Python Developer's Guide](https://devguide.python.org/getting-started/pull-request-lifecycle/index.html#making-good-commits).",
      "summary": "This guide is intended to help you start contributing to Langflow.\nAs an open-source codebase in a rapidly developing field, Langflow welcomes contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.",
      "keywords": [
        "additional",
        "build",
        "clone",
        "contribute",
        "contribution",
        "debug",
        "development",
        "documentation",
        "environment",
        "frontend",
        "guides",
        "install",
        "issues",
        "langflow",
        "note",
        "only",
        "open",
        "overview",
        "prerequisites",
        "pull"
      ]
    },
    "contributing-how-to-contribute": {
      "title": "Contribute to Langflow",
      "slug": "/contributing-how-to-contribute",
      "category": "contributing",
      "content": "This guide is intended to help you start contributing to Langflow.\nAs an open-source codebase in a rapidly developing field, Langflow welcomes contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\n\nTo contribute code or documentation to Langflow, follow the [pull request guide](#open-a-pull-request).\n\n## Langflow services overview\n\nThis overview will help you understand how to set up your development environment.\n\nLangflow consists of two main services:\n- Frontend: A React/TypeScript application that provides the user interface.\n- Backend: A Python/FastAPI service that handles API requests.\n\nDuring development, the frontend and backend services run separately on different ports:\n- Frontend development server: `http://localhost:3000` (with hot reload).\n- Backend API server: `http://localhost:7860`.\n\nWhen you install Langflow as an end user, the backend serves the frontend internally, making both services available on a single port (`7860` by default).\n\n## Install Langflow from source\n\nInstall Langflow from source by forking the repository and setting up your development environment.\n\n### Prerequisites\n\n* [uv](https://docs.astral.sh/uv/getting-started/installation/) version 0.4 or later\n* [Node.js](https://nodejs.org/en/download/package-manager)\n* [Make](https://www.gnu.org/software/make/#documentation) (Linux and macOS only)\n\n### Clone the Langflow repository\n\n1. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\n\n2. Add the new remote to your local repository on your local machine:\n```bash\ngit remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git\n```\nReplace the following:\n* `FORK_NAME`: A name for your fork of the repository\n* `GIT_USERNAME`: Your Git username\n\n### Run Langflow from source\n\nYou can run Langflow from source after cloning the repository, even if you aren't contributing to the codebase.\nThis builds the frontend and serves it through the backend on port `7860`.\n\nThe instructions below are for running Langflow from source. For development with hot reload, see [Set up development environment](#set-up-your-langflow-development-environment).\n\n<details>\n<summary>Run from source on macOS/Linux</summary>\n\nIn your terminal, navigate to the root of the Langflow directory, and then run `make run_cli`.\n\nThis command does the following:\n\n- Installs frontend and backend dependencies\n- Builds the frontend static files\n- Starts the application with default settings\n\nThe Langflow frontend is served at `http://localhost:7860`.\n</details>\n\n<details>\n<summary>Run from source with Windows CMD</summary>\n\nTo run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.\n\nDo one of the following:\n\n* To install and run Langflow with the included Windows Batch file, navigate to the `scripts/windows` directory, and then run the `build_and_run.bat` file.\n\n* To run Langflow from the Windows Command Line:\n\n    1. Build the frontend static files:\n\n        ```\n        cd src/frontend\n        npm install\n        npm run build\n        ```\n\n    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.\n\n    3. Start Langflow:\n\n        ```\n        uv run langflow run\n        ```\n\nThe Langflow frontend is served at `http://localhost:7860`.\n\n</details>\n\n<details>\n<summary>Run from source with PowerShell</summary>\n\nTo run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.\n\nDo one of the following:\n\n* To install and run Langflow with the included scripts, navigate to the `scripts/windows` directory, and then run the `build_and_run.ps1` file.\n\n* To run Langflow from a PowerShell terminal:\n\n    1. Build the frontend static files:\n\n        ```\n        cd src/frontend\n        npm install\n        npm run build\n        ```\n\n    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.\n\n    3. Start Langflow:\n\n        ```\n        uv run langflow run\n        ```\n\nThe Langflow frontend is served at `http://localhost:7860`.\n\n</details>\n\n### Set up your Langflow development environment\n\n**Note:** Run `make help` to display all available make commands.\n\nThis section is for contributors who want to develop and test code changes with hot reload enabled.\n\nIf you just want to run Langflow locally without making code changes, see [Run Langflow from source](#run-langflow-from-source).\n\n</Tabs>\n\n### Troubleshoot frontend build issues\n\nIf you encounter frontend build problems or are upgrading from an older version of Langflow, run `make run_clic` once.\n\n```bash\nmake run_clic\n```\n\nThis command cleans the build cache and rebuilds everything from scratch, which resolves most frontend-related issues when switching between versions.\n\n### Debug\n\nThe repo includes a `.vscode/launch.json` file for debugging the backend in VSCode, which is faster than debugging with Docker Compose.\n\nFor more information, see the [VSCode documentation](https://code.visualstudio.com/docs/debugtest/debugging#_start-a-debugging-session).\n\n### Additional contribution guides\n\n- [Contribute bundles](./contributing-bundles.mdx)\n- [Contribute components](./contributing-components.mdx)\n- [Contribute tests](./contributing-component-tests.mdx)\n- [Contribute templates](./contributing-templates.mdx)\n\n## Contribute documentation\n\nThe documentation is built using [Docusaurus](https://docusaurus.io/) and written in [Markdown](https://docusaurus.io/docs/markdown-features).\nFor style guidance, see the [Google Developer Documentation Style Guide](https://developers.google.com/style).\n\n1. Install [Node.js](https://nodejs.org/en/download/package-manager) and [Yarn](https://yarnpkg.com/)\n\n2. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).\n\n3. Add the new remote to your local repository on your local machine:\n\n    ```bash\n    git remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git\n    ```\n\n    Replace the following:\n\n    * `FORK_NAME`: A name for your fork of the repository\n    * `GIT_USERNAME`: Your Git username\n\n4. From the root of your local Langflow fork, change to the `/docs` directory:\n\n    ```bash\n    cd docs\n    ```\n\n    If you're using a dev container, run the documentation build from outside the container on your host terminal, not from within the dev container workspace.\n    The documentation build may not work properly when run inside the dev container.\n\n5. Install dependencies and start a local Docusaurus static site with hot reload:\n\n    ```bash\n    yarn install\n    yarn start\n    ```\n\n    The documentation is served at `http://localhost:3000`.\n\n6. To edit and create content, work with the `.mdx` files in the `langflow/docs/docs` directory.\n\n    Create new files in `.mdx` format.\n\n    Navigation is defined in `langflow/docs/sidebars.js`.\n\n    Most pages use a `slug` for shorthand cross-referencing, rather than supplying the full or relative directory path.\n    For example, if a page has a `slug` of `/cool-page`, you can link to it with `[Cool page](/cool-page)` from any other `/docs` page.\n\n7. Recommended: After making some changes, run `yarn build` to build the site locally with more robust logging.\nThis can help you find broken links before creating a PR.\n\n## Open a pull request\n\nTo submit a pull request, do the following:\n\n1. Open a new GitHub pull request with your patch against the `main` branch.\n2. Ensure the PR title follows semantic commit conventions. For example, features are `feat: add new feature` and fixes are `fix: correct issue with X`.\n\nSome additional guidance on pull request titles:\n* Ensure the pull request description clearly describes the problem and solution. If the PR fixes an issue, include a link to the fixed issue in the PR description with `Fixes #1234`.\n* Pull request titles appear in Langflow's release notes, so they should explain what the PR does as explicitly as possible.\n* Pull requests should strive to fix one thing **only**, and should contain a good description of what is being fixed.\n\n3. A Langflow maintainer will review your pull request and may request changes, so ensure you pay attention to your PRs. Thanks for your contribution!\n\nFor more information, see the [Python Developer's Guide](https://devguide.python.org/getting-started/pull-request-lifecycle/index.html#making-good-commits).",
      "summary": "This guide is intended to help you start contributing to Langflow.\nAs an open-source codebase in a rapidly developing field, Langflow welcomes contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.",
      "keywords": [
        "additional",
        "build",
        "clone",
        "contribute",
        "contribution",
        "debug",
        "development",
        "documentation",
        "environment",
        "frontend",
        "guides",
        "install",
        "issues",
        "langflow",
        "note",
        "only",
        "open",
        "overview",
        "prerequisites",
        "pull"
      ]
    },
    "/contributing-templates": {
      "title": "Contribute templates",
      "slug": "/contributing-templates",
      "category": "contributing",
      "content": "Follow these best practices when submitting a template to Langflow.\n\nFor template formatting examples, see [`/starter_projects`](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects) in the Langflow repository.\n\n## Create a PR to submit your template\n\nFollow these steps to submit your template:\n\n1. Fork the [Langflow repository](https://github.com/langflow-ai/langflow) on GitHub.\n2. On your fork, add your `template.json` file to `/starter_projects`.\n3. Include the [required items for template submission](#required-items-for-template-submission).\n4. Create a Pull Request (PR) from your fork to the main Langflow repository.\n5. Include a screenshot of your template in the PR.\n\nThe Langflow team will review your PR, offer feedback, and, if approved, merge the template.\n\n## Required items for template submission\n\nInclude the following items and follow these guidelines when submitting your template.\n\n### Name\n\nThe template name must be concise and contain no more than three words.\nCapitalize only the first letter of each word.\nFor example: **Blog Writer** or **Travel Planning Agent**.\n\n### Description\n\nA brief, informative description that is shown in the visual editor to help users understand the template's purpose and use cases.\nFor example:\n\n```json\n   \"description\": \"Auto-generate a customized blog post from instructions and referenced articles.\",\n```\n\n### Icons\n\nUse icons from the [Lucide](https://lucide.dev/icons/) icon library.\n\n### Flow\n\nUse only  **Core components** and  **Bundles**.\nDon't use custom components that aren't part of the Langflow codebase.\n\nInclude brief README, quickstart, or other essential details in a  **Note**. Notes accept Markdown syntax.\nFor example:\n\n```text\n# Financial Assistant Agents\n\nThe Financial Assistant Agent retrieves web content and writes reports about finance.\n\n## Prerequisites\n\n* [OpenAI API key](https://platform.openai.com/api-keys)\n* [Tavily AI Search key](https://docs.tavily.com/welcome)\n* [Sambanova API key](https://sambanova.ai/)\n\n## Quickstart\n\n1. In both **Agent** components, add your OpenAI API key.\n2. In the **Model Provider** field, select **Sambanova**, and select a model.\n3. In the **Sambanova** component, add your **Sambanova API key**.\n4. In the **Tavily Search** component, add your **Tavily API key**.\n5. Click the **Playground** and ask `Why did Nvidia stock drop in January?`\n```\n\n### Format\n\nSubmit the template in JSON format.\n\n### Tags\nAssign the template to one of the following categories:\n\n- Assistants\n- Classification\n- Coding\n- Content Generation\n- Q&A\n- Prompting\n- RAG\n- Agents\n\nFor more information, see the Langflow repository's [template categories](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/modals/templatesModal/index.tsx#L27-L57).",
      "summary": "Follow these best practices when submitting a template to Langflow.",
      "keywords": [
        "agent",
        "agents",
        "api",
        "assistant",
        "blog",
        "bundles",
        "components",
        "contribute",
        "core",
        "create",
        "description",
        "financial",
        "flow",
        "format",
        "icons",
        "items",
        "key",
        "model",
        "name",
        "note"
      ]
    },
    "contributing-templates": {
      "title": "Contribute templates",
      "slug": "/contributing-templates",
      "category": "contributing",
      "content": "Follow these best practices when submitting a template to Langflow.\n\nFor template formatting examples, see [`/starter_projects`](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects) in the Langflow repository.\n\n## Create a PR to submit your template\n\nFollow these steps to submit your template:\n\n1. Fork the [Langflow repository](https://github.com/langflow-ai/langflow) on GitHub.\n2. On your fork, add your `template.json` file to `/starter_projects`.\n3. Include the [required items for template submission](#required-items-for-template-submission).\n4. Create a Pull Request (PR) from your fork to the main Langflow repository.\n5. Include a screenshot of your template in the PR.\n\nThe Langflow team will review your PR, offer feedback, and, if approved, merge the template.\n\n## Required items for template submission\n\nInclude the following items and follow these guidelines when submitting your template.\n\n### Name\n\nThe template name must be concise and contain no more than three words.\nCapitalize only the first letter of each word.\nFor example: **Blog Writer** or **Travel Planning Agent**.\n\n### Description\n\nA brief, informative description that is shown in the visual editor to help users understand the template's purpose and use cases.\nFor example:\n\n```json\n   \"description\": \"Auto-generate a customized blog post from instructions and referenced articles.\",\n```\n\n### Icons\n\nUse icons from the [Lucide](https://lucide.dev/icons/) icon library.\n\n### Flow\n\nUse only  **Core components** and  **Bundles**.\nDon't use custom components that aren't part of the Langflow codebase.\n\nInclude brief README, quickstart, or other essential details in a  **Note**. Notes accept Markdown syntax.\nFor example:\n\n```text\n# Financial Assistant Agents\n\nThe Financial Assistant Agent retrieves web content and writes reports about finance.\n\n## Prerequisites\n\n* [OpenAI API key](https://platform.openai.com/api-keys)\n* [Tavily AI Search key](https://docs.tavily.com/welcome)\n* [Sambanova API key](https://sambanova.ai/)\n\n## Quickstart\n\n1. In both **Agent** components, add your OpenAI API key.\n2. In the **Model Provider** field, select **Sambanova**, and select a model.\n3. In the **Sambanova** component, add your **Sambanova API key**.\n4. In the **Tavily Search** component, add your **Tavily API key**.\n5. Click the **Playground** and ask `Why did Nvidia stock drop in January?`\n```\n\n### Format\n\nSubmit the template in JSON format.\n\n### Tags\nAssign the template to one of the following categories:\n\n- Assistants\n- Classification\n- Coding\n- Content Generation\n- Q&A\n- Prompting\n- RAG\n- Agents\n\nFor more information, see the Langflow repository's [template categories](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/modals/templatesModal/index.tsx#L27-L57).",
      "summary": "Follow these best practices when submitting a template to Langflow.",
      "keywords": [
        "agent",
        "agents",
        "api",
        "assistant",
        "blog",
        "bundles",
        "components",
        "contribute",
        "core",
        "create",
        "description",
        "financial",
        "flow",
        "format",
        "icons",
        "items",
        "key",
        "model",
        "name",
        "note"
      ]
    },
    "/deployment-architecture": {
      "title": "Langflow architecture on Kubernetes",
      "slug": "/deployment-architecture",
      "category": "deployment",
      "content": "There are two broad types of Langflow deployments:\n\n* **Langflow IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.\n\n    The Langflow repository's example [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file builds a Langflow IDE image.\n\n    For information about IDE deployments on Kubernetes, see [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev).\n\n* **Langflow runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor. The server exposes your flows as endpoints, and then runs only the processes necessary to serve each flow.\n\n    An external PostgreSQL database is strongly recommended with this deployment type to improve scalability and reliability as compared to the default SQLite database.\n\n    For information about runtime deployments on Kubernetes, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).\n\n    \n**Note:** You can start Langflow in headless mode with the `LANGFLOW_BACKEND_ONLY` [environment variable](/environment-variables).\n    \n\nYou can also [deploy the Langflow IDE and runtime on Docker](/deployment-docker).\n\n## Benefits of deploying Langflow on Kubernetes\n\nDeploying on Kubernetes offers the following advantages:\n\n* **Scalability**: Kubernetes allows you to scale the Langflow service to meet the demands of your workload.\n* **Availability and resilience**: Kubernetes provides built-in resilience features, such as automatic failover and self-healing, to ensure that the Langflow service is always available.\n* **Security**: Kubernetes provides security features, such as role-based access control and network isolation, to protect the Langflow service and its data.\n* **Portability**: Kubernetes is a portable platform, which means that you can deploy the Langflow service to any Kubernetes cluster, on-premises or in the cloud.\n\n    Langflow can be deployed on cloud platforms like AWS EKS, Google GKE, or Azure AKS. For more information, see the [Langflow Helm charts repository](https://github.com/langflow-ai/langflow-helm-charts).\n\n## Langflow deployment\n\nA typical Langflow deployment includes:\n\n* **Langflow services**: The Langflow API and, for IDE deployments, the visual editor.\n* **Kubernetes cluster**: The Kubernetes cluster provides a platform for deploying and managing Langflow and its supporting services.\n* **Persistent storage**: Persistent storage is used to store the Langflow service's data, such as models and training data.\n* **Ingress controller**: The ingress controller provides a single entry point for traffic to the Langflow service.\n* **Load balancer**: Balances traffic across multiple Langflow replicas.\n* **Vector database**: If you are using Langflow for RAG, you can integrate with the vector database in Astra Serverless.\n\n![Langflow reference architecture on Kubernetes](/img/langflow-reference-architecture.png)\n\n## Environment isolation\n\nIt is recommended to deploy and run separate development and production environments for Langflow:\n\n* Deploy the IDE in your development environment where your developers prototype and test new flows.\n* Deploy the runtime in your production environment to serve flows as standalone services.\n\n![Langflow environments](/img/langflow-env.png)\n\nThis separation is designed to enhance security, support an organized software development pipeline, and optimize infrastructure resource allocation:\n\n* **Isolation**: By separating the development and production environments, you can better isolate different phases of the application lifecycle. This isolation minimizes the risk of development-related issues impacting the production environments.\n* **Access control**: Different security policies and access controls can be applied to each environment. Developers may require broader access in the IDE for testing and debugging, while the runtime environment can be locked down with stricter security measures.\n* **Reduced attack surface**: The runtime environment is configured to include only essential components, reducing the attack surface and potential vulnerabilities.\n* **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n* **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n\n## Next steps\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "There are two broad types of Langflow deployments:",
      "keywords": [
        "access",
        "architecture",
        "attack",
        "availability",
        "balancer",
        "benefits",
        "cluster",
        "control",
        "controller",
        "cost",
        "database",
        "deploying",
        "deployment",
        "development",
        "efficiency",
        "environment",
        "ide",
        "ingress",
        "isolation",
        "kubernetes"
      ]
    },
    "deployment-architecture": {
      "title": "Langflow architecture on Kubernetes",
      "slug": "/deployment-architecture",
      "category": "deployment",
      "content": "There are two broad types of Langflow deployments:\n\n* **Langflow IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.\n\n    The Langflow repository's example [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file builds a Langflow IDE image.\n\n    For information about IDE deployments on Kubernetes, see [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev).\n\n* **Langflow runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor. The server exposes your flows as endpoints, and then runs only the processes necessary to serve each flow.\n\n    An external PostgreSQL database is strongly recommended with this deployment type to improve scalability and reliability as compared to the default SQLite database.\n\n    For information about runtime deployments on Kubernetes, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).\n\n    \n**Note:** You can start Langflow in headless mode with the `LANGFLOW_BACKEND_ONLY` [environment variable](/environment-variables).\n    \n\nYou can also [deploy the Langflow IDE and runtime on Docker](/deployment-docker).\n\n## Benefits of deploying Langflow on Kubernetes\n\nDeploying on Kubernetes offers the following advantages:\n\n* **Scalability**: Kubernetes allows you to scale the Langflow service to meet the demands of your workload.\n* **Availability and resilience**: Kubernetes provides built-in resilience features, such as automatic failover and self-healing, to ensure that the Langflow service is always available.\n* **Security**: Kubernetes provides security features, such as role-based access control and network isolation, to protect the Langflow service and its data.\n* **Portability**: Kubernetes is a portable platform, which means that you can deploy the Langflow service to any Kubernetes cluster, on-premises or in the cloud.\n\n    Langflow can be deployed on cloud platforms like AWS EKS, Google GKE, or Azure AKS. For more information, see the [Langflow Helm charts repository](https://github.com/langflow-ai/langflow-helm-charts).\n\n## Langflow deployment\n\nA typical Langflow deployment includes:\n\n* **Langflow services**: The Langflow API and, for IDE deployments, the visual editor.\n* **Kubernetes cluster**: The Kubernetes cluster provides a platform for deploying and managing Langflow and its supporting services.\n* **Persistent storage**: Persistent storage is used to store the Langflow service's data, such as models and training data.\n* **Ingress controller**: The ingress controller provides a single entry point for traffic to the Langflow service.\n* **Load balancer**: Balances traffic across multiple Langflow replicas.\n* **Vector database**: If you are using Langflow for RAG, you can integrate with the vector database in Astra Serverless.\n\n![Langflow reference architecture on Kubernetes](/img/langflow-reference-architecture.png)\n\n## Environment isolation\n\nIt is recommended to deploy and run separate development and production environments for Langflow:\n\n* Deploy the IDE in your development environment where your developers prototype and test new flows.\n* Deploy the runtime in your production environment to serve flows as standalone services.\n\n![Langflow environments](/img/langflow-env.png)\n\nThis separation is designed to enhance security, support an organized software development pipeline, and optimize infrastructure resource allocation:\n\n* **Isolation**: By separating the development and production environments, you can better isolate different phases of the application lifecycle. This isolation minimizes the risk of development-related issues impacting the production environments.\n* **Access control**: Different security policies and access controls can be applied to each environment. Developers may require broader access in the IDE for testing and debugging, while the runtime environment can be locked down with stricter security measures.\n* **Reduced attack surface**: The runtime environment is configured to include only essential components, reducing the attack surface and potential vulnerabilities.\n* **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.\n* **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.\n\n## Next steps\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "There are two broad types of Langflow deployments:",
      "keywords": [
        "access",
        "architecture",
        "attack",
        "availability",
        "balancer",
        "benefits",
        "cluster",
        "control",
        "controller",
        "cost",
        "database",
        "deploying",
        "deployment",
        "development",
        "efficiency",
        "environment",
        "ide",
        "ingress",
        "isolation",
        "kubernetes"
      ]
    },
    "/deployment-caddyfile": {
      "title": "Deploy Langflow on a remote server",
      "slug": "/deployment-caddyfile",
      "category": "deployment",
      "content": "Learn how to deploy Langflow on your own remote server with secure web access.\nThis guide walks you through setting up Langflow on a remote server using [Docker](https://docs.docker.com/) and configuring secure web access with [Caddy](https://caddyserver.com/docs/).\n\n## Prerequisites\n\n* A server with a dual-core CPU and at least 2 GB of RAM.\nThis example uses [Hetzner cloud](https://www.hetzner.com/) for hosting. Your deployment may vary.\n\n## Connect to your remote server with SSH\n\n1. Create an SSH key to connect to your server remotely.\nFor example:\n\n    ```bash\n    ssh-keygen -t ed25519 -C \"DANA@EXAMPLE.COM\"\n    ```\n\n    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.\n\n2. In your terminal, follow the instructions to create your SSH key pair.\nThis creates both a private and public key.\nTo copy the public key from your terminal, enter the following command:\n\n    ```bash\n    cat ~/Downloads/host-lf.pub | pbcopy\n    ```\n\n3. In your remote server, add the SSH key you copied in the previous step.\nFor example, if you are using a Hetzner cloud server, click **Server**, and then select **SSH keys** to add an SSH key.\n\n4. To connect to your server with SSH, enter the following command.\n\n    ```bash\n    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS\n    ```\n\n    Replace the following:\n\n    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server\n    * `SERVER_IP_ADDRESS`: Your server's IP address\n\n5. When prompted for a key fingerprint, type `yes`.\n\nThe terminal output indicates if the connection succeeds or fails.\nThe following response was returned after connecting to a Hetzner cloud server:\n\n```text\nSystem information as of Mon May 19 04:34:44 PM UTC 2025\n\nSystem load:  0.0               Processes:             129\nUsage of /:   1.5% of 74.79GB   Users logged in:       0\nMemory usage: 5%                IPv4 address for eth0: 5.161.250.132\nSwap usage:   0%                IPv6 address for eth0: 2a01:4ff:f0:4de7::1\n```\n\n## Deploy Langflow on your server\n\nNow that your local machine is connected to your remote server with SSH, you can install Docker, create a `docker-compose.yml` file, and serve it publicly with a reverse proxy, such as Caddy.\n\n1. Install Docker on your server.\n\n    Since this example server is an Ubuntu server, it can install snap packages.\n    If you aren't using Ubuntu or you prefer a different installation method, see the [official Docker installation guide](https://docs.docker.com/get-started/get-docker/) for instructions for your operating system.\n\n    ```bash\n    snap install docker\n    ```\n\n2. Create a file called `docker-compose.yml`, and then open it in a text editor:\n\n    ```bash\n    touch docker-compose.yml && nano docker-compose.yml\n    ```\n\n3. Add the following values to `docker-compose.yml`, and then save the file.\n\n    The following example defines the Langflow service from the `langflow:latest` image and a Caddy service to expose Langflow through a reverse proxy.\n\n    \n**Note:** The [host-langflow](https://github.com/datastax/host-langflow) repository offers pre-built copies of this `docker-compose.yml` and `Caddyfile`, if you prefer to fork the repository to your server.\n    \n\n    ```yml\n    version: \"3.8\"\n\n    services:\n      langflow:\n        image: langflowai/langflow:latest\n        ports:\n          - \"7860:7860\"\n        environment:\n          - LANGFLOW_HOST=0.0.0.0\n          - LANGFLOW_PORT=7860\n\n      caddy:\n        image: caddy:latest\n        ports:\n          - \"80:80\"\n          - \"443:443\"\n        volumes:\n          - ./Caddyfile:/etc/caddy/Caddyfile\n          - caddy_data:/data\n          - caddy_config:/config\n        depends_on:\n          - langflow\n\n    volumes:\n      caddy_data:\n      caddy_config:\n    ```\n\n4. Create a file called `Caddyfile`:\n\n    ```bash\n    touch Caddyfile && nano Caddyfile\n    ```\n\n5. Add the following values to `Caddyfile`, and then save the file.\n\n    This Caddyfile configures Caddy to listen on port 80, and forward all incoming requests to port 80 to the Langflow service at port 7860.\n\n    ```\n    :80 {\n        reverse_proxy langflow:7860\n    }\n    ```\n\n6. To deploy your server, run `docker-compose up`.\n\n    When the `Welcome to Langflow` message appears, Langflow is running and accessible internally at `http://0.0.0.0:7860` inside the Docker network.\n\n7. To access your Langflow server over the public internet, navigate to your server's public IP address, such as `http://5.161.250.132`.\nThis address uses HTTP because HTTPS isn't enabled yet.\n\n8. Recommended: Enable HTTPS:\n\n    1. Modify your domain's A record to point to your server's IP address. For example:\n\n        ```\n        Type: A\n        Name: langflow\n        Value: 5.161.250.132  # Set to your server's IP address\n        ```\n\n    2. Stop your server.\n\n    3. Modify your Caddyfile to include port `443` so Caddy can forward both HTTP (port 80) and HTTPS (port 443) requests to the Langflow service:\n\n        ```\n        :80, :443 {\n            reverse_proxy langflow:7860\n        }\n        ```\n\n    4. Start your server.\n\n        When users visit your domain, Caddy recognizes the incoming traffic and automatically routes it to your server with a secure, encrypted connection.\n\n9. To exit your SSH session, type `exit`.\n\n## See also\n\nTo package your local flows into a custom Docker image, see [Containerize a Langflow application](/develop-application).\n\nFor a step-by-step guide to deploying Langflow, including deployments to [fly.io](https://fly.io/) and [Flightcontrol.dev](https://www.flightcontrol.dev/), see [How to Host Langflow Anywhere](https://www.youtube.com/watch?v=q4qt5hSnte4).",
      "summary": "Learn how to deploy Langflow on your own remote server with secure web access.\nThis guide walks you through setting up Langflow on a remote server using [Docker](https://docs.docker.com/) and configuring secure web access with [Caddy](https://caddyserver.com/docs/).",
      "keywords": [
        "connect",
        "deploy",
        "keys",
        "langflow",
        "note",
        "prerequisites",
        "remote",
        "server",
        "ssh"
      ]
    },
    "deployment-caddyfile": {
      "title": "Deploy Langflow on a remote server",
      "slug": "/deployment-caddyfile",
      "category": "deployment",
      "content": "Learn how to deploy Langflow on your own remote server with secure web access.\nThis guide walks you through setting up Langflow on a remote server using [Docker](https://docs.docker.com/) and configuring secure web access with [Caddy](https://caddyserver.com/docs/).\n\n## Prerequisites\n\n* A server with a dual-core CPU and at least 2 GB of RAM.\nThis example uses [Hetzner cloud](https://www.hetzner.com/) for hosting. Your deployment may vary.\n\n## Connect to your remote server with SSH\n\n1. Create an SSH key to connect to your server remotely.\nFor example:\n\n    ```bash\n    ssh-keygen -t ed25519 -C \"DANA@EXAMPLE.COM\"\n    ```\n\n    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.\n\n2. In your terminal, follow the instructions to create your SSH key pair.\nThis creates both a private and public key.\nTo copy the public key from your terminal, enter the following command:\n\n    ```bash\n    cat ~/Downloads/host-lf.pub | pbcopy\n    ```\n\n3. In your remote server, add the SSH key you copied in the previous step.\nFor example, if you are using a Hetzner cloud server, click **Server**, and then select **SSH keys** to add an SSH key.\n\n4. To connect to your server with SSH, enter the following command.\n\n    ```bash\n    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS\n    ```\n\n    Replace the following:\n\n    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server\n    * `SERVER_IP_ADDRESS`: Your server's IP address\n\n5. When prompted for a key fingerprint, type `yes`.\n\nThe terminal output indicates if the connection succeeds or fails.\nThe following response was returned after connecting to a Hetzner cloud server:\n\n```text\nSystem information as of Mon May 19 04:34:44 PM UTC 2025\n\nSystem load:  0.0               Processes:             129\nUsage of /:   1.5% of 74.79GB   Users logged in:       0\nMemory usage: 5%                IPv4 address for eth0: 5.161.250.132\nSwap usage:   0%                IPv6 address for eth0: 2a01:4ff:f0:4de7::1\n```\n\n## Deploy Langflow on your server\n\nNow that your local machine is connected to your remote server with SSH, you can install Docker, create a `docker-compose.yml` file, and serve it publicly with a reverse proxy, such as Caddy.\n\n1. Install Docker on your server.\n\n    Since this example server is an Ubuntu server, it can install snap packages.\n    If you aren't using Ubuntu or you prefer a different installation method, see the [official Docker installation guide](https://docs.docker.com/get-started/get-docker/) for instructions for your operating system.\n\n    ```bash\n    snap install docker\n    ```\n\n2. Create a file called `docker-compose.yml`, and then open it in a text editor:\n\n    ```bash\n    touch docker-compose.yml && nano docker-compose.yml\n    ```\n\n3. Add the following values to `docker-compose.yml`, and then save the file.\n\n    The following example defines the Langflow service from the `langflow:latest` image and a Caddy service to expose Langflow through a reverse proxy.\n\n    \n**Note:** The [host-langflow](https://github.com/datastax/host-langflow) repository offers pre-built copies of this `docker-compose.yml` and `Caddyfile`, if you prefer to fork the repository to your server.\n    \n\n    ```yml\n    version: \"3.8\"\n\n    services:\n      langflow:\n        image: langflowai/langflow:latest\n        ports:\n          - \"7860:7860\"\n        environment:\n          - LANGFLOW_HOST=0.0.0.0\n          - LANGFLOW_PORT=7860\n\n      caddy:\n        image: caddy:latest\n        ports:\n          - \"80:80\"\n          - \"443:443\"\n        volumes:\n          - ./Caddyfile:/etc/caddy/Caddyfile\n          - caddy_data:/data\n          - caddy_config:/config\n        depends_on:\n          - langflow\n\n    volumes:\n      caddy_data:\n      caddy_config:\n    ```\n\n4. Create a file called `Caddyfile`:\n\n    ```bash\n    touch Caddyfile && nano Caddyfile\n    ```\n\n5. Add the following values to `Caddyfile`, and then save the file.\n\n    This Caddyfile configures Caddy to listen on port 80, and forward all incoming requests to port 80 to the Langflow service at port 7860.\n\n    ```\n    :80 {\n        reverse_proxy langflow:7860\n    }\n    ```\n\n6. To deploy your server, run `docker-compose up`.\n\n    When the `Welcome to Langflow` message appears, Langflow is running and accessible internally at `http://0.0.0.0:7860` inside the Docker network.\n\n7. To access your Langflow server over the public internet, navigate to your server's public IP address, such as `http://5.161.250.132`.\nThis address uses HTTP because HTTPS isn't enabled yet.\n\n8. Recommended: Enable HTTPS:\n\n    1. Modify your domain's A record to point to your server's IP address. For example:\n\n        ```\n        Type: A\n        Name: langflow\n        Value: 5.161.250.132  # Set to your server's IP address\n        ```\n\n    2. Stop your server.\n\n    3. Modify your Caddyfile to include port `443` so Caddy can forward both HTTP (port 80) and HTTPS (port 443) requests to the Langflow service:\n\n        ```\n        :80, :443 {\n            reverse_proxy langflow:7860\n        }\n        ```\n\n    4. Start your server.\n\n        When users visit your domain, Caddy recognizes the incoming traffic and automatically routes it to your server with a secure, encrypted connection.\n\n9. To exit your SSH session, type `exit`.\n\n## See also\n\nTo package your local flows into a custom Docker image, see [Containerize a Langflow application](/develop-application).\n\nFor a step-by-step guide to deploying Langflow, including deployments to [fly.io](https://fly.io/) and [Flightcontrol.dev](https://www.flightcontrol.dev/), see [How to Host Langflow Anywhere](https://www.youtube.com/watch?v=q4qt5hSnte4).",
      "summary": "Learn how to deploy Langflow on your own remote server with secure web access.\nThis guide walks you through setting up Langflow on a remote server using [Docker](https://docs.docker.com/) and configuring secure web access with [Caddy](https://caddyserver.com/docs/).",
      "keywords": [
        "connect",
        "deploy",
        "keys",
        "langflow",
        "note",
        "prerequisites",
        "remote",
        "server",
        "ssh"
      ]
    },
    "/deployment-docker": {
      "title": "Deploy Langflow on Docker",
      "slug": "/deployment-docker",
      "category": "deployment",
      "content": "Running applications in Docker containers ensures consistent behavior across different systems and eliminates dependency conflicts.\n\nYou can use the Langflow Docker image to start a Langflow container.\n\nThis guide demonstrates several ways to deploy Langflow with [Docker](https://docs.docker.com/) and [Docker Compose](https://docs.docker.com/compose/):\n\n* [Quickstart](#quickstart): Start a Langflow container with default values.\n* [Use Docker Compose](#clone): Clone the Langflow repo, and then use Docker Compose to build the Langflow Docker container.\nThis option provides more control over the configuration, including a persistent PostgreSQL database service, while still using the base Langflow Docker image.\n* [Create a custom flow image](#package-your-flow-as-a-docker-image): Use a Dockerfile to package a flow as a Docker image.\n* [Create a custom Langflow image](#customize-the-langflow-docker-image): Use a Dockerfile to package a custom Langflow Docker image that includes your own code, custom dependencies, or other modifications.\n\n## Quickstart: Start a Langflow container with default values {#quickstart}\n\nWith Docker installed and running on your system, run the following command:\n\n```shell\ndocker run -p 7860:7860 langflowai/langflow:latest\n```\n\nThen, access Langflow at `http://localhost:7860/`.\n\nThis container runs a pre-built Docker image with default settings.\nFor more control over the configuration, see [Clone the repo and run the Langflow Docker container](#clone).\n\n## Clone the repo and run the Langflow Docker container {#clone}\n\nCloning the Langflow repository and using Docker Compose gives you more control over your configuration, allowing you to customize environment variables, use a persistent PostgreSQL database service (instead of the default SQLite database), and include custom dependencies.\n\nThe default deployment with Docker Compose includes the following:\n\n- **Langflow service**: Runs the latest Langflow image with PostgreSQL as the database.\n- **PostgreSQL service**: Provides persistent data storage for flows, users, and settings.\n- **Persistent volumes**: Ensures your data survives container restarts.\n\nThe complete Docker Compose configuration is available in `docker_example/docker-compose.yml`.\n\n1. Clone the Langflow repository:\n\n   ```shell\n   git clone https://github.com/langflow-ai/langflow.git\n   ```\n\n2. Navigate to the `docker_example` directory:\n\n   ```shell\n   cd langflow/docker_example\n   ```\n\n3. Run the Docker Compose file:\n\n   ```shell\n   docker compose up\n   ```\n\n4. Access Langflow at `http://localhost:7860/`.\n\n### Customize your deployment\n\nYou can customize the Docker Compose configuration to fit your specific deployment.\n\nFor example, to configure the container's database credentials using a `.env` file, do the following:\n\n1. Create a `.env` file with your database credentials in the same directory as `docker-compose.yml`:\n\n    ```text\n    # Database credentials\n    POSTGRES_USER=myuser\n    POSTGRES_PASSWORD=mypassword\n    POSTGRES_DB=langflow\n\n    # Langflow configuration\n    LANGFLOW_DATABASE_URL=postgresql://myuser:mypassword@postgres:5432/langflow\n    LANGFLOW_CONFIG_DIR=/app/langflow\n    ```\n\n2. Modify the `docker-compose.yml` file to reference the `.env` file for both the `langflow` and `postgres` services:\n\n    ```yaml\n    services:\n      langflow:\n        environment:\n          - LANGFLOW_DATABASE_URL=${LANGFLOW_DATABASE_URL}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n      postgres:\n        environment:\n          - POSTGRES_USER=${POSTGRES_USER}\n          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n          - POSTGRES_DB=${POSTGRES_DB}\n    ```\n\nFor a complete list of available environment variables, see [Langflow environment variables](/environment-variables).\n\nFor more customization options, see [Customize the Langflow Docker image with your own code](#customize-the-langflow-docker-image).\n\n## Package your flow as a Docker image {#package-your-flow-as-a-docker-image}\n\nThis section shows you how to create a Dockerfile that builds a Docker image containing your Langflow flow. This approach is useful when you want to distribute a specific flow as a standalone container or deploy it to environments like Kubernetes.\n\nUnlike the previous sections that use pre-built images, this method builds a custom image with your flow embedded inside it.\n\n1. Create a project directory, and change directory into it.\n\n    ```bash\n    mkdir langflow-custom && cd langflow-custom\n    ```\n\n2. Add your flow's JSON file to the directory. You can download an example, or use your own:\n\n    ```bash\n    # Download an example flow\n    wget https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json\n\n    # Or copy your own flow file\n    cp /path/to/your/flow.json .\n    ```\n\n3. Create a Dockerfile to build your custom image:\n\n    ```dockerfile\n    FROM langflowai/langflow:latest\n    RUN mkdir /app/flows\n    COPY ./*.json /app/flows/\n    ENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows\n    ```\n\nThis Dockerfile uses the official Langflow image as the base, creates a directory for your flows, copies your JSON flow files into the directory, and sets the environment variable to tell Langflow where to find the flows.\n\n4. Build and test your custom image:\n\n    ```bash\n    docker build -t myuser/langflow-custom:1.0.0 .\n    docker run -p 7860:7860 myuser/langflow-custom:1.0.0\n    ```\n\n5. Push your image to Docker Hub (optional):\n\n    ```bash\n    docker push myuser/langflow-custom:1.0.0\n    ```\n\nYour custom image now contains your flow and can be deployed anywhere Docker runs. For Kubernetes deployment, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).\n\n## Customize the Langflow Docker image with your own code {#customize-the-langflow-docker-image}\n\nWhile the previous section showed how to package a flow with a Docker image, this section shows how to customize the Langflow application itself. This is useful when you need to add custom Python packages or dependencies, modify Langflow's configuration or settings, include custom components or tools, or add your own code to extend Langflow's functionality.\n\nThis example demonstrates how to customize the **Message History** component, but the same approach can be used for any code modifications.\n\n```dockerfile\nFROM langflowai/langflow:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy your modified memory component\nCOPY src/backend/base/langflow/components/helpers/memory.py /tmp/memory.py\n\n# Find the site-packages directory where langflow is installed\nRUN python -c \"import site; print(site.getsitepackages()[0])\" > /tmp/site_packages.txt\n\n# Replace the file in the site-packages location\nRUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \\\n    echo \"Site packages at: $SITE_PACKAGES\" && \\\n    mkdir -p \"$SITE_PACKAGES/langflow/components/helpers\" && \\\n    cp /tmp/memory.py \"$SITE_PACKAGES/langflow/components/helpers/\"\n\n# Clear Python cache in the site-packages directory only\nRUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \\\n    find \"$SITE_PACKAGES\" -name \"*.pyc\" -delete && \\\n    find \"$SITE_PACKAGES\" -name \"__pycache__\" -type d -exec rm -rf {} +\n\n# Expose the default Langflow port\nEXPOSE 7860\n\n# Command to run Langflow\nCMD [\"python\", \"-m\", \"langflow\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\nTo use this custom Dockerfile, do the following:\n\n1. Create a directory for your custom Langflow setup:\n\n    ```bash\n    mkdir langflow-custom && cd langflow-custom\n    ```\n\n2. Create the necessary directory structure for your custom code.\nIn this example, Langflow expects `memory.py` to exist in the `/helpers` directory, so you create a directory in that location.\n\n    ```bash\n    mkdir -p src/backend/base/langflow/components/helpers\n    ```\n\n3. Place your modified `memory.py` file in the `/helpers` directory.\n\n4. Create a new file named `Dockerfile` in your `langflow-custom` directory, and then copy the Dockerfile contents shown above into it.\n\n5. Build and run the image:\n\n    ```bash\n    docker build -t myuser/langflow-custom:1.0.0 .\n    docker run -p 7860:7860 myuser/langflow-custom:1.0.0\n    ```\n\nThis approach can be adapted for any other components or custom code you want to add to Langflow by modifying the file paths and component names.",
      "summary": "Running applications in Docker containers ensures consistent behavior across different systems and eliminates dependency conflicts.",
      "keywords": [
        "cache",
        "clear",
        "clone",
        "code",
        "command",
        "component",
        "container",
        "copy",
        "customize",
        "default",
        "deploy",
        "deployment",
        "directory",
        "docker",
        "expose",
        "file",
        "find",
        "flow",
        "history",
        "image"
      ]
    },
    "deployment-docker": {
      "title": "Deploy Langflow on Docker",
      "slug": "/deployment-docker",
      "category": "deployment",
      "content": "Running applications in Docker containers ensures consistent behavior across different systems and eliminates dependency conflicts.\n\nYou can use the Langflow Docker image to start a Langflow container.\n\nThis guide demonstrates several ways to deploy Langflow with [Docker](https://docs.docker.com/) and [Docker Compose](https://docs.docker.com/compose/):\n\n* [Quickstart](#quickstart): Start a Langflow container with default values.\n* [Use Docker Compose](#clone): Clone the Langflow repo, and then use Docker Compose to build the Langflow Docker container.\nThis option provides more control over the configuration, including a persistent PostgreSQL database service, while still using the base Langflow Docker image.\n* [Create a custom flow image](#package-your-flow-as-a-docker-image): Use a Dockerfile to package a flow as a Docker image.\n* [Create a custom Langflow image](#customize-the-langflow-docker-image): Use a Dockerfile to package a custom Langflow Docker image that includes your own code, custom dependencies, or other modifications.\n\n## Quickstart: Start a Langflow container with default values {#quickstart}\n\nWith Docker installed and running on your system, run the following command:\n\n```shell\ndocker run -p 7860:7860 langflowai/langflow:latest\n```\n\nThen, access Langflow at `http://localhost:7860/`.\n\nThis container runs a pre-built Docker image with default settings.\nFor more control over the configuration, see [Clone the repo and run the Langflow Docker container](#clone).\n\n## Clone the repo and run the Langflow Docker container {#clone}\n\nCloning the Langflow repository and using Docker Compose gives you more control over your configuration, allowing you to customize environment variables, use a persistent PostgreSQL database service (instead of the default SQLite database), and include custom dependencies.\n\nThe default deployment with Docker Compose includes the following:\n\n- **Langflow service**: Runs the latest Langflow image with PostgreSQL as the database.\n- **PostgreSQL service**: Provides persistent data storage for flows, users, and settings.\n- **Persistent volumes**: Ensures your data survives container restarts.\n\nThe complete Docker Compose configuration is available in `docker_example/docker-compose.yml`.\n\n1. Clone the Langflow repository:\n\n   ```shell\n   git clone https://github.com/langflow-ai/langflow.git\n   ```\n\n2. Navigate to the `docker_example` directory:\n\n   ```shell\n   cd langflow/docker_example\n   ```\n\n3. Run the Docker Compose file:\n\n   ```shell\n   docker compose up\n   ```\n\n4. Access Langflow at `http://localhost:7860/`.\n\n### Customize your deployment\n\nYou can customize the Docker Compose configuration to fit your specific deployment.\n\nFor example, to configure the container's database credentials using a `.env` file, do the following:\n\n1. Create a `.env` file with your database credentials in the same directory as `docker-compose.yml`:\n\n    ```text\n    # Database credentials\n    POSTGRES_USER=myuser\n    POSTGRES_PASSWORD=mypassword\n    POSTGRES_DB=langflow\n\n    # Langflow configuration\n    LANGFLOW_DATABASE_URL=postgresql://myuser:mypassword@postgres:5432/langflow\n    LANGFLOW_CONFIG_DIR=/app/langflow\n    ```\n\n2. Modify the `docker-compose.yml` file to reference the `.env` file for both the `langflow` and `postgres` services:\n\n    ```yaml\n    services:\n      langflow:\n        environment:\n          - LANGFLOW_DATABASE_URL=${LANGFLOW_DATABASE_URL}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n      postgres:\n        environment:\n          - POSTGRES_USER=${POSTGRES_USER}\n          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n          - POSTGRES_DB=${POSTGRES_DB}\n    ```\n\nFor a complete list of available environment variables, see [Langflow environment variables](/environment-variables).\n\nFor more customization options, see [Customize the Langflow Docker image with your own code](#customize-the-langflow-docker-image).\n\n## Package your flow as a Docker image {#package-your-flow-as-a-docker-image}\n\nThis section shows you how to create a Dockerfile that builds a Docker image containing your Langflow flow. This approach is useful when you want to distribute a specific flow as a standalone container or deploy it to environments like Kubernetes.\n\nUnlike the previous sections that use pre-built images, this method builds a custom image with your flow embedded inside it.\n\n1. Create a project directory, and change directory into it.\n\n    ```bash\n    mkdir langflow-custom && cd langflow-custom\n    ```\n\n2. Add your flow's JSON file to the directory. You can download an example, or use your own:\n\n    ```bash\n    # Download an example flow\n    wget https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json\n\n    # Or copy your own flow file\n    cp /path/to/your/flow.json .\n    ```\n\n3. Create a Dockerfile to build your custom image:\n\n    ```dockerfile\n    FROM langflowai/langflow:latest\n    RUN mkdir /app/flows\n    COPY ./*.json /app/flows/\n    ENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows\n    ```\n\nThis Dockerfile uses the official Langflow image as the base, creates a directory for your flows, copies your JSON flow files into the directory, and sets the environment variable to tell Langflow where to find the flows.\n\n4. Build and test your custom image:\n\n    ```bash\n    docker build -t myuser/langflow-custom:1.0.0 .\n    docker run -p 7860:7860 myuser/langflow-custom:1.0.0\n    ```\n\n5. Push your image to Docker Hub (optional):\n\n    ```bash\n    docker push myuser/langflow-custom:1.0.0\n    ```\n\nYour custom image now contains your flow and can be deployed anywhere Docker runs. For Kubernetes deployment, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).\n\n## Customize the Langflow Docker image with your own code {#customize-the-langflow-docker-image}\n\nWhile the previous section showed how to package a flow with a Docker image, this section shows how to customize the Langflow application itself. This is useful when you need to add custom Python packages or dependencies, modify Langflow's configuration or settings, include custom components or tools, or add your own code to extend Langflow's functionality.\n\nThis example demonstrates how to customize the **Message History** component, but the same approach can be used for any code modifications.\n\n```dockerfile\nFROM langflowai/langflow:latest\n\n# Set working directory\nWORKDIR /app\n\n# Copy your modified memory component\nCOPY src/backend/base/langflow/components/helpers/memory.py /tmp/memory.py\n\n# Find the site-packages directory where langflow is installed\nRUN python -c \"import site; print(site.getsitepackages()[0])\" > /tmp/site_packages.txt\n\n# Replace the file in the site-packages location\nRUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \\\n    echo \"Site packages at: $SITE_PACKAGES\" && \\\n    mkdir -p \"$SITE_PACKAGES/langflow/components/helpers\" && \\\n    cp /tmp/memory.py \"$SITE_PACKAGES/langflow/components/helpers/\"\n\n# Clear Python cache in the site-packages directory only\nRUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \\\n    find \"$SITE_PACKAGES\" -name \"*.pyc\" -delete && \\\n    find \"$SITE_PACKAGES\" -name \"__pycache__\" -type d -exec rm -rf {} +\n\n# Expose the default Langflow port\nEXPOSE 7860\n\n# Command to run Langflow\nCMD [\"python\", \"-m\", \"langflow\", \"run\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\nTo use this custom Dockerfile, do the following:\n\n1. Create a directory for your custom Langflow setup:\n\n    ```bash\n    mkdir langflow-custom && cd langflow-custom\n    ```\n\n2. Create the necessary directory structure for your custom code.\nIn this example, Langflow expects `memory.py` to exist in the `/helpers` directory, so you create a directory in that location.\n\n    ```bash\n    mkdir -p src/backend/base/langflow/components/helpers\n    ```\n\n3. Place your modified `memory.py` file in the `/helpers` directory.\n\n4. Create a new file named `Dockerfile` in your `langflow-custom` directory, and then copy the Dockerfile contents shown above into it.\n\n5. Build and run the image:\n\n    ```bash\n    docker build -t myuser/langflow-custom:1.0.0 .\n    docker run -p 7860:7860 myuser/langflow-custom:1.0.0\n    ```\n\nThis approach can be adapted for any other components or custom code you want to add to Langflow by modifying the file paths and component names.",
      "summary": "Running applications in Docker containers ensures consistent behavior across different systems and eliminates dependency conflicts.",
      "keywords": [
        "cache",
        "clear",
        "clone",
        "code",
        "command",
        "component",
        "container",
        "copy",
        "customize",
        "default",
        "deploy",
        "deployment",
        "directory",
        "docker",
        "expose",
        "file",
        "find",
        "flow",
        "history",
        "image"
      ]
    },
    "/deployment-gcp": {
      "title": "Deploy Langflow on Google Cloud Platform",
      "slug": "/deployment-gcp",
      "category": "deployment",
      "content": "This guide demonstrates how to deploy Langflow on [Google Cloud Platform](https://console.cloud.google.com/) with a Cloud Shell script that walks through the process of setting up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow development environment in GCP.\n\nTo use this script, you need a Google Cloud project with the necessary permissions to create resources.\n\n1. Follow this link to launch the Cloud Shell with the GCP deployment script from the Langflow repository:\n\n   [![Deploy to Google Cloud](https://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/langflow-ai/langflow&working_dir=scripts/gcp&shellonly=true&tutorial=walkthroughtutorial.md)\n\n2. Click **Trust repo**.\n\n    Some `gcloud` commands may not run in an ephemeral Cloud Shell environment.\n\n3. Click **Start**, and then follow the tutorial to deploy Langflow.\n\n**Note:** This deployment uses a [spot (preemptible) instance](https://cloud.google.com/compute/docs/instances/preemptible) as a cost-effective option to demonstrate how to deploy Langflow on GCP.\nHowever, due to the nature of spot instances, the VM can be terminated at any time if Google Cloud needs to reclaim the resources.\n\nFor a more stable deployment, consider using a regular VM instance instead of a spot instance.\n\nFor more information, see the [GCP pricing calculator](https://cloud.google.com/products/calculator?hl=en).",
      "summary": "This guide demonstrates how to deploy Langflow on [Google Cloud Platform](https://console.cloud.google.com/) with a Cloud Shell script that walks through the process of setting up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow development...",
      "keywords": [
        "cloud",
        "deploy",
        "google",
        "langflow",
        "note",
        "platform",
        "repo",
        "start",
        "trust"
      ]
    },
    "deployment-gcp": {
      "title": "Deploy Langflow on Google Cloud Platform",
      "slug": "/deployment-gcp",
      "category": "deployment",
      "content": "This guide demonstrates how to deploy Langflow on [Google Cloud Platform](https://console.cloud.google.com/) with a Cloud Shell script that walks through the process of setting up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow development environment in GCP.\n\nTo use this script, you need a Google Cloud project with the necessary permissions to create resources.\n\n1. Follow this link to launch the Cloud Shell with the GCP deployment script from the Langflow repository:\n\n   [![Deploy to Google Cloud](https://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/langflow-ai/langflow&working_dir=scripts/gcp&shellonly=true&tutorial=walkthroughtutorial.md)\n\n2. Click **Trust repo**.\n\n    Some `gcloud` commands may not run in an ephemeral Cloud Shell environment.\n\n3. Click **Start**, and then follow the tutorial to deploy Langflow.\n\n**Note:** This deployment uses a [spot (preemptible) instance](https://cloud.google.com/compute/docs/instances/preemptible) as a cost-effective option to demonstrate how to deploy Langflow on GCP.\nHowever, due to the nature of spot instances, the VM can be terminated at any time if Google Cloud needs to reclaim the resources.\n\nFor a more stable deployment, consider using a regular VM instance instead of a spot instance.\n\nFor more information, see the [GCP pricing calculator](https://cloud.google.com/products/calculator?hl=en).",
      "summary": "This guide demonstrates how to deploy Langflow on [Google Cloud Platform](https://console.cloud.google.com/) with a Cloud Shell script that walks through the process of setting up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow development...",
      "keywords": [
        "cloud",
        "deploy",
        "google",
        "langflow",
        "note",
        "platform",
        "repo",
        "start",
        "trust"
      ]
    },
    "/deployment-hugging-face-spaces": {
      "title": "Deploy Langflow on Hugging Face Spaces",
      "slug": "/deployment-hugging-face-spaces",
      "category": "deployment",
      "content": "This guide explains how to deploy Langflow on [Hugging Face Spaces](https://huggingface.co/spaces/).\n\n1. Go to the [Langflow Space](https://huggingface.co/spaces/Langflow/Langflow?duplicate=true).\n2. Click **Duplicate Space**.\n3. Configure the duplicated Space:\n\n   1. Enter a name for your Space.\n   2. Select either public or private visibility.\n   3. Click **Duplicate Space**.\n\n   ![Hugging Face deployment dialog](/img/hugging-face-deployment.png)\n\nWhen setup is complete, you're redirected to your new Space automatically, and your Langflow instance is ready to use.",
      "summary": "This guide explains how to deploy Langflow on [Hugging Face Spaces](https://huggingface.co/spaces/).",
      "keywords": [
        "deploy",
        "duplicate",
        "face",
        "hugging",
        "langflow",
        "space",
        "spaces"
      ]
    },
    "deployment-hugging-face-spaces": {
      "title": "Deploy Langflow on Hugging Face Spaces",
      "slug": "/deployment-hugging-face-spaces",
      "category": "deployment",
      "content": "This guide explains how to deploy Langflow on [Hugging Face Spaces](https://huggingface.co/spaces/).\n\n1. Go to the [Langflow Space](https://huggingface.co/spaces/Langflow/Langflow?duplicate=true).\n2. Click **Duplicate Space**.\n3. Configure the duplicated Space:\n\n   1. Enter a name for your Space.\n   2. Select either public or private visibility.\n   3. Click **Duplicate Space**.\n\n   ![Hugging Face deployment dialog](/img/hugging-face-deployment.png)\n\nWhen setup is complete, you're redirected to your new Space automatically, and your Langflow instance is ready to use.",
      "summary": "This guide explains how to deploy Langflow on [Hugging Face Spaces](https://huggingface.co/spaces/).",
      "keywords": [
        "deploy",
        "duplicate",
        "face",
        "hugging",
        "langflow",
        "space",
        "spaces"
      ]
    },
    "/deployment-kubernetes-dev": {
      "title": "Deploy the Langflow development environment on Kubernetes",
      "slug": "/deployment-kubernetes-dev",
      "category": "deployment",
      "content": "The [Langflow integrated development environment (IDE) Helm chart](https://github.com/langflow-ai/langflow-helm-charts/tree/main/charts/langflow-ide) is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the Langflow API and visual editor.\n\n## Prerequisites\n\n- A [Kubernetes](https://kubernetes.io/docs/setup/) cluster\n- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n- [Helm](https://helm.sh/docs/intro/install/)\n\n## Prepare a Kubernetes cluster\n\nThis example uses [Minikube](https://minikube.sigs.k8s.io/docs/start/), but you can use any Kubernetes cluster.\n\n1. Create a Kubernetes cluster on Minikube:\n\n\t```shell\n\tminikube start\n\t```\n\n2. Set `kubectl` to use Minikube:\n\n\t```shell\n\tkubectl config use-context minikube\n\t```\n\n## Install the Langflow IDE Helm chart\n\n1. Add the repository to Helm, and then update it:\n\n\t```shell\n\thelm repo add langflow https://langflow-ai.github.io/langflow-helm-charts\n\thelm repo update\n\t```\n\n2. Install Langflow with the default options in the `langflow` namespace:\n\n\t```shell\n\thelm install langflow-ide langflow/langflow-ide -n langflow --create-namespace\n\t```\n\n3. Check the status of the pods:\n\n\t```shell\n\tkubectl get pods -n langflow\n\t```\n\n## Access the Langflow IDE\n\nEnable local port forwarding to access Langflow from your local machine:\n\n1. Make the Langflow API accessible from your local machine at port 7860:\n\n    ```shell\n    kubectl port-forward -n langflow svc/langflow-service-backend 7860:7860\n    ```\n\n2. Make the visual editor accessible from your local machine at port 8080:\n\n    ```shell\n    kubectl port-forward -n langflow svc/langflow-service 8080:8080\n    ```\n\nNow you can do the following:\n\n- Access the Langflow API at `http://localhost:7860`.\n- Access the visual editor at `http://localhost:8080`.\n\n## Modify your Langflow IDE deployment\n\nYou can modify the Langflow IDE Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-ide/values.yaml) file to customize your deployment.\nThe following sections describe some common modifications.\n\nIf you need to set secrets, Kubernetes secrets are recommended.\n\n### Deploy a different Langflow version\n\nThe Langflow IDE Helm chart deploys the latest Langflow version by default.\n\nTo specify a different Langflow version, set the `langflow.backend.image.tag` and `langflow.frontend.image.tag` values to your preferred version.\nFor example:\n\n```yaml\nlangflow:\n  backend:\n    image:\n      tag: \"1.0.0a59\"\n  frontend:\n    image:\n      tag: \"1.0.0a59\"\n```\n\n### Use external storage for the Langflow database\n\nThe Langflow IDE Helm chart uses the default Langflow database configuration, specifically a SQLite database stored in a local persistent disk.\n\nIf you want to use an [external PostgreSQL database](/configuration-custom-database), use `postgresql` chart or `externalDatabase` to configure the database connection in `values.yaml`.\n\n</Tabs>\n\n### Configure scaling\n\nTo configure scaling for the Langflow IDE Helm chart deployment, you must set `replicaCount` (horizontal scaling) and `resources` (vertical scaling) for both the `langflow.backend` and `langflow.frontend`.\n\nIf your flows rely on a shared state, such as [built-in chat memory](/memory), you must also set up a shared database when scaling horizontally.\n\n```yaml\nlangflow:\n  backend:\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 0.5\n        memory: 1Gi\n      # limits:\n      #   cpu: 0.5\n      #   memory: 1Gi\n\n  frontend:\n    enabled: true\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 0.3\n        memory: 512Mi\n      # limits:\n      #   cpu: 0.3\n      #   memory: 512Mi\n```\n\n## See also\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)",
      "summary": "The [Langflow integrated development environment (IDE) Helm chart](https://github.com/langflow-ai/langflow-helm-charts/tree/main/charts/langflow-ide) is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the Langflow API and visual...",
      "keywords": [
        "access",
        "chart",
        "cluster",
        "configure",
        "database",
        "deploy",
        "deployment",
        "development",
        "different",
        "environment",
        "external",
        "helm",
        "ide",
        "install",
        "kubernetes",
        "langflow",
        "modify",
        "prepare",
        "prerequisites",
        "scaling"
      ]
    },
    "deployment-kubernetes-dev": {
      "title": "Deploy the Langflow development environment on Kubernetes",
      "slug": "/deployment-kubernetes-dev",
      "category": "deployment",
      "content": "The [Langflow integrated development environment (IDE) Helm chart](https://github.com/langflow-ai/langflow-helm-charts/tree/main/charts/langflow-ide) is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the Langflow API and visual editor.\n\n## Prerequisites\n\n- A [Kubernetes](https://kubernetes.io/docs/setup/) cluster\n- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n- [Helm](https://helm.sh/docs/intro/install/)\n\n## Prepare a Kubernetes cluster\n\nThis example uses [Minikube](https://minikube.sigs.k8s.io/docs/start/), but you can use any Kubernetes cluster.\n\n1. Create a Kubernetes cluster on Minikube:\n\n\t```shell\n\tminikube start\n\t```\n\n2. Set `kubectl` to use Minikube:\n\n\t```shell\n\tkubectl config use-context minikube\n\t```\n\n## Install the Langflow IDE Helm chart\n\n1. Add the repository to Helm, and then update it:\n\n\t```shell\n\thelm repo add langflow https://langflow-ai.github.io/langflow-helm-charts\n\thelm repo update\n\t```\n\n2. Install Langflow with the default options in the `langflow` namespace:\n\n\t```shell\n\thelm install langflow-ide langflow/langflow-ide -n langflow --create-namespace\n\t```\n\n3. Check the status of the pods:\n\n\t```shell\n\tkubectl get pods -n langflow\n\t```\n\n## Access the Langflow IDE\n\nEnable local port forwarding to access Langflow from your local machine:\n\n1. Make the Langflow API accessible from your local machine at port 7860:\n\n    ```shell\n    kubectl port-forward -n langflow svc/langflow-service-backend 7860:7860\n    ```\n\n2. Make the visual editor accessible from your local machine at port 8080:\n\n    ```shell\n    kubectl port-forward -n langflow svc/langflow-service 8080:8080\n    ```\n\nNow you can do the following:\n\n- Access the Langflow API at `http://localhost:7860`.\n- Access the visual editor at `http://localhost:8080`.\n\n## Modify your Langflow IDE deployment\n\nYou can modify the Langflow IDE Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-ide/values.yaml) file to customize your deployment.\nThe following sections describe some common modifications.\n\nIf you need to set secrets, Kubernetes secrets are recommended.\n\n### Deploy a different Langflow version\n\nThe Langflow IDE Helm chart deploys the latest Langflow version by default.\n\nTo specify a different Langflow version, set the `langflow.backend.image.tag` and `langflow.frontend.image.tag` values to your preferred version.\nFor example:\n\n```yaml\nlangflow:\n  backend:\n    image:\n      tag: \"1.0.0a59\"\n  frontend:\n    image:\n      tag: \"1.0.0a59\"\n```\n\n### Use external storage for the Langflow database\n\nThe Langflow IDE Helm chart uses the default Langflow database configuration, specifically a SQLite database stored in a local persistent disk.\n\nIf you want to use an [external PostgreSQL database](/configuration-custom-database), use `postgresql` chart or `externalDatabase` to configure the database connection in `values.yaml`.\n\n</Tabs>\n\n### Configure scaling\n\nTo configure scaling for the Langflow IDE Helm chart deployment, you must set `replicaCount` (horizontal scaling) and `resources` (vertical scaling) for both the `langflow.backend` and `langflow.frontend`.\n\nIf your flows rely on a shared state, such as [built-in chat memory](/memory), you must also set up a shared database when scaling horizontally.\n\n```yaml\nlangflow:\n  backend:\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 0.5\n        memory: 1Gi\n      # limits:\n      #   cpu: 0.5\n      #   memory: 1Gi\n\n  frontend:\n    enabled: true\n    replicaCount: 1\n    resources:\n      requests:\n        cpu: 0.3\n        memory: 512Mi\n      # limits:\n      #   cpu: 0.3\n      #   memory: 512Mi\n```\n\n## See also\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)",
      "summary": "The [Langflow integrated development environment (IDE) Helm chart](https://github.com/langflow-ai/langflow-helm-charts/tree/main/charts/langflow-ide) is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the Langflow API and visual...",
      "keywords": [
        "access",
        "chart",
        "cluster",
        "configure",
        "database",
        "deploy",
        "deployment",
        "development",
        "different",
        "environment",
        "external",
        "helm",
        "ide",
        "install",
        "kubernetes",
        "langflow",
        "modify",
        "prepare",
        "prerequisites",
        "scaling"
      ]
    },
    "/deployment-kubernetes-prod": {
      "title": "Deploy the Langflow production environment on Kubernetes",
      "slug": "/deployment-kubernetes-prod",
      "category": "deployment",
      "content": "The [Langflow runtime Helm chart](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation, and security to ensure that applications run reliably and efficiently.\n\n**Note:** For security reasons, the default Langflow runtime Helm chart sets [`readOnlyRootFilesystem: true`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml#L46). This setting prevents modifications to the container's root filesystem at runtime, which is a recommended security measure in production environments.\n\nIf `readOnlyRootFilesystem` is disabled (`false`), it degrades your deployment's security posture. Only disable this setting if you understand the security implications and you have implemented other security measures.\n\nFor more information, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).\n\n## Prerequisites\n\n- A [Kubernetes](https://kubernetes.io/docs/setup/) server\n- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n- [Helm](https://helm.sh/docs/intro/install/)\n\n## Install the Langflow runtime Helm chart\n\n1. Add the repository to Helm:\n\n    ```shell\n    helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts\n    helm repo update\n    ```\n\n2. Install the Langflow app with the default options in the `langflow` namespace.\n\n    \n    \n    </Tabs>\n\n3. Check the status of the pods:\n\n    ```shell\n    kubectl get pods -n langflow\n    ```\n\n## Access the Langflow runtime\n\n1. Get your service name:\n\n    ```shell\n    kubectl get svc -n langflow\n    ```\n\n    The service name is your release name suffixed by `-langflow-runtime`. For example, if you used `helm install my-langflow-app-with-flow`, then the service name is `my-langflow-app-with-flow-langflow-runtime`.\n\n2. Enable port forwarding to access Langflow from your local machine:\n\n    ```shell\n    kubectl port-forward -n langflow svc/my-langflow-app-with-flow-langflow-runtime 7860:7860\n    ```\n\n3. Confirm you can access the API by calling `http://localhost:7860/api/v1/flows/`:\n\n    ```shell\n    curl -v http://localhost:7860/api/v1/flows/\n    ```\n\n    A successful request returns a list of flows.\n\n4. Run a packaged flow.\nThe following example gets the first flow ID from the flows list, and then runs the flow:\n\n    ```shell\n    # Get flow ID\n    id=$(curl -s \"http://localhost:7860/api/v1/flows/\" | jq -r '.[0].id')\n\n    # Run flow\n    curl -X POST \\\n        \"http://localhost:7860/api/v1/run/$id?stream=false\" \\\n        -H 'Content-Type: application/json' \\\n        -d '{\n          \"input_value\": \"Hello!\",\n          \"output_type\": \"chat\",\n          \"input_type\": \"chat\"\n        }'\n    ```\n\n## Configure secrets and environment variables\n\nUse the `.env` section of the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to define environment variables for your Langflow deployment.\nThis includes built-in [Langflow environment variables](/environment-variables), as well as [global variables](/configuration-global-variables) used by your flows.\n\nLangflow can source global variables from your runtime environment, such as Kubernetes secrets referenced in `values.yaml`.\nFor example, the Langflow runtime Helm chart's [example flow JSON](https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json) uses a global variable that is a secret.\nIf you want to run this flow in your Langflow deployment on Kubernetes, you need to include the secret in your runtime configuration.\n\n**Note:** When you export flows as JSON files, it's recommended to omit secrets.\nWhether or not a secret is included depends on how you declare the secret in your flow and whether you use the **Save with my API keys** option.\nFor more information, see [Import and export flows](/concepts-flows-import).\n\n### Set secrets\n\nKubernetes secrets are the recommended way to store sensitive values and credentials.\n\nUse `secretKeyRef` to reference a Kubernetes secret in `values.yaml`:\n\n```yaml\nenv:\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: openai-credentials\n        key: openai-key\n```\n\n<details>\n<summary>Create and set secrets with `kubectl` and `helm`</summary>\n\nYou can use `kubectl` and `helm` commands to create and set secrets:\n\n1. Create a secret:\n\n    ```shell\n    kubectl create secret generic openai-credentials \\\n      --namespace langflow \\\n      --from-literal=OPENAI_API_KEY=sk...\n    ```\n\n2. Verify the secret exists:\n\n    ```shell\n    kubectl get secrets -n langflow openai-credentials\n    ```\n\n    The result is encrypted.\n\n3. Upgrade the Helm release to use the secret:\n\n    ```shell\n    helm upgrade my-langflow-app-image langflow/langflow-runtime -n langflow \\\n      --reuse-values \\\n      --set \"extraEnv[0].name=OPENAI_API_KEY\" \\\n      --set \"extraEnv[0].valueFrom.secretKeyRef.name=openai-credentials\" \\\n      --set \"extraEnv[0].valueFrom.secretKeyRef.key=OPENAI_API_KEY\"\n    ```\n\n    Escape square brackets if required by your shell.\n\n</details>\n\n### Set the log level and other configuration variables\n\nFor non-sensitive variables, such as `LANGFLOW_LOG_LEVEL`, you can set the value directly in `values.yaml`:\n\n```yaml\nenv:\n  - name: LANGFLOW_LOG_LEVEL\n    value: \"INFO\"\n```\n\n## Configure scaling\n\nUse `replicaCount` and `resources` in the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to configure scaling:\n\n* **Horizontal scaling**: Use `replicaCount` to set the number of replicas for your Langflow deployment.\n\n    ```yaml\n    replicaCount: 3\n    ```\n\n* **Vertical scaling**: Use the `resources` section to adjust pod resources depending on your application's needs.\n\n    ```yaml\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n    ```\n\n## See also\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)",
      "summary": "The [Langflow runtime Helm chart](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation, and security to ensure that applications run reliably and...",
      "keywords": [
        "access",
        "api",
        "chart",
        "configuration",
        "configure",
        "deploy",
        "environment",
        "helm",
        "horizontal",
        "install",
        "keys",
        "kubernetes",
        "langflow",
        "level",
        "log",
        "note",
        "prerequisites",
        "production",
        "runtime",
        "save"
      ]
    },
    "deployment-kubernetes-prod": {
      "title": "Deploy the Langflow production environment on Kubernetes",
      "slug": "/deployment-kubernetes-prod",
      "category": "deployment",
      "content": "The [Langflow runtime Helm chart](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation, and security to ensure that applications run reliably and efficiently.\n\n**Note:** For security reasons, the default Langflow runtime Helm chart sets [`readOnlyRootFilesystem: true`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml#L46). This setting prevents modifications to the container's root filesystem at runtime, which is a recommended security measure in production environments.\n\nIf `readOnlyRootFilesystem` is disabled (`false`), it degrades your deployment's security posture. Only disable this setting if you understand the security implications and you have implemented other security measures.\n\nFor more information, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).\n\n## Prerequisites\n\n- A [Kubernetes](https://kubernetes.io/docs/setup/) server\n- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n- [Helm](https://helm.sh/docs/intro/install/)\n\n## Install the Langflow runtime Helm chart\n\n1. Add the repository to Helm:\n\n    ```shell\n    helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts\n    helm repo update\n    ```\n\n2. Install the Langflow app with the default options in the `langflow` namespace.\n\n    \n    \n    </Tabs>\n\n3. Check the status of the pods:\n\n    ```shell\n    kubectl get pods -n langflow\n    ```\n\n## Access the Langflow runtime\n\n1. Get your service name:\n\n    ```shell\n    kubectl get svc -n langflow\n    ```\n\n    The service name is your release name suffixed by `-langflow-runtime`. For example, if you used `helm install my-langflow-app-with-flow`, then the service name is `my-langflow-app-with-flow-langflow-runtime`.\n\n2. Enable port forwarding to access Langflow from your local machine:\n\n    ```shell\n    kubectl port-forward -n langflow svc/my-langflow-app-with-flow-langflow-runtime 7860:7860\n    ```\n\n3. Confirm you can access the API by calling `http://localhost:7860/api/v1/flows/`:\n\n    ```shell\n    curl -v http://localhost:7860/api/v1/flows/\n    ```\n\n    A successful request returns a list of flows.\n\n4. Run a packaged flow.\nThe following example gets the first flow ID from the flows list, and then runs the flow:\n\n    ```shell\n    # Get flow ID\n    id=$(curl -s \"http://localhost:7860/api/v1/flows/\" | jq -r '.[0].id')\n\n    # Run flow\n    curl -X POST \\\n        \"http://localhost:7860/api/v1/run/$id?stream=false\" \\\n        -H 'Content-Type: application/json' \\\n        -d '{\n          \"input_value\": \"Hello!\",\n          \"output_type\": \"chat\",\n          \"input_type\": \"chat\"\n        }'\n    ```\n\n## Configure secrets and environment variables\n\nUse the `.env` section of the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to define environment variables for your Langflow deployment.\nThis includes built-in [Langflow environment variables](/environment-variables), as well as [global variables](/configuration-global-variables) used by your flows.\n\nLangflow can source global variables from your runtime environment, such as Kubernetes secrets referenced in `values.yaml`.\nFor example, the Langflow runtime Helm chart's [example flow JSON](https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json) uses a global variable that is a secret.\nIf you want to run this flow in your Langflow deployment on Kubernetes, you need to include the secret in your runtime configuration.\n\n**Note:** When you export flows as JSON files, it's recommended to omit secrets.\nWhether or not a secret is included depends on how you declare the secret in your flow and whether you use the **Save with my API keys** option.\nFor more information, see [Import and export flows](/concepts-flows-import).\n\n### Set secrets\n\nKubernetes secrets are the recommended way to store sensitive values and credentials.\n\nUse `secretKeyRef` to reference a Kubernetes secret in `values.yaml`:\n\n```yaml\nenv:\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: openai-credentials\n        key: openai-key\n```\n\n<details>\n<summary>Create and set secrets with `kubectl` and `helm`</summary>\n\nYou can use `kubectl` and `helm` commands to create and set secrets:\n\n1. Create a secret:\n\n    ```shell\n    kubectl create secret generic openai-credentials \\\n      --namespace langflow \\\n      --from-literal=OPENAI_API_KEY=sk...\n    ```\n\n2. Verify the secret exists:\n\n    ```shell\n    kubectl get secrets -n langflow openai-credentials\n    ```\n\n    The result is encrypted.\n\n3. Upgrade the Helm release to use the secret:\n\n    ```shell\n    helm upgrade my-langflow-app-image langflow/langflow-runtime -n langflow \\\n      --reuse-values \\\n      --set \"extraEnv[0].name=OPENAI_API_KEY\" \\\n      --set \"extraEnv[0].valueFrom.secretKeyRef.name=openai-credentials\" \\\n      --set \"extraEnv[0].valueFrom.secretKeyRef.key=OPENAI_API_KEY\"\n    ```\n\n    Escape square brackets if required by your shell.\n\n</details>\n\n### Set the log level and other configuration variables\n\nFor non-sensitive variables, such as `LANGFLOW_LOG_LEVEL`, you can set the value directly in `values.yaml`:\n\n```yaml\nenv:\n  - name: LANGFLOW_LOG_LEVEL\n    value: \"INFO\"\n```\n\n## Configure scaling\n\nUse `replicaCount` and `resources` in the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to configure scaling:\n\n* **Horizontal scaling**: Use `replicaCount` to set the number of replicas for your Langflow deployment.\n\n    ```yaml\n    replicaCount: 3\n    ```\n\n* **Vertical scaling**: Use the `resources` section to adjust pod resources depending on your application's needs.\n\n    ```yaml\n    resources:\n      requests:\n        memory: \"2Gi\"\n        cpu: \"1000m\"\n    ```\n\n## See also\n\n* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)",
      "summary": "The [Langflow runtime Helm chart](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation, and security to ensure that applications run reliably and...",
      "keywords": [
        "access",
        "api",
        "chart",
        "configuration",
        "configure",
        "deploy",
        "environment",
        "helm",
        "horizontal",
        "install",
        "keys",
        "kubernetes",
        "langflow",
        "level",
        "log",
        "note",
        "prerequisites",
        "production",
        "runtime",
        "save"
      ]
    },
    "/deployment-nginx-ssl": {
      "title": "Deploy Langflow with Nginx and SSL",
      "slug": "/deployment-nginx-ssl",
      "category": "deployment",
      "content": "Deploy Langflow on a Linux-based server using Nginx as a reverse proxy, Let's Encrypt for SSL certificates, and Certbot for automated certificate management.\n\nThis setup encrypts all communications between users and your Langflow server. SSL certificates ensure that sensitive data is protected from eavesdropping and tampering, and the automatic certificate management through Certbot eliminates the complexity of manual SSL configuration.\n\n## Prerequisites\n\n- An Ubuntu or Debian-based Linux server with a dual-core CPU and at least 2 GB of RAM\nThis example uses [Digital Ocean cloud](https://www.digitalocean.com) for hosting. Your deployment may vary.\n- A domain name with external DNS management access\n- A DNS record configured to point your domain to your server's external IP address\nFor example, if your server's IP is `203.0.113.1`, configure your DNS like this:\n    ```bash\n    Type: A\n    Name: langflow.example.com\n    Value: 203.0.113.1\n    ```\n\n## Connect to your server with SSH\n\n1. Create an SSH key to connect to your server remotely.\nFor example:\n\n    ```bash\n    ssh-keygen -t ed25519 -C \"DANA@EXAMPLE.COM\"\n    ```\n\n    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.\n\n2. In your terminal, follow the instructions to create your SSH key pair.\nThis creates both a private and public key.\nTo copy the public key from your terminal, enter the following command:\n\n    ```bash\n    cat ~/Downloads/host-lf.pub | pbcopy\n    ```\n\n3. In your server, add the SSH key you copied in the previous step.\nFor example, if you are using a Digital Ocean cloud server, add this SSH key when the server is created, or with the [Digital Ocean control panel](https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/).\n\n4. To connect to your server with SSH, enter the following command.\n\n    ```bash\n    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS\n    ```\n\n    Replace the following:\n\n    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server\n    * `SERVER_IP_ADDRESS`: Your server's IP address\n\n5. When prompted for a key fingerprint, type `yes`.\nThe terminal output indicates if the connection succeeds or fails.\nThe following response was returned after connecting to a Digital Ocean cloud server.\n    ```bash\n     System information as of Wed Oct  8 21:40:43 UTC 2025\n\n      System load:  0.02              Processes:             103\n      Usage of /:   4.1% of 47.35GB   Users logged in:       1\n      Memory usage: 10%               IPv4 address for eth0: 165.227.176.236\n      Swap usage:   0%                IPv4 address for eth0: 10.17.0.5\n    ```\n\n## Install Langflow on your server\n\nTo install Langflow on your server, do the following:\n\n1. Update system packages.\n    ```bash\n    sudo apt update && sudo apt upgrade -y\n    ```\n\n2. Install Python and pip.\n    ```bash\n    sudo apt install python3 python3-pip python3-venv -y\n    ```\n\n3. Install uv to manage Python packages.\n    Langflow recommends uv for faster installation.\n    ```bash\n    pip install uv\n    ```\n\n4. Create a virtual environment for Langflow.\n    ```bash\n    uv venv langflow-venv\n    source langflow-venv/bin/activate\n    ```\n\n5. Install Langflow using uv.\n    ```bash\n    uv pip install langflow\n    ```\n\n6. Optionally, start Langflow.\n    ```bash\n    uv run langflow run --host 127.0.0.1 --port 7860 &\n    ```\n\n    Test access at your `http://YOUR_PUBLIC_IP:7860`.\n    Since Langflow is running on `localhost`, you should not be successful.\n    In next steps, you will install Nginx as a reverse proxy to handle external access, and Certbot to configure SSL for secure HTTPS access.\n\n## Install Nginx\n\nNginx is a reverse proxy that receives external requests and forwards them to your Langflow server. In addition to SSL termination, Nginx includes features for load balancing and security. For more information, see the [Nginx documentation](https://nginx.org/en/docs/).\n\n1. Install Nginx on your server.\n\n    ```bash\n    sudo apt install nginx -y\n    ```\n\n2. Start and enable Nginx on your server.\n    ```bash\n    sudo systemctl start nginx\n    sudo systemctl enable nginx\n    ```\n\n3. Create an Nginx configuration file.\nReplace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.\n    ```bash\n    sudo nano /etc/nginx/sites-available/DOMAIN_NAME\n    ```\n\n4. Paste the following content to the configuration file you created at `/etc/nginx/sites-available/DOMAIN_NAME`.\n\n    Replace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.\n    ```nginx\n    server {\n        listen 80;\n        server_name DOMAIN_NAME;\n\n        # Increase client body size for file uploads\n        client_max_body_size 100M;\n\n        location / {\n            proxy_pass http://127.0.0.1:7860/;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # WebSocket support for Langflow\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n\n            # Timeout settings for long-running flows\n            proxy_connect_timeout 60s;\n            proxy_send_timeout 60s;\n            proxy_read_timeout 300s;\n\n            # Buffer settings\n            proxy_buffering off;\n            proxy_request_buffering off;\n        }\n    }\n    ```\n    This configuration listens on port 80 for standard HTTP. When you install Certbot in a later step, it will modify this configuration to add port 443 for HTTPS.\n\n5. To enable your site configuration, create a symlink between the `/sites-available` and `/sites-enabled` directories. `/sites-available` stores all site configurations, and `/sites-enabled` stores only active configurations which Nginx reads from. Creating this symlink switches this configuration ON.\n\n    ```bash\n    sudo ln -s /etc/nginx/sites-available/DOMAIN_NAME /etc/nginx/sites-enabled/DOMAIN_NAME\n    ```\n\n6. Check the Nginx configuration file's syntax.\n    ```bash\n    sudo nginx -t\n    ```\n\n7. Restart Nginx.\n    ```bash\n    sudo systemctl restart nginx\n    ```\n\n## Install Certbot and obtain SSL certificates\n\nWhile Nginx handles SSL termination in encrypting and decrypting traffic, Certbot automatically obtains SSL certificates from Let's Encrypt and configures Nginx to use them.\n\nThe Certbot client is [recommended by Let's Encrypt](https://letsencrypt.org/getting-started/#selecting-and-operating-an-acme-client-yourself) for automated certificate management. For more information, see the [Certbot documentation](https://certbot.eff.org/).\n\nInstall the Certbot client on your server to manage certificates, and install the [`python3-certbot-nginx` plugin](https://packages.ubuntu.com/source/jammy/python-certbot-nginx) to allow Certbot to modify your server's Nginx configuration.\n\n1. Install Certbot and the `python3-certbot-nginx` plugin.\n    ```bash\n    sudo apt install certbot python3-certbot-nginx -y\n    ```\n\n2. Obtain the SSL certificate for `DOMAIN_NAME` from Let's Encrypt.\n    ```bash\n    sudo certbot --nginx -d DOMAIN_NAME\n    ```\n\n    This result indicates that Certbot succeeded.\n    ```bash\n    Successfully received certificate.\n    Certificate is saved at: /etc/letsencrypt/live/DOMAIN_NAME/fullchain.pem\n    Key is saved at: /etc/letsencrypt/live/DOMAIN_NAME/privkey.pem\n    ```\n\n    When using `--nginx`, Certbot automatically injects the paths into your Nginx configuration as `ssl_certificate` and `ssl_certificate_key`.\n\n3. Start Langflow from the virtual environment.\n\n    a. Activate the virtual environment.\n    ```bash\n    source langflow-venv/bin/activate\n    ```\n\n    b. Start Langflow in the background.\n    ```bash\n    uv run langflow run --host 127.0.0.1 --port 7860 &\n    ```\n\n4. To test the deployment, navigate to `https://DOMAIN_NAME` in your browser.\n5. Verify the SSL certificate is working. Ensure the URL is `https://`, not `http://`.\nYour browser's address bar should display a  **Lock** icon.\nClick  **Lock** to view your SSL certificate details.\n\n## See also\n\n- [Deploy Langflow on a remote server with Caddy](/deployment-caddyfile)\n- [Deploy Langflow on Docker](/deployment-docker)",
      "summary": "Deploy Langflow on a Linux-based server using Nginx as a reverse proxy, Let's Encrypt for SSL certificates, and Certbot for automated certificate management.",
      "keywords": [
        "certbot",
        "certificates",
        "connect",
        "deploy",
        "install",
        "langflow",
        "lock",
        "nginx",
        "obtain",
        "prerequisites",
        "server",
        "ssh",
        "ssl"
      ]
    },
    "deployment-nginx-ssl": {
      "title": "Deploy Langflow with Nginx and SSL",
      "slug": "/deployment-nginx-ssl",
      "category": "deployment",
      "content": "Deploy Langflow on a Linux-based server using Nginx as a reverse proxy, Let's Encrypt for SSL certificates, and Certbot for automated certificate management.\n\nThis setup encrypts all communications between users and your Langflow server. SSL certificates ensure that sensitive data is protected from eavesdropping and tampering, and the automatic certificate management through Certbot eliminates the complexity of manual SSL configuration.\n\n## Prerequisites\n\n- An Ubuntu or Debian-based Linux server with a dual-core CPU and at least 2 GB of RAM\nThis example uses [Digital Ocean cloud](https://www.digitalocean.com) for hosting. Your deployment may vary.\n- A domain name with external DNS management access\n- A DNS record configured to point your domain to your server's external IP address\nFor example, if your server's IP is `203.0.113.1`, configure your DNS like this:\n    ```bash\n    Type: A\n    Name: langflow.example.com\n    Value: 203.0.113.1\n    ```\n\n## Connect to your server with SSH\n\n1. Create an SSH key to connect to your server remotely.\nFor example:\n\n    ```bash\n    ssh-keygen -t ed25519 -C \"DANA@EXAMPLE.COM\"\n    ```\n\n    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.\n\n2. In your terminal, follow the instructions to create your SSH key pair.\nThis creates both a private and public key.\nTo copy the public key from your terminal, enter the following command:\n\n    ```bash\n    cat ~/Downloads/host-lf.pub | pbcopy\n    ```\n\n3. In your server, add the SSH key you copied in the previous step.\nFor example, if you are using a Digital Ocean cloud server, add this SSH key when the server is created, or with the [Digital Ocean control panel](https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/).\n\n4. To connect to your server with SSH, enter the following command.\n\n    ```bash\n    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS\n    ```\n\n    Replace the following:\n\n    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server\n    * `SERVER_IP_ADDRESS`: Your server's IP address\n\n5. When prompted for a key fingerprint, type `yes`.\nThe terminal output indicates if the connection succeeds or fails.\nThe following response was returned after connecting to a Digital Ocean cloud server.\n    ```bash\n     System information as of Wed Oct  8 21:40:43 UTC 2025\n\n      System load:  0.02              Processes:             103\n      Usage of /:   4.1% of 47.35GB   Users logged in:       1\n      Memory usage: 10%               IPv4 address for eth0: 165.227.176.236\n      Swap usage:   0%                IPv4 address for eth0: 10.17.0.5\n    ```\n\n## Install Langflow on your server\n\nTo install Langflow on your server, do the following:\n\n1. Update system packages.\n    ```bash\n    sudo apt update && sudo apt upgrade -y\n    ```\n\n2. Install Python and pip.\n    ```bash\n    sudo apt install python3 python3-pip python3-venv -y\n    ```\n\n3. Install uv to manage Python packages.\n    Langflow recommends uv for faster installation.\n    ```bash\n    pip install uv\n    ```\n\n4. Create a virtual environment for Langflow.\n    ```bash\n    uv venv langflow-venv\n    source langflow-venv/bin/activate\n    ```\n\n5. Install Langflow using uv.\n    ```bash\n    uv pip install langflow\n    ```\n\n6. Optionally, start Langflow.\n    ```bash\n    uv run langflow run --host 127.0.0.1 --port 7860 &\n    ```\n\n    Test access at your `http://YOUR_PUBLIC_IP:7860`.\n    Since Langflow is running on `localhost`, you should not be successful.\n    In next steps, you will install Nginx as a reverse proxy to handle external access, and Certbot to configure SSL for secure HTTPS access.\n\n## Install Nginx\n\nNginx is a reverse proxy that receives external requests and forwards them to your Langflow server. In addition to SSL termination, Nginx includes features for load balancing and security. For more information, see the [Nginx documentation](https://nginx.org/en/docs/).\n\n1. Install Nginx on your server.\n\n    ```bash\n    sudo apt install nginx -y\n    ```\n\n2. Start and enable Nginx on your server.\n    ```bash\n    sudo systemctl start nginx\n    sudo systemctl enable nginx\n    ```\n\n3. Create an Nginx configuration file.\nReplace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.\n    ```bash\n    sudo nano /etc/nginx/sites-available/DOMAIN_NAME\n    ```\n\n4. Paste the following content to the configuration file you created at `/etc/nginx/sites-available/DOMAIN_NAME`.\n\n    Replace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.\n    ```nginx\n    server {\n        listen 80;\n        server_name DOMAIN_NAME;\n\n        # Increase client body size for file uploads\n        client_max_body_size 100M;\n\n        location / {\n            proxy_pass http://127.0.0.1:7860/;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # WebSocket support for Langflow\n            proxy_http_version 1.1;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n\n            # Timeout settings for long-running flows\n            proxy_connect_timeout 60s;\n            proxy_send_timeout 60s;\n            proxy_read_timeout 300s;\n\n            # Buffer settings\n            proxy_buffering off;\n            proxy_request_buffering off;\n        }\n    }\n    ```\n    This configuration listens on port 80 for standard HTTP. When you install Certbot in a later step, it will modify this configuration to add port 443 for HTTPS.\n\n5. To enable your site configuration, create a symlink between the `/sites-available` and `/sites-enabled` directories. `/sites-available` stores all site configurations, and `/sites-enabled` stores only active configurations which Nginx reads from. Creating this symlink switches this configuration ON.\n\n    ```bash\n    sudo ln -s /etc/nginx/sites-available/DOMAIN_NAME /etc/nginx/sites-enabled/DOMAIN_NAME\n    ```\n\n6. Check the Nginx configuration file's syntax.\n    ```bash\n    sudo nginx -t\n    ```\n\n7. Restart Nginx.\n    ```bash\n    sudo systemctl restart nginx\n    ```\n\n## Install Certbot and obtain SSL certificates\n\nWhile Nginx handles SSL termination in encrypting and decrypting traffic, Certbot automatically obtains SSL certificates from Let's Encrypt and configures Nginx to use them.\n\nThe Certbot client is [recommended by Let's Encrypt](https://letsencrypt.org/getting-started/#selecting-and-operating-an-acme-client-yourself) for automated certificate management. For more information, see the [Certbot documentation](https://certbot.eff.org/).\n\nInstall the Certbot client on your server to manage certificates, and install the [`python3-certbot-nginx` plugin](https://packages.ubuntu.com/source/jammy/python-certbot-nginx) to allow Certbot to modify your server's Nginx configuration.\n\n1. Install Certbot and the `python3-certbot-nginx` plugin.\n    ```bash\n    sudo apt install certbot python3-certbot-nginx -y\n    ```\n\n2. Obtain the SSL certificate for `DOMAIN_NAME` from Let's Encrypt.\n    ```bash\n    sudo certbot --nginx -d DOMAIN_NAME\n    ```\n\n    This result indicates that Certbot succeeded.\n    ```bash\n    Successfully received certificate.\n    Certificate is saved at: /etc/letsencrypt/live/DOMAIN_NAME/fullchain.pem\n    Key is saved at: /etc/letsencrypt/live/DOMAIN_NAME/privkey.pem\n    ```\n\n    When using `--nginx`, Certbot automatically injects the paths into your Nginx configuration as `ssl_certificate` and `ssl_certificate_key`.\n\n3. Start Langflow from the virtual environment.\n\n    a. Activate the virtual environment.\n    ```bash\n    source langflow-venv/bin/activate\n    ```\n\n    b. Start Langflow in the background.\n    ```bash\n    uv run langflow run --host 127.0.0.1 --port 7860 &\n    ```\n\n4. To test the deployment, navigate to `https://DOMAIN_NAME` in your browser.\n5. Verify the SSL certificate is working. Ensure the URL is `https://`, not `http://`.\nYour browser's address bar should display a  **Lock** icon.\nClick  **Lock** to view your SSL certificate details.\n\n## See also\n\n- [Deploy Langflow on a remote server with Caddy](/deployment-caddyfile)\n- [Deploy Langflow on Docker](/deployment-docker)",
      "summary": "Deploy Langflow on a Linux-based server using Nginx as a reverse proxy, Let's Encrypt for SSL certificates, and Certbot for automated certificate management.",
      "keywords": [
        "certbot",
        "certificates",
        "connect",
        "deploy",
        "install",
        "langflow",
        "lock",
        "nginx",
        "obtain",
        "prerequisites",
        "server",
        "ssh",
        "ssl"
      ]
    },
    "/deployment-overview": {
      "title": "Langflow deployment overview",
      "slug": "/deployment-overview",
      "category": "deployment",
      "content": "This section includes the different ways to bring your locally-built flows to the world.\n\n* To self-host your local server through an ngrok gateway, see [Deploy a public Langflow server](/deployment-public-server).\nThis approach uses [ngrok](https://ngrok.com/docs/getting-started/) to forward traffic and share your local Langflow server over the internet, without deploying to a cloud provider or exposing your network directly.\n\n* To build and deploy a Langflow container that includes your flow files, see [Containerize a Langflow application](develop-application).\nThis approach bundles your flows and dependencies into a portable, reproducible Docker image for easy deployment across different environments.\n\n* To deploy a Langflow server on a remote server with Docker and Caddy, see [Deploy Langflow on a remote server](/deployment-caddyfile).\nThis approach is good for hosting your own Langflow instance on a remote server with secure web access, using Docker containers and Caddy as a reverse proxy for HTTPS support.\n\n* To deploy Langflow with Nginx and automatic SSL certificates, see [Deploy Langflow with Nginx and Let's Encrypt](/deployment-nginx-ssl).\nThis approach uses Nginx as a reverse proxy with Let's Encrypt for automatic HTTPS certificate management to provide a secure deployment _without_ Docker.\n\n* To deploy Langflow on Kubernetes, see [Langflow Kubernetes architecture and best practices](/deployment-prod-best-practices)\nThis approach creates production-grade deployments with high availability, scalability, and robust orchestration.\n\n* For cloud provider-specific deployment guides, see your cloud provider's documentation.\nThe Langflow documentation provides a few examples, such as [Google Cloud Platform](/deployment-gcp) and [Hugging Face Spaces](/deployment-hugging-face-spaces), to help you get started.",
      "summary": "This section includes the different ways to bring your locally-built flows to the world.",
      "keywords": [
        "deployment",
        "langflow",
        "overview"
      ]
    },
    "deployment-overview": {
      "title": "Langflow deployment overview",
      "slug": "/deployment-overview",
      "category": "deployment",
      "content": "This section includes the different ways to bring your locally-built flows to the world.\n\n* To self-host your local server through an ngrok gateway, see [Deploy a public Langflow server](/deployment-public-server).\nThis approach uses [ngrok](https://ngrok.com/docs/getting-started/) to forward traffic and share your local Langflow server over the internet, without deploying to a cloud provider or exposing your network directly.\n\n* To build and deploy a Langflow container that includes your flow files, see [Containerize a Langflow application](develop-application).\nThis approach bundles your flows and dependencies into a portable, reproducible Docker image for easy deployment across different environments.\n\n* To deploy a Langflow server on a remote server with Docker and Caddy, see [Deploy Langflow on a remote server](/deployment-caddyfile).\nThis approach is good for hosting your own Langflow instance on a remote server with secure web access, using Docker containers and Caddy as a reverse proxy for HTTPS support.\n\n* To deploy Langflow with Nginx and automatic SSL certificates, see [Deploy Langflow with Nginx and Let's Encrypt](/deployment-nginx-ssl).\nThis approach uses Nginx as a reverse proxy with Let's Encrypt for automatic HTTPS certificate management to provide a secure deployment _without_ Docker.\n\n* To deploy Langflow on Kubernetes, see [Langflow Kubernetes architecture and best practices](/deployment-prod-best-practices)\nThis approach creates production-grade deployments with high availability, scalability, and robust orchestration.\n\n* For cloud provider-specific deployment guides, see your cloud provider's documentation.\nThe Langflow documentation provides a few examples, such as [Google Cloud Platform](/deployment-gcp) and [Hugging Face Spaces](/deployment-hugging-face-spaces), to help you get started.",
      "summary": "This section includes the different ways to bring your locally-built flows to the world.",
      "keywords": [
        "deployment",
        "langflow",
        "overview"
      ]
    },
    "/deployment-prod-best-practices": {
      "title": "Best practices for Langflow on Kubernetes",
      "slug": "/deployment-prod-best-practices",
      "category": "deployment",
      "content": "This guide provides best practices for deploying Langflow in production environments on Kubernetes.\n\n## Resources and scaling\n\nLangflow's minimum resource requirements vary by deployment type:\n\n* **IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.\n\n    The frontend service requires a minimum of 512Mi RAM and 0.3 CPU per instance with 1 replica.\n\n    The backend service requires a minimum of 1Gi RAM and 0.5 CPU per instance with 1 replica.\n\n* **Runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor.\n\n    Minimum requirements include 2Gi RAM and 1000m (1 CPU) per instance with 3 replicas.\n\nFor more information about Langflow deployment types, see [Langflow architecture on Kubernetes](/deployment-architecture).\n\n### Estimate, test, and adjust\n\nStart with the minimum recommended resources and replicas, then monitor and scale as needed based on your deployment's requirements and performance testing.\nConsider the following factors in your resource estimation and performance testing:\n\n* Flow complexity.\n* Volume of concurrent users and requests.\n\n   For IDE (development) deployments, consider that frontend activity also pings the backend service, so you typically need to scale both the frontend and backend together.\n\n* Request payload content and size, particularly for file uploads in production deployments.\n* Storage requirements for cache, file management, and the Langflow database.\n\n    An [external PostgreSQL database](#use-an-external-postgresql-database) is recommended for production deployments.\n\n* Infrastructure options that might require more resources, such as multi-core CPUs.\n\n### Use an external PostgreSQL database\n\nAn external PostgreSQL database is recommended for production deployments to improve scalability and reliability as compared to the default SQLite database.\n\nYour resource allocation and replication strategy must be able to support the PostgreSQL service and storage.\nFor example, for a runtime (production) deployment, you might allocate 4Gi RAM, 2 CPU, and multiple replicas for high availability.\nTune PostgreSQL parameters, such as `work_mem` and `shared_buffers`, as needed based on resource requirements and usage metrics.\n\nRecommended configurations include:\n\n* Persistent storage to prevent data loss on container shutdown\n* High Availability (HA) or Active-Active for automatic failover, scaling, and load balancing\n* Shared databases for multi-instance deployments\n* Shared storage, such as NFS or cloud storage, for multi-instance deployments to access large files stored on disk, such as in `/opt/langflow/data/`.\n\nFor more information, see [Configure an external PostgreSQL database](/configuration-custom-database) and [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n\n### Use HPA for dynamic scaling\n\nLoad balancing and dynamic scaling are recommended for runtime (production) deployments.\n\nFor example, consider using Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically scale based on CPU or memory usage.\nThe following example shows a Langflow HPA configuration with CPU-based scaling:\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: langflow-runtime-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: langflow-runtime\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 80\n```\n\n## Failure points\n\nLangflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability:\n\n* **Database failures**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n* **File system failures**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\nTo avoid this, use a shared, POSIX-compliant file system or cloud storage.\nUse persistent volumes instead of ramdisk solutions that result in data loss on container shutdown.\n* **Instance failures**: Deploy multiple replicas to avoid service disruptions when a single instance fails. Use health checks to detect and replace failed pods.\n* **Network and dependency failures**: External APIs or services used in flows may fail, causing flow errors. Implement retry logic and error handling in flows or application code. Monitor network latency and dependency health.\n\n## Monitoring\n\nEffective monitoring ensures Langflow operates reliably and performs well under varying loads:\n\n* **Database monitoring**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n* **Application logs**: Collect and analyze logs for errors, warnings, and flow execution issues. Centralize logs using tools like ELK Stack or Fluentd. You can also inspect [Langflow logs](/logging).\n* **Resource usage**: Track CPU, memory, and disk usage of Langflow instances. Use Prometheus and Grafana for real-time metrics collection and monitoring in Kubernetes.\n\n    To expose your Langflow server's Prometheus metrics, set `LANGFLOW_PROMETHEUS_ENABLED=True` (the default is `false`).\n    The default port for the Prometheus metrics is 9090.\n    To change the port, set `LANGFLOW_PROMETHEUS_PORT`.\n\n* **API performance**: Monitor response times, error rates, and request throughput. Set alerts for high latency or error spikes.\n* **Observability tools**: Integrate with [LangWatch](/integrations-langwatch) or [Opik](/integrations-opik) for detailed flow tracing and metrics. Use these tools to debug flow performance and optimize execution.\n\n## Security\n\nRunning Langflow in production requires robust security measures to protect the application, data, and users.\nFollow industry best practices and use secure Langflow configurations, such as the following:\n\n* **Container security**: Apply security best practices for containerized applications. For example, set `readOnlyRootFilesystem: true` in runtime (production) containers to prevent unauthorized modifications. Restrict access to files and codebases containing sensitive data and configuration files that shouldn't be exposed to unauthorized users.\n* **Secrets management**: Store sensitive data like API keys and PostgreSQL credentials in Kubernetes secrets or external secrets managers like HashiCorp Vault.\n* **Authentication, authorization, and access control**: Start your Langflow server with authentication enabled, as explained in [API keys and authentication](/api-keys-and-authentication). Restrict network and resource access with firewalls, network policies, network security groups, or VPCs. For example, restrict PostgreSQL database access to Langflow instances.\n* **Encryption and privacy**: Follow industry best practices and legal requirements for data privacy and encryption of data in transit and at rest, including GDPR requirements, HTTPS, TLS, and SSL. For example, configure PostgreSQL with valid SSL certificates and append `?sslmode=require` or `?sslmode=verify-full` to the connection string to enable SSL for database connections.\n* **Security posture maintenance**: Conduct regular security audits, keep current with software updates, and monitor for suspicious activity using intrusion detection systems.\n\n## See also\n\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)\n* [Langflow environment variables](/environment-variables)",
      "summary": "This guide provides best practices for deploying Langflow in production environments on Kubernetes.",
      "keywords": [
        "access",
        "adjust",
        "api",
        "application",
        "authentication",
        "authorization",
        "best",
        "container",
        "control",
        "database",
        "dependency",
        "development",
        "dynamic",
        "encryption",
        "estimate",
        "external",
        "failure",
        "failures",
        "file",
        "hpa"
      ]
    },
    "deployment-prod-best-practices": {
      "title": "Best practices for Langflow on Kubernetes",
      "slug": "/deployment-prod-best-practices",
      "category": "deployment",
      "content": "This guide provides best practices for deploying Langflow in production environments on Kubernetes.\n\n## Resources and scaling\n\nLangflow's minimum resource requirements vary by deployment type:\n\n* **IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.\n\n    The frontend service requires a minimum of 512Mi RAM and 0.3 CPU per instance with 1 replica.\n\n    The backend service requires a minimum of 1Gi RAM and 0.5 CPU per instance with 1 replica.\n\n* **Runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor.\n\n    Minimum requirements include 2Gi RAM and 1000m (1 CPU) per instance with 3 replicas.\n\nFor more information about Langflow deployment types, see [Langflow architecture on Kubernetes](/deployment-architecture).\n\n### Estimate, test, and adjust\n\nStart with the minimum recommended resources and replicas, then monitor and scale as needed based on your deployment's requirements and performance testing.\nConsider the following factors in your resource estimation and performance testing:\n\n* Flow complexity.\n* Volume of concurrent users and requests.\n\n   For IDE (development) deployments, consider that frontend activity also pings the backend service, so you typically need to scale both the frontend and backend together.\n\n* Request payload content and size, particularly for file uploads in production deployments.\n* Storage requirements for cache, file management, and the Langflow database.\n\n    An [external PostgreSQL database](#use-an-external-postgresql-database) is recommended for production deployments.\n\n* Infrastructure options that might require more resources, such as multi-core CPUs.\n\n### Use an external PostgreSQL database\n\nAn external PostgreSQL database is recommended for production deployments to improve scalability and reliability as compared to the default SQLite database.\n\nYour resource allocation and replication strategy must be able to support the PostgreSQL service and storage.\nFor example, for a runtime (production) deployment, you might allocate 4Gi RAM, 2 CPU, and multiple replicas for high availability.\nTune PostgreSQL parameters, such as `work_mem` and `shared_buffers`, as needed based on resource requirements and usage metrics.\n\nRecommended configurations include:\n\n* Persistent storage to prevent data loss on container shutdown\n* High Availability (HA) or Active-Active for automatic failover, scaling, and load balancing\n* Shared databases for multi-instance deployments\n* Shared storage, such as NFS or cloud storage, for multi-instance deployments to access large files stored on disk, such as in `/opt/langflow/data/`.\n\nFor more information, see [Configure an external PostgreSQL database](/configuration-custom-database) and [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n\n### Use HPA for dynamic scaling\n\nLoad balancing and dynamic scaling are recommended for runtime (production) deployments.\n\nFor example, consider using Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically scale based on CPU or memory usage.\nThe following example shows a Langflow HPA configuration with CPU-based scaling:\n\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: langflow-runtime-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: langflow-runtime\n  minReplicas: 1\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 80\n```\n\n## Failure points\n\nLangflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability:\n\n* **Database failures**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n* **File system failures**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.\nTo avoid this, use a shared, POSIX-compliant file system or cloud storage.\nUse persistent volumes instead of ramdisk solutions that result in data loss on container shutdown.\n* **Instance failures**: Deploy multiple replicas to avoid service disruptions when a single instance fails. Use health checks to detect and replace failed pods.\n* **Network and dependency failures**: External APIs or services used in flows may fail, causing flow errors. Implement retry logic and error handling in flows or application code. Monitor network latency and dependency health.\n\n## Monitoring\n\nEffective monitoring ensures Langflow operates reliably and performs well under varying loads:\n\n* **Database monitoring**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).\n* **Application logs**: Collect and analyze logs for errors, warnings, and flow execution issues. Centralize logs using tools like ELK Stack or Fluentd. You can also inspect [Langflow logs](/logging).\n* **Resource usage**: Track CPU, memory, and disk usage of Langflow instances. Use Prometheus and Grafana for real-time metrics collection and monitoring in Kubernetes.\n\n    To expose your Langflow server's Prometheus metrics, set `LANGFLOW_PROMETHEUS_ENABLED=True` (the default is `false`).\n    The default port for the Prometheus metrics is 9090.\n    To change the port, set `LANGFLOW_PROMETHEUS_PORT`.\n\n* **API performance**: Monitor response times, error rates, and request throughput. Set alerts for high latency or error spikes.\n* **Observability tools**: Integrate with [LangWatch](/integrations-langwatch) or [Opik](/integrations-opik) for detailed flow tracing and metrics. Use these tools to debug flow performance and optimize execution.\n\n## Security\n\nRunning Langflow in production requires robust security measures to protect the application, data, and users.\nFollow industry best practices and use secure Langflow configurations, such as the following:\n\n* **Container security**: Apply security best practices for containerized applications. For example, set `readOnlyRootFilesystem: true` in runtime (production) containers to prevent unauthorized modifications. Restrict access to files and codebases containing sensitive data and configuration files that shouldn't be exposed to unauthorized users.\n* **Secrets management**: Store sensitive data like API keys and PostgreSQL credentials in Kubernetes secrets or external secrets managers like HashiCorp Vault.\n* **Authentication, authorization, and access control**: Start your Langflow server with authentication enabled, as explained in [API keys and authentication](/api-keys-and-authentication). Restrict network and resource access with firewalls, network policies, network security groups, or VPCs. For example, restrict PostgreSQL database access to Langflow instances.\n* **Encryption and privacy**: Follow industry best practices and legal requirements for data privacy and encryption of data in transit and at rest, including GDPR requirements, HTTPS, TLS, and SSL. For example, configure PostgreSQL with valid SSL certificates and append `?sslmode=require` or `?sslmode=verify-full` to the connection string to enable SSL for database connections.\n* **Security posture maintenance**: Conduct regular security audits, keep current with software updates, and monitor for suspicious activity using intrusion detection systems.\n\n## See also\n\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)\n* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)\n* [Langflow environment variables](/environment-variables)",
      "summary": "This guide provides best practices for deploying Langflow in production environments on Kubernetes.",
      "keywords": [
        "access",
        "adjust",
        "api",
        "application",
        "authentication",
        "authorization",
        "best",
        "container",
        "control",
        "database",
        "dependency",
        "development",
        "dynamic",
        "encryption",
        "estimate",
        "external",
        "failure",
        "failures",
        "file",
        "hpa"
      ]
    },
    "/deployment-public-server": {
      "title": "Deploy a public Langflow server",
      "slug": "/deployment-public-server",
      "category": "deployment",
      "content": "By default, your Langflow server at `http://localhost:7860` isn't exposed to the public internet.\nHowever, you can forward Langflow server traffic with a forwarding platform like [ngrok](https://ngrok.com/docs/getting-started/) or [zrok](https://docs.zrok.io/docs/getting-started) to make your server public.\n\nWhen your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).\n\n## Prerequisites\n\nOn the machine where you plan to host your Langflow installation, [install Langflow](/get-started-installation) and a reverse proxy or forwarding service.\n\nThis guide uses ngrok, but you can use any similar reverse proxy or forwarding platform.\n\nIf you want to follow along with this guide, [install ngrok](https://ngrok.com/docs/getting-started/#1-install-ngrok) and [create an ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken).\n\n## Expose your Langflow server with ngrok\n\n1. Start Langflow:\n\n    ```bash\n    uv run langflow run\n    ```\n\n2. In another terminal window, use your ngrok authtoken to authenticate your local ngrok server:\n\n    ```bash\n    ngrok config add-authtoken NGROK_AUTHTOKEN\n    ```\n\n3. Use ngrok to expose your Langflow server to the public internet:\n\n    ```bash\n    ngrok http http://localhost:7860\n    ```\n\n    This example assumes that you use the default Langflow listening address at `http://localhost:7860`. If you have a different listening address, you must modify this command accordingly.\n\n    The ngrok session starts in your terminal and deploys an ephemeral domain with no authentication.\n    To add authentication or deploy a static domain, see the [ngrok documentation](https://ngrok.com/docs/).\n\n    The `Forwarding` line prints the forwarding address for your Langflow server:\n\n    ```\n    Forwarding https://94b1-76-64-171-14.ngrok-free.app -> http://localhost:7860\n    ```\n\n    The forwarding address acts as a reverse proxy for your Langflow server, and ngrok forwards your local traffic to this domain.\n\n4. To verify that your Langflow server is publicly available, navigate to the forwarding address URL, such as `https://94b1-76-64-171-14.ngrok-free.app`.\n\n## Use a public Langflow server\n\nWhen your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).\n\n### Deploy your MCP server externally\n\nAfter you deploy a public Langflow server, you can also access your Langflow projects' MCP servers publicly.\n\nTo do this, use your server's forwarding address when you [connect a client to a Langflow MCP server](/mcp-server#connect-clients-to-use-the-servers-actions).\n\n### Serve API requests\n\nTo send requests to a public Langflow server's [Langflow API](/api-reference-api-examples) endpoints, use the server's domain as the [base URL](/api-reference-api-examples#base-url) for your API requests.\nFor example:\n\n```bash\ncurl -X POST \\\n  \"PUBLIC_SERVER_DOMAIN/api/v1/webhook/FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: LANGFLOW_API_KEY\" \\\n  -d '{\"data\": \"example-data\"}'\n```\n\n**Note:** When you create flows on public Langflow servers, the code snippets generated in the [**API access** pane](/concepts-publish) automatically use your public server's domain.\n\nYou also use your public domain when making Langflow API calls in scripts, including the code snippets that are automatically generated by Langflow.\nFor example, the following code snippet calls an ngrok domain to trigger the specified flow (`d764c4b8...`):\n\n    ```python\n    import requests\n\n    url = \"https://3f7c-73-64-93-151.ngrok-free.app/api/v1/run/d764c4b8-5cec-4c0f-9de0-4b419b11901a\"  # The complete API endpoint URL for this flow\n\n    # Request payload configuration\n    payload = {\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"input_value\": \"Hello\"\n    }\n\n    # Request headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": \"LANGFLOW_API_KEY\"\n    }\n\n    try:\n        # Send API request\n        response = requests.request(\"POST\", url, json=payload, headers=headers)\n        response.raise_for_status()  # Raise exception for bad status codes\n\n        # Print response\n        print(response.text)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error making API request: {e}\")\n    except ValueError as e:\n        print(f\"Error parsing response: {e}\")\n    ```\n\nFor a demo of the Langflow API in a script, see the [Quickstart](/get-started-quickstart).\n\n### Share a flow's Playground\n\nAfter you deploy a public Langflow server, you can use the **Shareable Playground** option to make a flow's **Playground** available at a public URL.\nIf a user accesses this URL, they can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.\n\nFor more information, see [Share a flow's Playground](/concepts-playground#share-a-flows-playground).",
      "summary": "By default, your Langflow server at `http://localhost:7860` isn't exposed to the public internet.\nHowever, you can forward Langflow server traffic with a forwarding platform like [ngrok](https://ngrok.com/docs/getting-started/) or [zrok](https://docs.zrok.io/docs/getting-started) to make your...",
      "keywords": [
        "access",
        "api",
        "deploy",
        "expose",
        "externally",
        "flow",
        "langflow",
        "mcp",
        "ngrok",
        "note",
        "playground",
        "prerequisites",
        "public",
        "requests",
        "serve",
        "server",
        "share",
        "shareable"
      ]
    },
    "deployment-public-server": {
      "title": "Deploy a public Langflow server",
      "slug": "/deployment-public-server",
      "category": "deployment",
      "content": "By default, your Langflow server at `http://localhost:7860` isn't exposed to the public internet.\nHowever, you can forward Langflow server traffic with a forwarding platform like [ngrok](https://ngrok.com/docs/getting-started/) or [zrok](https://docs.zrok.io/docs/getting-started) to make your server public.\n\nWhen your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).\n\n## Prerequisites\n\nOn the machine where you plan to host your Langflow installation, [install Langflow](/get-started-installation) and a reverse proxy or forwarding service.\n\nThis guide uses ngrok, but you can use any similar reverse proxy or forwarding platform.\n\nIf you want to follow along with this guide, [install ngrok](https://ngrok.com/docs/getting-started/#1-install-ngrok) and [create an ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken).\n\n## Expose your Langflow server with ngrok\n\n1. Start Langflow:\n\n    ```bash\n    uv run langflow run\n    ```\n\n2. In another terminal window, use your ngrok authtoken to authenticate your local ngrok server:\n\n    ```bash\n    ngrok config add-authtoken NGROK_AUTHTOKEN\n    ```\n\n3. Use ngrok to expose your Langflow server to the public internet:\n\n    ```bash\n    ngrok http http://localhost:7860\n    ```\n\n    This example assumes that you use the default Langflow listening address at `http://localhost:7860`. If you have a different listening address, you must modify this command accordingly.\n\n    The ngrok session starts in your terminal and deploys an ephemeral domain with no authentication.\n    To add authentication or deploy a static domain, see the [ngrok documentation](https://ngrok.com/docs/).\n\n    The `Forwarding` line prints the forwarding address for your Langflow server:\n\n    ```\n    Forwarding https://94b1-76-64-171-14.ngrok-free.app -> http://localhost:7860\n    ```\n\n    The forwarding address acts as a reverse proxy for your Langflow server, and ngrok forwards your local traffic to this domain.\n\n4. To verify that your Langflow server is publicly available, navigate to the forwarding address URL, such as `https://94b1-76-64-171-14.ngrok-free.app`.\n\n## Use a public Langflow server\n\nWhen your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).\n\n### Deploy your MCP server externally\n\nAfter you deploy a public Langflow server, you can also access your Langflow projects' MCP servers publicly.\n\nTo do this, use your server's forwarding address when you [connect a client to a Langflow MCP server](/mcp-server#connect-clients-to-use-the-servers-actions).\n\n### Serve API requests\n\nTo send requests to a public Langflow server's [Langflow API](/api-reference-api-examples) endpoints, use the server's domain as the [base URL](/api-reference-api-examples#base-url) for your API requests.\nFor example:\n\n```bash\ncurl -X POST \\\n  \"PUBLIC_SERVER_DOMAIN/api/v1/webhook/FLOW_ID\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"x-api-key: LANGFLOW_API_KEY\" \\\n  -d '{\"data\": \"example-data\"}'\n```\n\n**Note:** When you create flows on public Langflow servers, the code snippets generated in the [**API access** pane](/concepts-publish) automatically use your public server's domain.\n\nYou also use your public domain when making Langflow API calls in scripts, including the code snippets that are automatically generated by Langflow.\nFor example, the following code snippet calls an ngrok domain to trigger the specified flow (`d764c4b8...`):\n\n    ```python\n    import requests\n\n    url = \"https://3f7c-73-64-93-151.ngrok-free.app/api/v1/run/d764c4b8-5cec-4c0f-9de0-4b419b11901a\"  # The complete API endpoint URL for this flow\n\n    # Request payload configuration\n    payload = {\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"input_value\": \"Hello\"\n    }\n\n    # Request headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": \"LANGFLOW_API_KEY\"\n    }\n\n    try:\n        # Send API request\n        response = requests.request(\"POST\", url, json=payload, headers=headers)\n        response.raise_for_status()  # Raise exception for bad status codes\n\n        # Print response\n        print(response.text)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error making API request: {e}\")\n    except ValueError as e:\n        print(f\"Error parsing response: {e}\")\n    ```\n\nFor a demo of the Langflow API in a script, see the [Quickstart](/get-started-quickstart).\n\n### Share a flow's Playground\n\nAfter you deploy a public Langflow server, you can use the **Shareable Playground** option to make a flow's **Playground** available at a public URL.\nIf a user accesses this URL, they can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.\n\nFor more information, see [Share a flow's Playground](/concepts-playground#share-a-flows-playground).",
      "summary": "By default, your Langflow server at `http://localhost:7860` isn't exposed to the public internet.\nHowever, you can forward Langflow server traffic with a forwarding platform like [ngrok](https://ngrok.com/docs/getting-started/) or [zrok](https://docs.zrok.io/docs/getting-started) to make your...",
      "keywords": [
        "access",
        "api",
        "deploy",
        "expose",
        "externally",
        "flow",
        "langflow",
        "mcp",
        "ngrok",
        "note",
        "playground",
        "prerequisites",
        "public",
        "requests",
        "serve",
        "server",
        "share",
        "shareable"
      ]
    },
    "/deployment-railway": {
      "title": "Deploy Langflow on Railway",
      "slug": "/deployment-railway",
      "category": "deployment",
      "content": "This guide explains how to [deploy Langflow on Railway](https://railway.com/?utm_medium=integration&utm_source=docs&utm_campaign=langflow), a cloud infrastructure platform that provides auto-deploy, managed databases, and automatic scaling.\n\n1. Create a Railway account.\n\n   A Hobby account on Railway is sufficient for Langflow's dual-core CPU and 2 GB RAM requirements. For more information, see [Railway pricing](https://railway.com/pricing?utm_medium=integration&utm_source=docs&utm_campaign=langflow).\n\n2. Follow this link to deploy the Langflow template on Railway:\n\n   [![Deploy on Railway](/logos/railway-deploy.svg)](https://railway.com/new/template/JMXEWp?referralCode=MnPSdg&utm_medium=integration&utm_source=docs&utm_campaign=langflow)\n\n3. Optional: Add any custom configuration for your Langflow deployment.\n\n   The Langflow Railway template automatically sets up the infrastructure, deploys Langflow, and then starts the application.\n\n4. Wait for the deployment to complete.\n\n5. Navigate to your Langflow instance at your deployment's public URL, such as `https://APP-NAME.up.railway.app`.",
      "summary": "This guide explains how to [deploy Langflow on Railway](https://railway.com/?utm_medium=integration&utm_source=docs&utm_campaign=langflow), a cloud infrastructure platform that provides auto-deploy, managed databases, and automatic scaling.",
      "keywords": [
        "deploy",
        "langflow",
        "railway"
      ]
    },
    "deployment-railway": {
      "title": "Deploy Langflow on Railway",
      "slug": "/deployment-railway",
      "category": "deployment",
      "content": "This guide explains how to [deploy Langflow on Railway](https://railway.com/?utm_medium=integration&utm_source=docs&utm_campaign=langflow), a cloud infrastructure platform that provides auto-deploy, managed databases, and automatic scaling.\n\n1. Create a Railway account.\n\n   A Hobby account on Railway is sufficient for Langflow's dual-core CPU and 2 GB RAM requirements. For more information, see [Railway pricing](https://railway.com/pricing?utm_medium=integration&utm_source=docs&utm_campaign=langflow).\n\n2. Follow this link to deploy the Langflow template on Railway:\n\n   [![Deploy on Railway](/logos/railway-deploy.svg)](https://railway.com/new/template/JMXEWp?referralCode=MnPSdg&utm_medium=integration&utm_source=docs&utm_campaign=langflow)\n\n3. Optional: Add any custom configuration for your Langflow deployment.\n\n   The Langflow Railway template automatically sets up the infrastructure, deploys Langflow, and then starts the application.\n\n4. Wait for the deployment to complete.\n\n5. Navigate to your Langflow instance at your deployment's public URL, such as `https://APP-NAME.up.railway.app`.",
      "summary": "This guide explains how to [deploy Langflow on Railway](https://railway.com/?utm_medium=integration&utm_source=docs&utm_campaign=langflow), a cloud infrastructure platform that provides auto-deploy, managed databases, and automatic scaling.",
      "keywords": [
        "deploy",
        "langflow",
        "railway"
      ]
    },
    "/deployment-render": {
      "title": "Deploy Langflow on Render",
      "slug": "/deployment-render",
      "category": "deployment",
      "content": "This guide explains how to deploy Langflow on [Render](https://render.com/), a cloud platform for deploying web applications and APIs.\n\n1. Prepare a Render instance that can support Langflow.\n\n   Langflow requires at least 2 GB of RAM to run, so you must use a Render instance type of **Standard** or better.\n   This requires a paid Render account.\n   For more information, see [Render Web Services](https://render.com/docs/web-services) and [Render pricing](https://render.com/pricing).\n\n2. Follow this link to start a Langflow deployment on Render:\n\n   [![Deploy to Render](/logos/render-deploy.svg)](https://render.com/deploy?repo=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow%2Ftree%2Fdev)\n\n3. Enter a blueprint name, select the branch for your `render.yaml` file, and then click **Deploy Blueprint**.\n\nWhen deployment is complete, your Langflow instance is ready to use.",
      "summary": "This guide explains how to deploy Langflow on [Render](https://render.com/), a cloud platform for deploying web applications and APIs.",
      "keywords": [
        "blueprint",
        "deploy",
        "langflow",
        "render",
        "standard"
      ]
    },
    "deployment-render": {
      "title": "Deploy Langflow on Render",
      "slug": "/deployment-render",
      "category": "deployment",
      "content": "This guide explains how to deploy Langflow on [Render](https://render.com/), a cloud platform for deploying web applications and APIs.\n\n1. Prepare a Render instance that can support Langflow.\n\n   Langflow requires at least 2 GB of RAM to run, so you must use a Render instance type of **Standard** or better.\n   This requires a paid Render account.\n   For more information, see [Render Web Services](https://render.com/docs/web-services) and [Render pricing](https://render.com/pricing).\n\n2. Follow this link to start a Langflow deployment on Render:\n\n   [![Deploy to Render](/logos/render-deploy.svg)](https://render.com/deploy?repo=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow%2Ftree%2Fdev)\n\n3. Enter a blueprint name, select the branch for your `render.yaml` file, and then click **Deploy Blueprint**.\n\nWhen deployment is complete, your Langflow instance is ready to use.",
      "summary": "This guide explains how to deploy Langflow on [Render](https://render.com/), a cloud platform for deploying web applications and APIs.",
      "keywords": [
        "blueprint",
        "deploy",
        "langflow",
        "render",
        "standard"
      ]
    },
    "/develop-application": {
      "title": "Containerize a Langflow application",
      "slug": "/develop-application",
      "category": "deployment",
      "content": "Designing flows in the visual editor is only the first step in building an application that uses Langflow.\n\nOnce you have a functional flow, you can use that flow in a larger application, such as a website or mobile app.\nBecause Langflow is both an IDE and a runtime, you can use Langflow to build and test your flows locally, and then package and serve your flows in a production environment.\n\nThis guide introduces application development with Langflow from initial setup through packaging and deployment.\nThis documentation doesn't explain how to write a complete application; it only describes how to include Langflow in the context of a larger application.\n\n## Directory structure\n\nThe following example describes the directory structure for a minimal Langflow application:\n\n```text\nLANGFLOW-APPLICATION/\n docker.env\n Dockerfile\n flows/\n    flow1.json\n    flow2.json\n langflow-config-dir/\n README.md\n```\n\nThis directory contains the following:\n\n* [`docker.env`](#docker-env): This file is copied to the Docker image as a `.env` file in the container root.\n* [`Dockerfile`](#dockerfile): This file controls how your Langflow image is built.\n* [`/flows`](#flows): This folder holds the flows you want to host, which are the flows that your application uses.\n* `/langflow-config-dir`: This folder is referenced in the Dockerfile as the location for your Langflow deployment's configuration files, database, and logs.\n* `README.md`: This is a typical README file for your application's documentation.\n\nThis is a minimal example of a Langflow application directory.\nYour application might have additional files and folders, such as a `/components` folder for custom components, or a `pyproject.toml` file for additional dependencies.\n\n### Package management\n\nThe base Langflow Docker image includes the Langflow core dependencies because it uses `langflowai/langflow:latest` as the parent image.\n\nIf your application requires additional dependencies, create a [`pyproject.toml`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml) file for the additional dependencies.\nFor more information, see [Install custom dependencies](/install-custom-dependencies).\n\nTo deploy an application with additional dependencies to Docker, you must copy the `pyproject.toml` and `uv.lock` files to the Docker image.\nTo do this, add the following to your Langflow application's Dockerfile:\n\n```text\nCOPY pyproject.toml uv.lock /app/\n```\n\n### Environment variables {#docker-env}\n\nThe `docker.env` file is a `.env` file loaded into your Docker image.\nIt contains [Langflow environment variables](/environment-variables) that are used in flows or control Langflow's behavior, such as authentication, database storage, API keys, and server configurations.\nFor example:\n\n```text\nLANGFLOW_AUTO_LOGIN=True\nLANGFLOW_SAVE_DB_IN_CONFIG_DIR=True\nLANGFLOW_BASE_URL=http://0.0.0.0:7860\nOPENAI_API_KEY=sk-...\n```\n\nYou can set environment variables in the Dockerfile as well.\nHowever, if you set an environment variable in both `docker.env` and the Dockerfile, Langflow uses the value set in `docker.env`.\n\nLangflow can also [create global variables from your environment variables](/configuration-global-variables#add-custom-global-variables-from-the-environment), or [use environment variables as a backup for missing global variables](/configuration-global-variables#use-environment-variables-for-missing-global-variables).\n\n### Secrets\n\nFor simplicity, the examples in the Langflow documentation might use direct references to API keys and other sensitive values.\nIn your own applications, you should always follow industry best practices for managing secrets, such as using environment variables or secret management tools.\n\nFor information about generating authentication keys and managing secrets in Langflow, see [API keys and authentication](/api-keys-and-authentication).\n\n### Storage\n\nBy default, Langflow uses an [SQLite](https://www.sqlite.org/) database for storage.\nIf you prefer to use PostgreSQL, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\nFor more information about storage, including cache and memory, see [Memory management options](/memory).\n\n### Flows {#flows}\n\nYour local Langflow instance might have many flows for different applications.\nWhen you package Langflow as a dependency of an application, you only want to include the flows your application uses.\n\n1. [Export flows](/concepts-flows-import) that are relevant to your application.\n\n    If you have chained flows (flows that trigger other flows), make sure you export _all_ necessary flows.\n\n2. Add the exported Langflow JSON files to the `/flows` folder in your application directory.\n\n### Components\n\nThe  **Core components** and  [**Bundles**] that you see in the Langflow visual editor are automatically included in the base Langflow Docker image.\n\nIf you have any [custom components](/components-custom-components) that you created for your application, you must include these components in your application directory:\n\n1. Create a `/components` folder in your application directory.\n2. Add your custom component files to the `/components` folder.\n3. Specify the path to `/components` in your `docker.env`.\n\n## Langflow Dockerfile {#dockerfile}\n\nThe Dockerfile determines how your Langflow image is built, including the dependencies, flows, components, and configuration files.\n\nAt minimum, you need to specify the base Langflow image, create the necessary folders in the container, copy folders and files to the container, and provide a startup command.\n\n```dockerfile\n# Use the latest version of the base Langflow image\nFROM langflowai/langflow:latest\n\n# Create folders and set the working directory in the container\nRUN mkdir /app/flows\nRUN mkdir /app/langflow-config-dir\nWORKDIR /app\n\n# Copy flows, langflow-config-dir, and docker.env to the container\nCOPY flows /app/flows\nCOPY langflow-config-dir /app/langflow-config-dir\nCOPY docker.env /app/.env\n\n# Optional: Copy custom components to the container\nCOPY components /app/components\n\n# Optional: Use custom dependencies\nCOPY pyproject.toml uv.lock /app/\n\n# Set environment variables if not set in docker.env\nENV PYTHONPATH=/app\nENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows\nENV LANGFLOW_CONFIG_DIR=/app/langflow-config-dir\nENV LANGFLOW_COMPONENTS_PATH=/app/components\nENV LANGFLOW_LOG_ENV=container\n\n# Command to run the Langflow server on port 7860\nEXPOSE 7860\nCMD [\"langflow\", \"run\", \"--backend-only\", \"--env-file\",\"/app/.env\",\"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\nThe environment variables set directly in this Dockerfile specify resource paths for Langflow.\nIf these variables are also set in `docker.env`, the values in `docker.env` override the values set in the Dockerfile.\n\nIn this example, `ENV LANGFLOW_LOG_ENV=container` sets the logging behavior for serialized JSON to `stdout` to track the application's behavior in a containerized environment. For more information, see [Logging](/logging).\n\n### Backend-only mode\n\nThe `--backend-only` flag in `CMD` starts Langflow in backend-only mode, which provides programmatic access only.\nThis is recommended when running Langflow as a dependency of an application where you don't need access to the visual editor.\n\nIf you want to serve the Langflow visual editor _and_ backend, then omit `--backend-only`.\n\nFor more information, see [Deploy Langflow on Docker](/deployment-docker).\n\n## Test your Langflow Docker image\n\nBuild and run your Langflow Docker image to test it.\n\nThis example runs the container locally.\nFor information about publishing your image on Docker Hub and running a Langflow container remotely, see [Deploy to Docker Hub and Kubernetes](#deploy-docker).\n\n1. Build the Docker image:\n\n    ```bash\n    docker build -t langflow-pokedex:1.2.0 .\n    ```\n\n2. Run the Docker container to start your Langflow server:\n\n    ```bash\n    docker run -p 7860:7860 langflow-pokedex:1.2.0\n    ```\n\n3. To confirm that the container is serving your flows as expected, use the Langflow API to run a flow:\n\n    1. Open one of the JSON files in your application's `/flows` folder, and then find the flow's `id` in the  [additional metadata and project information](/concepts-flows-import#additional-metadata-and-project-information).\n\n        There are many `id` values; make sure you get the ID for the entire flow, not the ID for an individual component.\n        If your flow is complex, try searching for the flow's name, which is typically near the flow's `id`.\n\n        ```json\n        \"name\": \"Basic Prompting\",\n        \"description\": \"Perform basic prompting with an OpenAI model.\",\n        \"id\": \"e4167236-938f-4aca-845b-21de3f399858\",\n        ```\n\n    2. Send a POST request to the [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow) endpoint using the flow ID from the previous step .\n\n        The following example runs a simple LLM chat flow that responds to a chat input string.\n        If necessary, modify the payload for your flow.\n        For example, if your flow doesn't have a **Chat Input** component, you must modify the payload to provide the expected input for your flow.\n\n        ```bash\n        curl --request POST \\\n          --url 'http://localhost:7860/api/v1/run/e4167236-938f-4aca-845b-21de3f399858?stream=true' \\\n          --header 'Content-Type: application/json' \\\n          --data '{\n            \"input_value\": \"Tell me about Charizard.\",\n            \"output_type\": \"chat\",\n            \"input_type\": \"chat\",\n            \"session_id\": \"charizard_test_request\"\n        }'\n        ```\n\n        <details>\n        <summary>About this example</summary>\n\n        This command runs the **Pokdex** template flow.\n        It provides chat input about a specific Pokmon, uses an optional custom `session_id`, and enables response streaming with `?stream=true`.\n\n        The default [session ID](/session-id) is the flow ID.\n        Custom session IDs can help isolate unique conversation threads to keep the LLM's context clean, and they can help identify specific conversations in flow logs to make debugging easier.\n\n        This command uses response streaming because the Pokdex flow can return a large amount of text.\n        To use batching, set `?stream=false`.\n\n        </details>\n\n      3. Verify that the request succeeds and the response is valid, depending on the specific flow you ran.\n\n          This confirms that your Langflow Docker image is correctly configured and this flow is accessible through the Langflow API server that is hosted on the container.\n          When you build and test your entire application stack, your front-end application can use Langflow API requests to trigger the flows served by your Langflow container in the same way you manually tested the flow in the previous step.\n\n          This example triggered a flow by sending chat input to the `/v1/run/$FLOW_ID` endpoint.\n          For more examples of flow triggers, see [Trigger flows with webhooks](/webhook) and the tutorial to [Create a chatbot that can ingest files](/chat-with-files).\n\n## Deploy to Docker Hub and Kubernetes {#deploy-docker}\n\nWhen you're ready to share your application with the world, you need to serve Langflow in a production environment.\nFor more information about deploying Langflow, see the following:\n\n* [Learn about Langflow deployments](/deployment-overview)\n* [Deploy Langflow on Docker](/deployment-docker)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "Designing flows in the visual editor is only the first step in building an application that uses Langflow.",
      "keywords": [
        "application",
        "backend",
        "base",
        "bundles",
        "chat",
        "command",
        "components",
        "config",
        "container",
        "containerize",
        "copy",
        "core",
        "create",
        "custom",
        "dependencies",
        "deploy",
        "dir",
        "directory",
        "docker",
        "dockerfile"
      ]
    },
    "develop-application": {
      "title": "Containerize a Langflow application",
      "slug": "/develop-application",
      "category": "deployment",
      "content": "Designing flows in the visual editor is only the first step in building an application that uses Langflow.\n\nOnce you have a functional flow, you can use that flow in a larger application, such as a website or mobile app.\nBecause Langflow is both an IDE and a runtime, you can use Langflow to build and test your flows locally, and then package and serve your flows in a production environment.\n\nThis guide introduces application development with Langflow from initial setup through packaging and deployment.\nThis documentation doesn't explain how to write a complete application; it only describes how to include Langflow in the context of a larger application.\n\n## Directory structure\n\nThe following example describes the directory structure for a minimal Langflow application:\n\n```text\nLANGFLOW-APPLICATION/\n docker.env\n Dockerfile\n flows/\n    flow1.json\n    flow2.json\n langflow-config-dir/\n README.md\n```\n\nThis directory contains the following:\n\n* [`docker.env`](#docker-env): This file is copied to the Docker image as a `.env` file in the container root.\n* [`Dockerfile`](#dockerfile): This file controls how your Langflow image is built.\n* [`/flows`](#flows): This folder holds the flows you want to host, which are the flows that your application uses.\n* `/langflow-config-dir`: This folder is referenced in the Dockerfile as the location for your Langflow deployment's configuration files, database, and logs.\n* `README.md`: This is a typical README file for your application's documentation.\n\nThis is a minimal example of a Langflow application directory.\nYour application might have additional files and folders, such as a `/components` folder for custom components, or a `pyproject.toml` file for additional dependencies.\n\n### Package management\n\nThe base Langflow Docker image includes the Langflow core dependencies because it uses `langflowai/langflow:latest` as the parent image.\n\nIf your application requires additional dependencies, create a [`pyproject.toml`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml) file for the additional dependencies.\nFor more information, see [Install custom dependencies](/install-custom-dependencies).\n\nTo deploy an application with additional dependencies to Docker, you must copy the `pyproject.toml` and `uv.lock` files to the Docker image.\nTo do this, add the following to your Langflow application's Dockerfile:\n\n```text\nCOPY pyproject.toml uv.lock /app/\n```\n\n### Environment variables {#docker-env}\n\nThe `docker.env` file is a `.env` file loaded into your Docker image.\nIt contains [Langflow environment variables](/environment-variables) that are used in flows or control Langflow's behavior, such as authentication, database storage, API keys, and server configurations.\nFor example:\n\n```text\nLANGFLOW_AUTO_LOGIN=True\nLANGFLOW_SAVE_DB_IN_CONFIG_DIR=True\nLANGFLOW_BASE_URL=http://0.0.0.0:7860\nOPENAI_API_KEY=sk-...\n```\n\nYou can set environment variables in the Dockerfile as well.\nHowever, if you set an environment variable in both `docker.env` and the Dockerfile, Langflow uses the value set in `docker.env`.\n\nLangflow can also [create global variables from your environment variables](/configuration-global-variables#add-custom-global-variables-from-the-environment), or [use environment variables as a backup for missing global variables](/configuration-global-variables#use-environment-variables-for-missing-global-variables).\n\n### Secrets\n\nFor simplicity, the examples in the Langflow documentation might use direct references to API keys and other sensitive values.\nIn your own applications, you should always follow industry best practices for managing secrets, such as using environment variables or secret management tools.\n\nFor information about generating authentication keys and managing secrets in Langflow, see [API keys and authentication](/api-keys-and-authentication).\n\n### Storage\n\nBy default, Langflow uses an [SQLite](https://www.sqlite.org/) database for storage.\nIf you prefer to use PostgreSQL, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\nFor more information about storage, including cache and memory, see [Memory management options](/memory).\n\n### Flows {#flows}\n\nYour local Langflow instance might have many flows for different applications.\nWhen you package Langflow as a dependency of an application, you only want to include the flows your application uses.\n\n1. [Export flows](/concepts-flows-import) that are relevant to your application.\n\n    If you have chained flows (flows that trigger other flows), make sure you export _all_ necessary flows.\n\n2. Add the exported Langflow JSON files to the `/flows` folder in your application directory.\n\n### Components\n\nThe  **Core components** and  [**Bundles**] that you see in the Langflow visual editor are automatically included in the base Langflow Docker image.\n\nIf you have any [custom components](/components-custom-components) that you created for your application, you must include these components in your application directory:\n\n1. Create a `/components` folder in your application directory.\n2. Add your custom component files to the `/components` folder.\n3. Specify the path to `/components` in your `docker.env`.\n\n## Langflow Dockerfile {#dockerfile}\n\nThe Dockerfile determines how your Langflow image is built, including the dependencies, flows, components, and configuration files.\n\nAt minimum, you need to specify the base Langflow image, create the necessary folders in the container, copy folders and files to the container, and provide a startup command.\n\n```dockerfile\n# Use the latest version of the base Langflow image\nFROM langflowai/langflow:latest\n\n# Create folders and set the working directory in the container\nRUN mkdir /app/flows\nRUN mkdir /app/langflow-config-dir\nWORKDIR /app\n\n# Copy flows, langflow-config-dir, and docker.env to the container\nCOPY flows /app/flows\nCOPY langflow-config-dir /app/langflow-config-dir\nCOPY docker.env /app/.env\n\n# Optional: Copy custom components to the container\nCOPY components /app/components\n\n# Optional: Use custom dependencies\nCOPY pyproject.toml uv.lock /app/\n\n# Set environment variables if not set in docker.env\nENV PYTHONPATH=/app\nENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows\nENV LANGFLOW_CONFIG_DIR=/app/langflow-config-dir\nENV LANGFLOW_COMPONENTS_PATH=/app/components\nENV LANGFLOW_LOG_ENV=container\n\n# Command to run the Langflow server on port 7860\nEXPOSE 7860\nCMD [\"langflow\", \"run\", \"--backend-only\", \"--env-file\",\"/app/.env\",\"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\nThe environment variables set directly in this Dockerfile specify resource paths for Langflow.\nIf these variables are also set in `docker.env`, the values in `docker.env` override the values set in the Dockerfile.\n\nIn this example, `ENV LANGFLOW_LOG_ENV=container` sets the logging behavior for serialized JSON to `stdout` to track the application's behavior in a containerized environment. For more information, see [Logging](/logging).\n\n### Backend-only mode\n\nThe `--backend-only` flag in `CMD` starts Langflow in backend-only mode, which provides programmatic access only.\nThis is recommended when running Langflow as a dependency of an application where you don't need access to the visual editor.\n\nIf you want to serve the Langflow visual editor _and_ backend, then omit `--backend-only`.\n\nFor more information, see [Deploy Langflow on Docker](/deployment-docker).\n\n## Test your Langflow Docker image\n\nBuild and run your Langflow Docker image to test it.\n\nThis example runs the container locally.\nFor information about publishing your image on Docker Hub and running a Langflow container remotely, see [Deploy to Docker Hub and Kubernetes](#deploy-docker).\n\n1. Build the Docker image:\n\n    ```bash\n    docker build -t langflow-pokedex:1.2.0 .\n    ```\n\n2. Run the Docker container to start your Langflow server:\n\n    ```bash\n    docker run -p 7860:7860 langflow-pokedex:1.2.0\n    ```\n\n3. To confirm that the container is serving your flows as expected, use the Langflow API to run a flow:\n\n    1. Open one of the JSON files in your application's `/flows` folder, and then find the flow's `id` in the  [additional metadata and project information](/concepts-flows-import#additional-metadata-and-project-information).\n\n        There are many `id` values; make sure you get the ID for the entire flow, not the ID for an individual component.\n        If your flow is complex, try searching for the flow's name, which is typically near the flow's `id`.\n\n        ```json\n        \"name\": \"Basic Prompting\",\n        \"description\": \"Perform basic prompting with an OpenAI model.\",\n        \"id\": \"e4167236-938f-4aca-845b-21de3f399858\",\n        ```\n\n    2. Send a POST request to the [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow) endpoint using the flow ID from the previous step .\n\n        The following example runs a simple LLM chat flow that responds to a chat input string.\n        If necessary, modify the payload for your flow.\n        For example, if your flow doesn't have a **Chat Input** component, you must modify the payload to provide the expected input for your flow.\n\n        ```bash\n        curl --request POST \\\n          --url 'http://localhost:7860/api/v1/run/e4167236-938f-4aca-845b-21de3f399858?stream=true' \\\n          --header 'Content-Type: application/json' \\\n          --data '{\n            \"input_value\": \"Tell me about Charizard.\",\n            \"output_type\": \"chat\",\n            \"input_type\": \"chat\",\n            \"session_id\": \"charizard_test_request\"\n        }'\n        ```\n\n        <details>\n        <summary>About this example</summary>\n\n        This command runs the **Pokdex** template flow.\n        It provides chat input about a specific Pokmon, uses an optional custom `session_id`, and enables response streaming with `?stream=true`.\n\n        The default [session ID](/session-id) is the flow ID.\n        Custom session IDs can help isolate unique conversation threads to keep the LLM's context clean, and they can help identify specific conversations in flow logs to make debugging easier.\n\n        This command uses response streaming because the Pokdex flow can return a large amount of text.\n        To use batching, set `?stream=false`.\n\n        </details>\n\n      3. Verify that the request succeeds and the response is valid, depending on the specific flow you ran.\n\n          This confirms that your Langflow Docker image is correctly configured and this flow is accessible through the Langflow API server that is hosted on the container.\n          When you build and test your entire application stack, your front-end application can use Langflow API requests to trigger the flows served by your Langflow container in the same way you manually tested the flow in the previous step.\n\n          This example triggered a flow by sending chat input to the `/v1/run/$FLOW_ID` endpoint.\n          For more examples of flow triggers, see [Trigger flows with webhooks](/webhook) and the tutorial to [Create a chatbot that can ingest files](/chat-with-files).\n\n## Deploy to Docker Hub and Kubernetes {#deploy-docker}\n\nWhen you're ready to share your application with the world, you need to serve Langflow in a production environment.\nFor more information about deploying Langflow, see the following:\n\n* [Learn about Langflow deployments](/deployment-overview)\n* [Deploy Langflow on Docker](/deployment-docker)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "Designing flows in the visual editor is only the first step in building an application that uses Langflow.",
      "keywords": [
        "application",
        "backend",
        "base",
        "bundles",
        "chat",
        "command",
        "components",
        "config",
        "container",
        "containerize",
        "copy",
        "core",
        "create",
        "custom",
        "dependencies",
        "deploy",
        "dir",
        "directory",
        "docker",
        "dockerfile"
      ]
    },
    "/api-keys-and-authentication": {
      "title": "API keys and authentication",
      "slug": "/api-keys-and-authentication",
      "category": "develop",
      "content": "**Note:** Never expose Langflow ports directly to the internet without proper security measures.\nSet `LANGFLOW_AUTO_LOGIN=False`, use a non-default `LANGFLOW_SECRET_KEY`, and deploy your Langflow server behind a reverse proxy with authentication enabled.\nFor more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).\n\nAuthentication credentials help prevent unauthorized access to your Langflow server, flows, and services connected through components.\n\nThere are three types of credentials that you use in Langflow:\n\n* [Langflow API keys](#langflow-api-keys): For authentication with the Langflow API and authorizing server-side Langflow actions like running flows and uploading files.\n* [Component API keys](#component-api-keys): For authentication between Langflow and a service connected through a component, such as a model provider or third-party API.\n* [Authentication environment variables](#authentication-environment-variables): These environment variables configure how Langflow handles user authentication and authorization.\n\n## Langflow API keys {#langflow-api-keys}\n\nYou can use Langflow API keys to interact with Langflow programmatically.\n\nBy default, most Langflow API endpoints, such as `/v1/run/$FLOW_ID`, require authentication with a Langflow API key.\n\nTo configure authentication for Langflow MCP servers, see [Use Langflow as an MCP server](/mcp-server).\n\n### Langflow API key permissions\n\nA Langflow API key adopts the privileges of the user who created it.\nThis means that API keys you create have the same permissions and access that you do, including access to your flows, components, and Langflow database.\nA Langflow API key cannot be used to access resources outside of your own Langflow server.\n\nIn single-user environments, you are always a superuser, and your Langflow API keys always have superuser privileges.\n\nIn multi-user environments, users who aren't superusers cannot use their API keys to access other users' resources.\nYou must [start your Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled) to allow user management and creation of non-superuser accounts.\n\n### Create a Langflow API key\n\nYou can generate a Langflow API key in your Langflow **Settings** or with the Langflow CLI.\n\nThe CLI option is required if your Langflow server is running in `--backend-only` mode.\n\n</Tabs>\n\n### Use a Langflow API key\n\nTo authenticate Langflow API requests, pass your Langflow API key an `x-api-key` header or query parameter.\n\n</Tabs>\n\nFor more information about forming Langflow API requests, see [Get started with the Langflow API](/api-reference-api-examples) and [Trigger flows with the Langflow API](/concepts-publish).\n\n### Track API key usage\n\nBy default, Langflow tracks API key usage through `total_uses` and `last_used_at` records in your [Langflow database](/memory).\n\nTo disable API key tracking, set `LANGFLOW_DISABLE_TRACK_APIKEY_USAGE=True` in your [Langflow environment variables](/environment-variables).\nThis can help avoid database contention during periods of high concurrency.\n\n### Revoke an API key\n\nTo revoke and delete an API key, do the following:\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n2. Click **Langflow API Keys**.\n3. Select the keys you want to delete, and then click  **Delete**.\n\nThis action immediately invalidates the key and prevents it from being used again.\n\n## Component API keys {#component-api-keys}\n\nComponent API keys authorize access to external services that are called by components in your flows, such as model providers, databases, or third-party APIs.\nThese aren't Langflow API keys or general application credentials.\n\nIn Langflow, you can store component API keys in global variables in your **Settings** or import them from your runtime environment.\nFor more information, see [Global variables](/configuration-global-variables).\n\nYou create and manage component API keys within the service provider's platform.\nLangflow only stores the encrypted key value or a secure reference to a key stored elsewhere; it doesn't manage the actual credentials at the source.\nThis means that deleting a global variable from Langflow doesn't delete or invalidate the actual API key in the service provider's system.\nYou must delete or rotate component API keys directly using the service provider's interface or API.\n\nFor added security, you can set `LANGFLOW_REMOVE_API_KEYS=True` to omit API keys and tokens from flow data in your [Langflow database](/memory).\nAdditionally, when [exporting flows](/concepts-flows-import), you can choose to omit API keys from the exported flow JSON.\n\n## Authentication environment variables\n\nThis section describes the available authentication configuration variables.\n\nYou can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.\n\n### LANGFLOW_AUTO_LOGIN {#langflow-auto-login}\n\nThis variable controls whether authentication is required to access your Langflow server, including the visual editor, API, and Langflow CLI:\n\n* If `LANGFLOW_AUTO_LOGIN=False`, automatic login is disabled. Users must sign in to the visual editor, authenticate as a superuser to run certain Langflow CLI commands, and use a Langflow API key for Langflow API requests.\nIf `false`, the Langflow team recommends that you also explicitly set [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](#langflow-superuser) to avoid using the insecure default values.\n\n* If `LANGFLOW_AUTO_LOGIN=True` (default), all API requests require authentication with a Langflow API key, but the visual editor automatically signs in all users as superusers, and Langflow uses _only_ the default [superuser credentials](/api-keys-and-authentication#langflow-superuser).\nAll users access the same visual editor environment without password protection, they can run all Langflow CLI commands as superusers, and Langflow automatically authenticates internal requests between the backend and frontend based on the users' superuser privileges.\nIf you also want to bypass authentication for Langflow API requests in addition to other bypassed authentication, see [`LANGFLOW_SKIP_AUTH_AUTO_LOGIN`](/api-keys-and-authentication#langflow-skip-auth-auto-login).\n\nLangflow doesn't allow users to simultaneously edit the same flow in real time.\nIf two users edit the same flow, Langflow saves only the work of the most recent editor based on the state of that user's [workspace](/concepts-overview#workspace). Any changes made by the other user in the interim are overwritten.\n\n#### Default authentication enforcement and LANGFLOW_SKIP_AUTH_AUTO_LOGIN {#langflow-skip-auth-auto-login}\n\nIn Langflow version 1.6, the default settings are `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.\nThis enforces authentication for API requests only, as explained in the preceding section.\n\nFor temporary backwards compatibility, you can revert to the fully unauthenticated behavior from earlier versions by setting both variables to `true`.\nHowever, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.\nAt that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.\n\n<details>\n<summary>Authentication enforcement in earlier versions</summary>\n\nLangflow version 1.5 was the first version that could enforce authentication for Langflow API requests, regardless of the value of `LANGFLOW_AUTO_LOGIN`.\nAs a temporary bypass for backwards compatibility, this version added the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable and set both variables to `true` by default to preserve the fully unauthenticated behavior from earlier versions.\nThis allowed users to upgrade to version 1.5 with no change in the authentication behavior.\n\nIn Langflow versions earlier than 1.5, Langflow API requests didn't require authentication.\nAdditionally, the default setting of `LANGFLOW_AUTO_LOGIN=True` automatically granted all users superuser privileges in the visual editor, and it allowed all users to run all Langflow CLI commands as superusers.\n</details>\n\n### LANGFLOW_ENABLE_SUPERUSER_CLI {#langflow-enable-superuser-cli}\n\nControls the availability of the `langflow superuser` command in the Langflow CLI.\nThe default is `true`, but `false` is recommended to prevent unrestricted superuser creation.\nFor more information, see [`langflow superuser`](/configuration-cli#langflow-superuser).\n\n### LANGFLOW_SUPERUSER and LANGFLOW_SUPERUSER_PASSWORD {#langflow-superuser}\n\nThese variables specify the username and password for the Langflow server's superuser.\n\n```text\nLANGFLOW_SUPERUSER=administrator\nLANGFLOW_SUPERUSER_PASSWORD=securepassword\n```\n\nThey are required if `LANGFLOW_AUTO_LOGIN=False`.\nOtherwise, they aren't relevant.\n\nWhen you [start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled), if these variables are required but _not_ set, then Langflow uses the default values of `langflow` and `langflow`.\nThese defaults don't apply when using the Langflow CLI command [`langflow superuser`](/configuration-cli#langflow-superuser).\n\n### LANGFLOW_SECRET_KEY {#langflow-secret-key}\n\nThis environment variable stores a secret key used for encrypting sensitive data like API keys.\nLangflow uses the [Fernet](https://pypi.org/project/cryptography/) library for secret key encryption.\n\nIf no secret key is provided, Langflow automatically generates one.\n\nHowever, you should generate and explicitly set your own key in production environments.\nThis is particularly important for multi-instance deployments like Kubernetes to ensure consistent encryption across instances.\n\nTo generate a secret encryption key for `LANGFLOW_SECRET_KEY`, do the following:\n\n1. Run the command to generate and copy a secret to the clipboard.\n\n    \n    \n    </Tabs>\n\n2. Paste the value into your `.env` file:\n\n    ```text\n    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb\n    ```\n\n    If you're running Langflow on Docker, reference the `LANGFLOW_SECRET_KEY` from your `.env` file in the `docker-compose.yml` file like this:\n\n        ```yaml\n        environment:\n          - LANGFLOW_SECRET_KEY=${LANGFLOW_SECRET_KEY}\n        ```\n\n### LANGFLOW_NEW_USER_IS_ACTIVE {#langflow-new-user-is-active}\n\nWhen `LANGFLOW_NEW_USER_IS_ACTIVE=False` (default), a superuser must explicitly activate a new user's account before they can sign in to the visual editor.\nThe superuser can also deactivate a user's account as needed.\n\nWhen `LANGFLOW_NEW_USER_IS_ACTIVE=True`, new user accounts are automatically activated.\n\n```text\nLANGFLOW_NEW_USER_IS_ACTIVE=False\n```\n\nOnly superusers can manage user accounts for a Langflow server, but user management only matters if your server has authentication enabled.\nFor more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).\n\n### LANGFLOW_CORS_* {#cors-configuration-for-authentication}\n\nCross-Origin Resource Sharing (CORS) configuration controls how authentication credentials are handled when your Langflow frontend and backend are served from different origins.\nThe following `LANGFLOW_CORS_*` environment variables are available:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_CORS_ALLOW_CREDENTIALS` | Boolean | `True` | Whether to allow credentials, such as cookies and authorization headers, in CORS requests. |\n| `LANGFLOW_CORS_ALLOW_HEADERS` | List[String] or String | `*` | The allowed headers for CORS requests. Provide a comma-separated list of headers or use `*` to allow all headers. |\n| `LANGFLOW_CORS_ALLOW_METHODS` | List[String] or String | `*` | The allowed HTTP methods for CORS requests. Provide a comma-separated list of methods or use `*` to allow all methods. |\n| `LANGFLOW_CORS_ORIGINS` | String | `*` | The allowed CORS origins. Provide a comma-separated list of origins or use `*` for all origins. |\n\nThe default configuration enables CORS credentials and uses wildcards (`*`) to allow all origins, headers, and methods:\n\n```text\nLANGFLOW_CORS_ORIGINS=*\nLANGFLOW_CORS_ALLOW_CREDENTIALS=True\nLANGFLOW_CORS_ALLOW_HEADERS=*\nLANGFLOW_CORS_ALLOW_METHODS=*\n```\n\n**Note:** Langflow's default CORS settings can be a security risk in production environments because any website can make requests to your Langflow API, and any website can include credentials in cross-origin requests, including authentication cookies and authorization headers.\n\nIn production deployments, specify exact origins in `LANGFLOW_CORS_ORIGINS`.\nYou can also specify allowed headers and methods, if needed.\nFor example:\n\n```text\nLANGFLOW_CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com\nLANGFLOW_CORS_ALLOW_CREDENTIALS=True\nLANGFLOW_CORS_ALLOW_HEADERS=Content-Type,Authorization\nLANGFLOW_CORS_ALLOW_METHODS=GET,POST,PUT\n```\n\n## Start a Langflow server with authentication enabled\n\nThis section shows you how to use the [authentication environment variables](/api-keys-and-authentication#authentication-environment-variables) to deploy a Langflow server with authentication enabled.\nThis involves disabling automatic login, setting superuser credentials, generating a secret encryption key, and enabling user management.\n\nThis configuration is recommended for any deployment where Langflow is exposed to a shared or public network, or where multiple users access the same Langflow server.\n\nWith authentication enabled, all users must sign in to the visual editor with valid credentials, and API requests require authentication with a Langflow API key.\nAdditionally, you must sign in as a superuser to manage users and [create a Langflow API key](#create-a-langflow-api-key) with superuser privileges.\n\n### Start the Langflow server\n\n1. Create a `.env` file with the following variables:\n\n    ```text\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_SUPERUSER=\n    LANGFLOW_SUPERUSER_PASSWORD=\n    LANGFLOW_SECRET_KEY=\n    LANGFLOW_NEW_USER_IS_ACTIVE=False\n    LANGFLOW_ENABLE_SUPERUSER_CLI=False\n    ```\n\n    Your `.env` file can have other environment variables.\n    This example focuses on authentication variables.\n\n2. Set `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD` to your desired superuser credentials.\n\n    For a one-time test, you can use basic credentials like `administrator` and `password`.\n    Strong, securely-stored credentials are recommended in genuine development and production environments.\n\n3. Recommended: Generate and set a `LANGFLOW_SECRET_KEY` for encrypting sensitive data.\n\n    If you don't set a secret key, Langflow generates one automatically, but this isn't recommended for production environments.\n\n    For instructions on generating at setting a secret key, see [`LANGFLOW_SECRET_KEY`](#langflow-secret-key).\n\n4. Save your `.env` file with the populated variables. For example:\n\n    ```text\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_SUPERUSER=administrator\n    LANGFLOW_SUPERUSER_PASSWORD=securepassword\n    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb\n    LANGFLOW_NEW_USER_IS_ACTIVE=False\n    LANGFLOW_ENABLE_SUPERUSER_CLI=False\n    ```\n\n5. Start Langflow with the configuration from your `.env` file:\n\n    ```text\n    uv run langflow run --env-file .env\n    ```\n\n    Starting Langflow with a `.env` file automatically authenticates you as the superuser set in `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`.\n    If you don't explicitly set these variables, the default values are `langflow` and `langflow` for system auto-login.\n\n6. Verify the server is running. The default location is `http://localhost:7860`.\n\nNext, you can add users to your Langflow server to collaborate with others on flows.\n\n### Manage users as an administrator\n\n1. To complete your first-time login as a superuser, go to `http://localhost:7860/login`.\n\n    If you aren't using the default location, replace `localhost:7860` with your server's address.\n\n2. Log in with the superuser credentials you set in your `.env` (`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`).\n\n3. To manage users on your server, navigate to `/admin`, such as `http://localhost:7860/admin`, click your profile icon, and then click **Admin Page**.\n\n    As a superuser, you can add users, set permissions, reset passwords, and delete accounts.\n\n4. To add a user, click **New User**, and then complete the user account form:\n\n    1. Enter a username and password.\n    2. To activate the account immediately, select **Active**. Inactive users cannot sign in or access flows they created before becoming inactive.\n    3. Deselect **Superuser** if you don't want the user to have full administrative privileges.\n    4. Click **Save**. The new user appears in the **Admin Page**.\n\n5. To test the new user's access, sign out of Langflow, and then sign in with the new user's credentials.\n\n    Try to access the `/admin` page.\n    You are redirected to the `/flows` page if the new user isn't a superuser.\n\n## See also\n\n* [Langflow environment variables](/environment-variables)",
      "summary": "**Note:** Never expose Langflow ports directly to the internet without proper security measures.\nSet `LANGFLOW_AUTO_LOGIN=False`, use a non-default `LANGFLOW_SECRET_KEY`, and deploy your Langflow server behind a reverse proxy with authentication enabled.\nFor more information, see [Start a Langflow...",
      "keywords": [
        "active",
        "admin",
        "administrator",
        "api",
        "auth",
        "authentication",
        "auto",
        "cli",
        "component",
        "configuration",
        "cors",
        "create",
        "default",
        "delete",
        "enable",
        "enabled",
        "enforcement",
        "environment",
        "key",
        "keys"
      ]
    },
    "api-keys-and-authentication": {
      "title": "API keys and authentication",
      "slug": "/api-keys-and-authentication",
      "category": "develop",
      "content": "**Note:** Never expose Langflow ports directly to the internet without proper security measures.\nSet `LANGFLOW_AUTO_LOGIN=False`, use a non-default `LANGFLOW_SECRET_KEY`, and deploy your Langflow server behind a reverse proxy with authentication enabled.\nFor more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).\n\nAuthentication credentials help prevent unauthorized access to your Langflow server, flows, and services connected through components.\n\nThere are three types of credentials that you use in Langflow:\n\n* [Langflow API keys](#langflow-api-keys): For authentication with the Langflow API and authorizing server-side Langflow actions like running flows and uploading files.\n* [Component API keys](#component-api-keys): For authentication between Langflow and a service connected through a component, such as a model provider or third-party API.\n* [Authentication environment variables](#authentication-environment-variables): These environment variables configure how Langflow handles user authentication and authorization.\n\n## Langflow API keys {#langflow-api-keys}\n\nYou can use Langflow API keys to interact with Langflow programmatically.\n\nBy default, most Langflow API endpoints, such as `/v1/run/$FLOW_ID`, require authentication with a Langflow API key.\n\nTo configure authentication for Langflow MCP servers, see [Use Langflow as an MCP server](/mcp-server).\n\n### Langflow API key permissions\n\nA Langflow API key adopts the privileges of the user who created it.\nThis means that API keys you create have the same permissions and access that you do, including access to your flows, components, and Langflow database.\nA Langflow API key cannot be used to access resources outside of your own Langflow server.\n\nIn single-user environments, you are always a superuser, and your Langflow API keys always have superuser privileges.\n\nIn multi-user environments, users who aren't superusers cannot use their API keys to access other users' resources.\nYou must [start your Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled) to allow user management and creation of non-superuser accounts.\n\n### Create a Langflow API key\n\nYou can generate a Langflow API key in your Langflow **Settings** or with the Langflow CLI.\n\nThe CLI option is required if your Langflow server is running in `--backend-only` mode.\n\n</Tabs>\n\n### Use a Langflow API key\n\nTo authenticate Langflow API requests, pass your Langflow API key an `x-api-key` header or query parameter.\n\n</Tabs>\n\nFor more information about forming Langflow API requests, see [Get started with the Langflow API](/api-reference-api-examples) and [Trigger flows with the Langflow API](/concepts-publish).\n\n### Track API key usage\n\nBy default, Langflow tracks API key usage through `total_uses` and `last_used_at` records in your [Langflow database](/memory).\n\nTo disable API key tracking, set `LANGFLOW_DISABLE_TRACK_APIKEY_USAGE=True` in your [Langflow environment variables](/environment-variables).\nThis can help avoid database contention during periods of high concurrency.\n\n### Revoke an API key\n\nTo revoke and delete an API key, do the following:\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n2. Click **Langflow API Keys**.\n3. Select the keys you want to delete, and then click  **Delete**.\n\nThis action immediately invalidates the key and prevents it from being used again.\n\n## Component API keys {#component-api-keys}\n\nComponent API keys authorize access to external services that are called by components in your flows, such as model providers, databases, or third-party APIs.\nThese aren't Langflow API keys or general application credentials.\n\nIn Langflow, you can store component API keys in global variables in your **Settings** or import them from your runtime environment.\nFor more information, see [Global variables](/configuration-global-variables).\n\nYou create and manage component API keys within the service provider's platform.\nLangflow only stores the encrypted key value or a secure reference to a key stored elsewhere; it doesn't manage the actual credentials at the source.\nThis means that deleting a global variable from Langflow doesn't delete or invalidate the actual API key in the service provider's system.\nYou must delete or rotate component API keys directly using the service provider's interface or API.\n\nFor added security, you can set `LANGFLOW_REMOVE_API_KEYS=True` to omit API keys and tokens from flow data in your [Langflow database](/memory).\nAdditionally, when [exporting flows](/concepts-flows-import), you can choose to omit API keys from the exported flow JSON.\n\n## Authentication environment variables\n\nThis section describes the available authentication configuration variables.\n\nYou can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.\n\n### LANGFLOW_AUTO_LOGIN {#langflow-auto-login}\n\nThis variable controls whether authentication is required to access your Langflow server, including the visual editor, API, and Langflow CLI:\n\n* If `LANGFLOW_AUTO_LOGIN=False`, automatic login is disabled. Users must sign in to the visual editor, authenticate as a superuser to run certain Langflow CLI commands, and use a Langflow API key for Langflow API requests.\nIf `false`, the Langflow team recommends that you also explicitly set [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](#langflow-superuser) to avoid using the insecure default values.\n\n* If `LANGFLOW_AUTO_LOGIN=True` (default), all API requests require authentication with a Langflow API key, but the visual editor automatically signs in all users as superusers, and Langflow uses _only_ the default [superuser credentials](/api-keys-and-authentication#langflow-superuser).\nAll users access the same visual editor environment without password protection, they can run all Langflow CLI commands as superusers, and Langflow automatically authenticates internal requests between the backend and frontend based on the users' superuser privileges.\nIf you also want to bypass authentication for Langflow API requests in addition to other bypassed authentication, see [`LANGFLOW_SKIP_AUTH_AUTO_LOGIN`](/api-keys-and-authentication#langflow-skip-auth-auto-login).\n\nLangflow doesn't allow users to simultaneously edit the same flow in real time.\nIf two users edit the same flow, Langflow saves only the work of the most recent editor based on the state of that user's [workspace](/concepts-overview#workspace). Any changes made by the other user in the interim are overwritten.\n\n#### Default authentication enforcement and LANGFLOW_SKIP_AUTH_AUTO_LOGIN {#langflow-skip-auth-auto-login}\n\nIn Langflow version 1.6, the default settings are `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.\nThis enforces authentication for API requests only, as explained in the preceding section.\n\nFor temporary backwards compatibility, you can revert to the fully unauthenticated behavior from earlier versions by setting both variables to `true`.\nHowever, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.\nAt that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.\n\n<details>\n<summary>Authentication enforcement in earlier versions</summary>\n\nLangflow version 1.5 was the first version that could enforce authentication for Langflow API requests, regardless of the value of `LANGFLOW_AUTO_LOGIN`.\nAs a temporary bypass for backwards compatibility, this version added the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable and set both variables to `true` by default to preserve the fully unauthenticated behavior from earlier versions.\nThis allowed users to upgrade to version 1.5 with no change in the authentication behavior.\n\nIn Langflow versions earlier than 1.5, Langflow API requests didn't require authentication.\nAdditionally, the default setting of `LANGFLOW_AUTO_LOGIN=True` automatically granted all users superuser privileges in the visual editor, and it allowed all users to run all Langflow CLI commands as superusers.\n</details>\n\n### LANGFLOW_ENABLE_SUPERUSER_CLI {#langflow-enable-superuser-cli}\n\nControls the availability of the `langflow superuser` command in the Langflow CLI.\nThe default is `true`, but `false` is recommended to prevent unrestricted superuser creation.\nFor more information, see [`langflow superuser`](/configuration-cli#langflow-superuser).\n\n### LANGFLOW_SUPERUSER and LANGFLOW_SUPERUSER_PASSWORD {#langflow-superuser}\n\nThese variables specify the username and password for the Langflow server's superuser.\n\n```text\nLANGFLOW_SUPERUSER=administrator\nLANGFLOW_SUPERUSER_PASSWORD=securepassword\n```\n\nThey are required if `LANGFLOW_AUTO_LOGIN=False`.\nOtherwise, they aren't relevant.\n\nWhen you [start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled), if these variables are required but _not_ set, then Langflow uses the default values of `langflow` and `langflow`.\nThese defaults don't apply when using the Langflow CLI command [`langflow superuser`](/configuration-cli#langflow-superuser).\n\n### LANGFLOW_SECRET_KEY {#langflow-secret-key}\n\nThis environment variable stores a secret key used for encrypting sensitive data like API keys.\nLangflow uses the [Fernet](https://pypi.org/project/cryptography/) library for secret key encryption.\n\nIf no secret key is provided, Langflow automatically generates one.\n\nHowever, you should generate and explicitly set your own key in production environments.\nThis is particularly important for multi-instance deployments like Kubernetes to ensure consistent encryption across instances.\n\nTo generate a secret encryption key for `LANGFLOW_SECRET_KEY`, do the following:\n\n1. Run the command to generate and copy a secret to the clipboard.\n\n    \n    \n    </Tabs>\n\n2. Paste the value into your `.env` file:\n\n    ```text\n    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb\n    ```\n\n    If you're running Langflow on Docker, reference the `LANGFLOW_SECRET_KEY` from your `.env` file in the `docker-compose.yml` file like this:\n\n        ```yaml\n        environment:\n          - LANGFLOW_SECRET_KEY=${LANGFLOW_SECRET_KEY}\n        ```\n\n### LANGFLOW_NEW_USER_IS_ACTIVE {#langflow-new-user-is-active}\n\nWhen `LANGFLOW_NEW_USER_IS_ACTIVE=False` (default), a superuser must explicitly activate a new user's account before they can sign in to the visual editor.\nThe superuser can also deactivate a user's account as needed.\n\nWhen `LANGFLOW_NEW_USER_IS_ACTIVE=True`, new user accounts are automatically activated.\n\n```text\nLANGFLOW_NEW_USER_IS_ACTIVE=False\n```\n\nOnly superusers can manage user accounts for a Langflow server, but user management only matters if your server has authentication enabled.\nFor more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).\n\n### LANGFLOW_CORS_* {#cors-configuration-for-authentication}\n\nCross-Origin Resource Sharing (CORS) configuration controls how authentication credentials are handled when your Langflow frontend and backend are served from different origins.\nThe following `LANGFLOW_CORS_*` environment variables are available:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_CORS_ALLOW_CREDENTIALS` | Boolean | `True` | Whether to allow credentials, such as cookies and authorization headers, in CORS requests. |\n| `LANGFLOW_CORS_ALLOW_HEADERS` | List[String] or String | `*` | The allowed headers for CORS requests. Provide a comma-separated list of headers or use `*` to allow all headers. |\n| `LANGFLOW_CORS_ALLOW_METHODS` | List[String] or String | `*` | The allowed HTTP methods for CORS requests. Provide a comma-separated list of methods or use `*` to allow all methods. |\n| `LANGFLOW_CORS_ORIGINS` | String | `*` | The allowed CORS origins. Provide a comma-separated list of origins or use `*` for all origins. |\n\nThe default configuration enables CORS credentials and uses wildcards (`*`) to allow all origins, headers, and methods:\n\n```text\nLANGFLOW_CORS_ORIGINS=*\nLANGFLOW_CORS_ALLOW_CREDENTIALS=True\nLANGFLOW_CORS_ALLOW_HEADERS=*\nLANGFLOW_CORS_ALLOW_METHODS=*\n```\n\n**Note:** Langflow's default CORS settings can be a security risk in production environments because any website can make requests to your Langflow API, and any website can include credentials in cross-origin requests, including authentication cookies and authorization headers.\n\nIn production deployments, specify exact origins in `LANGFLOW_CORS_ORIGINS`.\nYou can also specify allowed headers and methods, if needed.\nFor example:\n\n```text\nLANGFLOW_CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com\nLANGFLOW_CORS_ALLOW_CREDENTIALS=True\nLANGFLOW_CORS_ALLOW_HEADERS=Content-Type,Authorization\nLANGFLOW_CORS_ALLOW_METHODS=GET,POST,PUT\n```\n\n## Start a Langflow server with authentication enabled\n\nThis section shows you how to use the [authentication environment variables](/api-keys-and-authentication#authentication-environment-variables) to deploy a Langflow server with authentication enabled.\nThis involves disabling automatic login, setting superuser credentials, generating a secret encryption key, and enabling user management.\n\nThis configuration is recommended for any deployment where Langflow is exposed to a shared or public network, or where multiple users access the same Langflow server.\n\nWith authentication enabled, all users must sign in to the visual editor with valid credentials, and API requests require authentication with a Langflow API key.\nAdditionally, you must sign in as a superuser to manage users and [create a Langflow API key](#create-a-langflow-api-key) with superuser privileges.\n\n### Start the Langflow server\n\n1. Create a `.env` file with the following variables:\n\n    ```text\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_SUPERUSER=\n    LANGFLOW_SUPERUSER_PASSWORD=\n    LANGFLOW_SECRET_KEY=\n    LANGFLOW_NEW_USER_IS_ACTIVE=False\n    LANGFLOW_ENABLE_SUPERUSER_CLI=False\n    ```\n\n    Your `.env` file can have other environment variables.\n    This example focuses on authentication variables.\n\n2. Set `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD` to your desired superuser credentials.\n\n    For a one-time test, you can use basic credentials like `administrator` and `password`.\n    Strong, securely-stored credentials are recommended in genuine development and production environments.\n\n3. Recommended: Generate and set a `LANGFLOW_SECRET_KEY` for encrypting sensitive data.\n\n    If you don't set a secret key, Langflow generates one automatically, but this isn't recommended for production environments.\n\n    For instructions on generating at setting a secret key, see [`LANGFLOW_SECRET_KEY`](#langflow-secret-key).\n\n4. Save your `.env` file with the populated variables. For example:\n\n    ```text\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_SUPERUSER=administrator\n    LANGFLOW_SUPERUSER_PASSWORD=securepassword\n    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb\n    LANGFLOW_NEW_USER_IS_ACTIVE=False\n    LANGFLOW_ENABLE_SUPERUSER_CLI=False\n    ```\n\n5. Start Langflow with the configuration from your `.env` file:\n\n    ```text\n    uv run langflow run --env-file .env\n    ```\n\n    Starting Langflow with a `.env` file automatically authenticates you as the superuser set in `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`.\n    If you don't explicitly set these variables, the default values are `langflow` and `langflow` for system auto-login.\n\n6. Verify the server is running. The default location is `http://localhost:7860`.\n\nNext, you can add users to your Langflow server to collaborate with others on flows.\n\n### Manage users as an administrator\n\n1. To complete your first-time login as a superuser, go to `http://localhost:7860/login`.\n\n    If you aren't using the default location, replace `localhost:7860` with your server's address.\n\n2. Log in with the superuser credentials you set in your `.env` (`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`).\n\n3. To manage users on your server, navigate to `/admin`, such as `http://localhost:7860/admin`, click your profile icon, and then click **Admin Page**.\n\n    As a superuser, you can add users, set permissions, reset passwords, and delete accounts.\n\n4. To add a user, click **New User**, and then complete the user account form:\n\n    1. Enter a username and password.\n    2. To activate the account immediately, select **Active**. Inactive users cannot sign in or access flows they created before becoming inactive.\n    3. Deselect **Superuser** if you don't want the user to have full administrative privileges.\n    4. Click **Save**. The new user appears in the **Admin Page**.\n\n5. To test the new user's access, sign out of Langflow, and then sign in with the new user's credentials.\n\n    Try to access the `/admin` page.\n    You are redirected to the `/flows` page if the new user isn't a superuser.\n\n## See also\n\n* [Langflow environment variables](/environment-variables)",
      "summary": "**Note:** Never expose Langflow ports directly to the internet without proper security measures.\nSet `LANGFLOW_AUTO_LOGIN=False`, use a non-default `LANGFLOW_SECRET_KEY`, and deploy your Langflow server behind a reverse proxy with authentication enabled.\nFor more information, see [Start a Langflow...",
      "keywords": [
        "active",
        "admin",
        "administrator",
        "api",
        "auth",
        "authentication",
        "auto",
        "cli",
        "component",
        "configuration",
        "cors",
        "create",
        "default",
        "delete",
        "enable",
        "enabled",
        "enforcement",
        "environment",
        "key",
        "keys"
      ]
    },
    "/concepts-file-management": {
      "title": "Manage files",
      "slug": "/concepts-file-management",
      "category": "develop",
      "content": "Each Langflow server has a file management system where you can store files that you want to use in your flows.\n\nFiles uploaded to Langflow file management are stored locally in your [Langflow configuration directory](/memory), and they are available to all of your flows.\nLocal storage is set by `LANGFLOW_STORAGE_TYPE`, which has only one allowed value (`local`).\n\nUploading files to Langflow file management keeps your files in a central location, and allows you to reuse files across flows without repeated manual uploads.\n\n## Use the file management UI\n\nYou can use the file management UI to upload files from your local machine to your own Langflow server.\nYou can also manage all files that have been uploaded to your Langflow server.\n\n1. Navigate to Langflow file management:\n\n    * **Langflow Desktop**: In Langflow, on the [**Projects** page](/concepts-flows#projects) page, click **My Files** below the list of projects.\n    * **Langflow OSS**: From a browser, navigate to your Langflow server's `/files` endpoint, such as `http://localhost:7860/files`. Modify the base URL as needed for your Langflow server.\n    * **Backend-only**: For programmatic file management, use the [Langflow API files endpoints](/api-files). However, the following steps assume you're using the file management UI.\n\n2. On the **My Files** page, click **Upload**.\n\n3. Select one or more files to upload.\n\nAfter uploading files, you can rename, download, copy, or delete files within the file management UI.\nTo delete a file, hover over a file's icon, select it, and then click  **Delete**.\nYou can delete multiple files in a single action.\nTo download a file, hover over a file's icon, select it, and then click  **Download**.\nIf you download multiple files in a single action, they are saved together in a zip file.\n\n## Upload and manage files with the Langflow API\n\nWith the Langflow API, you can upload and manage files in Langflow file management, and you can send files to flows programmatically at runtime.\n\nFor more information and examples, see [Files endpoints](/api-files) and [Create a chatbot that can ingest files](/chat-with-files).\n\n## Set the maximum file size\n\nBy default, the maximum file size is 1024 MB.\nTo modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n## Use files in a flow\n\nTo use files in your Langflow file management system in a flow, add a component that accepts file input to your flow, such as the **File** component.\n\nFor example, add a **File** component to your flow, click **Select files**, and then select files from the **My Files** list.\n\nThis list includes all files in your server's file management system, but you can only select [file types that are supported by the **File** component](/components-data#file).\nIf you need another file type, you must use a different component that supports that file type, or you need to convert it to a supported type before uploading it.\n\nFor more information about the **File** component and other data loading components, see [Data components](/components-data).\n\n### Load files at runtime\n\nYou can use preloaded files in your flows, and you can load files at runtime, if your flow accepts file input.\nTo enable file input in your flow, do the following:\n\n1. Add a [**File** component](/components-data#file) to your flow.\n\n2. Click **Share**, select **API access**, and then click **Input Schema** to add [`tweaks`](/concepts-publish#input-schema) to the request payload in the flow's automatically generated code snippets.\n\n3. Expand the **File** section, find the **Files** row, and then enable **Expose Input** to allow the parameter to be set at runtime through the Langflow API.\n\n4. Close the **Input Schema** pane to return to the **API access** pane.\nThe payload in each code snippet now includes `tweaks` with your **File** component's ID and the `path` key that you enabled in **Input Schema**:\n\n    ```json\n    \"tweaks\": {\n\t    \"File-qYD5w\": {\n\t\t    \"path\": []\n\t    }\n\t}\n    ```\n\n5. When you run this flow programmatically, your script must upload a file to Langflow file management, and then pass the returned `file_path` to the `path` tweak in the `/run` request:\n\n    ```json\n    \"tweaks\": {\n        \"FILE_COMPONENT_ID\": {\n            \"path\": [ \"file_path\" ]\n        }\n    }\n    ```\n\n    For a complete example see [Create a chatbot that can ingest files](/chat-with-files) and [Files endpoints](/api-files).\n\n    If you want to upload multiple files, you can pass multiple `file_path` values in the `path` array, such as `[ \"path1\", \"path2\" ]`.\n\n## Upload images\n\nLangflow supports base64 images in the following formats:\n\n* PNG\n* JPG/JPEG\n* GIF\n* BMP\n* WebP\n\nYou can upload images to the **Playground** chat interface and as runtime input with the Langflow API.\n\n* In the **Playground**, you can drag-and-drop images into the chat input area, or you can click the **Attach image** icon to select an image to upload.\n\n* When you trigger a flow with the `/api/v1/run/$FLOW_ID` endpoint, you can use the `files` parameter to attach image data as a base64-encoded string:\n\n   ```bash\n   curl -X POST \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n   -H \"Content-Type: application/json\" \\\n   -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n   -d '{\n      \"session_id\": \"custom_session_123\",\n      \"input_value\": \"What is in this image?\",\n      \"input_type\": \"chat\",\n      \"output_type\": \"chat\",\n      \"files\": [\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\"]\n   }'\n   ```\n\nFor more specialized image processing, browse  [**Bundles**] or [create your own components](/components-custom-components).\n\n## Work with video files\n\nFor videos, see the **Twelve Labs** and **YouTube**  [**Bundles**](/components-bundle-components).\n\n## See also\n\n* [Data components](/components-data)\n* [Processing components](/components-processing)",
      "summary": "Each Langflow server has a file management system where you can store files that you want to use in your flows.",
      "keywords": [
        "access",
        "api",
        "attach",
        "backend",
        "bundles",
        "delete",
        "desktop",
        "download",
        "expose",
        "file",
        "files",
        "flow",
        "image",
        "images",
        "input",
        "labs",
        "langflow",
        "load",
        "manage",
        "management"
      ]
    },
    "concepts-file-management": {
      "title": "Manage files",
      "slug": "/concepts-file-management",
      "category": "develop",
      "content": "Each Langflow server has a file management system where you can store files that you want to use in your flows.\n\nFiles uploaded to Langflow file management are stored locally in your [Langflow configuration directory](/memory), and they are available to all of your flows.\nLocal storage is set by `LANGFLOW_STORAGE_TYPE`, which has only one allowed value (`local`).\n\nUploading files to Langflow file management keeps your files in a central location, and allows you to reuse files across flows without repeated manual uploads.\n\n## Use the file management UI\n\nYou can use the file management UI to upload files from your local machine to your own Langflow server.\nYou can also manage all files that have been uploaded to your Langflow server.\n\n1. Navigate to Langflow file management:\n\n    * **Langflow Desktop**: In Langflow, on the [**Projects** page](/concepts-flows#projects) page, click **My Files** below the list of projects.\n    * **Langflow OSS**: From a browser, navigate to your Langflow server's `/files` endpoint, such as `http://localhost:7860/files`. Modify the base URL as needed for your Langflow server.\n    * **Backend-only**: For programmatic file management, use the [Langflow API files endpoints](/api-files). However, the following steps assume you're using the file management UI.\n\n2. On the **My Files** page, click **Upload**.\n\n3. Select one or more files to upload.\n\nAfter uploading files, you can rename, download, copy, or delete files within the file management UI.\nTo delete a file, hover over a file's icon, select it, and then click  **Delete**.\nYou can delete multiple files in a single action.\nTo download a file, hover over a file's icon, select it, and then click  **Download**.\nIf you download multiple files in a single action, they are saved together in a zip file.\n\n## Upload and manage files with the Langflow API\n\nWith the Langflow API, you can upload and manage files in Langflow file management, and you can send files to flows programmatically at runtime.\n\nFor more information and examples, see [Files endpoints](/api-files) and [Create a chatbot that can ingest files](/chat-with-files).\n\n## Set the maximum file size\n\nBy default, the maximum file size is 1024 MB.\nTo modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).\n\n## Use files in a flow\n\nTo use files in your Langflow file management system in a flow, add a component that accepts file input to your flow, such as the **File** component.\n\nFor example, add a **File** component to your flow, click **Select files**, and then select files from the **My Files** list.\n\nThis list includes all files in your server's file management system, but you can only select [file types that are supported by the **File** component](/components-data#file).\nIf you need another file type, you must use a different component that supports that file type, or you need to convert it to a supported type before uploading it.\n\nFor more information about the **File** component and other data loading components, see [Data components](/components-data).\n\n### Load files at runtime\n\nYou can use preloaded files in your flows, and you can load files at runtime, if your flow accepts file input.\nTo enable file input in your flow, do the following:\n\n1. Add a [**File** component](/components-data#file) to your flow.\n\n2. Click **Share**, select **API access**, and then click **Input Schema** to add [`tweaks`](/concepts-publish#input-schema) to the request payload in the flow's automatically generated code snippets.\n\n3. Expand the **File** section, find the **Files** row, and then enable **Expose Input** to allow the parameter to be set at runtime through the Langflow API.\n\n4. Close the **Input Schema** pane to return to the **API access** pane.\nThe payload in each code snippet now includes `tweaks` with your **File** component's ID and the `path` key that you enabled in **Input Schema**:\n\n    ```json\n    \"tweaks\": {\n\t    \"File-qYD5w\": {\n\t\t    \"path\": []\n\t    }\n\t}\n    ```\n\n5. When you run this flow programmatically, your script must upload a file to Langflow file management, and then pass the returned `file_path` to the `path` tweak in the `/run` request:\n\n    ```json\n    \"tweaks\": {\n        \"FILE_COMPONENT_ID\": {\n            \"path\": [ \"file_path\" ]\n        }\n    }\n    ```\n\n    For a complete example see [Create a chatbot that can ingest files](/chat-with-files) and [Files endpoints](/api-files).\n\n    If you want to upload multiple files, you can pass multiple `file_path` values in the `path` array, such as `[ \"path1\", \"path2\" ]`.\n\n## Upload images\n\nLangflow supports base64 images in the following formats:\n\n* PNG\n* JPG/JPEG\n* GIF\n* BMP\n* WebP\n\nYou can upload images to the **Playground** chat interface and as runtime input with the Langflow API.\n\n* In the **Playground**, you can drag-and-drop images into the chat input area, or you can click the **Attach image** icon to select an image to upload.\n\n* When you trigger a flow with the `/api/v1/run/$FLOW_ID` endpoint, you can use the `files` parameter to attach image data as a base64-encoded string:\n\n   ```bash\n   curl -X POST \"http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID\" \\\n   -H \"Content-Type: application/json\" \\\n   -H \"x-api-key: $LANGFLOW_API_KEY\" \\\n   -d '{\n      \"session_id\": \"custom_session_123\",\n      \"input_value\": \"What is in this image?\",\n      \"input_type\": \"chat\",\n      \"output_type\": \"chat\",\n      \"files\": [\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...\"]\n   }'\n   ```\n\nFor more specialized image processing, browse  [**Bundles**] or [create your own components](/components-custom-components).\n\n## Work with video files\n\nFor videos, see the **Twelve Labs** and **YouTube**  [**Bundles**](/components-bundle-components).\n\n## See also\n\n* [Data components](/components-data)\n* [Processing components](/components-processing)",
      "summary": "Each Langflow server has a file management system where you can store files that you want to use in your flows.",
      "keywords": [
        "access",
        "api",
        "attach",
        "backend",
        "bundles",
        "delete",
        "desktop",
        "download",
        "expose",
        "file",
        "files",
        "flow",
        "image",
        "images",
        "input",
        "labs",
        "langflow",
        "load",
        "manage",
        "management"
      ]
    },
    "/concepts-voice-mode": {
      "title": "Use voice mode",
      "slug": "/concepts-voice-mode",
      "category": "develop",
      "content": "**Note:** Voice mode is not available in Langflow Desktop.\nTo use voice mode, [Install the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n\nYou can use Langflow's voice mode to interact with your flows verbally through a microphone and speakers.\n\n## Prerequisites\n\nVoice mode requires the following:\n\n* A flow with **Chat Input**, **Language Model**, and **Chat Output** components.\n\n    If your flow has an **Agent** component, make sure the tools in your flow have accurate names and descriptions to help the agent choose which tools to use.\n\n    Additionally, be aware that voice mode overrides typed instructions in the **Agent** component's **Agent Instructions** field.\n\n* An [OpenAI](https://platform.openai.com/) account and an OpenAI API key because Langflow uses the OpenAI API to process voice input and generate responses.\n\n* Optional: An [ElevenLabs](https://elevenlabs.io) API key to enable more voice options for the LLM's response.\n\n* A microphone and speakers.\n\n    A high quality microphone and minimal background noise are recommended for optimal voice comprehension.\n\n## Test voice mode in the Playground\n\nIn the **Playground**, click the  **Microphone** to enable voice mode and verbally interact with your flows through a microphone and speakers.\n\nThe following steps use the **Simple Agent** template to demonstrate how to enable voice mode:\n\n1. Create a flow based on the **Simple Agent** template.\n\n2. Add your **OpenAI API key** credentials to the **Agent** component.\n\n3. Click **Playground**.\n\n4. Click the  **Microphone** icon to open the **Voice mode** dialog.\n\n5. Enter your OpenAI API key, and then click **Save**. Langflow saves the key as a [global variable](/configuration-global-variables).\n\n6. If you are prompted to grant microphone access, you must allow microphone access to use voice mode.\nIf microphone access is blocked, you won't be able to provide verbal input.\n\n7. For **Audio Input**, select the input device to use with voice mode.\n\n8. Optional: Add an ElevenLabs API key to enable more voices for the LLM's response.\nLangflow saves this key as a global variable.\n\n9. For **Preferred Language**, select the language you want to use for your conversations with the LLM.\nThis option changes both the expected input language and the response language.\n\n10. Speak into your microphone to start the chat.\n\n    If configured correctly, the waveform registers your input, and then the agent's logic and response are described verbally and in the **Playground**.\n\n## Develop applications with websockets endpoints\n\nLangflow exposes two OpenAI Realtime API-compatible websocket endpoints for your flows.\nYou can build applications against these endpoints the same way you would build against [OpenAI Realtime API websockets](https://platform.openai.com/docs/guides/realtime#connect-with-websockets).\n\nThe Langflow API's websockets endpoints require an [OpenAI API key](https://platform.openai.com/docs/overview) for authentication, and they support an optional [ElevenLabs](https://elevenlabs.io) integration with an ElevenLabs API key.\n\nAdditionally, both endpoints require that you provide the flow ID in the endpoint path.\n\n### Voice-to-voice audio streaming\n\nThe `/ws/flow_as_tool/$FLOW_ID` endpoint establishes a connection to OpenAI Realtime voice, and then invokes the specified flow as a tool according to the [OpenAI Realtime model](https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets).\n\nThis approach is ideal for low latency applications, but it is less deterministic because the OpenAI voice-to-voice model determines when to call your flow.\n\n### Speech-to-text audio transcription\n\nThe `/ws/flow_tts/$FLOW_ID` endpoint converts audio to text using [OpenAI Realtime voice transcription](https://platform.openai.com/docs/guides/realtime-transcription), and then directly invokes the specified flow for each transcript.\n\nThis approach is more deterministic but has higher latency.\n\nThis is the mode used in the Langflow **Playground**.\n\n### Session IDs for websockets endpoints\n\nBoth endpoints accept an optional `/$SESSION_ID` path parameter to provide a unique ID for the conversation.\nIf omitted, Langflow uses the flow ID as the [session ID](/session-id).\n\nHowever, be aware that voice mode only maintains context within the current conversation instance.\nWhen you close the **Playground** or end a chat, verbal chat history is discarded and not available for future chat sessions.\n\n## See also\n\n* [Test flows in the Playground](/concepts-playground)",
      "summary": "**Note:** Voice mode is not available in Langflow Desktop.\nTo use voice mode, [Install the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).",
      "keywords": [
        "agent",
        "api",
        "applications",
        "audio",
        "chat",
        "develop",
        "endpoints",
        "ids",
        "input",
        "instructions",
        "key",
        "language",
        "microphone",
        "mode",
        "model",
        "note",
        "openai",
        "output",
        "playground",
        "preferred"
      ]
    },
    "concepts-voice-mode": {
      "title": "Use voice mode",
      "slug": "/concepts-voice-mode",
      "category": "develop",
      "content": "**Note:** Voice mode is not available in Langflow Desktop.\nTo use voice mode, [Install the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n\nYou can use Langflow's voice mode to interact with your flows verbally through a microphone and speakers.\n\n## Prerequisites\n\nVoice mode requires the following:\n\n* A flow with **Chat Input**, **Language Model**, and **Chat Output** components.\n\n    If your flow has an **Agent** component, make sure the tools in your flow have accurate names and descriptions to help the agent choose which tools to use.\n\n    Additionally, be aware that voice mode overrides typed instructions in the **Agent** component's **Agent Instructions** field.\n\n* An [OpenAI](https://platform.openai.com/) account and an OpenAI API key because Langflow uses the OpenAI API to process voice input and generate responses.\n\n* Optional: An [ElevenLabs](https://elevenlabs.io) API key to enable more voice options for the LLM's response.\n\n* A microphone and speakers.\n\n    A high quality microphone and minimal background noise are recommended for optimal voice comprehension.\n\n## Test voice mode in the Playground\n\nIn the **Playground**, click the  **Microphone** to enable voice mode and verbally interact with your flows through a microphone and speakers.\n\nThe following steps use the **Simple Agent** template to demonstrate how to enable voice mode:\n\n1. Create a flow based on the **Simple Agent** template.\n\n2. Add your **OpenAI API key** credentials to the **Agent** component.\n\n3. Click **Playground**.\n\n4. Click the  **Microphone** icon to open the **Voice mode** dialog.\n\n5. Enter your OpenAI API key, and then click **Save**. Langflow saves the key as a [global variable](/configuration-global-variables).\n\n6. If you are prompted to grant microphone access, you must allow microphone access to use voice mode.\nIf microphone access is blocked, you won't be able to provide verbal input.\n\n7. For **Audio Input**, select the input device to use with voice mode.\n\n8. Optional: Add an ElevenLabs API key to enable more voices for the LLM's response.\nLangflow saves this key as a global variable.\n\n9. For **Preferred Language**, select the language you want to use for your conversations with the LLM.\nThis option changes both the expected input language and the response language.\n\n10. Speak into your microphone to start the chat.\n\n    If configured correctly, the waveform registers your input, and then the agent's logic and response are described verbally and in the **Playground**.\n\n## Develop applications with websockets endpoints\n\nLangflow exposes two OpenAI Realtime API-compatible websocket endpoints for your flows.\nYou can build applications against these endpoints the same way you would build against [OpenAI Realtime API websockets](https://platform.openai.com/docs/guides/realtime#connect-with-websockets).\n\nThe Langflow API's websockets endpoints require an [OpenAI API key](https://platform.openai.com/docs/overview) for authentication, and they support an optional [ElevenLabs](https://elevenlabs.io) integration with an ElevenLabs API key.\n\nAdditionally, both endpoints require that you provide the flow ID in the endpoint path.\n\n### Voice-to-voice audio streaming\n\nThe `/ws/flow_as_tool/$FLOW_ID` endpoint establishes a connection to OpenAI Realtime voice, and then invokes the specified flow as a tool according to the [OpenAI Realtime model](https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets).\n\nThis approach is ideal for low latency applications, but it is less deterministic because the OpenAI voice-to-voice model determines when to call your flow.\n\n### Speech-to-text audio transcription\n\nThe `/ws/flow_tts/$FLOW_ID` endpoint converts audio to text using [OpenAI Realtime voice transcription](https://platform.openai.com/docs/guides/realtime-transcription), and then directly invokes the specified flow for each transcript.\n\nThis approach is more deterministic but has higher latency.\n\nThis is the mode used in the Langflow **Playground**.\n\n### Session IDs for websockets endpoints\n\nBoth endpoints accept an optional `/$SESSION_ID` path parameter to provide a unique ID for the conversation.\nIf omitted, Langflow uses the flow ID as the [session ID](/session-id).\n\nHowever, be aware that voice mode only maintains context within the current conversation instance.\nWhen you close the **Playground** or end a chat, verbal chat history is discarded and not available for future chat sessions.\n\n## See also\n\n* [Test flows in the Playground](/concepts-playground)",
      "summary": "**Note:** Voice mode is not available in Langflow Desktop.\nTo use voice mode, [Install the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).",
      "keywords": [
        "agent",
        "api",
        "applications",
        "audio",
        "chat",
        "develop",
        "endpoints",
        "ids",
        "input",
        "instructions",
        "key",
        "language",
        "microphone",
        "mode",
        "model",
        "note",
        "openai",
        "output",
        "playground",
        "preferred"
      ]
    },
    "/configuration-cli": {
      "title": "Langflow CLI",
      "slug": "/configuration-cli",
      "category": "develop",
      "content": "The Langflow command line interface is the main interface for managing and running the Langflow server.\n\nThe Langflow CLI is automatically installed when you [install the Langflow package](/get-started-installation).\nIt isn't available for Langflow Desktop.\n\n## How to use the CLI\n\nThe Langflow CLI can be invoked in several ways, depending on your installation method and environment.\n\nThe recommended approach is to run the CLI with `uv run` from within a virtual environment where Langflow is installed.\n\nFor example, to start Langflow on the default port, run the following command:\n\n    ```bash\n    uv run langflow run\n    ```\n\nIf Langflow is installed globally or added to your PATH, you can execute the CLI directly with `langflow`.\n\n    ```bash\n    langflow run\n    ```\n\n## Precedence\n\nLangflow CLI options override the values of [environment variables](/environment-variables) set in your terminal or primary `.env` file.\n\nFor example, if you have `LANGFLOW_PORT=7860` defined as an environment variable, and you run the CLI with `--port 7880`, then Langflow sets the port to `7880` because the CLI option overrides the environment variable.\n\nThis also applies to Boolean environment variables.\nFor example, if you set `LANGFLOW_REMOVE_API_KEYS=True` in your `.env` file, you can change it to `False` at runtime by running the CLI with `--no-remove-api-keys`.\n\n## Langflow CLI options\n\nAll Langflow CLI commands support options that modify the command's behavior or set environment variables.\n\nTo set values for options, you can use either of the following syntax styles:\n\n* `--option value`\n* `--option=value`\n\nValues with spaces must be surrounded by quotation marks:\n\n* `--option 'Value with Spaces'`\n* `--option=\"Value with Spaces\"`\n\n### Boolean options\n\nBoolean options enable and disable settings.\nThey have a true (enabled) and false (disabled) form:\n\n* Enabled (true): `--option`\n* Disabled (false): `--no-option`\n\nThe following examples compare Boolean option forms for `REMOVE_API_KEYS`.\n\n</Tabs>\n\nIn the following command references, default values for Booleans include both the CLI flag and the equivalent Boolean evaluation, such as \"`--option` (true)\" and \"`--no-option` (false)\".\n\n### Universal options\n\nThe following options are available for all Langflow CLI commands:\n\n* `--version`, `-v`: Show the version and exit.\n* `--install-completion`: Install auto-completion for the current shell.\n* `--show-completion`: Show the location of the auto-completion config file, if installed.\n* `--help`: Print information about command usage, options, and arguments.\n\n## CLI commands\n\nThe following sections describe the available CLI commands and any additional options (beyond the [universal options](#universal-options)) available for each command.\n\n### langflow\n\nRunning the CLI without any arguments prints a list of available options and commands.\n\n  \n</Tabs>\n\n### langflow api-key {#langflow-api-key}\n\nCreates a Langflow API key.\n\nYou must be a superuser to create API keys with the CLI.\nFor more information, see [Langflow API keys](/api-keys-and-authentication#langflow-api-keys).\n\n  \n</Tabs>\n\n#### Options\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |\n\n### langflow copy-db\n\nCopies the Langflow database files from the cache directory to the current Langflow installation directory, which is the directory containing `__main__.py`.\nYou can find the copy target directory by running `which langflow`.\n\nThe following files are copied if they exist in the cache directory:\n\n* `langflow.db`: The main Langflow database, stored in the user cache directory\n* `langflow-pre.db`: The pre-release database, if it exists\n\n  \n</Tabs>\n\n### langflow migration\n\nManages Langflow database schema changes using [Alembic](https://alembic.sqlalchemy.org/en/latest/), a database migration tool for SQLAlchemy.\n\nThe `migration` command has two modes:\n\n* **Test mode (default)**: Checks if migrations can be applied safely without actually running the migrations.\nUse this mode to previews the changes that would be made to the database schema before proceeding with the migrations.\n\n* **Fix mode**: Applies the migrations to update the database schema.\n\n    \n**Note:** `langflow migration --fix` is a destructive operation that can delete data.\n    Always run `langflow migration` first to preview the changes.\n    \n\n  \n</Tabs>\n\n### langflow run {#langflow-run}\n\nStarts the Langflow server.\n\n  \n</Tabs>\n\n#### Options\n\nThis command supports some common and non-sensitive configuration options for your Langflow server.\nOther options must be set in the `.env` or your terminal.\nFor more information Langflow configuration options, see [Langflow environment variables](/environment-variables).\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--auto-saving` | `--auto-saving` (true) | Boolean | Whether to enable flow auto-saving in the visual editor. Use `--no-auto-saving` to disable flow auto-saving. |\n| `--auto-saving-interval` | `1000` | Integer | The interval for flow auto-saving in milliseconds. |\n| `--backend-only` | `--no-backend-only` (false) | Boolean | Whether to run Langflow's backend service only (no frontend). Omit or use `--no-backend-only` to start both the frontend and backend. See [Start Langflow in headless mode](#start-langflow-in-headless-mode). |\n| `--cache` | `async` | String | The type of [cache storage](/memory) to use. One of `async`, `redis`, `memory`, or `disk`. |\n| `--components-path` | Not set | String | The path to the directory containing your custom components. |\n| `--dev` | `--no-dev` (false) | Boolean | Whether to run in development mode (may contain bugs). |\n| `--env-file` | Not set | String | The path to the `.env` file containing Langflow environment variables. See [Start Langflow with a specific .env file](#start-langflow-with-a-specific-env-file). |\n| `--frontend-path` | Not set | String | The path to the frontend directory containing build files. This is only used when [contributing to the Langflow codebase](/contributing-how-to-contribute) or developing a custom Langflow image that includes customized frontend code. |\n| `--health-check-max-retries` | `5` | Integer | The maximum number of retries for your Langflow server's health check. |\n| `--host` | `localhost` | String | The host on which the Langflow server will run. |\n| `--log-file` | `logs/langflow.log` | String | The path to the log file for Langflow. |\n| `--log-level` | `critical` | String | The logging level as one of `debug`, `info`, `warning`, `error`, or `critical`. |\n| `--log-rotation` | Not set | String | The log rotation interval, either a time duration or file size. |\n| `--max-file-size-upload` | `1024` | Integer | The maximum size in megabytes for file uploads. |\n| `--open-browser` | `--no-open-browser` (false) | Boolean | Whether to open the system web browser on startup. Use `--open-browser` to open the system's default web browser when Langflow starts. |\n| `--port` | `7860` | Integer | The port on which the Langflow server will run. The server automatically selects a free port if the specified port is in use. |\n| `--remove-api-keys` | `--no-remove-api-keys` (false) | Boolean | Whether to remove API keys and tokens from flows saved in the Langflow database. |\n| `--ssl-cert-file-path` | Not set | String | The path to the SSL certificate file on the local system for SSL-encrypted connections. |\n| `--ssl-key-file-path` | Not set | String | The path to the SSL key file on the local system for SSL-encrypted connections. |\n| `--worker-timeout` | `300` | Integer | The Langflow server worker timeout in seconds. |\n| `--workers` | `1` | Integer | The number of Langflow server worker processes. |\n\n#### Start Langflow with a specific .env file {#start-langflow-with-a-specific-env-file}\n\nThe `--env-file` option starts Langflow using the configuration defined in the given `.env` file.\nAdditional options appended to this command override the values in the `.env` file if there are duplicates.\n\nIf `--env-file` is omitted or doesn't include all required variables, Langflow uses the default values for those variables.\n\n  \n</Tabs>\n\n#### Start Langflow in headless mode {#start-langflow-in-headless-mode}\n\nThe `--backend-only` option starts Langflow's backend service only.\nThis headless mode has no frontend (visual editor), and you can only access the server programmatically with the Langflow API and CLI.\n\n  \n</Tabs>\n\n### langflow superuser {#langflow-superuser}\n\nCreates a superuser account with the given username and password.\n\n  \n</Tabs>\n\n#### Options\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |\n\nFor this command, `--username` and `--password` aren't optional, and they have no default value.\nThe command fails if you don't provide these arguments.\nFor more information, see [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](/api-keys-and-authentication#langflow-superuser).\n\n#### Disable CLI superuser creation\n\nThe `langflow superuser` command is controlled by the [`LANGFLOW_ENABLE_SUPERUSER_CLI`](/api-keys-and-authentication#langflow-enable-superuser-cli) environment variable:\n\n* **`LANGFLOW_ENABLE_SUPERUSER_CLI=True` (default)**: The `langflow superuser` command is available, and superuser creation is unrestricted.\n* **`LANGFLOW_ENABLE_SUPERUSER_CLI=False` (recommended)**: Disables the `langflow superuser` command.\nFor security reasons, this is recommended to prevent unauthorized superuser creation, especially in production environments.\n\nTo disable the `langflow superuser` command, you must set `LANGFLOW_ENABLE_SUPERUSER_CLI=False` in your Langflow `.env` file, and then [start Langflow with your `.env` file](#start-langflow-with-a-specific-env-file).",
      "summary": "The Langflow command line interface is the main interface for managing and running the Langflow server.",
      "keywords": [
        "api",
        "boolean",
        "cli",
        "commands",
        "copy",
        "creation",
        "default",
        "disable",
        "env",
        "false",
        "file",
        "fix",
        "headless",
        "key",
        "langflow",
        "migration",
        "mode",
        "note",
        "options",
        "precedence"
      ]
    },
    "configuration-cli": {
      "title": "Langflow CLI",
      "slug": "/configuration-cli",
      "category": "develop",
      "content": "The Langflow command line interface is the main interface for managing and running the Langflow server.\n\nThe Langflow CLI is automatically installed when you [install the Langflow package](/get-started-installation).\nIt isn't available for Langflow Desktop.\n\n## How to use the CLI\n\nThe Langflow CLI can be invoked in several ways, depending on your installation method and environment.\n\nThe recommended approach is to run the CLI with `uv run` from within a virtual environment where Langflow is installed.\n\nFor example, to start Langflow on the default port, run the following command:\n\n    ```bash\n    uv run langflow run\n    ```\n\nIf Langflow is installed globally or added to your PATH, you can execute the CLI directly with `langflow`.\n\n    ```bash\n    langflow run\n    ```\n\n## Precedence\n\nLangflow CLI options override the values of [environment variables](/environment-variables) set in your terminal or primary `.env` file.\n\nFor example, if you have `LANGFLOW_PORT=7860` defined as an environment variable, and you run the CLI with `--port 7880`, then Langflow sets the port to `7880` because the CLI option overrides the environment variable.\n\nThis also applies to Boolean environment variables.\nFor example, if you set `LANGFLOW_REMOVE_API_KEYS=True` in your `.env` file, you can change it to `False` at runtime by running the CLI with `--no-remove-api-keys`.\n\n## Langflow CLI options\n\nAll Langflow CLI commands support options that modify the command's behavior or set environment variables.\n\nTo set values for options, you can use either of the following syntax styles:\n\n* `--option value`\n* `--option=value`\n\nValues with spaces must be surrounded by quotation marks:\n\n* `--option 'Value with Spaces'`\n* `--option=\"Value with Spaces\"`\n\n### Boolean options\n\nBoolean options enable and disable settings.\nThey have a true (enabled) and false (disabled) form:\n\n* Enabled (true): `--option`\n* Disabled (false): `--no-option`\n\nThe following examples compare Boolean option forms for `REMOVE_API_KEYS`.\n\n</Tabs>\n\nIn the following command references, default values for Booleans include both the CLI flag and the equivalent Boolean evaluation, such as \"`--option` (true)\" and \"`--no-option` (false)\".\n\n### Universal options\n\nThe following options are available for all Langflow CLI commands:\n\n* `--version`, `-v`: Show the version and exit.\n* `--install-completion`: Install auto-completion for the current shell.\n* `--show-completion`: Show the location of the auto-completion config file, if installed.\n* `--help`: Print information about command usage, options, and arguments.\n\n## CLI commands\n\nThe following sections describe the available CLI commands and any additional options (beyond the [universal options](#universal-options)) available for each command.\n\n### langflow\n\nRunning the CLI without any arguments prints a list of available options and commands.\n\n  \n</Tabs>\n\n### langflow api-key {#langflow-api-key}\n\nCreates a Langflow API key.\n\nYou must be a superuser to create API keys with the CLI.\nFor more information, see [Langflow API keys](/api-keys-and-authentication#langflow-api-keys).\n\n  \n</Tabs>\n\n#### Options\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |\n\n### langflow copy-db\n\nCopies the Langflow database files from the cache directory to the current Langflow installation directory, which is the directory containing `__main__.py`.\nYou can find the copy target directory by running `which langflow`.\n\nThe following files are copied if they exist in the cache directory:\n\n* `langflow.db`: The main Langflow database, stored in the user cache directory\n* `langflow-pre.db`: The pre-release database, if it exists\n\n  \n</Tabs>\n\n### langflow migration\n\nManages Langflow database schema changes using [Alembic](https://alembic.sqlalchemy.org/en/latest/), a database migration tool for SQLAlchemy.\n\nThe `migration` command has two modes:\n\n* **Test mode (default)**: Checks if migrations can be applied safely without actually running the migrations.\nUse this mode to previews the changes that would be made to the database schema before proceeding with the migrations.\n\n* **Fix mode**: Applies the migrations to update the database schema.\n\n    \n**Note:** `langflow migration --fix` is a destructive operation that can delete data.\n    Always run `langflow migration` first to preview the changes.\n    \n\n  \n</Tabs>\n\n### langflow run {#langflow-run}\n\nStarts the Langflow server.\n\n  \n</Tabs>\n\n#### Options\n\nThis command supports some common and non-sensitive configuration options for your Langflow server.\nOther options must be set in the `.env` or your terminal.\nFor more information Langflow configuration options, see [Langflow environment variables](/environment-variables).\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--auto-saving` | `--auto-saving` (true) | Boolean | Whether to enable flow auto-saving in the visual editor. Use `--no-auto-saving` to disable flow auto-saving. |\n| `--auto-saving-interval` | `1000` | Integer | The interval for flow auto-saving in milliseconds. |\n| `--backend-only` | `--no-backend-only` (false) | Boolean | Whether to run Langflow's backend service only (no frontend). Omit or use `--no-backend-only` to start both the frontend and backend. See [Start Langflow in headless mode](#start-langflow-in-headless-mode). |\n| `--cache` | `async` | String | The type of [cache storage](/memory) to use. One of `async`, `redis`, `memory`, or `disk`. |\n| `--components-path` | Not set | String | The path to the directory containing your custom components. |\n| `--dev` | `--no-dev` (false) | Boolean | Whether to run in development mode (may contain bugs). |\n| `--env-file` | Not set | String | The path to the `.env` file containing Langflow environment variables. See [Start Langflow with a specific .env file](#start-langflow-with-a-specific-env-file). |\n| `--frontend-path` | Not set | String | The path to the frontend directory containing build files. This is only used when [contributing to the Langflow codebase](/contributing-how-to-contribute) or developing a custom Langflow image that includes customized frontend code. |\n| `--health-check-max-retries` | `5` | Integer | The maximum number of retries for your Langflow server's health check. |\n| `--host` | `localhost` | String | The host on which the Langflow server will run. |\n| `--log-file` | `logs/langflow.log` | String | The path to the log file for Langflow. |\n| `--log-level` | `critical` | String | The logging level as one of `debug`, `info`, `warning`, `error`, or `critical`. |\n| `--log-rotation` | Not set | String | The log rotation interval, either a time duration or file size. |\n| `--max-file-size-upload` | `1024` | Integer | The maximum size in megabytes for file uploads. |\n| `--open-browser` | `--no-open-browser` (false) | Boolean | Whether to open the system web browser on startup. Use `--open-browser` to open the system's default web browser when Langflow starts. |\n| `--port` | `7860` | Integer | The port on which the Langflow server will run. The server automatically selects a free port if the specified port is in use. |\n| `--remove-api-keys` | `--no-remove-api-keys` (false) | Boolean | Whether to remove API keys and tokens from flows saved in the Langflow database. |\n| `--ssl-cert-file-path` | Not set | String | The path to the SSL certificate file on the local system for SSL-encrypted connections. |\n| `--ssl-key-file-path` | Not set | String | The path to the SSL key file on the local system for SSL-encrypted connections. |\n| `--worker-timeout` | `300` | Integer | The Langflow server worker timeout in seconds. |\n| `--workers` | `1` | Integer | The number of Langflow server worker processes. |\n\n#### Start Langflow with a specific .env file {#start-langflow-with-a-specific-env-file}\n\nThe `--env-file` option starts Langflow using the configuration defined in the given `.env` file.\nAdditional options appended to this command override the values in the `.env` file if there are duplicates.\n\nIf `--env-file` is omitted or doesn't include all required variables, Langflow uses the default values for those variables.\n\n  \n</Tabs>\n\n#### Start Langflow in headless mode {#start-langflow-in-headless-mode}\n\nThe `--backend-only` option starts Langflow's backend service only.\nThis headless mode has no frontend (visual editor), and you can only access the server programmatically with the Langflow API and CLI.\n\n  \n</Tabs>\n\n### langflow superuser {#langflow-superuser}\n\nCreates a superuser account with the given username and password.\n\n  \n</Tabs>\n\n#### Options\n\n| Option | Default | Type | Description |\n|--------|---------|--------|-------------|\n| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |\n\nFor this command, `--username` and `--password` aren't optional, and they have no default value.\nThe command fails if you don't provide these arguments.\nFor more information, see [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](/api-keys-and-authentication#langflow-superuser).\n\n#### Disable CLI superuser creation\n\nThe `langflow superuser` command is controlled by the [`LANGFLOW_ENABLE_SUPERUSER_CLI`](/api-keys-and-authentication#langflow-enable-superuser-cli) environment variable:\n\n* **`LANGFLOW_ENABLE_SUPERUSER_CLI=True` (default)**: The `langflow superuser` command is available, and superuser creation is unrestricted.\n* **`LANGFLOW_ENABLE_SUPERUSER_CLI=False` (recommended)**: Disables the `langflow superuser` command.\nFor security reasons, this is recommended to prevent unauthorized superuser creation, especially in production environments.\n\nTo disable the `langflow superuser` command, you must set `LANGFLOW_ENABLE_SUPERUSER_CLI=False` in your Langflow `.env` file, and then [start Langflow with your `.env` file](#start-langflow-with-a-specific-env-file).",
      "summary": "The Langflow command line interface is the main interface for managing and running the Langflow server.",
      "keywords": [
        "api",
        "boolean",
        "cli",
        "commands",
        "copy",
        "creation",
        "default",
        "disable",
        "env",
        "false",
        "file",
        "fix",
        "headless",
        "key",
        "langflow",
        "migration",
        "mode",
        "note",
        "options",
        "precedence"
      ]
    },
    "/configuration-custom-database": {
      "title": "Configure an external PostgreSQL database",
      "slug": "/configuration-custom-database",
      "category": "develop",
      "content": "Langflow's default database is [SQLite](https://www.sqlite.org/docs.html), but you can configure Langflow to use PostgreSQL instead.\n\nThis guide walks you through setting up an external database for Langflow by replacing the default SQLite connection string `sqlite:///./langflow.db` with PostgreSQL, both in local and containerized environments.\n\nIn this configuration, all structured application data from Langflow, including flows, message history, and logs, is instead managed by PostgreSQL.\nPostgreSQL is better suited for production environments due to its robust support for concurrent users, advanced data integrity features, and scalability.\nLangflow can more efficiently handle multiple users and larger workloads by using PostgreSQL as the database.\n\n## Prerequisites\n\n- A [PostgreSQL](https://www.pgadmin.org/download/) database\n\n## Connect Langflow to a local PostgreSQL database\n\n1. If Langflow is running, stop Langflow with <kbd>Ctrl+C</kbd>.\n\n2. Find your PostgreSQL database's connection string in the format `postgresql://user:password@host:port/dbname`.\n\n    The hostname in your connection string depends on how you're running PostgreSQL:\n\n    - If you're running PostgreSQL directly on your machine, use `localhost`.\n    - If you're running PostgreSQL in Docker Compose, use the service name, such as `postgres`.\n    - If you're running PostgreSQL in a separate Docker container with `docker run`, use the container's IP address or network alias.\n    - If you're running a cloud-hosted PostgreSQL, your provider will share your connection string, which includes a username and password.\n\n3. Edit or create a Langflow `.env` file:\n\n    ```\n    touch .env\n    ```\n\n    You can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.\n\n4. In your `.env` file, set `LANGFLOW_DATABASE_URL` to your PostgreSQL connection string:\n\n    ```text\n    LANGFLOW_DATABASE_URL=\"postgresql://user:password@localhost:5432/dbname\"\n    ```\n\n    Langflow uses [SQLAlchemy](https://www.sqlalchemy.org/) with the [psycopg](https://www.psycopg.org/) driver to pass SSL parameters directly to the PostgreSQL connection. The following SSL modes are supported:\n\n    - **`sslmode=require`**: Requires SSL connection but doesn't verify server certificate. This option is the least secure, but acceptable for most use cases.\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user:password@localhost:5432/dbname?sslmode=require\"\n        ```\n\n    - **`sslmode=verify-ca`**: Requires SSL and verifies the server certificate against the Certificate Authority (CA). Add the certificate paths to your connection string:\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user@localhost:5432/dbname?sslmode=verify-ca&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt\"\n        ```\n\n    - **`sslmode=verify-full`**: Requires SSL, verifies the server certificate, and checks the request hostname against the certificate hostname. The `db.example.com` hostname in this example must match the server certificate's CN. This option is the most secure.\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user@db.example.com:5432/dbname?sslmode=verify-full&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt\"\n        ```\n\n        Do not use the Langflow environment variables [`LANGFLOW_SSL_CERT_FILE`](/environment-variables#server) and [`LANGFLOW_SSL_KEY_FILE`](/environment-variables#server) for your PostgreSQL certificates: these variables are for enabling HTTPS on the Langflow server, not for PostgreSQL database connections.\n\n        For more on managing SSL certificates in PostgreSQL, see the [PostgreSQL documentation](https://www.postgresql.org/docs/9.1/ssl-tcp.html).\n\n5. Save your changes, and then start Langflow with your `.env` file:\n\n    ```bash\n    uv run langflow run --env-file .env\n    ```\n\n    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).\n\n6. In Langflow, run any flow to create traffic.\n\n7. Inspect your PostgreSQL database's tables and activity to verify that new tables and traffic were created after you ran a flow.\n\n## Deploy Langflow and PostgreSQL containers with docker-compose.yml\n\nLaunching Langflow and PostgreSQL containers in the same Docker network ensures proper connectivity between services.\nFor an example, see the [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file in the Langflow repository.\n\nThe configuration in the example `docker-compose.yml` also sets up persistent volumes for both Langflow and PostgreSQL data.\nPersistent volumes map directories inside of containers to storage on the host machine, so data persists through container restarts.\n\nDocker Compose creates an isolated network for all services defined in `docker-compose.yml`. This ensures that the services can communicate with each other using their service names as hostnames, such as `postgres` in the database URL.\nIn contrast, if you run PostgreSQL separately with `docker run`, it launches in a different network than the Langflow container, and this prevents Langflow from connecting to PostgreSQL using the service name.\n\nTo start the Langflow and PostgreSQL services with the example Docker Compose file, navigate to the `langflow/docker_example` directory, and then run `docker-compose up`.\nIf you're using a different `docker-compose.yml` file, run the `docker-compose up` command from the same directory as your `docker-compose.yml` file.\n\n## Deploy multiple Langflow instances with a shared PostgreSQL database\n\nTo configure multiple Langflow instances that share the same PostgreSQL database, modify your `docker-compose.yml` file to include multiple Langflow services.\n\nThis example populates the values in `docker-compose.yml` with values from your Langflow `.env` file.\nThis approach means you only have to manage deployment variables in one file, instead of copying values across multiple files.\n\n1. Update your `.env` file with values for your PostgreSQL database:\n\n    ```text\n    POSTGRES_USER=langflow\n    POSTGRES_PASSWORD=your_secure_password\n    POSTGRES_DB=langflow\n    POSTGRES_HOST=postgres\n    POSTGRES_PORT=5432\n    LANGFLOW_CONFIG_DIR=app/langflow\n    LANGFLOW_PORT_1=7860\n    LANGFLOW_PORT_2=7861\n    LANGFLOW_HOST=0.0.0.0\n    ```\n\n    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).\n\n2. Reference these variables in your `docker-compose.yml`.\nFor example:\n\n    ```yaml\n    services:\n      postgres:\n        image: postgres:16\n        environment:\n          - POSTGRES_USER=${POSTGRES_USER}\n          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n          - POSTGRES_DB=${POSTGRES_DB}\n        ports:\n          - \"${POSTGRES_PORT}:5432\"\n        volumes:\n          - langflow-postgres:/var/lib/postgresql/data\n\n      langflow-1:\n        image: langflowai/langflow:latest\n        pull_policy: always\n        ports:\n          - \"${LANGFLOW_PORT_1}:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n          - LANGFLOW_HOST=${LANGFLOW_HOST}\n          - PORT=7860\n        volumes:\n          - langflow-data-1:/app/langflow\n\n      langflow-2:\n        image: langflowai/langflow:latest\n        pull_policy: always\n        ports:\n          - \"${LANGFLOW_PORT_2}:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n          - LANGFLOW_HOST=${LANGFLOW_HOST}\n          - PORT=7860\n        volumes:\n          - langflow-data-2:/app/langflow\n\n    volumes:\n      langflow-postgres:\n      langflow-data-1:\n      langflow-data-2:\n    ```\n\n3. Deploy the file with `docker-compose up`.\nYou can access the first Langflow instance at `http://localhost:7860`, and the second Langflow instance at `http://localhost:7861`.\n\n4. To confirm both instances are using the same database, run the `docker exec` command to start `psql` in your PostgreSQL container.\nYour container name may vary.\n\n    ```bash\n    docker exec -it docker-test-postgres-1 psql -U langflow -d langflow\n    ```\n\n5. Query the database for active connections:\n\n    ```sql\n    langflow=# SELECT * FROM pg_stat_activity WHERE datname = 'langflow';\n    ```\n\n6. Examine the query results for multiple connections with different `client_addr` values, for example `172.21.0.3` and `172.21.0.4`.\nSince each Langflow instance runs in its own container on the Docker network, using different incoming IP addresses confirms that both instances are actively connected to the PostgreSQL database.\n\n7. To quit `psql`, type `quit`.\n\n## See also\n\n* [Langflow database guide for enterprise DBAs](/enterprise-database-guide)\n* [Memory management options](/memory)\n* [Logs](/logging)",
      "summary": "Langflow's default database is [SQLite](https://www.sqlite.org/docs.html), but you can configure Langflow to use PostgreSQL instead.",
      "keywords": [
        "compose",
        "configure",
        "connect",
        "containers",
        "database",
        "deploy",
        "docker",
        "external",
        "full",
        "instances",
        "langflow",
        "local",
        "multiple",
        "postgresql",
        "prerequisites",
        "require",
        "shared",
        "sslmode",
        "verify",
        "yml"
      ]
    },
    "configuration-custom-database": {
      "title": "Configure an external PostgreSQL database",
      "slug": "/configuration-custom-database",
      "category": "develop",
      "content": "Langflow's default database is [SQLite](https://www.sqlite.org/docs.html), but you can configure Langflow to use PostgreSQL instead.\n\nThis guide walks you through setting up an external database for Langflow by replacing the default SQLite connection string `sqlite:///./langflow.db` with PostgreSQL, both in local and containerized environments.\n\nIn this configuration, all structured application data from Langflow, including flows, message history, and logs, is instead managed by PostgreSQL.\nPostgreSQL is better suited for production environments due to its robust support for concurrent users, advanced data integrity features, and scalability.\nLangflow can more efficiently handle multiple users and larger workloads by using PostgreSQL as the database.\n\n## Prerequisites\n\n- A [PostgreSQL](https://www.pgadmin.org/download/) database\n\n## Connect Langflow to a local PostgreSQL database\n\n1. If Langflow is running, stop Langflow with <kbd>Ctrl+C</kbd>.\n\n2. Find your PostgreSQL database's connection string in the format `postgresql://user:password@host:port/dbname`.\n\n    The hostname in your connection string depends on how you're running PostgreSQL:\n\n    - If you're running PostgreSQL directly on your machine, use `localhost`.\n    - If you're running PostgreSQL in Docker Compose, use the service name, such as `postgres`.\n    - If you're running PostgreSQL in a separate Docker container with `docker run`, use the container's IP address or network alias.\n    - If you're running a cloud-hosted PostgreSQL, your provider will share your connection string, which includes a username and password.\n\n3. Edit or create a Langflow `.env` file:\n\n    ```\n    touch .env\n    ```\n\n    You can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.\n\n4. In your `.env` file, set `LANGFLOW_DATABASE_URL` to your PostgreSQL connection string:\n\n    ```text\n    LANGFLOW_DATABASE_URL=\"postgresql://user:password@localhost:5432/dbname\"\n    ```\n\n    Langflow uses [SQLAlchemy](https://www.sqlalchemy.org/) with the [psycopg](https://www.psycopg.org/) driver to pass SSL parameters directly to the PostgreSQL connection. The following SSL modes are supported:\n\n    - **`sslmode=require`**: Requires SSL connection but doesn't verify server certificate. This option is the least secure, but acceptable for most use cases.\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user:password@localhost:5432/dbname?sslmode=require\"\n        ```\n\n    - **`sslmode=verify-ca`**: Requires SSL and verifies the server certificate against the Certificate Authority (CA). Add the certificate paths to your connection string:\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user@localhost:5432/dbname?sslmode=verify-ca&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt\"\n        ```\n\n    - **`sslmode=verify-full`**: Requires SSL, verifies the server certificate, and checks the request hostname against the certificate hostname. The `db.example.com` hostname in this example must match the server certificate's CN. This option is the most secure.\n        ```bash\n        LANGFLOW_DATABASE_URL=\"postgresql://user@db.example.com:5432/dbname?sslmode=verify-full&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt\"\n        ```\n\n        Do not use the Langflow environment variables [`LANGFLOW_SSL_CERT_FILE`](/environment-variables#server) and [`LANGFLOW_SSL_KEY_FILE`](/environment-variables#server) for your PostgreSQL certificates: these variables are for enabling HTTPS on the Langflow server, not for PostgreSQL database connections.\n\n        For more on managing SSL certificates in PostgreSQL, see the [PostgreSQL documentation](https://www.postgresql.org/docs/9.1/ssl-tcp.html).\n\n5. Save your changes, and then start Langflow with your `.env` file:\n\n    ```bash\n    uv run langflow run --env-file .env\n    ```\n\n    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).\n\n6. In Langflow, run any flow to create traffic.\n\n7. Inspect your PostgreSQL database's tables and activity to verify that new tables and traffic were created after you ran a flow.\n\n## Deploy Langflow and PostgreSQL containers with docker-compose.yml\n\nLaunching Langflow and PostgreSQL containers in the same Docker network ensures proper connectivity between services.\nFor an example, see the [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file in the Langflow repository.\n\nThe configuration in the example `docker-compose.yml` also sets up persistent volumes for both Langflow and PostgreSQL data.\nPersistent volumes map directories inside of containers to storage on the host machine, so data persists through container restarts.\n\nDocker Compose creates an isolated network for all services defined in `docker-compose.yml`. This ensures that the services can communicate with each other using their service names as hostnames, such as `postgres` in the database URL.\nIn contrast, if you run PostgreSQL separately with `docker run`, it launches in a different network than the Langflow container, and this prevents Langflow from connecting to PostgreSQL using the service name.\n\nTo start the Langflow and PostgreSQL services with the example Docker Compose file, navigate to the `langflow/docker_example` directory, and then run `docker-compose up`.\nIf you're using a different `docker-compose.yml` file, run the `docker-compose up` command from the same directory as your `docker-compose.yml` file.\n\n## Deploy multiple Langflow instances with a shared PostgreSQL database\n\nTo configure multiple Langflow instances that share the same PostgreSQL database, modify your `docker-compose.yml` file to include multiple Langflow services.\n\nThis example populates the values in `docker-compose.yml` with values from your Langflow `.env` file.\nThis approach means you only have to manage deployment variables in one file, instead of copying values across multiple files.\n\n1. Update your `.env` file with values for your PostgreSQL database:\n\n    ```text\n    POSTGRES_USER=langflow\n    POSTGRES_PASSWORD=your_secure_password\n    POSTGRES_DB=langflow\n    POSTGRES_HOST=postgres\n    POSTGRES_PORT=5432\n    LANGFLOW_CONFIG_DIR=app/langflow\n    LANGFLOW_PORT_1=7860\n    LANGFLOW_PORT_2=7861\n    LANGFLOW_HOST=0.0.0.0\n    ```\n\n    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).\n\n2. Reference these variables in your `docker-compose.yml`.\nFor example:\n\n    ```yaml\n    services:\n      postgres:\n        image: postgres:16\n        environment:\n          - POSTGRES_USER=${POSTGRES_USER}\n          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}\n          - POSTGRES_DB=${POSTGRES_DB}\n        ports:\n          - \"${POSTGRES_PORT}:5432\"\n        volumes:\n          - langflow-postgres:/var/lib/postgresql/data\n\n      langflow-1:\n        image: langflowai/langflow:latest\n        pull_policy: always\n        ports:\n          - \"${LANGFLOW_PORT_1}:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n          - LANGFLOW_HOST=${LANGFLOW_HOST}\n          - PORT=7860\n        volumes:\n          - langflow-data-1:/app/langflow\n\n      langflow-2:\n        image: langflowai/langflow:latest\n        pull_policy: always\n        ports:\n          - \"${LANGFLOW_PORT_2}:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}\n          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}\n          - LANGFLOW_HOST=${LANGFLOW_HOST}\n          - PORT=7860\n        volumes:\n          - langflow-data-2:/app/langflow\n\n    volumes:\n      langflow-postgres:\n      langflow-data-1:\n      langflow-data-2:\n    ```\n\n3. Deploy the file with `docker-compose up`.\nYou can access the first Langflow instance at `http://localhost:7860`, and the second Langflow instance at `http://localhost:7861`.\n\n4. To confirm both instances are using the same database, run the `docker exec` command to start `psql` in your PostgreSQL container.\nYour container name may vary.\n\n    ```bash\n    docker exec -it docker-test-postgres-1 psql -U langflow -d langflow\n    ```\n\n5. Query the database for active connections:\n\n    ```sql\n    langflow=# SELECT * FROM pg_stat_activity WHERE datname = 'langflow';\n    ```\n\n6. Examine the query results for multiple connections with different `client_addr` values, for example `172.21.0.3` and `172.21.0.4`.\nSince each Langflow instance runs in its own container on the Docker network, using different incoming IP addresses confirms that both instances are actively connected to the PostgreSQL database.\n\n7. To quit `psql`, type `quit`.\n\n## See also\n\n* [Langflow database guide for enterprise DBAs](/enterprise-database-guide)\n* [Memory management options](/memory)\n* [Logs](/logging)",
      "summary": "Langflow's default database is [SQLite](https://www.sqlite.org/docs.html), but you can configure Langflow to use PostgreSQL instead.",
      "keywords": [
        "compose",
        "configure",
        "connect",
        "containers",
        "database",
        "deploy",
        "docker",
        "external",
        "full",
        "instances",
        "langflow",
        "local",
        "multiple",
        "postgresql",
        "prerequisites",
        "require",
        "shared",
        "sslmode",
        "verify",
        "yml"
      ]
    },
    "/configuration-global-variables": {
      "title": "Global variables",
      "slug": "/configuration-global-variables",
      "category": "develop",
      "content": "Use global variables to store and reuse credentials and generic values across all of your flows.\nGlobal variables are typically used by components in flow, and you can use them in any field with the  global variable icon.\n\nIn contrast, [environment variables](/environment-variables), like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, are generally for broader settings that configure how Langflow runs.\nHowever, Langflow can also source global variables from environment variables.\n\nLangflow stores global variables in its internal database, and it encrypts the values using a secret key.\n\n## Create a global variable\n\nTo create a new global variable, follow these steps.\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n2. Click **Global Variables**.\n\n3. Click **Add New**.\n\n4. In the **Create Variable** dialog, enter a name for your variable in the **Variable Name** field.\n\n5. Optional: Select a **Type** for your global variable. The available types are **Generic** (default) and **Credential**.\n\n   Langflow encrypts both **Generic** and **Credential** type global variables.\n   However, **Generic** variables aren't masked in the visual editor, whereas **Credential** variables are masked.\n   **Session ID** fields don't accept **Credential** (masked) variables.\n\n6. Enter the **Value** for your global variable.\n\n7. Optional: Use the **Apply To Fields** menu to select one or more fields that you want Langflow to automatically apply your global variable to. For example, if you select **OpenAI API Key**, Langflow automatically applies the variable to any **OpenAI API Key** field.\n\n8. Click **Save Variable**.\n\nYou can now select your global variable from any text input field that displays the  **Globe** icon.\n\n## Edit a global variable\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n\n2. Click **Global Variables**.\n\n3. Click on the global variable you want to edit.\n\n4. In the **Update Variable** dialog, you can edit the following fields: **Variable Name**, **Value**, and **Apply To Fields**.\n\n5. Click **Update Variable**.\n\n## Delete a global variable\n\nDeleting a global variable permanently deletes the value from the database.\nFlows that reference the deleted global variable will fail.\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n\n2. Click **Global Variables**.\n\n3. Click the checkbox next to the global variable that you want to delete.\n\n4. Click  **Delete**.\n\nThe global variable is deleted from the database.\n\n## Add custom global variables from the environment {#add-custom-global-variables-from-the-environment}\n\nLangflow can source custom global variables from your runtime environment.\nFor information about how Langflow detects and applies environment variables, see [Langflow environment variables](/environment-variables).\n\nLangflow automatically generates global variables based on [`constants.py`](https://github.com/langflow-ai/langflow/blob/main/src/lfx/src/lfx/services/settings/constants.py) if it detects any matching environment variables.\nFor example, if you set `OPENAI_API_KEY` in your runtime environment, Langflow automatically generates a global variable using that value.\n\nYou can declare additional variables in `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT`.\nFor example, `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=WATSONX_PROJECT_ID,WATSONX_API_KEY` creates global variables named `WATSONX_PROJECT_ID` and `WATSONX_API_KEY` in Langflow's database.\nThen, you can use these variables wherever they are needed in your component settings.\n\n</Tabs>\n\nAfter starting Langflow, go to your Langflow **Settings** to confirm that the variables were created.\n\nOnly the **Name** and **Value** are taken from the environment.\nYou can edit the variables in your Langflow **Settings** if you want to configure additional options, such as the **Apply To Fields** option.\n\nGlobal variables sourced from the environment are assigned the **Credential** type, which masks the values in the visual editor.\nHowever, Langflow automatically encrypts _all_ global variables stored in the database.\n\n## Disallow global variables from the environment\n\nIf you want to explicitly prevent Langflow from sourcing global variables from the environment, set `LANGFLOW_STORE_ENVIRONMENT_VARIABLES=False` in your `.env` file.\n\n## Use environment variables for missing global variables {#use-environment-variables-for-missing-global-variables}\n\nIf you want to automatically set fallback values for your global variables to environment variables, set `LANGFLOW_FALLBACK_TO_ENV_VAR=True` in your `.env` file.\nWhen this setting is enabled, if a global variable isn't found, Langflow attempts to use an environment variable with the same name as a backup.\n\nFor example, assume you have the following Langflow `.env` configuration, and your flow has a component that expects a `WATSONX_API_KEY` global variable:\n\n```text\nLANGFLOW_FALLBACK_TO_ENV_VAR=True\nWATSONX_PROJECT_ID=your_project_id\nWATSONX_API_KEY=your_api_key\n```\n\nWhen you run the flow, if there is no global variable named `WATSONX_API_KEY`, Langflow looks for an environment variable named `WATSONX_API_KEY`.\nIn this example, Langflow uses the `WATSONX_API_KEY` value from the `.env` to run the flow.",
      "summary": "Use global variables to store and reuse credentials and generic values across all of your flows.\nGlobal variables are typically used by components in flow, and you can use them in any field with the  global variable icon.",
      "keywords": [
        "add",
        "api",
        "apply",
        "create",
        "credential",
        "custom",
        "delete",
        "disallow",
        "edit",
        "environment",
        "fields",
        "generic",
        "global",
        "globe",
        "key",
        "missing",
        "name",
        "new",
        "openai",
        "save"
      ]
    },
    "configuration-global-variables": {
      "title": "Global variables",
      "slug": "/configuration-global-variables",
      "category": "develop",
      "content": "Use global variables to store and reuse credentials and generic values across all of your flows.\nGlobal variables are typically used by components in flow, and you can use them in any field with the  global variable icon.\n\nIn contrast, [environment variables](/environment-variables), like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, are generally for broader settings that configure how Langflow runs.\nHowever, Langflow can also source global variables from environment variables.\n\nLangflow stores global variables in its internal database, and it encrypts the values using a secret key.\n\n## Create a global variable\n\nTo create a new global variable, follow these steps.\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n2. Click **Global Variables**.\n\n3. Click **Add New**.\n\n4. In the **Create Variable** dialog, enter a name for your variable in the **Variable Name** field.\n\n5. Optional: Select a **Type** for your global variable. The available types are **Generic** (default) and **Credential**.\n\n   Langflow encrypts both **Generic** and **Credential** type global variables.\n   However, **Generic** variables aren't masked in the visual editor, whereas **Credential** variables are masked.\n   **Session ID** fields don't accept **Credential** (masked) variables.\n\n6. Enter the **Value** for your global variable.\n\n7. Optional: Use the **Apply To Fields** menu to select one or more fields that you want Langflow to automatically apply your global variable to. For example, if you select **OpenAI API Key**, Langflow automatically applies the variable to any **OpenAI API Key** field.\n\n8. Click **Save Variable**.\n\nYou can now select your global variable from any text input field that displays the  **Globe** icon.\n\n## Edit a global variable\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n\n2. Click **Global Variables**.\n\n3. Click on the global variable you want to edit.\n\n4. In the **Update Variable** dialog, you can edit the following fields: **Variable Name**, **Value**, and **Apply To Fields**.\n\n5. Click **Update Variable**.\n\n## Delete a global variable\n\nDeleting a global variable permanently deletes the value from the database.\nFlows that reference the deleted global variable will fail.\n\n1. In the Langflow header, click your profile icon, and then select **Settings**.\n\n2. Click **Global Variables**.\n\n3. Click the checkbox next to the global variable that you want to delete.\n\n4. Click  **Delete**.\n\nThe global variable is deleted from the database.\n\n## Add custom global variables from the environment {#add-custom-global-variables-from-the-environment}\n\nLangflow can source custom global variables from your runtime environment.\nFor information about how Langflow detects and applies environment variables, see [Langflow environment variables](/environment-variables).\n\nLangflow automatically generates global variables based on [`constants.py`](https://github.com/langflow-ai/langflow/blob/main/src/lfx/src/lfx/services/settings/constants.py) if it detects any matching environment variables.\nFor example, if you set `OPENAI_API_KEY` in your runtime environment, Langflow automatically generates a global variable using that value.\n\nYou can declare additional variables in `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT`.\nFor example, `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=WATSONX_PROJECT_ID,WATSONX_API_KEY` creates global variables named `WATSONX_PROJECT_ID` and `WATSONX_API_KEY` in Langflow's database.\nThen, you can use these variables wherever they are needed in your component settings.\n\n</Tabs>\n\nAfter starting Langflow, go to your Langflow **Settings** to confirm that the variables were created.\n\nOnly the **Name** and **Value** are taken from the environment.\nYou can edit the variables in your Langflow **Settings** if you want to configure additional options, such as the **Apply To Fields** option.\n\nGlobal variables sourced from the environment are assigned the **Credential** type, which masks the values in the visual editor.\nHowever, Langflow automatically encrypts _all_ global variables stored in the database.\n\n## Disallow global variables from the environment\n\nIf you want to explicitly prevent Langflow from sourcing global variables from the environment, set `LANGFLOW_STORE_ENVIRONMENT_VARIABLES=False` in your `.env` file.\n\n## Use environment variables for missing global variables {#use-environment-variables-for-missing-global-variables}\n\nIf you want to automatically set fallback values for your global variables to environment variables, set `LANGFLOW_FALLBACK_TO_ENV_VAR=True` in your `.env` file.\nWhen this setting is enabled, if a global variable isn't found, Langflow attempts to use an environment variable with the same name as a backup.\n\nFor example, assume you have the following Langflow `.env` configuration, and your flow has a component that expects a `WATSONX_API_KEY` global variable:\n\n```text\nLANGFLOW_FALLBACK_TO_ENV_VAR=True\nWATSONX_PROJECT_ID=your_project_id\nWATSONX_API_KEY=your_api_key\n```\n\nWhen you run the flow, if there is no global variable named `WATSONX_API_KEY`, Langflow looks for an environment variable named `WATSONX_API_KEY`.\nIn this example, Langflow uses the `WATSONX_API_KEY` value from the `.env` to run the flow.",
      "summary": "Use global variables to store and reuse credentials and generic values across all of your flows.\nGlobal variables are typically used by components in flow, and you can use them in any field with the  global variable icon.",
      "keywords": [
        "add",
        "api",
        "apply",
        "create",
        "credential",
        "custom",
        "delete",
        "disallow",
        "edit",
        "environment",
        "fields",
        "generic",
        "global",
        "globe",
        "key",
        "missing",
        "name",
        "new",
        "openai",
        "save"
      ]
    },
    "/contributing-telemetry": {
      "title": "Telemetry",
      "slug": "/contributing-telemetry",
      "category": "develop",
      "content": "Langflow uses anonymous telemetry to collect statistics about feature usage and performance.\nThe Langflow team uses this data to identify popular features and areas that need improvement based on actual usage patterns.\nThis helps prioritize development efforts on the most impactful changes and popular features.\n\n## Privacy\n\nThe Langflow team respects your privacy, and the team is committed to protecting your data.\n\nLangflow telemetry doesn't collect any personal information or sensitive data.\nAll telemetry data is anonymized and used solely for improving Langflow.\n\n## Opt out of telemetry\n\nTo opt out of telemetry, set `DO_NOT_TRACK=True` in your [Langflow environment variables](/environment-variables) before starting Langflow. This disables telemetry data collection.\n\n## Data that Langflow collects\n\nLangflow telemetry collects data on flow runs, your environment, and component usage.\n\n### Run\n\nThis telemetry event is sent every time a flow is executed.\n\n- **IsWebhook**: Indicates whether the operation was triggered with a webhook.\n- **Seconds**: Duration in seconds for how long the operation lasted, providing insights into performance.\n- **Success**: Boolean value indicating whether the operation was successful, helping identify potential errors or issues.\n- **ErrorMessage**: Provides error message details if the operation was unsuccessful, aiding in troubleshooting and enhancements.\n\n### Shutdown\n\nThis telemetry event captures information about application lifecycle and runtime duration.\n\n- **TimeRunning**: Total runtime before shutdown, which is useful for understanding the application lifecycle and optimizing uptime.\n\n### Version\n\nThis telemetry event is sent once when the telemetry service starts.\n\n- **Version**: The specific version of Langflow used, which helps in tracking feature adoption and compatibility.\n- **Platform**: Operating system of the host machine, which helps determine the most popular platforms for development and testing efforts.\n- **Python**: The version of Python used, assisting in maintaining compatibility and support for various Python versions.\n- **Arch**: Architecture of the system, such as x86 or ARM, which helps prioritize hardware optimization and testing in the Langflow codebase.\n- **AutoLogin**: Indicates whether the auto-login feature is enabled, reflecting user preference settings.\n- **CacheType**: Type of caching mechanism used, which impacts performance and efficiency.\n- **BackendOnly**: Boolean indicating whether Langflow is running in backend-only mode, useful for understanding deployment configurations.\n- **Desktop**: Indicates whether Langflow is running in desktop mode (Langflow Desktop), helping to understand usage patterns across different deployment types.\n\n### Playground\n\nThis telemetry event monitors performance and usage patterns in the **Playground** environment.\n\n- **Seconds**: Duration in seconds for **Playground** execution, offering insights into performance during testing or experimental stages.\n- **ComponentCount**: Number of components used in the **Playground**, which helps understand complexity and usage patterns.\n- **Success**: Success status of the **Playground** operation, aiding in identifying the stability of experimental features.\n\n### Component\n\nThis telemetry event is sent for each component execution.\n\n- **Name**: Identifies the component, providing data on which components are most utilized or prone to issues.\n- **Seconds**: Time taken by the component to execute, offering performance metrics.\n- **Success**: Whether the component operated successfully, which helps in quality control.\n- **ErrorMessage**: Details of any errors encountered, crucial for debugging and improvement.\n\n### Exception\n\nThis telemetry event is sent when an unhandled exception is captured by Langflow's lifecycle or global exception handler.\n\n- **Type**: The exception class name, such as `ValueError`.\n- **Message**: The exception message that was raised.\n- **Context**: Additional contextual information related to where the exception occurred, such as route, component, or operation details, when available.\n- **StackTraceHash**: A hash of the stack trace used to group similar exceptions for easier analysis.",
      "summary": "Langflow uses anonymous telemetry to collect statistics about feature usage and performance.\nThe Langflow team uses this data to identify popular features and areas that need improvement based on actual usage patterns.\nThis helps prioritize development efforts on the most impactful changes and...",
      "keywords": [
        "arch",
        "autologin",
        "backendonly",
        "cachetype",
        "collects",
        "component",
        "componentcount",
        "context",
        "data",
        "desktop",
        "errormessage",
        "exception",
        "iswebhook",
        "langflow",
        "message",
        "name",
        "opt",
        "out",
        "platform",
        "playground"
      ]
    },
    "contributing-telemetry": {
      "title": "Telemetry",
      "slug": "/contributing-telemetry",
      "category": "develop",
      "content": "Langflow uses anonymous telemetry to collect statistics about feature usage and performance.\nThe Langflow team uses this data to identify popular features and areas that need improvement based on actual usage patterns.\nThis helps prioritize development efforts on the most impactful changes and popular features.\n\n## Privacy\n\nThe Langflow team respects your privacy, and the team is committed to protecting your data.\n\nLangflow telemetry doesn't collect any personal information or sensitive data.\nAll telemetry data is anonymized and used solely for improving Langflow.\n\n## Opt out of telemetry\n\nTo opt out of telemetry, set `DO_NOT_TRACK=True` in your [Langflow environment variables](/environment-variables) before starting Langflow. This disables telemetry data collection.\n\n## Data that Langflow collects\n\nLangflow telemetry collects data on flow runs, your environment, and component usage.\n\n### Run\n\nThis telemetry event is sent every time a flow is executed.\n\n- **IsWebhook**: Indicates whether the operation was triggered with a webhook.\n- **Seconds**: Duration in seconds for how long the operation lasted, providing insights into performance.\n- **Success**: Boolean value indicating whether the operation was successful, helping identify potential errors or issues.\n- **ErrorMessage**: Provides error message details if the operation was unsuccessful, aiding in troubleshooting and enhancements.\n\n### Shutdown\n\nThis telemetry event captures information about application lifecycle and runtime duration.\n\n- **TimeRunning**: Total runtime before shutdown, which is useful for understanding the application lifecycle and optimizing uptime.\n\n### Version\n\nThis telemetry event is sent once when the telemetry service starts.\n\n- **Version**: The specific version of Langflow used, which helps in tracking feature adoption and compatibility.\n- **Platform**: Operating system of the host machine, which helps determine the most popular platforms for development and testing efforts.\n- **Python**: The version of Python used, assisting in maintaining compatibility and support for various Python versions.\n- **Arch**: Architecture of the system, such as x86 or ARM, which helps prioritize hardware optimization and testing in the Langflow codebase.\n- **AutoLogin**: Indicates whether the auto-login feature is enabled, reflecting user preference settings.\n- **CacheType**: Type of caching mechanism used, which impacts performance and efficiency.\n- **BackendOnly**: Boolean indicating whether Langflow is running in backend-only mode, useful for understanding deployment configurations.\n- **Desktop**: Indicates whether Langflow is running in desktop mode (Langflow Desktop), helping to understand usage patterns across different deployment types.\n\n### Playground\n\nThis telemetry event monitors performance and usage patterns in the **Playground** environment.\n\n- **Seconds**: Duration in seconds for **Playground** execution, offering insights into performance during testing or experimental stages.\n- **ComponentCount**: Number of components used in the **Playground**, which helps understand complexity and usage patterns.\n- **Success**: Success status of the **Playground** operation, aiding in identifying the stability of experimental features.\n\n### Component\n\nThis telemetry event is sent for each component execution.\n\n- **Name**: Identifies the component, providing data on which components are most utilized or prone to issues.\n- **Seconds**: Time taken by the component to execute, offering performance metrics.\n- **Success**: Whether the component operated successfully, which helps in quality control.\n- **ErrorMessage**: Details of any errors encountered, crucial for debugging and improvement.\n\n### Exception\n\nThis telemetry event is sent when an unhandled exception is captured by Langflow's lifecycle or global exception handler.\n\n- **Type**: The exception class name, such as `ValueError`.\n- **Message**: The exception message that was raised.\n- **Context**: Additional contextual information related to where the exception occurred, such as route, component, or operation details, when available.\n- **StackTraceHash**: A hash of the stack trace used to group similar exceptions for easier analysis.",
      "summary": "Langflow uses anonymous telemetry to collect statistics about feature usage and performance.\nThe Langflow team uses this data to identify popular features and areas that need improvement based on actual usage patterns.\nThis helps prioritize development efforts on the most impactful changes and...",
      "keywords": [
        "arch",
        "autologin",
        "backendonly",
        "cachetype",
        "collects",
        "component",
        "componentcount",
        "context",
        "data",
        "desktop",
        "errormessage",
        "exception",
        "iswebhook",
        "langflow",
        "message",
        "name",
        "opt",
        "out",
        "platform",
        "playground"
      ]
    },
    "/data-types": {
      "title": "Langflow data types",
      "slug": "/data-types",
      "category": "develop",
      "content": "Langflow components are designed to accept and produce specific types of inputs and outputs.\nInput and output data types define the structure and flow of information between components.\nUnderstanding these structures helps you build applications that provide valid input and correctly anticipate the output format.\n\n[Component ports](/concepts-components#component-ports) represent the data types that each component can send and receive.\nSome data types are self-evident from the fields they are attached to; for example, a **System Message** field accepts [message data](#message).\n[Port colors](/concepts-components#port-colors) also indicate the port's data type.\nFor example **Data** ports, represented by , either accept or emit [structured data objects](#data).\n\nWhen building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.\n\n**Note:** * In the [workspace](/concepts-overview#workspace), hover over a port to see connection details for that port.\nClick a port to  **Search** for compatible components.\n\n* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.\n\n## Data\n\n**Data** ports  accept or produce the `Data` type, which is a structured data object, like a JSON payload that you might send to an API.\nThis data type is used to pass key-value pairs between components, such as user profiles, settings, or other structured information.\n\n`Data` objects include a primary text field, indicated by a `text_key`, and additional metadata.\n\n### Schema and attributes\n\nThe schema is defined in [`data.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/data.py).\n\nThe following attributes are available:\n\n- `data`: A `Data` object stores key-value pairs within the `.data` attribute. This is the `Data` object's core dictionary. Each key is a field name, and the values can be any supported data type.\n- `text_key`: The key in `data` that is considered the primary text value.\n- `default_value`: Fallback if `text_key` is missing. The default `text_key` is `\"text\"`.\n\n```python\ndata_obj = Data(\n    text_key=\"text\",\n    data={\n        \"text\": \"Hello world\",\n        \"name\": \"Charlie\",\n        \"age\": 28\n    },\n    default_value=\"\"\n)\n```\n\n`Data` objects can be serialized to JSON, created from JSON, or created from other dictionary data.\nHowever, the resulting `Data` object is a structured object with validation and methods, not a plain dictionary.\nFor example, when serialized into JSON, the previous Python example becomes the following JSON object:\n\n```json\n{\n  \"text_key\": \"text\",\n  \"data\": {\n    \"text\": \"Hello world\",\n    \"name\": \"Charlie\",\n    \"age\": 28\n  },\n  \"default_value\": \"\"\n}\n```\n\n## DataFrame\n\n**DataFrame** ports  accept or produce [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), which are similar to the tabular CSV data.\n\nUse the `DataFrame` type to work with data containing multiple rows or records.\n\n### Schema and attributes\n\nThe schema is defined in [`dataframe.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/dataframe.py).\n\nThe following attributes are available:\n\n- **Full pandas compatibility**: All pandas DataFrame methods and functionality are supported\n\n- **Langflow integration**: Accepts lists of [`Data` objects](#data), dictionaries, or existing DataFrames.\n\n- **Convenience methods**:\n   - `to_data_list()`\n   - `add_row()`\n   - `add_rows()`\n   - `to_lc_documents()`\n   - `to_data()`\n   - `to_message()`\n\n- **Text key support**: Maintains `text_key` and `default_value` attributes for [`Data` object](#data) compatibility.\n\n### DataFrame structure\n\nA DataFrame has a tabular data structure with rows and columns.\nKeys are columns, and each object in the array is a row.\n\n```json\n[\n  {\n    \"name\": \"Charlie Lastname\",\n    \"age\": 28,\n    \"email\": \"charlie.lastname@example.com\"\n  },\n  {\n    \"name\": \"Alexandra Example\",\n    \"age\": 34,\n    \"email\": \"alexandra@example.com\"\n  }\n]\n```\n\nWhen represented as tabular data, the preceding DataFrame object is structured as follows:\n\n```text\n| name | age | email |\n|------|-----|-------|\n| Charlie Lastname | 28 | charlie.lastname@example.com |\n| Alexandra Example | 34 | alexandra@example.com |\n```\n\n## Embeddings\n\n**Embeddings** ports  emit or ingest vector embeddings to support functions like similarity search.\n\nThe `Embeddings` data type is used specifically by components that either produce or consume vector embeddings, such as the [embedding model components](/components-embedding-models) and vector store components.\n\nFor example, embedding model components output `Embeddings` data that you can connect to an **Embedding** input port on a vector store component.\n\nFor information about the underlying Python classes that produce `Embeddings`, see the [LangChain Embedding models documentation](https://docs.langchain.com/oss/python/integrations/text_embedding).\n\n## LanguageModel\n\nThe `LanguageModel` type is a specific data type that can be produced by language model components and accepted by components that use an LLM.\n\nWhen you change a language model component's output type from **Model Response** to **Language Model**, the component's output port changes from a **Message** port to a **Language Model** port .\nThen, you connect the outgoing **Language Model** port to a **Language Model** input port on a compatible component, such as a **Smart Function** component.\n\nFor more information about using these components in flows and toggling `LanguageModel` output, see [Language model components](/components-models#language-model-output-types).\n\n<details>\n<summary>LanguageModel is an instance of LangChain ChatModel</summary>\n\nBecause Langflow is built on LangChain, `LanguageModel` is actually an instance of a [LangChain chat model](https://docs.langchain.com/oss/python/integrations/chat) that uses the configuration parameters set in the originating component.\n\nOften, components produce an instance of an integrated chat model that is designed to support the specific model provider, such as [`ChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/openai) or [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic).\n\nYou can inspect the [component code](/concepts-components#component-code) to see the specific `Chat` instance it produces.\n\n</details>\n\n## Memory\n\n**Memory** ports  are used to integrate a **Message History** component with external chat memory storage.\n\nFor more information, see the [**Message History** component](/components-helpers#message-history).\n\n## Message\n\n**Message** ports  accept or produce `Message` data, which extends the [`Data` type](#data) with additional fields and methods for text input typically used in chat flows.\n\nThis data type is used by many components.\n\n**Note:** Components that accept or produce `Message` data may not include all attributes in the incoming or outgoing `Message` data.\nAs long as the data is compatible with the `Message` schema, it can be valid.\n\nWhen building flows, focus on the fields shown on each component in the workspace, rather than the data types passed between components.\nThe details of a particular data type are often only relevant when you are debugging a flow or component that isn't producing the expected output.\n\nFor example, a **Chat Input** component only requires the content of the **Input Text** (`input_value`) field.\nThe component then constructs a complete `Message` object before passing the data to other components in the flow.\n\n### Schema, structure, and attributes\n\nThe `Message` schema is defined in [`message.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/message.py).\nSome `Message` attributes have their own schema definitions, such as [`content_block.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/content_block.py).\n\n`Message` data is structured as a JSON object.\nFor example:\n\n```json\n{\n  \"text\": \"Name: Charlie Lastname, Age: 28, Email: charlie.lastname@example.com\",\n  \"sender\": \"User\",\n  \"sender_name\": \"Charlie Lastname\",\n  \"session_id\": \"some-session-id\",\n  \"timestamp\": \"2024-06-01T12:00:00Z\",\n  \"files\": [],\n  \"content_blocks\": [],\n  \"category\": \"message\"\n}\n```\n\nThe attributes included in a specific `Message` object depend on the context, including the component type, flow activity, and whether the message is a query or response.\nSome common attributes include the following:\n\n- `text`: The main message content.\n- `sender`: Identifies the originator of a chat message as either `User` or `Language Model`.\n- `sender_name`: The display name for the sender. Defaults to `User` or `Language Model`.\n- `session_id`: The chat [session identifier](/session-id).\n- `flow_id`: The ID of the flow that the message is associated with. `flow_id` and `session_id` are the same if the flow doesn't use custom session IDs.\n- `timestamp`: The UTC timestamp that the message was sent.\n- `files`: A list of file paths or images included with the message\n- `content_blocks`: Container for rich content input, such as text, media, or code. Also contains error message information if the LLM can't process the input.\n- `category`: `\"message\"`, `\"error\"`, `\"warning\"`, or `\"info\"`.\n\nNot all attributes are required, and some components accept message-compatible input, such as raw text input.\nThe strictness depends on the component.\n\n### Message data in Input and Output components\n\nIn flows with [**Chat Input and Output** components](/components-io#chat-io), `Message` data provides a consistent structure for chat interactions, and it is ideal for chatbots, conversational analysis, and other use cases based on a dialogue with an LLM or agent.\nIn these flows, the **Playground** chat interface prints only the `Message` attributes that are relevant to the conversation, such as `text`, `files`, and error messages from `content_blocks`.\nTo see all `Message` attributes, inspect the message logs in the **Playground**.\n\nIn flows with [**Text Input and Output** components](/components-io#text-io), `Message` data is used to pass simple text strings without the chat-related metadata.\nThese components handle `Message` data as independent text strings, not as part of an ongoing conversation.\nFor this reason, a flow with only **Text Input and Output** components isn't compatible with the **Playground**.\nFor more information, see [Input and output components](/components-io).\n\nWhen using the Langflow API, the response includes the `Message` object along with other response data from the flow run.\nLangflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.\nFor an example, see the [Langflow quickstart](/get-started-quickstart).\n\nAdditionally, input sent to the input port of input/output components does _not_ need to be a complete `Message` object because the component constructs the `Message` object that is then passed to other components in the flow or returned as flow output.\nIn fact, some components shouldn't receive a complete `Message` object because some attributes, like `timestamp` should be added by the component for accuracy.\n\n## Tool\n\n**Tool** ports  connect tools to an **Agent** component.\n\nTools can be other components where you enabled **Tool Mode**, they can be the dedicated **MCP Tools** component, or they can be other components that only support **Tool Mode**.\nMultiple tools can be connected to the same **Agent** component at the same port.\n\nFunctionally, `Tool` data is a LangChain `StructuredTool` object that can be used in agent flows.\n\nFor more information, see [Configure tools for agents](/agents-tools) and [Use Langflow as an MCP client](/mcp-client).\n\n## Unknown or multiple types\n\nIf a port can accept or produce multiple data types, it is represented by the gray port icon .\n\nHover over the port to see the accepted or produced data types.\n\n## View data types in flows\n\nIn Langflow, you can use  **Inspect output** to view the output of individual components.\nThis can help you learn about the different data type and debug problems with invalid or malformed inputs and output.\n\nThe following example shows how to inspect the output of a [**Type Convert** component](/components-processing#type-convert), which can convert data from one type to another:\n\n1. Create a flow, and then connect a **Chat Input** component to a **Type Convert** component.\n\n2. In the **Chat Input** component, enter some text for the type converter to process.\n\n3. On the **Type Convert** component, click  **Run component**, and then click  **Inspect output**.\n\n    The default output is `Message` data, which is the same as the input coming from the **Chat Input** component.\n    To see the `Message` data converted to `Data` or `DataFrame`, change the **Output Type** on the **Type Convert** component, and then rerun the component.\n\n    \n    \n    \n    </Tabs>\n\n## See also\n\n- [Processing components](/components-processing)\n- [Custom components](/components-custom-components)\n- [Pydantic Models](https://docs.pydantic.dev/latest/api/base_model/)\n- [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)",
      "summary": "Langflow components are designed to accept and produce specific types of inputs and outputs.\nInput and output data types define the structure and flow of information between components.\nUnderstanding these structures helps you build applications that provide valid input and correctly anticipate the...",
      "keywords": [
        "agent",
        "attributes",
        "chat",
        "compatibility",
        "component",
        "components",
        "convenience",
        "convert",
        "data",
        "dataframe",
        "embedding",
        "embeddings",
        "flows",
        "full",
        "function",
        "history",
        "input",
        "inspect",
        "integration",
        "key"
      ]
    },
    "data-types": {
      "title": "Langflow data types",
      "slug": "/data-types",
      "category": "develop",
      "content": "Langflow components are designed to accept and produce specific types of inputs and outputs.\nInput and output data types define the structure and flow of information between components.\nUnderstanding these structures helps you build applications that provide valid input and correctly anticipate the output format.\n\n[Component ports](/concepts-components#component-ports) represent the data types that each component can send and receive.\nSome data types are self-evident from the fields they are attached to; for example, a **System Message** field accepts [message data](#message).\n[Port colors](/concepts-components#port-colors) also indicate the port's data type.\nFor example **Data** ports, represented by , either accept or emit [structured data objects](#data).\n\nWhen building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.\n\n**Note:** * In the [workspace](/concepts-overview#workspace), hover over a port to see connection details for that port.\nClick a port to  **Search** for compatible components.\n\n* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.\n\n## Data\n\n**Data** ports  accept or produce the `Data` type, which is a structured data object, like a JSON payload that you might send to an API.\nThis data type is used to pass key-value pairs between components, such as user profiles, settings, or other structured information.\n\n`Data` objects include a primary text field, indicated by a `text_key`, and additional metadata.\n\n### Schema and attributes\n\nThe schema is defined in [`data.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/data.py).\n\nThe following attributes are available:\n\n- `data`: A `Data` object stores key-value pairs within the `.data` attribute. This is the `Data` object's core dictionary. Each key is a field name, and the values can be any supported data type.\n- `text_key`: The key in `data` that is considered the primary text value.\n- `default_value`: Fallback if `text_key` is missing. The default `text_key` is `\"text\"`.\n\n```python\ndata_obj = Data(\n    text_key=\"text\",\n    data={\n        \"text\": \"Hello world\",\n        \"name\": \"Charlie\",\n        \"age\": 28\n    },\n    default_value=\"\"\n)\n```\n\n`Data` objects can be serialized to JSON, created from JSON, or created from other dictionary data.\nHowever, the resulting `Data` object is a structured object with validation and methods, not a plain dictionary.\nFor example, when serialized into JSON, the previous Python example becomes the following JSON object:\n\n```json\n{\n  \"text_key\": \"text\",\n  \"data\": {\n    \"text\": \"Hello world\",\n    \"name\": \"Charlie\",\n    \"age\": 28\n  },\n  \"default_value\": \"\"\n}\n```\n\n## DataFrame\n\n**DataFrame** ports  accept or produce [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), which are similar to the tabular CSV data.\n\nUse the `DataFrame` type to work with data containing multiple rows or records.\n\n### Schema and attributes\n\nThe schema is defined in [`dataframe.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/dataframe.py).\n\nThe following attributes are available:\n\n- **Full pandas compatibility**: All pandas DataFrame methods and functionality are supported\n\n- **Langflow integration**: Accepts lists of [`Data` objects](#data), dictionaries, or existing DataFrames.\n\n- **Convenience methods**:\n   - `to_data_list()`\n   - `add_row()`\n   - `add_rows()`\n   - `to_lc_documents()`\n   - `to_data()`\n   - `to_message()`\n\n- **Text key support**: Maintains `text_key` and `default_value` attributes for [`Data` object](#data) compatibility.\n\n### DataFrame structure\n\nA DataFrame has a tabular data structure with rows and columns.\nKeys are columns, and each object in the array is a row.\n\n```json\n[\n  {\n    \"name\": \"Charlie Lastname\",\n    \"age\": 28,\n    \"email\": \"charlie.lastname@example.com\"\n  },\n  {\n    \"name\": \"Alexandra Example\",\n    \"age\": 34,\n    \"email\": \"alexandra@example.com\"\n  }\n]\n```\n\nWhen represented as tabular data, the preceding DataFrame object is structured as follows:\n\n```text\n| name | age | email |\n|------|-----|-------|\n| Charlie Lastname | 28 | charlie.lastname@example.com |\n| Alexandra Example | 34 | alexandra@example.com |\n```\n\n## Embeddings\n\n**Embeddings** ports  emit or ingest vector embeddings to support functions like similarity search.\n\nThe `Embeddings` data type is used specifically by components that either produce or consume vector embeddings, such as the [embedding model components](/components-embedding-models) and vector store components.\n\nFor example, embedding model components output `Embeddings` data that you can connect to an **Embedding** input port on a vector store component.\n\nFor information about the underlying Python classes that produce `Embeddings`, see the [LangChain Embedding models documentation](https://docs.langchain.com/oss/python/integrations/text_embedding).\n\n## LanguageModel\n\nThe `LanguageModel` type is a specific data type that can be produced by language model components and accepted by components that use an LLM.\n\nWhen you change a language model component's output type from **Model Response** to **Language Model**, the component's output port changes from a **Message** port to a **Language Model** port .\nThen, you connect the outgoing **Language Model** port to a **Language Model** input port on a compatible component, such as a **Smart Function** component.\n\nFor more information about using these components in flows and toggling `LanguageModel` output, see [Language model components](/components-models#language-model-output-types).\n\n<details>\n<summary>LanguageModel is an instance of LangChain ChatModel</summary>\n\nBecause Langflow is built on LangChain, `LanguageModel` is actually an instance of a [LangChain chat model](https://docs.langchain.com/oss/python/integrations/chat) that uses the configuration parameters set in the originating component.\n\nOften, components produce an instance of an integrated chat model that is designed to support the specific model provider, such as [`ChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/openai) or [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic).\n\nYou can inspect the [component code](/concepts-components#component-code) to see the specific `Chat` instance it produces.\n\n</details>\n\n## Memory\n\n**Memory** ports  are used to integrate a **Message History** component with external chat memory storage.\n\nFor more information, see the [**Message History** component](/components-helpers#message-history).\n\n## Message\n\n**Message** ports  accept or produce `Message` data, which extends the [`Data` type](#data) with additional fields and methods for text input typically used in chat flows.\n\nThis data type is used by many components.\n\n**Note:** Components that accept or produce `Message` data may not include all attributes in the incoming or outgoing `Message` data.\nAs long as the data is compatible with the `Message` schema, it can be valid.\n\nWhen building flows, focus on the fields shown on each component in the workspace, rather than the data types passed between components.\nThe details of a particular data type are often only relevant when you are debugging a flow or component that isn't producing the expected output.\n\nFor example, a **Chat Input** component only requires the content of the **Input Text** (`input_value`) field.\nThe component then constructs a complete `Message` object before passing the data to other components in the flow.\n\n### Schema, structure, and attributes\n\nThe `Message` schema is defined in [`message.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/message.py).\nSome `Message` attributes have their own schema definitions, such as [`content_block.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/content_block.py).\n\n`Message` data is structured as a JSON object.\nFor example:\n\n```json\n{\n  \"text\": \"Name: Charlie Lastname, Age: 28, Email: charlie.lastname@example.com\",\n  \"sender\": \"User\",\n  \"sender_name\": \"Charlie Lastname\",\n  \"session_id\": \"some-session-id\",\n  \"timestamp\": \"2024-06-01T12:00:00Z\",\n  \"files\": [],\n  \"content_blocks\": [],\n  \"category\": \"message\"\n}\n```\n\nThe attributes included in a specific `Message` object depend on the context, including the component type, flow activity, and whether the message is a query or response.\nSome common attributes include the following:\n\n- `text`: The main message content.\n- `sender`: Identifies the originator of a chat message as either `User` or `Language Model`.\n- `sender_name`: The display name for the sender. Defaults to `User` or `Language Model`.\n- `session_id`: The chat [session identifier](/session-id).\n- `flow_id`: The ID of the flow that the message is associated with. `flow_id` and `session_id` are the same if the flow doesn't use custom session IDs.\n- `timestamp`: The UTC timestamp that the message was sent.\n- `files`: A list of file paths or images included with the message\n- `content_blocks`: Container for rich content input, such as text, media, or code. Also contains error message information if the LLM can't process the input.\n- `category`: `\"message\"`, `\"error\"`, `\"warning\"`, or `\"info\"`.\n\nNot all attributes are required, and some components accept message-compatible input, such as raw text input.\nThe strictness depends on the component.\n\n### Message data in Input and Output components\n\nIn flows with [**Chat Input and Output** components](/components-io#chat-io), `Message` data provides a consistent structure for chat interactions, and it is ideal for chatbots, conversational analysis, and other use cases based on a dialogue with an LLM or agent.\nIn these flows, the **Playground** chat interface prints only the `Message` attributes that are relevant to the conversation, such as `text`, `files`, and error messages from `content_blocks`.\nTo see all `Message` attributes, inspect the message logs in the **Playground**.\n\nIn flows with [**Text Input and Output** components](/components-io#text-io), `Message` data is used to pass simple text strings without the chat-related metadata.\nThese components handle `Message` data as independent text strings, not as part of an ongoing conversation.\nFor this reason, a flow with only **Text Input and Output** components isn't compatible with the **Playground**.\nFor more information, see [Input and output components](/components-io).\n\nWhen using the Langflow API, the response includes the `Message` object along with other response data from the flow run.\nLangflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.\nFor an example, see the [Langflow quickstart](/get-started-quickstart).\n\nAdditionally, input sent to the input port of input/output components does _not_ need to be a complete `Message` object because the component constructs the `Message` object that is then passed to other components in the flow or returned as flow output.\nIn fact, some components shouldn't receive a complete `Message` object because some attributes, like `timestamp` should be added by the component for accuracy.\n\n## Tool\n\n**Tool** ports  connect tools to an **Agent** component.\n\nTools can be other components where you enabled **Tool Mode**, they can be the dedicated **MCP Tools** component, or they can be other components that only support **Tool Mode**.\nMultiple tools can be connected to the same **Agent** component at the same port.\n\nFunctionally, `Tool` data is a LangChain `StructuredTool` object that can be used in agent flows.\n\nFor more information, see [Configure tools for agents](/agents-tools) and [Use Langflow as an MCP client](/mcp-client).\n\n## Unknown or multiple types\n\nIf a port can accept or produce multiple data types, it is represented by the gray port icon .\n\nHover over the port to see the accepted or produced data types.\n\n## View data types in flows\n\nIn Langflow, you can use  **Inspect output** to view the output of individual components.\nThis can help you learn about the different data type and debug problems with invalid or malformed inputs and output.\n\nThe following example shows how to inspect the output of a [**Type Convert** component](/components-processing#type-convert), which can convert data from one type to another:\n\n1. Create a flow, and then connect a **Chat Input** component to a **Type Convert** component.\n\n2. In the **Chat Input** component, enter some text for the type converter to process.\n\n3. On the **Type Convert** component, click  **Run component**, and then click  **Inspect output**.\n\n    The default output is `Message` data, which is the same as the input coming from the **Chat Input** component.\n    To see the `Message` data converted to `Data` or `DataFrame`, change the **Output Type** on the **Type Convert** component, and then rerun the component.\n\n    \n    \n    \n    </Tabs>\n\n## See also\n\n- [Processing components](/components-processing)\n- [Custom components](/components-custom-components)\n- [Pydantic Models](https://docs.pydantic.dev/latest/api/base_model/)\n- [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)",
      "summary": "Langflow components are designed to accept and produce specific types of inputs and outputs.\nInput and output data types define the structure and flow of information between components.\nUnderstanding these structures helps you build applications that provide valid input and correctly anticipate the...",
      "keywords": [
        "agent",
        "attributes",
        "chat",
        "compatibility",
        "component",
        "components",
        "convenience",
        "convert",
        "data",
        "dataframe",
        "embedding",
        "embeddings",
        "flows",
        "full",
        "function",
        "history",
        "input",
        "inspect",
        "integration",
        "key"
      ]
    },
    "/enterprise-database-guide": {
      "title": "Langflow database guide for enterprise DBAs",
      "slug": "/enterprise-database-guide",
      "category": "develop",
      "content": "The Langflow database stores data that is essential for more Langflow operations, including startup, flow execution, user interactions, and administrative tasks.\nThe database supports both frontend (visual editor) and backend (API) operations, making its availability critical to Langflow's stability and functionality.\nFor details about the database schema, see [Memory management options](/memory).\n\nThis guide is designed for enterprise database administrators (DBAs) and operators responsible for deploying and managing Langflow in production environments.\nIt explains how to configure Langflow to use PostgreSQL, including high availability (HA) and active-active configurations, as well as best practices for monitoring, maintenance, and security.\n\n## Configure Langflow with PostgreSQL\n\nLangflow's default database is SQLite.\nHowever, PostgreSQL is recommended for production deployments due to its scalability, performance, and robustness.\n\nThe following steps explain how to configure Langflow to use PostgreSQL for a standalone or containerized deployment.\nFor more information, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\n1. Set up PostgreSQL:\n\n   1. Deploy a PostgreSQL instance (version 12 or higher recommended) using a local server, Docker, or a managed cloud service.\n   2. Create a database for Langflow.\n   3. Create a PostgreSQL user with appropriate, minimal permissions to manage and write to the database, such as CREATE, SELECT, INSERT, UPDATE, DELETE on your Langflow tables.\n\n2. Obtain the connection string in the format `postgresql://user:password@host:port/dbname`, such as`postgresql://langflow:securepassword@postgres:5432/langflow`.\n\n    For High Availability, use the virtual IP or proxy hostname instead of the direct database host.\n    For more information, see [High Availability for PostgreSQL](#high-availability-for-postgresql).\n\n3. Configure Langflow with the `.env` or `docker-compose.yml` files.\n\n    \n    \n    </Tabs>\n\n4. Start Langflow with your PostgreSQL connection:\n\n    \n      \n    </Tabs>\n\n5. Optional: Run migrations.\n\n    Langflow uses migrations to manage its database schema.\n    When you first connect to PostgreSQL, Langflow automatically runs migrations to create the necessary tables.\n\n    Direct schema modification can cause conflicts with Langflow's built-in schema management.\n    If you need to update the schema, you can manually run migrations with the Langflow CLI:\n\n    1. Run `langflow migration` to preview the changes.\n\n    2. Review the changes to ensure that it's safe to proceed with the migration.\n\n    3. Run `langflow migration --fix` to run the migration and permanently apply the changes.\n\n        This is a destructive operation that can delete data.\n        For more information, see [`langflow migration`](/configuration-cli#langflow-migration).\n\n6. To verify the configuration, create any flow using the Langflow visual editor or API, and then query your database to confirm the tables and activity are recorded there. The content of the flow doesn't matter; you only need to confirm that the flow is stored in your PostgreSQL database.\nYou can query the database in two ways:\n\n    * Query the database container:\n\n        ```\n        docker exec -it <postgres-container> psql -U langflow -d langflow\n        ```\n\n    * Use SQL:\n\n        ```\n        SELECT * FROM pg_stat_activity WHERE datname = 'langflow';\n        ```\n\n### High Availability for PostgreSQL {#high-availability-for-postgresql}\n\nTo further improve performance, reliability, and scalability, use a High Availability (HA) or Active-Active HA PostgreSQL configuration.\nThis is recommended for production deployments to minimize downtime and ensure continuous operations if your database server fails, especially when multiple Langflow instances rely on the same database.\n\n</Tabs>\n\nAfter implementing HA or Active-Active HA, monitor failover events and ensure replicas are in sync.\nLangflow, through [SQLAlchemy](https://docs.sqlalchemy.org/en/20/), supports reconnection attempts if `LANGFLOW_DATABASE_CONNECTION_RETRY=True`, ensures recovery after failover, and reduces disruption once the database is back online.\n\nAlthough PostgreSQL handles concurrent connections well, you must still monitor for contention, deadlocks, or other performance degradation during high load.\n\n## Impact of database failure\n\nIf the PostgreSQL database becomes unavailable, the following Langflow functions will fail:\n\n* **Flow Retrieval**: Cannot load new or existing flows from the database.\n* **Flow Saving**: Unable to save new flows or updates to existing flows.\n* **User Authentication**: Login and user management functions fail.\n* **Project Collection Access**: Cannot access or share community/custom project collections.\n* **Configuration Retrieval**: Unable to load application settings.\n* **Configuration Updates**: Changes to settings cannot be saved.\n* **Execution Log Access**: Cannot retrieve historical flow execution logs.\n* **Log Writing**: New execution or system activity logs cannot be recorded.\n* **Multi-User Collaboration**: Sharing flows or projects across users fails.\n* **API Flow Loading**: API requests to load new flows (non-cached) fail.\n\nFlows already loaded in memory may continue to function with cached configurations.\nHowever, any operation requiring database access fails until the database is restored.\nFor example, a cached flow might run, but it won't record logs or message history to the database.\n\nTo minimize the possibility and impact of database failure, use [HA configurations](#high-availability-for-postgresql) and record backups regularly.\nFor example, you can use `pg_dump` to create logical backups or set up continuous archiving with write-ahead logs (WAL) for point-in-time recovery.\nTest restoration procedures regularly to ensure your team understands how to execute them in a disaster recovery scenario.\n\n## Database monitoring\n\nMonitor your PostgreSQL database to ensure optimal performance and reliability:\n\n* Use tools like pgAdmin, Prometheus with PostgreSQL exporter, or cloud-based monitoring for PostgreSQL.\n* Track performance metrics such as CPU, memory, and disk I/O usage.\n* Monitor replica health, availability, lag, and synchronization.\nFor example, use `pg_stat_activity` to monitor connection counts and contention.\n* Set up alerts and notifications for high latency, failover events, or replication issues.\n* Enable PostgreSQL logging, such as `log_connections` and `log_statements`, to track access and changes.\n\n## See also\n\n* [Configure an external PostgreSQL database](/configuration-custom-database)\n* [Langflow architecture on Kubernetes](/deployment-architecture)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "The Langflow database stores data that is essential for more Langflow operations, including startup, flow execution, user interactions, and administrative tasks.\nThe database supports both frontend (visual editor) and backend (API) operations, making its availability critical to Langflow's...",
      "keywords": [
        "access",
        "api",
        "authentication",
        "availability",
        "collaboration",
        "collection",
        "configuration",
        "configure",
        "database",
        "dbas",
        "enterprise",
        "execution",
        "failure",
        "flow",
        "guide",
        "high",
        "impact",
        "langflow",
        "loading",
        "log"
      ]
    },
    "enterprise-database-guide": {
      "title": "Langflow database guide for enterprise DBAs",
      "slug": "/enterprise-database-guide",
      "category": "develop",
      "content": "The Langflow database stores data that is essential for more Langflow operations, including startup, flow execution, user interactions, and administrative tasks.\nThe database supports both frontend (visual editor) and backend (API) operations, making its availability critical to Langflow's stability and functionality.\nFor details about the database schema, see [Memory management options](/memory).\n\nThis guide is designed for enterprise database administrators (DBAs) and operators responsible for deploying and managing Langflow in production environments.\nIt explains how to configure Langflow to use PostgreSQL, including high availability (HA) and active-active configurations, as well as best practices for monitoring, maintenance, and security.\n\n## Configure Langflow with PostgreSQL\n\nLangflow's default database is SQLite.\nHowever, PostgreSQL is recommended for production deployments due to its scalability, performance, and robustness.\n\nThe following steps explain how to configure Langflow to use PostgreSQL for a standalone or containerized deployment.\nFor more information, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\n1. Set up PostgreSQL:\n\n   1. Deploy a PostgreSQL instance (version 12 or higher recommended) using a local server, Docker, or a managed cloud service.\n   2. Create a database for Langflow.\n   3. Create a PostgreSQL user with appropriate, minimal permissions to manage and write to the database, such as CREATE, SELECT, INSERT, UPDATE, DELETE on your Langflow tables.\n\n2. Obtain the connection string in the format `postgresql://user:password@host:port/dbname`, such as`postgresql://langflow:securepassword@postgres:5432/langflow`.\n\n    For High Availability, use the virtual IP or proxy hostname instead of the direct database host.\n    For more information, see [High Availability for PostgreSQL](#high-availability-for-postgresql).\n\n3. Configure Langflow with the `.env` or `docker-compose.yml` files.\n\n    \n    \n    </Tabs>\n\n4. Start Langflow with your PostgreSQL connection:\n\n    \n      \n    </Tabs>\n\n5. Optional: Run migrations.\n\n    Langflow uses migrations to manage its database schema.\n    When you first connect to PostgreSQL, Langflow automatically runs migrations to create the necessary tables.\n\n    Direct schema modification can cause conflicts with Langflow's built-in schema management.\n    If you need to update the schema, you can manually run migrations with the Langflow CLI:\n\n    1. Run `langflow migration` to preview the changes.\n\n    2. Review the changes to ensure that it's safe to proceed with the migration.\n\n    3. Run `langflow migration --fix` to run the migration and permanently apply the changes.\n\n        This is a destructive operation that can delete data.\n        For more information, see [`langflow migration`](/configuration-cli#langflow-migration).\n\n6. To verify the configuration, create any flow using the Langflow visual editor or API, and then query your database to confirm the tables and activity are recorded there. The content of the flow doesn't matter; you only need to confirm that the flow is stored in your PostgreSQL database.\nYou can query the database in two ways:\n\n    * Query the database container:\n\n        ```\n        docker exec -it <postgres-container> psql -U langflow -d langflow\n        ```\n\n    * Use SQL:\n\n        ```\n        SELECT * FROM pg_stat_activity WHERE datname = 'langflow';\n        ```\n\n### High Availability for PostgreSQL {#high-availability-for-postgresql}\n\nTo further improve performance, reliability, and scalability, use a High Availability (HA) or Active-Active HA PostgreSQL configuration.\nThis is recommended for production deployments to minimize downtime and ensure continuous operations if your database server fails, especially when multiple Langflow instances rely on the same database.\n\n</Tabs>\n\nAfter implementing HA or Active-Active HA, monitor failover events and ensure replicas are in sync.\nLangflow, through [SQLAlchemy](https://docs.sqlalchemy.org/en/20/), supports reconnection attempts if `LANGFLOW_DATABASE_CONNECTION_RETRY=True`, ensures recovery after failover, and reduces disruption once the database is back online.\n\nAlthough PostgreSQL handles concurrent connections well, you must still monitor for contention, deadlocks, or other performance degradation during high load.\n\n## Impact of database failure\n\nIf the PostgreSQL database becomes unavailable, the following Langflow functions will fail:\n\n* **Flow Retrieval**: Cannot load new or existing flows from the database.\n* **Flow Saving**: Unable to save new flows or updates to existing flows.\n* **User Authentication**: Login and user management functions fail.\n* **Project Collection Access**: Cannot access or share community/custom project collections.\n* **Configuration Retrieval**: Unable to load application settings.\n* **Configuration Updates**: Changes to settings cannot be saved.\n* **Execution Log Access**: Cannot retrieve historical flow execution logs.\n* **Log Writing**: New execution or system activity logs cannot be recorded.\n* **Multi-User Collaboration**: Sharing flows or projects across users fails.\n* **API Flow Loading**: API requests to load new flows (non-cached) fail.\n\nFlows already loaded in memory may continue to function with cached configurations.\nHowever, any operation requiring database access fails until the database is restored.\nFor example, a cached flow might run, but it won't record logs or message history to the database.\n\nTo minimize the possibility and impact of database failure, use [HA configurations](#high-availability-for-postgresql) and record backups regularly.\nFor example, you can use `pg_dump` to create logical backups or set up continuous archiving with write-ahead logs (WAL) for point-in-time recovery.\nTest restoration procedures regularly to ensure your team understands how to execute them in a disaster recovery scenario.\n\n## Database monitoring\n\nMonitor your PostgreSQL database to ensure optimal performance and reliability:\n\n* Use tools like pgAdmin, Prometheus with PostgreSQL exporter, or cloud-based monitoring for PostgreSQL.\n* Track performance metrics such as CPU, memory, and disk I/O usage.\n* Monitor replica health, availability, lag, and synchronization.\nFor example, use `pg_stat_activity` to monitor connection counts and contention.\n* Set up alerts and notifications for high latency, failover events, or replication issues.\n* Enable PostgreSQL logging, such as `log_connections` and `log_statements`, to track access and changes.\n\n## See also\n\n* [Configure an external PostgreSQL database](/configuration-custom-database)\n* [Langflow architecture on Kubernetes](/deployment-architecture)\n* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)",
      "summary": "The Langflow database stores data that is essential for more Langflow operations, including startup, flow execution, user interactions, and administrative tasks.\nThe database supports both frontend (visual editor) and backend (API) operations, making its availability critical to Langflow's...",
      "keywords": [
        "access",
        "api",
        "authentication",
        "availability",
        "collaboration",
        "collection",
        "configuration",
        "configure",
        "database",
        "dbas",
        "enterprise",
        "execution",
        "failure",
        "flow",
        "guide",
        "high",
        "impact",
        "langflow",
        "loading",
        "log"
      ]
    },
    "/environment-variables": {
      "title": "Environment variables",
      "slug": "/environment-variables",
      "category": "develop",
      "content": "In general, environment variables, like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, configure how Langflow runs.\nThese are broad settings that apply to your entire Langflow deployment.\n\nIn contrast, global variables are user-defined values stored in Langflow's database for use in flows, such as `OPENAI_API_KEY`.\nLangflow can also source global variables from environment variables.\nFor more information, see [Langflow global variables](/configuration-global-variables).\n\n## Configure environment variables for Langflow OSS\n\nLangflow recognizes [supported environment variables](#supported-variables) from the following sources:\n\n- Environment variables set in your terminal.\n- Environment variables imported from a `.env` file when starting Langflow.\n- Environment variables set with the [Langflow CLI](./configuration-cli), including the `--env-file` option and direct options, such as `--port`.\n\nYou can choose to use one or more of these sources.\n\n### Precedence {#precedence}\n\nIf the same environment variable is set in multiple places, the following hierarchy applies:\n\n1. Langflow CLI options override all other sources.\n2. The `.env` file overrides system environment variables.\n3. System environment variables are used only if not set elsewhere.\nWhen running a Langflow Docker image, the `-e` flag can be used to set additional system environment variables.\n\nFor example:\n\n* If you set `LANGFLOW_PORT=8080` in your system environment and `LANGFLOW_PORT=7860` in `.env`, Langflow uses `7860` from `.env`.\n* If you use the Langflow CLI to run `langflow run --env-file .env --port 9000`, and you set `LANGFLOW_PORT=7860` in `.env`, then Langflow uses `9000` from the CLI option.\n\n### Set environment variables in your terminal {#configure-variables-terminal}\n\nRun the following commands to set environment variables for your current terminal session:\n\n</Tabs>\n\nWhen you start Langflow, it looks for environment variables that you've set in your terminal.\nIf it detects a supported environment variable, then it automatically adopts the specified value, subject to [precedence rules](#precedence).\n\n### Import environment variables from a .env file {#configure-variables-env-file}\n\n1. If Langflow is running, quit Langflow.\n\n2. Create a `.env` file, and then open it in your preferred editor.\n\n3. Define [Langflow environment variables](#supported-variables) in the `.env` file.\n\n    <details>\n    <summary>Example: .env</summary>\n\n    ```text\n    DO_NOT_TRACK=True\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_AUTO_SAVING=True\n    LANGFLOW_AUTO_SAVING_INTERVAL=1000\n    LANGFLOW_BACKEND_ONLY=False\n    LANGFLOW_BUNDLE_URLS=[\"https://github.com/user/repo/commit/hash\"]\n    LANGFLOW_CACHE_TYPE=async\n    LANGFLOW_COMPONENTS_PATH=/path/to/components/\n    LANGFLOW_CONFIG_DIR=/path/to/config/\n    LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow\n    LANGFLOW_DEV=False\n    LANGFLOW_FALLBACK_TO_ENV_VAR=False\n    LANGFLOW_HEALTH_CHECK_MAX_RETRIES=5\n    LANGFLOW_HOST=localhost\n    LANGFLOW_LANGCHAIN_CACHE=InMemoryCache\n    LANGFLOW_MAX_FILE_SIZE_UPLOAD=10000\n    LANGFLOW_MAX_ITEMS_LENGTH=100\n    LANGFLOW_MAX_TEXT_LENGTH=1000\n    LANGFLOW_LOG_LEVEL=error\n    LANGFLOW_OPEN_BROWSER=False\n    LANGFLOW_PORT=7860\n    LANGFLOW_REMOVE_API_KEYS=False\n    LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True\n    LANGFLOW_SECRET_KEY=somesecretkey\n    LANGFLOW_STORE_ENVIRONMENT_VARIABLES=True\n    LANGFLOW_SUPERUSER=adminuser\n    LANGFLOW_SUPERUSER_PASSWORD=adminpass\n    LANGFLOW_WORKER_TIMEOUT=60000\n    LANGFLOW_WORKERS=3\n    ```\n\n    For additional examples, see [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) in the Langflow repository.\n\n    </details>\n\n4. Save and close `.env`.\n\n5. Start Langflow with your `.env` file:\n\n    \n    \n    </Tabs>\n\n    If your `.env` file isn't in the same directory, provide the path to your `.env` file.\n\nOn startup, Langflow imports the environment variables from your `.env` file, as well as any others that you set in your terminal, and then adopts their specified values.\n\n### Configure environment variables for development\n\nThe following examples show how to configure Langflow using environment variables in different development scenarios.\n\n</Tabs>\n\n## Set environment variables for Langflow Desktop\n\nEnvironment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when you launch them from the Windows or macOS GUI.\n\nFor Windows, this means any GUI-based app launched from the Start menu, desktop shortcuts, or Windows Explorer.\n\nFor macOS, this means any GUI-based app launched from Finder, Spotlight, Launchpad, or the Dock.\n\nTo set environment variables for Langflow Desktop, you need to use specific commands or files, depending on your OS.\n\n</Tabs>\n\n## Supported environment variables {#supported-variables}\n\nThe following sections provide information about specific Langflow environment variables.\n\n### Authentication and security\n\nSee [API keys and authentication](/api-keys-and-authentication).\n\n### Global variables\n\nFor information about the relationship between Langflow global variables and environment variables, as well as environment variables that control handling of global variables, see [Global variables](/configuration-global-variables).\n\n### Logs {#logging}\n\nSee [Configure log options](/logging#log-storage).\n\n### MCP servers {#mcp}\n\nSee [Use Langflow as an MCP server](/mcp-server).\n\n### Monitoring and metrics\n\nFor environment variables for specific monitoring service providers, see the Langflow monitoring integration guides, such as [Langfuse](/integrations-langfuse) and [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices).\n\n### Server\n\nThe following environment variables set base Langflow server configuration, such as where the server is hosted, required files for SSL encryption, and the deployment type (frontend and backend, backend-only, development mode).\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_HOST` | String | `localhost` | The host on which the Langflow server will run. |\n| `LANGFLOW_PORT` | Integer | `7860` | The port on which the Langflow server runs. The server automatically selects a free port if the specified port is in use. |\n| `LANGFLOW_BACKEND_ONLY` | Boolean | `False` | Run only the Langflow backend service (no frontend). |\n| `LANGFLOW_DEV` | Boolean | `False` | Whether to run Langflow in development mode (may contain bugs). |\n| `LANGFLOW_OPEN_BROWSER` | Boolean | `False` | Open the system web browser on startup. |\n| `LANGFLOW_HEALTH_CHECK_MAX_RETRIES` | Integer | `5` | Set the maximum number of retries for Langflow's server status health checks. |\n| `LANGFLOW_WORKERS` | Integer | `1` | Number of worker processes. |\n| `LANGFLOW_WORKER_TIMEOUT` | Integer | `300` | Worker timeout in seconds. |\n| `LANGFLOW_SSL_CERT_FILE` | String | Not set | Path to the SSL certificate file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |\n| `LANGFLOW_SSL_KEY_FILE` | String | Not set | Path to the SSL key file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |\n| `LANGFLOW_DEACTIVATE_TRACING` | Boolean | `False` | Deactivate tracing functionality. |\n| `LANGFLOW_CELERY_ENABLED` | Boolean | `False` | Enable Celery for distributed task processing. |\n\nFor more information about deploying Langflow servers, see [Langflow deployment overview](/deployment-overview).\n\n### Storage\n\nSee [Memory management options](/memory) and [Manage files](/concepts-file-management).\n\n### Telemetry\n\nSee [Telemetry](/contributing-telemetry).\n\n### Visual editor and Playground behavior\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_AUTO_SAVING` | Boolean | `True` | Whether to automatically save flows. |\n| `LANGFLOW_AUTO_SAVING_INTERVAL` | Integer | `1000` | Set the auto-save interval in milliseconds if `LANGFLOW_AUTO_SAVING=True`. |\n| `LANGFLOW_BUNDLE_URLS` | List[String] | `[]` | A list of URLs from which to load custom bundles. Supports GitHub URLs. If `LANGFLOW_AUTO_LOGIN=True`, flows from these bundles are loaded into the database. |\n| `LANGFLOW_COMPONENTS_PATH` | String | Not set | Path to a directory containing custom components. Typically used if you have local custom components or you are building a Docker image with custom components. |\n| `LANGFLOW_LOAD_FLOWS_PATH` | String | Not set | Path to a directory containing flow JSON files to be loaded on startup. Typically used when creating a Docker image with prepackaged flows. Requires `LANGFLOW_AUTO_LOGIN=True`. |\n| `LANGFLOW_CREATE_STARTER_PROJECTS` | Boolean | `True` | Whether to create templates during initialization. If `false`, Langflow doesn't create templates, and `LANGFLOW_UPDATE_STARTER_PROJECTS` is treated as `false`. |\n| `LANGFLOW_UPDATE_STARTER_PROJECTS` | Boolean | `True` | Whether to update templates with the latest component versions when initializing after an upgrade. |\n| `LANGFLOW_LAZY_LOAD_COMPONENTS` | Boolean | `False` | If `true`, Langflow only partially loads components at startup and fully loads them on demand. This significantly reduces startup time but can cause a slight delay when a component is first used. |\n| `LANGFLOW_EVENT_DELIVERY` | String | `streaming` | How to deliver build events to the frontend: `polling`, `streaming` or `direct`. |\n| `LANGFLOW_FRONTEND_PATH` | String | `./frontend` | Path to the frontend directory containing build files. For development purposes only when you need to serve specific frontend code. |\n| `LANGFLOW_MAX_ITEMS_LENGTH` | Integer | `100` | Maximum number of items to store and display in the visual editor. Lists longer than this will be truncated when displayed in the visual editor. Doesn't affect outputs or data passed between components. |\n| `LANGFLOW_MAX_TEXT_LENGTH` | Integer | `1000` | Maximum number of characters to store and display in the visual editor. Responses longer than this will be truncated when displayed in the visual editor. Doesn't truncate outputs or responses passed between components. |\n| `LANGFLOW_MAX_TRANSACTIONS_TO_KEEP` | Integer | `3000` | Maximum number of flow transaction events to keep in the database. |\n| `LANGFLOW_MAX_VERTEX_BUILDS_TO_KEEP` | Integer | `3000` | Maximum number of vertex builds to keep in the database. Relates to [Playground](/concepts-playground) functionality. |\n| `LANGFLOW_MAX_VERTEX_BUILDS_PER_VERTEX` | Integer | `2` | Maximum number of builds to keep per vertex. Older builds are deleted. Relates to [Playground](/concepts-playground) functionality. |\n| `LANGFLOW_PUBLIC_FLOW_CLEANUP_INTERVAL` | Integer | `3600` | The interval in seconds at which data for [shared Playground](/concepts-playground#share-a-flows-playground) flows are cleaned up. Default: 3600 seconds (1 hour). Minimum: 600 seconds (10 minutes). |\n| `LANGFLOW_PUBLIC_FLOW_EXPIRATION` | Integer | `86400` | The time in seconds after which a [shared Playground](/concepts-playground#share-a-flows-playground) flow is considered expired and eligible for cleanup. Default: 86400 seconds (24 hours). Minimum: 600 seconds (10 minutes). |",
      "summary": "In general, environment variables, like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, configure how Langflow runs.\nThese are broad settings that apply to your entire Langflow deployment.",
      "keywords": [
        "authentication",
        "behavior",
        "configure",
        "desktop",
        "development",
        "editor",
        "env",
        "environment",
        "file",
        "global",
        "import",
        "langflow",
        "logging",
        "logs",
        "mcp",
        "metrics",
        "monitoring",
        "oss",
        "playground",
        "precedence"
      ]
    },
    "environment-variables": {
      "title": "Environment variables",
      "slug": "/environment-variables",
      "category": "develop",
      "content": "In general, environment variables, like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, configure how Langflow runs.\nThese are broad settings that apply to your entire Langflow deployment.\n\nIn contrast, global variables are user-defined values stored in Langflow's database for use in flows, such as `OPENAI_API_KEY`.\nLangflow can also source global variables from environment variables.\nFor more information, see [Langflow global variables](/configuration-global-variables).\n\n## Configure environment variables for Langflow OSS\n\nLangflow recognizes [supported environment variables](#supported-variables) from the following sources:\n\n- Environment variables set in your terminal.\n- Environment variables imported from a `.env` file when starting Langflow.\n- Environment variables set with the [Langflow CLI](./configuration-cli), including the `--env-file` option and direct options, such as `--port`.\n\nYou can choose to use one or more of these sources.\n\n### Precedence {#precedence}\n\nIf the same environment variable is set in multiple places, the following hierarchy applies:\n\n1. Langflow CLI options override all other sources.\n2. The `.env` file overrides system environment variables.\n3. System environment variables are used only if not set elsewhere.\nWhen running a Langflow Docker image, the `-e` flag can be used to set additional system environment variables.\n\nFor example:\n\n* If you set `LANGFLOW_PORT=8080` in your system environment and `LANGFLOW_PORT=7860` in `.env`, Langflow uses `7860` from `.env`.\n* If you use the Langflow CLI to run `langflow run --env-file .env --port 9000`, and you set `LANGFLOW_PORT=7860` in `.env`, then Langflow uses `9000` from the CLI option.\n\n### Set environment variables in your terminal {#configure-variables-terminal}\n\nRun the following commands to set environment variables for your current terminal session:\n\n</Tabs>\n\nWhen you start Langflow, it looks for environment variables that you've set in your terminal.\nIf it detects a supported environment variable, then it automatically adopts the specified value, subject to [precedence rules](#precedence).\n\n### Import environment variables from a .env file {#configure-variables-env-file}\n\n1. If Langflow is running, quit Langflow.\n\n2. Create a `.env` file, and then open it in your preferred editor.\n\n3. Define [Langflow environment variables](#supported-variables) in the `.env` file.\n\n    <details>\n    <summary>Example: .env</summary>\n\n    ```text\n    DO_NOT_TRACK=True\n    LANGFLOW_AUTO_LOGIN=False\n    LANGFLOW_AUTO_SAVING=True\n    LANGFLOW_AUTO_SAVING_INTERVAL=1000\n    LANGFLOW_BACKEND_ONLY=False\n    LANGFLOW_BUNDLE_URLS=[\"https://github.com/user/repo/commit/hash\"]\n    LANGFLOW_CACHE_TYPE=async\n    LANGFLOW_COMPONENTS_PATH=/path/to/components/\n    LANGFLOW_CONFIG_DIR=/path/to/config/\n    LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow\n    LANGFLOW_DEV=False\n    LANGFLOW_FALLBACK_TO_ENV_VAR=False\n    LANGFLOW_HEALTH_CHECK_MAX_RETRIES=5\n    LANGFLOW_HOST=localhost\n    LANGFLOW_LANGCHAIN_CACHE=InMemoryCache\n    LANGFLOW_MAX_FILE_SIZE_UPLOAD=10000\n    LANGFLOW_MAX_ITEMS_LENGTH=100\n    LANGFLOW_MAX_TEXT_LENGTH=1000\n    LANGFLOW_LOG_LEVEL=error\n    LANGFLOW_OPEN_BROWSER=False\n    LANGFLOW_PORT=7860\n    LANGFLOW_REMOVE_API_KEYS=False\n    LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True\n    LANGFLOW_SECRET_KEY=somesecretkey\n    LANGFLOW_STORE_ENVIRONMENT_VARIABLES=True\n    LANGFLOW_SUPERUSER=adminuser\n    LANGFLOW_SUPERUSER_PASSWORD=adminpass\n    LANGFLOW_WORKER_TIMEOUT=60000\n    LANGFLOW_WORKERS=3\n    ```\n\n    For additional examples, see [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) in the Langflow repository.\n\n    </details>\n\n4. Save and close `.env`.\n\n5. Start Langflow with your `.env` file:\n\n    \n    \n    </Tabs>\n\n    If your `.env` file isn't in the same directory, provide the path to your `.env` file.\n\nOn startup, Langflow imports the environment variables from your `.env` file, as well as any others that you set in your terminal, and then adopts their specified values.\n\n### Configure environment variables for development\n\nThe following examples show how to configure Langflow using environment variables in different development scenarios.\n\n</Tabs>\n\n## Set environment variables for Langflow Desktop\n\nEnvironment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when you launch them from the Windows or macOS GUI.\n\nFor Windows, this means any GUI-based app launched from the Start menu, desktop shortcuts, or Windows Explorer.\n\nFor macOS, this means any GUI-based app launched from Finder, Spotlight, Launchpad, or the Dock.\n\nTo set environment variables for Langflow Desktop, you need to use specific commands or files, depending on your OS.\n\n</Tabs>\n\n## Supported environment variables {#supported-variables}\n\nThe following sections provide information about specific Langflow environment variables.\n\n### Authentication and security\n\nSee [API keys and authentication](/api-keys-and-authentication).\n\n### Global variables\n\nFor information about the relationship between Langflow global variables and environment variables, as well as environment variables that control handling of global variables, see [Global variables](/configuration-global-variables).\n\n### Logs {#logging}\n\nSee [Configure log options](/logging#log-storage).\n\n### MCP servers {#mcp}\n\nSee [Use Langflow as an MCP server](/mcp-server).\n\n### Monitoring and metrics\n\nFor environment variables for specific monitoring service providers, see the Langflow monitoring integration guides, such as [Langfuse](/integrations-langfuse) and [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices).\n\n### Server\n\nThe following environment variables set base Langflow server configuration, such as where the server is hosted, required files for SSL encryption, and the deployment type (frontend and backend, backend-only, development mode).\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_HOST` | String | `localhost` | The host on which the Langflow server will run. |\n| `LANGFLOW_PORT` | Integer | `7860` | The port on which the Langflow server runs. The server automatically selects a free port if the specified port is in use. |\n| `LANGFLOW_BACKEND_ONLY` | Boolean | `False` | Run only the Langflow backend service (no frontend). |\n| `LANGFLOW_DEV` | Boolean | `False` | Whether to run Langflow in development mode (may contain bugs). |\n| `LANGFLOW_OPEN_BROWSER` | Boolean | `False` | Open the system web browser on startup. |\n| `LANGFLOW_HEALTH_CHECK_MAX_RETRIES` | Integer | `5` | Set the maximum number of retries for Langflow's server status health checks. |\n| `LANGFLOW_WORKERS` | Integer | `1` | Number of worker processes. |\n| `LANGFLOW_WORKER_TIMEOUT` | Integer | `300` | Worker timeout in seconds. |\n| `LANGFLOW_SSL_CERT_FILE` | String | Not set | Path to the SSL certificate file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |\n| `LANGFLOW_SSL_KEY_FILE` | String | Not set | Path to the SSL key file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |\n| `LANGFLOW_DEACTIVATE_TRACING` | Boolean | `False` | Deactivate tracing functionality. |\n| `LANGFLOW_CELERY_ENABLED` | Boolean | `False` | Enable Celery for distributed task processing. |\n\nFor more information about deploying Langflow servers, see [Langflow deployment overview](/deployment-overview).\n\n### Storage\n\nSee [Memory management options](/memory) and [Manage files](/concepts-file-management).\n\n### Telemetry\n\nSee [Telemetry](/contributing-telemetry).\n\n### Visual editor and Playground behavior\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_AUTO_SAVING` | Boolean | `True` | Whether to automatically save flows. |\n| `LANGFLOW_AUTO_SAVING_INTERVAL` | Integer | `1000` | Set the auto-save interval in milliseconds if `LANGFLOW_AUTO_SAVING=True`. |\n| `LANGFLOW_BUNDLE_URLS` | List[String] | `[]` | A list of URLs from which to load custom bundles. Supports GitHub URLs. If `LANGFLOW_AUTO_LOGIN=True`, flows from these bundles are loaded into the database. |\n| `LANGFLOW_COMPONENTS_PATH` | String | Not set | Path to a directory containing custom components. Typically used if you have local custom components or you are building a Docker image with custom components. |\n| `LANGFLOW_LOAD_FLOWS_PATH` | String | Not set | Path to a directory containing flow JSON files to be loaded on startup. Typically used when creating a Docker image with prepackaged flows. Requires `LANGFLOW_AUTO_LOGIN=True`. |\n| `LANGFLOW_CREATE_STARTER_PROJECTS` | Boolean | `True` | Whether to create templates during initialization. If `false`, Langflow doesn't create templates, and `LANGFLOW_UPDATE_STARTER_PROJECTS` is treated as `false`. |\n| `LANGFLOW_UPDATE_STARTER_PROJECTS` | Boolean | `True` | Whether to update templates with the latest component versions when initializing after an upgrade. |\n| `LANGFLOW_LAZY_LOAD_COMPONENTS` | Boolean | `False` | If `true`, Langflow only partially loads components at startup and fully loads them on demand. This significantly reduces startup time but can cause a slight delay when a component is first used. |\n| `LANGFLOW_EVENT_DELIVERY` | String | `streaming` | How to deliver build events to the frontend: `polling`, `streaming` or `direct`. |\n| `LANGFLOW_FRONTEND_PATH` | String | `./frontend` | Path to the frontend directory containing build files. For development purposes only when you need to serve specific frontend code. |\n| `LANGFLOW_MAX_ITEMS_LENGTH` | Integer | `100` | Maximum number of items to store and display in the visual editor. Lists longer than this will be truncated when displayed in the visual editor. Doesn't affect outputs or data passed between components. |\n| `LANGFLOW_MAX_TEXT_LENGTH` | Integer | `1000` | Maximum number of characters to store and display in the visual editor. Responses longer than this will be truncated when displayed in the visual editor. Doesn't truncate outputs or responses passed between components. |\n| `LANGFLOW_MAX_TRANSACTIONS_TO_KEEP` | Integer | `3000` | Maximum number of flow transaction events to keep in the database. |\n| `LANGFLOW_MAX_VERTEX_BUILDS_TO_KEEP` | Integer | `3000` | Maximum number of vertex builds to keep in the database. Relates to [Playground](/concepts-playground) functionality. |\n| `LANGFLOW_MAX_VERTEX_BUILDS_PER_VERTEX` | Integer | `2` | Maximum number of builds to keep per vertex. Older builds are deleted. Relates to [Playground](/concepts-playground) functionality. |\n| `LANGFLOW_PUBLIC_FLOW_CLEANUP_INTERVAL` | Integer | `3600` | The interval in seconds at which data for [shared Playground](/concepts-playground#share-a-flows-playground) flows are cleaned up. Default: 3600 seconds (1 hour). Minimum: 600 seconds (10 minutes). |\n| `LANGFLOW_PUBLIC_FLOW_EXPIRATION` | Integer | `86400` | The time in seconds after which a [shared Playground](/concepts-playground#share-a-flows-playground) flow is considered expired and eligible for cleanup. Default: 86400 seconds (24 hours). Minimum: 600 seconds (10 minutes). |",
      "summary": "In general, environment variables, like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, configure how Langflow runs.\nThese are broad settings that apply to your entire Langflow deployment.",
      "keywords": [
        "authentication",
        "behavior",
        "configure",
        "desktop",
        "development",
        "editor",
        "env",
        "environment",
        "file",
        "global",
        "import",
        "langflow",
        "logging",
        "logs",
        "mcp",
        "metrics",
        "monitoring",
        "oss",
        "playground",
        "precedence"
      ]
    },
    "/install-custom-dependencies": {
      "title": "Install custom dependencies",
      "slug": "/install-custom-dependencies",
      "category": "develop",
      "content": "Langflow provides optional dependency groups and support for custom dependencies to extend Langflow functionality. This guide covers how to add dependencies for different Langflow installations, including Langflow Desktop and Langflow OSS.\n\nThe Langflow codebase uses two `pyproject.toml` files to manage dependencies, with one for `base` and one for `main`:\n\n* The `main` package is managed by the root level `pyproject.toml`, and it includes end-user features and main application code, such as Langchain and OpenAI.\n* The `base` package is managed at `src/backend/base/pyproject.toml`, and it includes core infrastructure, such as the FastAPI web framework.\n\n## Install custom dependencies in Langflow Desktop {#langflow-desktop}\n\nTo add dependencies to Langflow Desktop, add an entry for the package to the application's `requirements.txt` file:\n\n    * On macOS, the file is located at `/Users/USER/.langflow/data/requirements.txt`.\n    * On Windows, the file is located at `C:\\Users\\USER\\AppData\\Roaming\\com.Langflow\\data\\requirements.txt`.\n\nAdd each dependency to `requirements.txt` on its own line in the format `DEPENDENCY==VERSION`, such as `matplotlib==3.10.0`.\n\nRestart Langflow Desktop to install the dependencies.\n\nIf you need to change or uninstall custom dependencies, edit the `requirements.txt` file, and then restart Langflow Desktop.\n\n## Install custom dependencies in Langflow OSS\n\nTo install your own custom dependencies in your Langflow environment, add them with your package manager.\n\nIf you're working within a cloned Langflow repository, add dependencies with `uv add` because there is already a `pyproject.toml` file for uv to reference:\n\n```bash\nuv add DEPENDENCY\n```\n\n### Install optional dependency groups\n\nLangflow OSS provides optional dependency groups that extend its functionality.\n\nThese dependencies are listed in the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L194) file under `[project.optional-dependencies]`.\n\nInstall dependency groups using pip's `[extras]` syntax. For example, to install Langflow with the `postgresql` dependency group, enter the following command:\n\n```bash\nuv pip install \"langflow[postgresql]\"\n```\n\nTo install multiple extras, use commas to separate each dependency group:\n\n```bash\nuv pip install \"langflow[local,postgresql]\"\n```\n\n### Use a virtual environment to test custom dependencies\n\nWhen testing locally, use a virtual environment to isolate your dependencies and prevent conflicts with other Python projects.\n\nFor example, if you want to experiment with `matplotlib` with Langflow:\n\n```bash\n# Create and activate a virtual environment\nuv venv YOUR_LANGFLOW_VENV\nsource YOUR_LANGFLOW_VENV/bin/activate\n\n# Install langflow and your additional dependency\nuv pip install langflow matplotlib\n```\n\nIf you're working within a cloned Langflow repository, add dependencies with `uv add` to reference the existing `pyproject.toml` files:\n\n```bash\nuv add matplotlib\n```\n\nThe `uv add` commands automatically update the `uv.lock` file in the appropriate location.\n\n## Add dependencies to the Langflow codebase\n\nWhen contributing to the Langflow codebase, you might need to add dependencies to Langflow.\n\nLangflow uses a workspace with two packages, each with different types of dependencies.\n\nTo add a dependency to the `main` package, run `uv add DEPENDENCY` from the project root.\nFor example:\n\n```bash\nuv add matplotlib\n```\n\nDependencies can be added to the `main` package as regular dependencies at `[project.dependencies]` or optional dependencies at `[project.optional-dependencies]`.\n\nTo add a dependency to the `base` package, navigate to `src/backend/base` and run:\n```bash\ncd src/backend/base && uv add DEPENDENCY\n```\n\nTo add a development dependency for testing, linting, or debugging, navigate to `src/backend/base` and run:\n```bash\ncd src/backend/base && uv add --group dev DEPENDENCY\n```\n\nDependencies can be added to the `base` package as regular dependencies at `[project.dependencies]`, development dependencies at `[dependency-groups.dev]`, or optional dependencies at `[project.optional-dependencies]`.\n\nYou can optionally use `make add` instead of `uv add`:\n\n```bash\n# Equivalent to: uv add matplotlib\nmake add main=\"matplotlib\"\n\n# Equivalent to: cd src/backend/base && uv add --group dev matplotlib\nmake add devel=\"matplotlib\"\n\n# Equivalent to: cd src/backend/base && uv add matplotlib\nmake add base=\"matplotlib\"\n```\n\nAlternatively, you can add these dependencies manually to the appropriate `pyproject.toml` file:\n\n```\n[project]\ndependencies = [\n    \"matplotlib>=3.8.0\"\n]\n```\n\nOr as an optional dependency in the main package:\n\n```\n[project.optional-dependencies]\nplotting = [\n    \"matplotlib>=3.8.0\",\n]\n```\n\nOr as a development dependency in the base package:\n\n```\n[dependency-groups]\ndev = [\n    \"matplotlib>=3.8.0\",\n]\n```\n\n## See also\n\n* [Containerize a Langflow application](/develop-application)\n* [Create custom Python components](/components-custom-components)",
      "summary": "Langflow provides optional dependency groups and support for custom dependencies to extend Langflow functionality. This guide covers how to add dependencies for different Langflow installations, including Langflow Desktop and Langflow OSS.",
      "keywords": [
        "activate",
        "add",
        "additional",
        "backend",
        "base",
        "codebase",
        "create",
        "custom",
        "dependencies",
        "dependency",
        "desktop",
        "dev",
        "environment",
        "equivalent",
        "group",
        "groups",
        "install",
        "langflow",
        "matplotlib",
        "optional"
      ]
    },
    "install-custom-dependencies": {
      "title": "Install custom dependencies",
      "slug": "/install-custom-dependencies",
      "category": "develop",
      "content": "Langflow provides optional dependency groups and support for custom dependencies to extend Langflow functionality. This guide covers how to add dependencies for different Langflow installations, including Langflow Desktop and Langflow OSS.\n\nThe Langflow codebase uses two `pyproject.toml` files to manage dependencies, with one for `base` and one for `main`:\n\n* The `main` package is managed by the root level `pyproject.toml`, and it includes end-user features and main application code, such as Langchain and OpenAI.\n* The `base` package is managed at `src/backend/base/pyproject.toml`, and it includes core infrastructure, such as the FastAPI web framework.\n\n## Install custom dependencies in Langflow Desktop {#langflow-desktop}\n\nTo add dependencies to Langflow Desktop, add an entry for the package to the application's `requirements.txt` file:\n\n    * On macOS, the file is located at `/Users/USER/.langflow/data/requirements.txt`.\n    * On Windows, the file is located at `C:\\Users\\USER\\AppData\\Roaming\\com.Langflow\\data\\requirements.txt`.\n\nAdd each dependency to `requirements.txt` on its own line in the format `DEPENDENCY==VERSION`, such as `matplotlib==3.10.0`.\n\nRestart Langflow Desktop to install the dependencies.\n\nIf you need to change or uninstall custom dependencies, edit the `requirements.txt` file, and then restart Langflow Desktop.\n\n## Install custom dependencies in Langflow OSS\n\nTo install your own custom dependencies in your Langflow environment, add them with your package manager.\n\nIf you're working within a cloned Langflow repository, add dependencies with `uv add` because there is already a `pyproject.toml` file for uv to reference:\n\n```bash\nuv add DEPENDENCY\n```\n\n### Install optional dependency groups\n\nLangflow OSS provides optional dependency groups that extend its functionality.\n\nThese dependencies are listed in the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L194) file under `[project.optional-dependencies]`.\n\nInstall dependency groups using pip's `[extras]` syntax. For example, to install Langflow with the `postgresql` dependency group, enter the following command:\n\n```bash\nuv pip install \"langflow[postgresql]\"\n```\n\nTo install multiple extras, use commas to separate each dependency group:\n\n```bash\nuv pip install \"langflow[local,postgresql]\"\n```\n\n### Use a virtual environment to test custom dependencies\n\nWhen testing locally, use a virtual environment to isolate your dependencies and prevent conflicts with other Python projects.\n\nFor example, if you want to experiment with `matplotlib` with Langflow:\n\n```bash\n# Create and activate a virtual environment\nuv venv YOUR_LANGFLOW_VENV\nsource YOUR_LANGFLOW_VENV/bin/activate\n\n# Install langflow and your additional dependency\nuv pip install langflow matplotlib\n```\n\nIf you're working within a cloned Langflow repository, add dependencies with `uv add` to reference the existing `pyproject.toml` files:\n\n```bash\nuv add matplotlib\n```\n\nThe `uv add` commands automatically update the `uv.lock` file in the appropriate location.\n\n## Add dependencies to the Langflow codebase\n\nWhen contributing to the Langflow codebase, you might need to add dependencies to Langflow.\n\nLangflow uses a workspace with two packages, each with different types of dependencies.\n\nTo add a dependency to the `main` package, run `uv add DEPENDENCY` from the project root.\nFor example:\n\n```bash\nuv add matplotlib\n```\n\nDependencies can be added to the `main` package as regular dependencies at `[project.dependencies]` or optional dependencies at `[project.optional-dependencies]`.\n\nTo add a dependency to the `base` package, navigate to `src/backend/base` and run:\n```bash\ncd src/backend/base && uv add DEPENDENCY\n```\n\nTo add a development dependency for testing, linting, or debugging, navigate to `src/backend/base` and run:\n```bash\ncd src/backend/base && uv add --group dev DEPENDENCY\n```\n\nDependencies can be added to the `base` package as regular dependencies at `[project.dependencies]`, development dependencies at `[dependency-groups.dev]`, or optional dependencies at `[project.optional-dependencies]`.\n\nYou can optionally use `make add` instead of `uv add`:\n\n```bash\n# Equivalent to: uv add matplotlib\nmake add main=\"matplotlib\"\n\n# Equivalent to: cd src/backend/base && uv add --group dev matplotlib\nmake add devel=\"matplotlib\"\n\n# Equivalent to: cd src/backend/base && uv add matplotlib\nmake add base=\"matplotlib\"\n```\n\nAlternatively, you can add these dependencies manually to the appropriate `pyproject.toml` file:\n\n```\n[project]\ndependencies = [\n    \"matplotlib>=3.8.0\"\n]\n```\n\nOr as an optional dependency in the main package:\n\n```\n[project.optional-dependencies]\nplotting = [\n    \"matplotlib>=3.8.0\",\n]\n```\n\nOr as a development dependency in the base package:\n\n```\n[dependency-groups]\ndev = [\n    \"matplotlib>=3.8.0\",\n]\n```\n\n## See also\n\n* [Containerize a Langflow application](/develop-application)\n* [Create custom Python components](/components-custom-components)",
      "summary": "Langflow provides optional dependency groups and support for custom dependencies to extend Langflow functionality. This guide covers how to add dependencies for different Langflow installations, including Langflow Desktop and Langflow OSS.",
      "keywords": [
        "activate",
        "add",
        "additional",
        "backend",
        "base",
        "codebase",
        "create",
        "custom",
        "dependencies",
        "dependency",
        "desktop",
        "dev",
        "environment",
        "equivalent",
        "group",
        "groups",
        "install",
        "langflow",
        "matplotlib",
        "optional"
      ]
    },
    "/integrations-arize": {
      "title": "Arize",
      "slug": "/integrations-arize",
      "category": "develop",
      "content": "Arize is a tool built on [OpenTelemetry](https://opentelemetry.io/) and [OpenInference](https://docs.arize.com/phoenix/reference/open-inference) for monitoring and optimizing LLM applications.\n\nTo enable Arize tracing, set the required Arize environment variables in your Langflow deployment.\nArize begins monitoring and collecting telemetry data from your LLM applications automatically.\n\n**Note:** Instructions for integrating Langflow and Arize are also available in the Arize documentation:\n\n* [Langflow tracing with Arize Platform](https://arize.com/docs/ax/integrations/frameworks-and-platforms/langflow/langflow-tracing)\n* [Langflow tracing with Arize Phoenix](https://arize.com/docs/phoenix/integrations/langflow/langflow-tracing)\n\n## Prerequisites\n\n* If you are using the [standard Arize platform](https://docs.arize.com/arize), you need an **Arize Space ID** and **Arize API Key**.\n* If you are using the open-source [Arize Phoenix platform](https://docs.arize.com/phoenix), you need an **Arize Phoenix API key**.\n\n## Connect Arize to Langflow\n\n</Tabs>\n\n## Run a flow and view metrics in Arize\n\n1. In Langflow, run a flow that has an LLM-driven component, such as an **Agent** component or any language model component.\nYou must chat with the flow or trigger the LLM to produce traffic for Arize to trace.\n\n    For example, you can create a flow with the **Simple Agent** template, add your OpenAI API key to the **Agent** component, and then click **Playground** to chat with the flow and generate traffic.\n\n2. In Arize, open your project dashboard, and then wait for Arize to process the data.\nThis can take a few minutes.\n\n3. To view metrics for your flows, go to the **LLM Tracing** tab.\n\n    Each Langflow execution generates two traces in Arize:\n\n    * The `AgentExecutor` trace is the Arize trace of LangChain's `AgentExecutor`.\n    * The `UUID` trace is the trace of the Langflow components.\n\n4. To view traces, go to the **Traces** tab.\n\n    A _trace_ is the complete journey of a request, made of multiple _spans_.\n\n5. To view spans, go to the **Spans** tab.\n\n    A _span_ is a single operation within a trace.\n    For example, a _span_ could be a single API call to OpenAI or a single function call to a custom tool.\n\n    For information about tracing metrics in Arize, see the [Arize LLM tracing documentation](https://docs.arize.com/arize/llm-tracing/tracing).\n\n6. To add a span to a [dataset](https://docs.arize.com/arize/llm-datasets-and-experiments/datasets-and-experiments), click **Add to Dataset**.\n\n    All metrics on the **LLM Tracing** tab can be added to datasets.\n\n7. To view a dataset, click the **Datasets** tab, and then select your dataset.",
      "summary": "Arize is a tool built on [OpenTelemetry](https://opentelemetry.io/) and [OpenInference](https://docs.arize.com/phoenix/reference/open-inference) for monitoring and optimizing LLM applications.",
      "keywords": [
        "add",
        "agent",
        "api",
        "arize",
        "connect",
        "dataset",
        "datasets",
        "flow",
        "key",
        "langflow",
        "llm",
        "metrics",
        "note",
        "phoenix",
        "playground",
        "prerequisites",
        "run",
        "simple",
        "space",
        "spans"
      ]
    },
    "integrations-arize": {
      "title": "Arize",
      "slug": "/integrations-arize",
      "category": "develop",
      "content": "Arize is a tool built on [OpenTelemetry](https://opentelemetry.io/) and [OpenInference](https://docs.arize.com/phoenix/reference/open-inference) for monitoring and optimizing LLM applications.\n\nTo enable Arize tracing, set the required Arize environment variables in your Langflow deployment.\nArize begins monitoring and collecting telemetry data from your LLM applications automatically.\n\n**Note:** Instructions for integrating Langflow and Arize are also available in the Arize documentation:\n\n* [Langflow tracing with Arize Platform](https://arize.com/docs/ax/integrations/frameworks-and-platforms/langflow/langflow-tracing)\n* [Langflow tracing with Arize Phoenix](https://arize.com/docs/phoenix/integrations/langflow/langflow-tracing)\n\n## Prerequisites\n\n* If you are using the [standard Arize platform](https://docs.arize.com/arize), you need an **Arize Space ID** and **Arize API Key**.\n* If you are using the open-source [Arize Phoenix platform](https://docs.arize.com/phoenix), you need an **Arize Phoenix API key**.\n\n## Connect Arize to Langflow\n\n</Tabs>\n\n## Run a flow and view metrics in Arize\n\n1. In Langflow, run a flow that has an LLM-driven component, such as an **Agent** component or any language model component.\nYou must chat with the flow or trigger the LLM to produce traffic for Arize to trace.\n\n    For example, you can create a flow with the **Simple Agent** template, add your OpenAI API key to the **Agent** component, and then click **Playground** to chat with the flow and generate traffic.\n\n2. In Arize, open your project dashboard, and then wait for Arize to process the data.\nThis can take a few minutes.\n\n3. To view metrics for your flows, go to the **LLM Tracing** tab.\n\n    Each Langflow execution generates two traces in Arize:\n\n    * The `AgentExecutor` trace is the Arize trace of LangChain's `AgentExecutor`.\n    * The `UUID` trace is the trace of the Langflow components.\n\n4. To view traces, go to the **Traces** tab.\n\n    A _trace_ is the complete journey of a request, made of multiple _spans_.\n\n5. To view spans, go to the **Spans** tab.\n\n    A _span_ is a single operation within a trace.\n    For example, a _span_ could be a single API call to OpenAI or a single function call to a custom tool.\n\n    For information about tracing metrics in Arize, see the [Arize LLM tracing documentation](https://docs.arize.com/arize/llm-tracing/tracing).\n\n6. To add a span to a [dataset](https://docs.arize.com/arize/llm-datasets-and-experiments/datasets-and-experiments), click **Add to Dataset**.\n\n    All metrics on the **LLM Tracing** tab can be added to datasets.\n\n7. To view a dataset, click the **Datasets** tab, and then select your dataset.",
      "summary": "Arize is a tool built on [OpenTelemetry](https://opentelemetry.io/) and [OpenInference](https://docs.arize.com/phoenix/reference/open-inference) for monitoring and optimizing LLM applications.",
      "keywords": [
        "add",
        "agent",
        "api",
        "arize",
        "connect",
        "dataset",
        "datasets",
        "flow",
        "key",
        "langflow",
        "llm",
        "metrics",
        "note",
        "phoenix",
        "playground",
        "prerequisites",
        "run",
        "simple",
        "space",
        "spans"
      ]
    },
    "/integrations-instana-traceloop": {
      "title": "Traceloop",
      "slug": "/integrations-instana-traceloop",
      "category": "develop",
      "content": "Traceloop SDK is a lightweight instrumentation toolkit designed for LLM applications.\nIt enables developers to automatically capture and export traces, metrics, and key observability signals from their LLM-powered workflows.\n\nWhen combined with Instana, the exported telemetry data from Traceloop provides end-to-end visibility, allowing users to visualize traces, analyze performance bottlenecks, and ensure reliable operation of LLM-driven applications.\n\nThis guide demonstrates how to integrate the Instana observability platform with your Langflow application using the Traceloop SDK so you can monitor and analyze LLM performance.\n\n## Prerequisites\n\n- Create a [Traceloop API key](https://app.traceloop.com/settings/api-key)\n- Create an [Instana endpoint and Instana key](https://www.ibm.com/docs/en/instana-observability/1.0.302)\n- [Install Langflow](/get-started-installation)\n\n## Configure environment variables\n\n1. In the root folder of your Langflow application, edit your existing Langflow `.env` file or create a new one.\n\n2. Enter the following environment variables, and then replace the placeholders with the values for your deployment or requirements:\n\n   ```text\n   TRACELOOP_API_KEY=tl_dummy_1234567890abcdef1234567890abcdef\n   TRACELOOP_BASE_URL=https://otlp-magenta-saas.instana.rocks:4318\n   TRACELOOP_HEADERS=\"x-instana-key=INSTANA_KEY\"\n   OTEL_EXPORTER_OTLP_INSECURE=false\n   TRACELOOP_METRICS_ENDPOINT=HOST:8000\n   TRACELOOP_METRICS_ENABLED=true\n   OTEL_METRIC_EXPORT_INTERVAL=10000\n   ```\n\n   Set the necessary values for each environment variable:\n\n   - **`TRACELOOP_API_KEY`**: A Traceloop API key to authenticate your application with Traceloop's monitoring service.\n   You can get this from your Traceloop account dashboard.\n\n      This integration uses the Traceloop SDK for instrumentation, which requires a Traceloop API key to initialize properly.\n      If you don't have a Traceloop API key, you can proceed with the placeholder API key in the preceding example.\n\n   - **`TRACELOOP_BASE_URL`**: The Instana endpoint URL for telemetry data collection, which is your Instana backend endpoint, such as `https://otlp-magenta-saas.instana.rocks:4318`.\n   You can get this from your Instana configuration or by contacting your Instana administrator.\n\n   - **`TRACELOOP_HEADERS`**: Authentication headers for Instana data collection. Set this to `\"x-instana-key=INSTANA_KEY\"`, replacing `INSTANA_KEY` with the Instana key from your Instana setup.\n\n   - **`OTEL_EXPORTER_OTLP_INSECURE`**: Security setting for OpenTelemetry Protocol connections. Set to `false` for secure HTTPS/TLS connections. This is recommended for production Instana SaaS endpoints. Set to `true` for insecure HTTP connections during local development.\n\n   - **`TRACELOOP_METRICS_ENDPOINT`**: Separate metrics endpoint configuration, in the form of `OTEL_DC_LLM_HOST:8000`. Typically set to `host.docker.internal:8000` for Docker environments. Adjust the host and port based on your deployment setup.\n\n   - **`TRACELOOP_METRICS_ENABLED`**: Boolean to enable metrics collection. Set to `true` to activate metrics gathering.\n\n   - **`OTEL_METRIC_EXPORT_INTERVAL`**: Interval in milliseconds for metrics export. Set to `10000` for 10-second export intervals, or adjust based on your monitoring requirements.\n\n3. Make sure the OpenTelemetry Data Collector (OTel DC) is running and correctly configured.\nOpen your Collector's `config.yaml` file, enter the following configuration, and then replace the placeholder values with the values from your data collector setup or requirements:\n\n   ```yaml\n   llm.application: \"LLM_DC\"\n   instances:\n      -  otel.agentless.mode: true\n         # Example endpoint: https://otlp-magenta-saas.instana.rocks:4318\n         otel.backend.url: \"INSTANA_ENDPOINT\"\n         otel.backend.using.http: false\n         callback.interval: 10\n         otel.service.name: \"DC1\"\n         otel.service.port: 8000\n         currency: \"USD\"\n   ```\n\n   This configuration enables the OTel Collector to operate in agentless mode and route telemetry data to your Instana backend with proper service identification and collection intervals for effective monitoring integration.\n\n## Start Langflow with Traceloop environment variables\n\nLaunch your Langflow application with your `.env` file:\n\n```bash\nuv run langflow run --env-file .env\n```\n\nTraceloop automatically begins monitoring and collecting telemetry data from your LLM applications.\n\n## Verify the integration\n\nTo verify that observability is working correctly:\n\n1. Run a flow in Langflow to generate traffic.\n2. To view traces in Instana, open Instana and click **Applications**.\n3. In **Services**, search for `Langflow`.\n4. Click **Langflow** to view and analyze the associated calls.\n\n   ![Instana Traces dashboard](/img/instana-traces-dashboard.png)\n\n5. To view metrics in Instana, open Instana and click **Infrastructure**.\n6. In **Analyze Infrastructure**, click **Otel LLMonitor**.\n7. To view your Metrics dashboard, click `LLM:DC1@your_machine_name.local`.\n\n   ![Instana Metrics dashboard](/img/instana-metrics-dashboard.png)\n\n## See also\n\n* [Traceloop documentation](https://www.traceloop.com/docs/introduction)\n* [Instana setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-instana-setup)\n* [Otel DC setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-install-otel-data-collector-llm-odcl)",
      "summary": "Traceloop SDK is a lightweight instrumentation toolkit designed for LLM applications.\nIt enables developers to automatically capture and export traces, metrics, and key observability signals from their LLM-powered workflows.",
      "keywords": [
        "analyze",
        "applications",
        "configure",
        "environment",
        "infrastructure",
        "integration",
        "langflow",
        "llmonitor",
        "otel",
        "prerequisites",
        "services",
        "start",
        "traceloop",
        "variables",
        "verify"
      ]
    },
    "integrations-instana-traceloop": {
      "title": "Traceloop",
      "slug": "/integrations-instana-traceloop",
      "category": "develop",
      "content": "Traceloop SDK is a lightweight instrumentation toolkit designed for LLM applications.\nIt enables developers to automatically capture and export traces, metrics, and key observability signals from their LLM-powered workflows.\n\nWhen combined with Instana, the exported telemetry data from Traceloop provides end-to-end visibility, allowing users to visualize traces, analyze performance bottlenecks, and ensure reliable operation of LLM-driven applications.\n\nThis guide demonstrates how to integrate the Instana observability platform with your Langflow application using the Traceloop SDK so you can monitor and analyze LLM performance.\n\n## Prerequisites\n\n- Create a [Traceloop API key](https://app.traceloop.com/settings/api-key)\n- Create an [Instana endpoint and Instana key](https://www.ibm.com/docs/en/instana-observability/1.0.302)\n- [Install Langflow](/get-started-installation)\n\n## Configure environment variables\n\n1. In the root folder of your Langflow application, edit your existing Langflow `.env` file or create a new one.\n\n2. Enter the following environment variables, and then replace the placeholders with the values for your deployment or requirements:\n\n   ```text\n   TRACELOOP_API_KEY=tl_dummy_1234567890abcdef1234567890abcdef\n   TRACELOOP_BASE_URL=https://otlp-magenta-saas.instana.rocks:4318\n   TRACELOOP_HEADERS=\"x-instana-key=INSTANA_KEY\"\n   OTEL_EXPORTER_OTLP_INSECURE=false\n   TRACELOOP_METRICS_ENDPOINT=HOST:8000\n   TRACELOOP_METRICS_ENABLED=true\n   OTEL_METRIC_EXPORT_INTERVAL=10000\n   ```\n\n   Set the necessary values for each environment variable:\n\n   - **`TRACELOOP_API_KEY`**: A Traceloop API key to authenticate your application with Traceloop's monitoring service.\n   You can get this from your Traceloop account dashboard.\n\n      This integration uses the Traceloop SDK for instrumentation, which requires a Traceloop API key to initialize properly.\n      If you don't have a Traceloop API key, you can proceed with the placeholder API key in the preceding example.\n\n   - **`TRACELOOP_BASE_URL`**: The Instana endpoint URL for telemetry data collection, which is your Instana backend endpoint, such as `https://otlp-magenta-saas.instana.rocks:4318`.\n   You can get this from your Instana configuration or by contacting your Instana administrator.\n\n   - **`TRACELOOP_HEADERS`**: Authentication headers for Instana data collection. Set this to `\"x-instana-key=INSTANA_KEY\"`, replacing `INSTANA_KEY` with the Instana key from your Instana setup.\n\n   - **`OTEL_EXPORTER_OTLP_INSECURE`**: Security setting for OpenTelemetry Protocol connections. Set to `false` for secure HTTPS/TLS connections. This is recommended for production Instana SaaS endpoints. Set to `true` for insecure HTTP connections during local development.\n\n   - **`TRACELOOP_METRICS_ENDPOINT`**: Separate metrics endpoint configuration, in the form of `OTEL_DC_LLM_HOST:8000`. Typically set to `host.docker.internal:8000` for Docker environments. Adjust the host and port based on your deployment setup.\n\n   - **`TRACELOOP_METRICS_ENABLED`**: Boolean to enable metrics collection. Set to `true` to activate metrics gathering.\n\n   - **`OTEL_METRIC_EXPORT_INTERVAL`**: Interval in milliseconds for metrics export. Set to `10000` for 10-second export intervals, or adjust based on your monitoring requirements.\n\n3. Make sure the OpenTelemetry Data Collector (OTel DC) is running and correctly configured.\nOpen your Collector's `config.yaml` file, enter the following configuration, and then replace the placeholder values with the values from your data collector setup or requirements:\n\n   ```yaml\n   llm.application: \"LLM_DC\"\n   instances:\n      -  otel.agentless.mode: true\n         # Example endpoint: https://otlp-magenta-saas.instana.rocks:4318\n         otel.backend.url: \"INSTANA_ENDPOINT\"\n         otel.backend.using.http: false\n         callback.interval: 10\n         otel.service.name: \"DC1\"\n         otel.service.port: 8000\n         currency: \"USD\"\n   ```\n\n   This configuration enables the OTel Collector to operate in agentless mode and route telemetry data to your Instana backend with proper service identification and collection intervals for effective monitoring integration.\n\n## Start Langflow with Traceloop environment variables\n\nLaunch your Langflow application with your `.env` file:\n\n```bash\nuv run langflow run --env-file .env\n```\n\nTraceloop automatically begins monitoring and collecting telemetry data from your LLM applications.\n\n## Verify the integration\n\nTo verify that observability is working correctly:\n\n1. Run a flow in Langflow to generate traffic.\n2. To view traces in Instana, open Instana and click **Applications**.\n3. In **Services**, search for `Langflow`.\n4. Click **Langflow** to view and analyze the associated calls.\n\n   ![Instana Traces dashboard](/img/instana-traces-dashboard.png)\n\n5. To view metrics in Instana, open Instana and click **Infrastructure**.\n6. In **Analyze Infrastructure**, click **Otel LLMonitor**.\n7. To view your Metrics dashboard, click `LLM:DC1@your_machine_name.local`.\n\n   ![Instana Metrics dashboard](/img/instana-metrics-dashboard.png)\n\n## See also\n\n* [Traceloop documentation](https://www.traceloop.com/docs/introduction)\n* [Instana setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-instana-setup)\n* [Otel DC setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-install-otel-data-collector-llm-odcl)",
      "summary": "Traceloop SDK is a lightweight instrumentation toolkit designed for LLM applications.\nIt enables developers to automatically capture and export traces, metrics, and key observability signals from their LLM-powered workflows.",
      "keywords": [
        "analyze",
        "applications",
        "configure",
        "environment",
        "infrastructure",
        "integration",
        "langflow",
        "llmonitor",
        "otel",
        "prerequisites",
        "services",
        "start",
        "traceloop",
        "variables",
        "verify"
      ]
    },
    "/integrations-langfuse": {
      "title": "Langfuse",
      "slug": "/integrations-langfuse",
      "category": "develop",
      "content": "[Langfuse](https://langfuse.com) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks, including workflow builders and runtimes like Langflow.\n\nThis guide explains how to configure Langflow to collect [tracing](https://langfuse.com/docs/tracing) data about your flow executions and automatically send the data to Langfuse.\n\n<iframe width=\"760\" height=\"415\" src=\"https://www.youtube.com/embed/SA9gGbzwNGU?si=eDKvdtvhb3fJCSbl\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Prerequisites\n\n- An account in a [Langfuse Cloud](https://cloud.langfuse.com) or [Langfuse self-hosted](https://langfuse.com/self-hosting) instance\n- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace\n\n**Note:** If you need a flow to test the Langfuse integration, see the [Langflow quickstart](/get-started-quickstart).\n\n## Set Langfuse credentials as environment variables {#langfuse-credentials}\n\n1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).\n\n2. Copy the following API key information:\n\n    - Secret Key\n    - Public Key\n    - Host URL\n\n3. Set your Langfuse project credentials as environment variables in the same environment where you run Langflow.\n\n    In the following examples, replace `SECRET_KEY`, `PUBLIC_KEY`, and `HOST_URL` with your API key details from Langfuse.\n\n    \n    \n    </Tabs>\n\n## Start Langflow and view traces in Langfuse\n\n1. Start Langflow in the same environment where you set the Langfuse environment variables:\n\n    ```bash\n    uv run langflow run\n    ```\n\n2. Run a flow.\n\n    Langflow automatically collects and sends tracing data about the flow execution to Langfuse.\n\n3. View the collected data in your [Langfuse dashboard](https://langfuse.com/docs/analytics/overview).\n\n    Langfuse also provides a [public live trace example dashboard](https://cloud.langfuse.com/project/cm0nywmaa005c3ol2msoisiho/traces/f016ae6d-4527-43f5-93ba-9d78388cd3d9).\n\n## Disable Langfuse tracing\n\nTo disable the Langfuse integration, remove the [Langfuse environment variables](#langfuse-credentials), and then restart Langflow.\n\n## Run Langfuse and Langflow with Docker Compose\n\nAs an alternative to the previous setup, particularly for self-hosted Langfuse, you can run both services with Docker Compose.\n\n1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).\n\n2. Copy the following API key information:\n\n    - Secret Key\n    - Public Key\n    - Host URL\n\n3. Add your Langflow credentials to your Langflow `docker-compose.yml` file in the `environment` section.\n\n    The following example is based on the [example `docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml).\n\n    ```yml\n    services:\n      langflow:\n        image: langflowai/langflow:latest # or another version tag on https://hub.docker.com/r/langflowai/langflow\n        pull_policy: always               # set to 'always' when using 'latest' image\n        ports:\n          - \"7860:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow\n          # This variable defines where the logs, file storage, monitor data and secret keys are stored.\n          - LANGFLOW_CONFIG_DIR=app/langflow\n          - LANGFUSE_SECRET_KEY=sk-...\n          - LANGFUSE_PUBLIC_KEY=pk-...\n          - LANGFUSE_HOST=https://us.cloud.langfuse.com\n        volumes:\n          - langflow-data:/app/langflow\n\n      postgres:\n        image: postgres:16\n        environment:\n          POSTGRES_USER: langflow\n          POSTGRES_PASSWORD: langflow\n          POSTGRES_DB: langflow\n        ports:\n          - \"5432:5432\"\n        volumes:\n          - langflow-postgres:/var/lib/postgresql/data\n\n    volumes:\n      langflow-postgres:\n      langflow-data:\n    ```\n\n4. Start the Docker container:\n\n    ```text\n    docker-compose up\n    ```\n\n5. To confirm Langfuse is connected to your Langflow container, run the following command:\n\n    ```sh\n    docker compose exec langflow python -c \"import requests, os; addr = os.environ.get('LANGFUSE_HOST'); print(addr); res = requests.get(addr, timeout=5); print(res.status_code)\"\n    ```\n\n    If there is an error, make sure you have set the `LANGFUSE_HOST` environment variable in your terminal session.\n\n    Output similar to the following indicates success:\n\n    ```text\n    https://us.cloud.langfuse.com\n    200\n    ```\n\n## See also\n\n* [Langfuse GitHub repository](https://github.com/langfuse/langfuse)",
      "summary": "[Langfuse](https://langfuse.com) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks, including workflow...",
      "keywords": [
        "compose",
        "credentials",
        "disable",
        "docker",
        "environment",
        "langflow",
        "langfuse",
        "note",
        "prerequisites",
        "run",
        "set",
        "start",
        "traces",
        "tracing",
        "variables",
        "view"
      ]
    },
    "integrations-langfuse": {
      "title": "Langfuse",
      "slug": "/integrations-langfuse",
      "category": "develop",
      "content": "[Langfuse](https://langfuse.com) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks, including workflow builders and runtimes like Langflow.\n\nThis guide explains how to configure Langflow to collect [tracing](https://langfuse.com/docs/tracing) data about your flow executions and automatically send the data to Langfuse.\n\n<iframe width=\"760\" height=\"415\" src=\"https://www.youtube.com/embed/SA9gGbzwNGU?si=eDKvdtvhb3fJCSbl\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n## Prerequisites\n\n- An account in a [Langfuse Cloud](https://cloud.langfuse.com) or [Langfuse self-hosted](https://langfuse.com/self-hosting) instance\n- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace\n\n**Note:** If you need a flow to test the Langfuse integration, see the [Langflow quickstart](/get-started-quickstart).\n\n## Set Langfuse credentials as environment variables {#langfuse-credentials}\n\n1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).\n\n2. Copy the following API key information:\n\n    - Secret Key\n    - Public Key\n    - Host URL\n\n3. Set your Langfuse project credentials as environment variables in the same environment where you run Langflow.\n\n    In the following examples, replace `SECRET_KEY`, `PUBLIC_KEY`, and `HOST_URL` with your API key details from Langfuse.\n\n    \n    \n    </Tabs>\n\n## Start Langflow and view traces in Langfuse\n\n1. Start Langflow in the same environment where you set the Langfuse environment variables:\n\n    ```bash\n    uv run langflow run\n    ```\n\n2. Run a flow.\n\n    Langflow automatically collects and sends tracing data about the flow execution to Langfuse.\n\n3. View the collected data in your [Langfuse dashboard](https://langfuse.com/docs/analytics/overview).\n\n    Langfuse also provides a [public live trace example dashboard](https://cloud.langfuse.com/project/cm0nywmaa005c3ol2msoisiho/traces/f016ae6d-4527-43f5-93ba-9d78388cd3d9).\n\n## Disable Langfuse tracing\n\nTo disable the Langfuse integration, remove the [Langfuse environment variables](#langfuse-credentials), and then restart Langflow.\n\n## Run Langfuse and Langflow with Docker Compose\n\nAs an alternative to the previous setup, particularly for self-hosted Langfuse, you can run both services with Docker Compose.\n\n1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).\n\n2. Copy the following API key information:\n\n    - Secret Key\n    - Public Key\n    - Host URL\n\n3. Add your Langflow credentials to your Langflow `docker-compose.yml` file in the `environment` section.\n\n    The following example is based on the [example `docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml).\n\n    ```yml\n    services:\n      langflow:\n        image: langflowai/langflow:latest # or another version tag on https://hub.docker.com/r/langflowai/langflow\n        pull_policy: always               # set to 'always' when using 'latest' image\n        ports:\n          - \"7860:7860\"\n        depends_on:\n          - postgres\n        environment:\n          - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow\n          # This variable defines where the logs, file storage, monitor data and secret keys are stored.\n          - LANGFLOW_CONFIG_DIR=app/langflow\n          - LANGFUSE_SECRET_KEY=sk-...\n          - LANGFUSE_PUBLIC_KEY=pk-...\n          - LANGFUSE_HOST=https://us.cloud.langfuse.com\n        volumes:\n          - langflow-data:/app/langflow\n\n      postgres:\n        image: postgres:16\n        environment:\n          POSTGRES_USER: langflow\n          POSTGRES_PASSWORD: langflow\n          POSTGRES_DB: langflow\n        ports:\n          - \"5432:5432\"\n        volumes:\n          - langflow-postgres:/var/lib/postgresql/data\n\n    volumes:\n      langflow-postgres:\n      langflow-data:\n    ```\n\n4. Start the Docker container:\n\n    ```text\n    docker-compose up\n    ```\n\n5. To confirm Langfuse is connected to your Langflow container, run the following command:\n\n    ```sh\n    docker compose exec langflow python -c \"import requests, os; addr = os.environ.get('LANGFUSE_HOST'); print(addr); res = requests.get(addr, timeout=5); print(res.status_code)\"\n    ```\n\n    If there is an error, make sure you have set the `LANGFUSE_HOST` environment variable in your terminal session.\n\n    Output similar to the following indicates success:\n\n    ```text\n    https://us.cloud.langfuse.com\n    200\n    ```\n\n## See also\n\n* [Langfuse GitHub repository](https://github.com/langfuse/langfuse)",
      "summary": "[Langfuse](https://langfuse.com) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks, including workflow...",
      "keywords": [
        "compose",
        "credentials",
        "disable",
        "docker",
        "environment",
        "langflow",
        "langfuse",
        "note",
        "prerequisites",
        "run",
        "set",
        "start",
        "traces",
        "tracing",
        "variables",
        "view"
      ]
    },
    "/integrations-langsmith": {
      "title": "LangSmith",
      "slug": "/integrations-langsmith",
      "category": "develop",
      "content": "LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, add your LangChain API key and configuration as Langflow environment variables, and then start Langflow.\n\n1. Obtain your LangChain API key from [https://smith.langchain.com](https://smith.langchain.com/)\n2. Set the following environment variables in your Langflow `.env` file, replacing `LANGCHAIN_API_KEY` and `LANGSMITH_PROJECT_NAME` with your own values:\n\n    ```text\n    LANGSMITH_TRACING=True\n    LANGSMITH_ENDPOINT=https://api.smith.langchain.com/\n    LANGSMITH_API_KEY=LANGCHAIN_API_KEY\n    LANGSMITH_PROJECT=LANGSMITH_PROJECT_NAME\n    ```\n\n    Alternatively, you can export the environment variables in your terminal instead of adding them to the `.env` file:\n\n    ```bash\n    export LANGSMITH_TRACING=True && export LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com/\" && export LANGSMITH_API_KEY=\"LANGCHAIN_API_KEY\" && export LANGSMITH_PROJECT=\"LANGSMITH_PROJECT_NAME\"\n    ```\n\n3. Restart Langflow with your modified `.env` file or from the terminal where you set your environment variables:\n\n    ```bash\n    langflow run --env-file .env\n    ```\n\n    If you set the environment variables in your terminal, you can omit `--env-file`.\n    However, Langflow can source environment variables from `.env` _and_ your terminal.\n    For more information, see [Langflow environment variables](/environment-variables).\n\n4. Run a flow in Langflow to generate some activity.\n\n5. View the LangSmith dashboard for monitoring and observability.\n\n    ![LangSmith dashboard](/img/langsmith-dashboard.png)",
      "summary": "LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, add your LangChain API key and configuration as Langflow environment variables, and then start Langflow.",
      "keywords": [
        "langsmith"
      ]
    },
    "integrations-langsmith": {
      "title": "LangSmith",
      "slug": "/integrations-langsmith",
      "category": "develop",
      "content": "LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, add your LangChain API key and configuration as Langflow environment variables, and then start Langflow.\n\n1. Obtain your LangChain API key from [https://smith.langchain.com](https://smith.langchain.com/)\n2. Set the following environment variables in your Langflow `.env` file, replacing `LANGCHAIN_API_KEY` and `LANGSMITH_PROJECT_NAME` with your own values:\n\n    ```text\n    LANGSMITH_TRACING=True\n    LANGSMITH_ENDPOINT=https://api.smith.langchain.com/\n    LANGSMITH_API_KEY=LANGCHAIN_API_KEY\n    LANGSMITH_PROJECT=LANGSMITH_PROJECT_NAME\n    ```\n\n    Alternatively, you can export the environment variables in your terminal instead of adding them to the `.env` file:\n\n    ```bash\n    export LANGSMITH_TRACING=True && export LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com/\" && export LANGSMITH_API_KEY=\"LANGCHAIN_API_KEY\" && export LANGSMITH_PROJECT=\"LANGSMITH_PROJECT_NAME\"\n    ```\n\n3. Restart Langflow with your modified `.env` file or from the terminal where you set your environment variables:\n\n    ```bash\n    langflow run --env-file .env\n    ```\n\n    If you set the environment variables in your terminal, you can omit `--env-file`.\n    However, Langflow can source environment variables from `.env` _and_ your terminal.\n    For more information, see [Langflow environment variables](/environment-variables).\n\n4. Run a flow in Langflow to generate some activity.\n\n5. View the LangSmith dashboard for monitoring and observability.\n\n    ![LangSmith dashboard](/img/langsmith-dashboard.png)",
      "summary": "LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, add your LangChain API key and configuration as Langflow environment variables, and then start Langflow.",
      "keywords": [
        "langsmith"
      ]
    },
    "/integrations-langwatch": {
      "title": "LangWatch",
      "slug": "/integrations-langwatch",
      "category": "develop",
      "content": "[LangWatch](https://app.langwatch.ai/) is an all-in-one LLMOps platform for monitoring, observability, analytics, evaluations and alerting for getting user insights and improve your LLM workflows.\n\n## Integrate LangWatch observability\n\nTo integrate with Langflow, add your LangWatch API key as a Langflow environment variable:\n\n1. Get a LangWatch API key from your LangWatch account.\n\n2. Add the key to your Langflow `.env` file:\n\n    ```shell\n    LANGWATCH_API_KEY=\"API_KEY_STRING\"\n    ```\n\n    Alternatively, you can set the environment variable in your terminal session:\n\n    ```shell\n    export LANGWATCH_API_KEY=\"API_KEY_STRING\"\n    ```\n\n3. Restart Langflow with your `.env` file, if you modified the Langflow `.env`:\n\n    ```\n    langflow run --env-file .env\n    ```\n\n4. Run a flow.\n\n5. View the LangWatch dashboard for monitoring and observability.\n\n![LangWatch dashboard](/img/langwatch-dashboard.png)\n\n## Use the LangWatch Evaluator\n\nIn your flows, you can use the **LangWatch Evaluator** component to use LangWatch's evaluation endpoints to assess a model's performance.\nThis component is available in the **LangWatch** [bundle](/components-bundle-components).",
      "summary": "[LangWatch](https://app.langwatch.ai/) is an all-in-one LLMOps platform for monitoring, observability, analytics, evaluations and alerting for getting user insights and improve your LLM workflows.",
      "keywords": [
        "evaluator",
        "integrate",
        "langwatch",
        "observability"
      ]
    },
    "integrations-langwatch": {
      "title": "LangWatch",
      "slug": "/integrations-langwatch",
      "category": "develop",
      "content": "[LangWatch](https://app.langwatch.ai/) is an all-in-one LLMOps platform for monitoring, observability, analytics, evaluations and alerting for getting user insights and improve your LLM workflows.\n\n## Integrate LangWatch observability\n\nTo integrate with Langflow, add your LangWatch API key as a Langflow environment variable:\n\n1. Get a LangWatch API key from your LangWatch account.\n\n2. Add the key to your Langflow `.env` file:\n\n    ```shell\n    LANGWATCH_API_KEY=\"API_KEY_STRING\"\n    ```\n\n    Alternatively, you can set the environment variable in your terminal session:\n\n    ```shell\n    export LANGWATCH_API_KEY=\"API_KEY_STRING\"\n    ```\n\n3. Restart Langflow with your `.env` file, if you modified the Langflow `.env`:\n\n    ```\n    langflow run --env-file .env\n    ```\n\n4. Run a flow.\n\n5. View the LangWatch dashboard for monitoring and observability.\n\n![LangWatch dashboard](/img/langwatch-dashboard.png)\n\n## Use the LangWatch Evaluator\n\nIn your flows, you can use the **LangWatch Evaluator** component to use LangWatch's evaluation endpoints to assess a model's performance.\nThis component is available in the **LangWatch** [bundle](/components-bundle-components).",
      "summary": "[LangWatch](https://app.langwatch.ai/) is an all-in-one LLMOps platform for monitoring, observability, analytics, evaluations and alerting for getting user insights and improve your LLM workflows.",
      "keywords": [
        "evaluator",
        "integrate",
        "langwatch",
        "observability"
      ]
    },
    "/integrations-opik": {
      "title": "Opik",
      "slug": "/integrations-opik",
      "category": "develop",
      "content": "[Opik](https://www.comet.com/site/products/opik/) is an open-source platform designed for evaluating, testing, and monitoring large language model (LLM) applications. Developed by Comet, it aims to facilitate more intuitive collaboration, testing, and monitoring of LLM-based applications.\n\nYou can configure Langflow to collect [tracing](https://www.comet.com/docs/opik/tracing/log_traces) data about your flow executions and automatically send the data to Opik.\n\n## Prerequisites\n\n- An [Open-Source Opik server or an Opik Cloud account](https://www.comet.com/docs/opik/faq#what-is-the-difference-between-opik-cloud-and-the-open-source-opik-platform-)\n- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace\n\n**Note:** If you need a flow to test the Opik integration, see the [Langflow quickstart](/get-started-quickstart).\n\n## Integrate Opik with Langflow\n\n1. If you use Opik Cloud, get an [Opik API key](https://www.comet.com/docs/opik/faq#where-can-i-find-my-opik-api-key-).\n\n    An API key isn't required with an Open-Source Opik server.\n\n2. Call the `opik configure` CLI to save your Opik configuration in the same environment where you run Langflow:\n\n    ```bash\n    opik configure\n    ```\n\n    For self-hosted Opik, you can also use the following Opik CLI command:\n\n    ```bash\n    opik configure --use_local\n    ```\n\n    For more information, see the [Opik SDK configuration documentation](https://www.comet.com/docs/opik/tracing/sdk_configuration).\n\n3. Start Langflow in the same terminal or environment where you set the environment variables:\n\n    ```bash\n    uv run langflow run\n    ```\n\n4. In Langflow, run a flow to produce activity for Opik to trace.\n\n5. Navigate to your Opik project dashboard and view the collected tracing data.\n\n## Disable the Opik integration\n\nTo disable the Opik integration, remove the environment variables you set with `opik configure`, and then restart Langflow.",
      "summary": "[Opik](https://www.comet.com/site/products/opik/) is an open-source platform designed for evaluating, testing, and monitoring large language model (LLM) applications. Developed by Comet, it aims to facilitate more intuitive collaboration, testing, and monitoring of LLM-based applications.",
      "keywords": [
        "disable",
        "integrate",
        "integration",
        "langflow",
        "note",
        "opik",
        "prerequisites"
      ]
    },
    "integrations-opik": {
      "title": "Opik",
      "slug": "/integrations-opik",
      "category": "develop",
      "content": "[Opik](https://www.comet.com/site/products/opik/) is an open-source platform designed for evaluating, testing, and monitoring large language model (LLM) applications. Developed by Comet, it aims to facilitate more intuitive collaboration, testing, and monitoring of LLM-based applications.\n\nYou can configure Langflow to collect [tracing](https://www.comet.com/docs/opik/tracing/log_traces) data about your flow executions and automatically send the data to Opik.\n\n## Prerequisites\n\n- An [Open-Source Opik server or an Opik Cloud account](https://www.comet.com/docs/opik/faq#what-is-the-difference-between-opik-cloud-and-the-open-source-opik-platform-)\n- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace\n\n**Note:** If you need a flow to test the Opik integration, see the [Langflow quickstart](/get-started-quickstart).\n\n## Integrate Opik with Langflow\n\n1. If you use Opik Cloud, get an [Opik API key](https://www.comet.com/docs/opik/faq#where-can-i-find-my-opik-api-key-).\n\n    An API key isn't required with an Open-Source Opik server.\n\n2. Call the `opik configure` CLI to save your Opik configuration in the same environment where you run Langflow:\n\n    ```bash\n    opik configure\n    ```\n\n    For self-hosted Opik, you can also use the following Opik CLI command:\n\n    ```bash\n    opik configure --use_local\n    ```\n\n    For more information, see the [Opik SDK configuration documentation](https://www.comet.com/docs/opik/tracing/sdk_configuration).\n\n3. Start Langflow in the same terminal or environment where you set the environment variables:\n\n    ```bash\n    uv run langflow run\n    ```\n\n4. In Langflow, run a flow to produce activity for Opik to trace.\n\n5. Navigate to your Opik project dashboard and view the collected tracing data.\n\n## Disable the Opik integration\n\nTo disable the Opik integration, remove the environment variables you set with `opik configure`, and then restart Langflow.",
      "summary": "[Opik](https://www.comet.com/site/products/opik/) is an open-source platform designed for evaluating, testing, and monitoring large language model (LLM) applications. Developed by Comet, it aims to facilitate more intuitive collaboration, testing, and monitoring of LLM-based applications.",
      "keywords": [
        "disable",
        "integrate",
        "integration",
        "langflow",
        "note",
        "opik",
        "prerequisites"
      ]
    },
    "/logging": {
      "title": "Logs",
      "slug": "/logging",
      "category": "develop",
      "content": "Langflow produces logs for individual flows and the Langflow application itself using the [structlog](https://www.structlog.org) library for logging.\n\nThe default, primary logfile is named `langflow.log`.\n\nLog files are stored in JSON format with structured metadata.\n\n## Log storage\n\nLangflow logs are stored in the config directory specified in the `LANGFLOW_CONFIG_DIR` environment variable.\nThe default config directory location depends on your operating system and installation method:\n\n- **Langflow Desktop**:\n\n    - **macOS**: `/Users/<username>/Library/Logs/com.LangflowDesktop`\n    - **Windows**: `C:\\Users\\<username>\\AppData\\Local\\com.LangflowDesktop\\logs`\n\n- **OSS Langflow**:\n\n    - **macOS with `uv pip install`**: `/Users/<username>/Library/Caches/langflow`\n    - **Linux with `uv pip install`**: `/home/<username>/.cache/langflow`\n    - **Windows/WSL with `uv pip install`**: `C:\\Users\\<username>\\AppData\\Local\\langflow\\langflow\\Cache`\n    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/`\n\nTo customize log storage locations and behaviors, set the following [Langflow environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_CONFIG_DIR` | String | Varies | Set the Langflow configuration directory where files and logs are stored. Default path depends on your installation, as described in the preceding list. |\n| `LANGFLOW_LOG_LEVEL` | String | `ERROR` | Sets the log level as one of `DEBUG`, `ERROR`, `INFO`, `WARNING`, and `CRITICAL`. For example, `LANGFLOW_LOG_LEVEL=DEBUG`. |\n| `LANGFLOW_LOG_FILE` | String | Not set | Sets the log file storage location if you want to use a non-default location. For example, `LANGFLOW_LOG_FILE=path/to/logfile.log`. If not set, logs are written to stdout. |\n| `LANGFLOW_LOG_ENV` | String | `default` | This variable is the primary log format controller. `container`: JSON format for Docker/structured logging. `container_csv`: Key-value format for data analysis. `default` or unset: Uses `LANGFLOW_PRETTY_LOGS` to determine format. |\n| `LANGFLOW_PRETTY_LOGS` | Boolean | `True` | This variable controls log output format when `LANGFLOW_LOG_ENV=default` or unset. When `true`, uses structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). When `false`, outputs logs in JSON format.  |\n| `LANGFLOW_LOG_FORMAT` | String | Not set | Switch between key-value format and console format. Set to `key_value` for key-value format or `console` to use structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). This variable only works when `LANGFLOW_LOG_ENV=default` and `LANGFLOW_PRETTY_LOGS=true`. |\n| `LANGFLOW_LOG_ROTATION` | String | `1 day` | Controls when the log file is rotated, either based on time or file size. For time-based rotation, set to `1 day`, `12 hours`, or `1 week`. For size-based rotation, set to `10 MB` or `1 GB`. To disable rotation, set to `None`. If disabled, log files grow without limit. |\n| `LANGFLOW_ENABLE_LOG_RETRIEVAL` | Boolean | `False` | Enables retrieval of logs from your Langflow instance with [Logs endpoints](/api-logs). |\n| `LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE` | Integer | `10000` | Set the buffer size for log retrieval if `LANGFLOW_ENABLE_LOG_RETRIEVAL=True`. Must be greater than `0` for log retrieval to function. |\n\n## View logs in real-time\n\nTo monitor Langflow logs as they are generated, you can follow the log file:\n\n1. Change to your [Langflow config directory](#log-storage):\n\n    \n    \n    </Tabs>\n\n2. Tail the main log file:\n\n    \n    \n    </Tabs>\n\n    If you don't see new log entries, check that Langflow is running, and perform some actions to generate logs events. You can also check the terminal where you started Langflow to see if logs are being printed there.\n\n## Flow and component logs\n\nAfter you run a flow, you can inspect the logs for the each component and flow run.\nFor example, you can inspect `Message` objects ingested and generated by [Input and Output components](/components-io).\n\n### View flow logs\n\nIn the visual editor, click **Logs** to view logs for the entire flow:\n\n![Logs pane](/img/logs.png)\n\nThen, click the cells in the **inputs** and **outputs** columns to inspect the `Message` objects.\nFor example, the following `Message` data could be the output from a **Chat Input** component:\n\n```text\n    \"messages\": [\n    {\n        \"message\": \"What's the recommended way to install Docker on Mac M1?\",\n        \"sender\": \"User\",\n        \"sender_name\": \"User\",\n        \"session_id\": \"Session Apr 21, 17:37:04\",\n        \"stream_url\": null,\n        \"component_id\": \"ChatInput-4WKag\",\n        \"files\": [],\n        \"type\": \"text\"\n    }\n    ],\n```\n\nIn the case of Input/Output components, the original input might not be structured as a `Message` object.\nFor example, a language model component can pass a raw text response to a **Chat Output** component that is then transformed into a `Message` object.\n\nYou can find `.log` files for flows at your Langflow installation's log storage location.\nFor filepaths, see [Log storage](#log-storage).\n\n### View chat logs\n\nIn the **Playground**, you can inspect the chat history for each chat session.\nFor more information, see [View chat history](/concepts-playground#view-chat-history).\n\n### View output from a single component\n\nWhen debugging issues with the format or content of a flow's output, it can help to inspect each component's output to determine where data is being lost or malformed.\n\nTo view the output produced by a single component during the most recent run, click  **Inspect output** on the component in the visual editor.\n\n## Access Langflow Desktop logs {#desktop-logs}\n\nIf you encounter issues with Langflow Desktop, you might need to access startup logs for debugging.\nFollow the steps for your operating system.\n\n</Tabs>\n\nYou can use the log file to investigate the issue on your own, add context to a [GitHub Issue](/contributing-github-issues), or send it to [support](/luna-for-langflow) for debugging assistance.\n\nThe log file is only created when Langflow Desktop runs. If you don't see a log file, try starting Langflow Desktop first, then check for the log file.\n\n## See also\n\n* [Logs endpoints](/api-logs)\n* [Memory management options](/memory)\n* [Configure an external PostgreSQL database](/configuration-custom-database)",
      "summary": "Langflow produces logs for individual flows and the Langflow application itself using the [structlog](https://www.structlog.org) library for logging.",
      "keywords": [
        "access",
        "chat",
        "clone",
        "component",
        "desktop",
        "flow",
        "git",
        "input",
        "inputs",
        "inspect",
        "install",
        "langflow",
        "linux",
        "log",
        "logs",
        "macos",
        "oss",
        "output",
        "outputs",
        "pip"
      ]
    },
    "logging": {
      "title": "Logs",
      "slug": "/logging",
      "category": "develop",
      "content": "Langflow produces logs for individual flows and the Langflow application itself using the [structlog](https://www.structlog.org) library for logging.\n\nThe default, primary logfile is named `langflow.log`.\n\nLog files are stored in JSON format with structured metadata.\n\n## Log storage\n\nLangflow logs are stored in the config directory specified in the `LANGFLOW_CONFIG_DIR` environment variable.\nThe default config directory location depends on your operating system and installation method:\n\n- **Langflow Desktop**:\n\n    - **macOS**: `/Users/<username>/Library/Logs/com.LangflowDesktop`\n    - **Windows**: `C:\\Users\\<username>\\AppData\\Local\\com.LangflowDesktop\\logs`\n\n- **OSS Langflow**:\n\n    - **macOS with `uv pip install`**: `/Users/<username>/Library/Caches/langflow`\n    - **Linux with `uv pip install`**: `/home/<username>/.cache/langflow`\n    - **Windows/WSL with `uv pip install`**: `C:\\Users\\<username>\\AppData\\Local\\langflow\\langflow\\Cache`\n    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/`\n\nTo customize log storage locations and behaviors, set the following [Langflow environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:\n\n| Variable | Format | Default | Description |\n|----------|--------|---------|-------------|\n| `LANGFLOW_CONFIG_DIR` | String | Varies | Set the Langflow configuration directory where files and logs are stored. Default path depends on your installation, as described in the preceding list. |\n| `LANGFLOW_LOG_LEVEL` | String | `ERROR` | Sets the log level as one of `DEBUG`, `ERROR`, `INFO`, `WARNING`, and `CRITICAL`. For example, `LANGFLOW_LOG_LEVEL=DEBUG`. |\n| `LANGFLOW_LOG_FILE` | String | Not set | Sets the log file storage location if you want to use a non-default location. For example, `LANGFLOW_LOG_FILE=path/to/logfile.log`. If not set, logs are written to stdout. |\n| `LANGFLOW_LOG_ENV` | String | `default` | This variable is the primary log format controller. `container`: JSON format for Docker/structured logging. `container_csv`: Key-value format for data analysis. `default` or unset: Uses `LANGFLOW_PRETTY_LOGS` to determine format. |\n| `LANGFLOW_PRETTY_LOGS` | Boolean | `True` | This variable controls log output format when `LANGFLOW_LOG_ENV=default` or unset. When `true`, uses structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). When `false`, outputs logs in JSON format.  |\n| `LANGFLOW_LOG_FORMAT` | String | Not set | Switch between key-value format and console format. Set to `key_value` for key-value format or `console` to use structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). This variable only works when `LANGFLOW_LOG_ENV=default` and `LANGFLOW_PRETTY_LOGS=true`. |\n| `LANGFLOW_LOG_ROTATION` | String | `1 day` | Controls when the log file is rotated, either based on time or file size. For time-based rotation, set to `1 day`, `12 hours`, or `1 week`. For size-based rotation, set to `10 MB` or `1 GB`. To disable rotation, set to `None`. If disabled, log files grow without limit. |\n| `LANGFLOW_ENABLE_LOG_RETRIEVAL` | Boolean | `False` | Enables retrieval of logs from your Langflow instance with [Logs endpoints](/api-logs). |\n| `LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE` | Integer | `10000` | Set the buffer size for log retrieval if `LANGFLOW_ENABLE_LOG_RETRIEVAL=True`. Must be greater than `0` for log retrieval to function. |\n\n## View logs in real-time\n\nTo monitor Langflow logs as they are generated, you can follow the log file:\n\n1. Change to your [Langflow config directory](#log-storage):\n\n    \n    \n    </Tabs>\n\n2. Tail the main log file:\n\n    \n    \n    </Tabs>\n\n    If you don't see new log entries, check that Langflow is running, and perform some actions to generate logs events. You can also check the terminal where you started Langflow to see if logs are being printed there.\n\n## Flow and component logs\n\nAfter you run a flow, you can inspect the logs for the each component and flow run.\nFor example, you can inspect `Message` objects ingested and generated by [Input and Output components](/components-io).\n\n### View flow logs\n\nIn the visual editor, click **Logs** to view logs for the entire flow:\n\n![Logs pane](/img/logs.png)\n\nThen, click the cells in the **inputs** and **outputs** columns to inspect the `Message` objects.\nFor example, the following `Message` data could be the output from a **Chat Input** component:\n\n```text\n    \"messages\": [\n    {\n        \"message\": \"What's the recommended way to install Docker on Mac M1?\",\n        \"sender\": \"User\",\n        \"sender_name\": \"User\",\n        \"session_id\": \"Session Apr 21, 17:37:04\",\n        \"stream_url\": null,\n        \"component_id\": \"ChatInput-4WKag\",\n        \"files\": [],\n        \"type\": \"text\"\n    }\n    ],\n```\n\nIn the case of Input/Output components, the original input might not be structured as a `Message` object.\nFor example, a language model component can pass a raw text response to a **Chat Output** component that is then transformed into a `Message` object.\n\nYou can find `.log` files for flows at your Langflow installation's log storage location.\nFor filepaths, see [Log storage](#log-storage).\n\n### View chat logs\n\nIn the **Playground**, you can inspect the chat history for each chat session.\nFor more information, see [View chat history](/concepts-playground#view-chat-history).\n\n### View output from a single component\n\nWhen debugging issues with the format or content of a flow's output, it can help to inspect each component's output to determine where data is being lost or malformed.\n\nTo view the output produced by a single component during the most recent run, click  **Inspect output** on the component in the visual editor.\n\n## Access Langflow Desktop logs {#desktop-logs}\n\nIf you encounter issues with Langflow Desktop, you might need to access startup logs for debugging.\nFollow the steps for your operating system.\n\n</Tabs>\n\nYou can use the log file to investigate the issue on your own, add context to a [GitHub Issue](/contributing-github-issues), or send it to [support](/luna-for-langflow) for debugging assistance.\n\nThe log file is only created when Langflow Desktop runs. If you don't see a log file, try starting Langflow Desktop first, then check for the log file.\n\n## See also\n\n* [Logs endpoints](/api-logs)\n* [Memory management options](/memory)\n* [Configure an external PostgreSQL database](/configuration-custom-database)",
      "summary": "Langflow produces logs for individual flows and the Langflow application itself using the [structlog](https://www.structlog.org) library for logging.",
      "keywords": [
        "access",
        "chat",
        "clone",
        "component",
        "desktop",
        "flow",
        "git",
        "input",
        "inputs",
        "inspect",
        "install",
        "langflow",
        "linux",
        "log",
        "logs",
        "macos",
        "oss",
        "output",
        "outputs",
        "pip"
      ]
    },
    "/memory": {
      "title": "Memory management options",
      "slug": "/memory",
      "category": "develop",
      "content": "Langflow provides flexible memory management options for storage and retrieval of data relevant to your flows and your Langflow server.\nThis includes essential Langflow database tables, file management, and caching, as well as chat memory.\n\n## Storage options and paths\n\nLangflow supports both local memory and external memory options.\n\nLangflow's default storage option is a [SQLite](https://www.sqlite.org/) database.\nThe default storage path depends on your operating system and installation method:\n\n- **Langflow Desktop**:\n    - **macOS**: `/Users/<username>/.langflow/data/database.db`\n    - **Windows**: `C:\\Users\\<name>\\AppData\\Roaming\\com.LangflowDesktop\\data\\database.db`\n- **Langflow OSS**\n    - **macOS/Windows/Linux/WSL with `uv pip install`**: `<path_to_venv>/lib/python3.12/site-packages/langflow/langflow.db` (Python version can vary. Database isn't shared between virtual environments because it is tied to the venv path.)\n    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/langflow.db`\n\nLangflow offers a few alternatives to the default database path:\n\n* **Config directory**: Set `LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True` to store the database in your Langflow config directory as set in [`LANGFLOW_CONFIG_DIR`](/logging).\n\n* **External PostgreSQL database**: You can use an external PostgreSQL database for all of your Langflow storage.\nFor more information, see [Configure external memory](#configure-external-memory)\n\n    External storage can be useful if you want to preserve the data after uninstalling Langflow or to share the same database between multiple virtual environments.\n\n* **Separate chat memory**: You can selectively use external storage for chat memory only, separate from other Langflow storage.\nFor more information, see [Store chat memory](#store-chat-memory).\n\n* **No database**: To disable all database operations and run a no-op session, set `LANGFLOW_USE_NOOP_DATABASE=True` in your [Langflow environment variables](/environment-variables).\nThis is useful for testing when you don't want to persist any data.\n\n## Langflow database tables\n\nThe following tables are stored in `langflow.db`:\n\n **ApiKey**: Manages Langflow API authentication keys. Component API keys are stored in the **Variables** table. For more information, see [API keys and authentication](/api-keys-and-authentication).\n\n **File**: Stores metadata for files uploaded to Langflow's file management system, including file names, paths, sizes, and storage providers. For more information, see [Manage files](/concepts-file-management).\n\n **Flow**: Contains flow definitions, including nodes, edges, and components, stored as JSON or database records. For more information, see [Build flows](/concepts-flows).\n\n    \n**Note:** To automatically remove API keys and tokens from flow data before saving a flow to the database, set `LANGFLOW_REMOVE_API_KEYS=True` in your [Langflow environment variables](/environment-variables).\n    When `true`, any field marked as a password field that _also_ has `api`, `key`, or `token` in its name is set to `null` before the flow is saved.\n    This helps prevent credentials from being stored in the database.\n    \n\n **Folder**: Provides a structure for flow storage, including single-user folders and shared folders accessed by multiple users. For more information, see [Manage flows in projects](/concepts-flows#projects).\n\n **Message**: Stores chat messages and interactions that occur between components. For more information, see [Message objects](/data-types#message) and [Store chat memory](#store-chat-memory).\n\n **Transactions**: Records execution history and results of flow runs. This information is used for [logging](/logging).\n\n **User**: Stores user account information including credentials, permissions, profiles, and user management settings. For more information, see [API keys and authentication](/api-keys-and-authentication).\n\n **Variables**: Stores global encrypted values and credentials. For more information, see [Global variables](/configuration-global-variables) and [Component API keys](/api-keys-and-authentication#component-api-keys).\n\n **VertexBuild**: Tracks the build status of individual nodes within flows. For more information, see [Test flows in the Playground](/concepts-playground).\n\nFor more information, see the database models in the [source code](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/services/database/models).\n\n## Configure external memory {#configure-external-memory}\n\nTo replace the default Langflow SQLite database with another database, set the `LANGFLOW_DATABASE_URL` environment variable to your database URL, and then start Langflow with your `.env` file.\nFor more information and examples, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\n```text\nLANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow\n```\n\nTo fine-tune your database connection pool and timeout settings, you can set the following additional environment variables:\n\n* `LANGFLOW_DATABASE_CONNECTION_RETRY`: Whether to retry lost connections to your Langflow database. If `true`, Langflow tries to connect to the database again if the connection fails. Default: `false`.\n\n* `LANGFLOW_DB_CONNECT_TIMEOUT`: The number of seconds to wait before giving up on a lock to be released or establishing a connection to the database. This may be separate from the `pool_timeout` in `LANGFLOW_DB_CONNECTION_SETTINGS`. Default: 30.\n\n* `LANGFLOW_DB_CONNECTION_SETTINGS`: A JSON dictionary containing the following database connection pool settings:\n\n    - `pool_size`: The base number of connections to keep open in the connection pool. Default: 20.\n    - `max_overflow`: Maximum number of connections that can be created in excess of `pool_size` if needed. Default: 30.\n    - `pool_timeout`: Number of seconds to wait for a connection from the pool before timing out. Default: 30.\n    - `pool_pre_ping`: If `true`, the pool tests connections for liveness upon each checkout. Default: `true`.\n    - `pool_recycle`: Number of seconds after which a connection is automatically recycled. Default: 1800 (30 minutes).\n    - `echo`: If `true`, SQL queries are logged for debugging purposes. Default: `false`.\n\n    For example:\n\n    ```text\n    LANGFLOW_DB_CONNECTION_SETTINGS='{\"pool_size\": 20, \"max_overflow\": 30, \"pool_timeout\": 30, \"pool_pre_ping\": true, \"pool_recycle\": 1800, \"echo\": false}'\n    ```\n\n    Don't use the deprecated environment variables `LANGFLOW_DB_POOL_SIZE` or `LANGFLOW_DB_MAX_OVERFLOW`.\n    Instead, use `pool_size` and `max_overflow` in `LANGFLOW_DB_CONNECTION_SETTINGS`.\n\n## Configure cache memory\n\nThe default Langflow caching behavior is an asynchronous, in-memory cache:\n\n```text\nLANGFLOW_LANGCHAIN_CACHE=InMemoryCache\nLANGFLOW_CACHE_TYPE=async\n```\n\nLangflow officially supports only the default asynchronous, in-memory cache, which is suitable for most use cases.\nOther cache options, such as Redis, are experimental and can change without notice.\nIf you want to use a non-default cache setting, you can use the following environment variables:\n\n| Variable | Type | Default | Description |\n|----------|------|---------|-------------|\n| `LANGFLOW_CACHE_TYPE` | String | `async` | Set the cache type for Langflow's internal caching system. Possible values: `async`, `redis`, `memory`, `disk`. If you set the type to `redis`, then you must also set the `LANGFLOW_REDIS_*` environment variables. |\n| `LANGFLOW_LANGCHAIN_CACHE` | String | `InMemoryCache` | Set the cache storage type for the LangChain caching system (a Langflow dependency), either `InMemoryCache` or `SQLiteCache`. |\n| `LANGFLOW_REDIS_HOST` | String | `localhost` | Redis server hostname if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_PORT` | Integer | `6379` | Redis server port if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_DB` | Integer | `0` | Redis database number if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_CACHE_EXPIRE` | Integer | `3600` | Cache expiration time in seconds if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_PASSWORD` | String | Not set | Optional password for Redis authentication if `LANGFLOW_CACHE_TYPE=redis`. |\n\n## Store chat memory\n\nChat-based flows with a **Chat Input** or **Chat Output** component produce chat history that is stored in the Langflow `messages` table.\nAt minimum, this serves as a chat log, but it isn't functionally the same as chat memory that provides historical context to an LLM.\n\nTo store and retrieve chat memories in flows, you can use a **Message History** component or the **Agent** component's built-in chat memory.\n\n<details>\n<summary>How does chat memory work?</summary>\n\nChat memory is a cache for an LLM or agent to preserve past conversations to retain and reference that context in future interactions.\nFor example, if a user has already told the LLM their name, the LLM can retrieve that information from chat memory rather than asking the user to repeat themselves in future conversations or messages.\n\nChat memory is distinct from vector store memory because it is built specifically for storing and retrieving chat messages from databases.\n\nComponents that support chat memory (such as the **Agent** and **Message History** components) provide access to their respective databases _as memory_.\nRetrieval as memory is an important distinction for LLMs and agents because this storage and retrieval mechanism is specifically designed to recall context from past conversations.\nUnlike vector stores, which are designed for semantic search and retrieval of text chunks, chat memory is designed to store and retrieve chat messages in a way that is optimized for conversation history.\n\n</details>\n\n### Session ID and chat memory\n\nChat history and memories are grouped by [session ID (`session_id`)](/session-id).\n\nThe default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one large chat session.\n\nFor better segregation of chat memory, especially in flows used by multiple users, consider using custom session IDs.\nFor example, if you use user IDs as session IDs, then each user's chat history is stored separately, isolating the context of their chats from other users' chats.\n\n### Chat memory options\n\nWhere and how chat memory is stored depends on the components used in your flow:\n\n* **Agent component**: This component has built-in chat memory that is enabled by default.\nThis memory allows the agent to retrieve and reference messages from previous conversations associated with the same session ID.\nAll messages are stored in [Langflow storage](#storage-options-and-paths), and the component provides minimal memory configuration options, such as the number of messages to retrieve.\n\n    The **Agent** component's built-in chat memory is sufficient for most use cases.\n\n    If you want to use external chat memory storage, retrieve memories outside the context of a chat, or use chat memory with a language model component (not an agent), you must use the **Message History** component (with or without a third-party chat memory component).\n\n* **Message History component**: By default, this component stores and retrieves memories from Langflow storage, unless you attach a third-party chat memory component. It provides a few more options for sorting and filtering memories, although most of these options are also built-in to the **Agent** component as configurable or fixed parameters.\n\n    You can use the **Message History** component with or without a language model or agent.\n    For example, if you need to retrieve data from memories outside of chat, you can use the **Message History** component to fetch that data directly from your chat memory database without feeding it into a chat.\n\n* **Third-party chat memory components**: Use one of these components only if you need to store or retrieve chat memories from a dedicated external chat memory database.\nTypically, this is necessary only if you have specific storage needs that aren't met by Langflow storage.\nFor example, if you want to manage chat memory data by directly working with the database, or if you want to use a different database than the default Langflow storage.\n\nFor more information and examples, see [**Message History** component](/components-helpers#message-history) and [Agent memory](/agents#agent-memory).\n\n## See also\n\n* [Langflow file management](/concepts-file-management)\n* [Langflow logs](/logging)\n* [Langflow environment variables](/environment-variables)",
      "summary": "Langflow provides flexible memory management options for storage and retrieval of data relevant to your flows and your Langflow server.\nThis includes essential Langflow database tables, file management, and caching, as well as chat memory.",
      "keywords": [
        "agent",
        "apikey",
        "cache",
        "chat",
        "clone",
        "component",
        "components",
        "config",
        "configure",
        "database",
        "desktop",
        "directory",
        "external",
        "file",
        "flow",
        "folder",
        "git",
        "history",
        "input",
        "install"
      ]
    },
    "memory": {
      "title": "Memory management options",
      "slug": "/memory",
      "category": "develop",
      "content": "Langflow provides flexible memory management options for storage and retrieval of data relevant to your flows and your Langflow server.\nThis includes essential Langflow database tables, file management, and caching, as well as chat memory.\n\n## Storage options and paths\n\nLangflow supports both local memory and external memory options.\n\nLangflow's default storage option is a [SQLite](https://www.sqlite.org/) database.\nThe default storage path depends on your operating system and installation method:\n\n- **Langflow Desktop**:\n    - **macOS**: `/Users/<username>/.langflow/data/database.db`\n    - **Windows**: `C:\\Users\\<name>\\AppData\\Roaming\\com.LangflowDesktop\\data\\database.db`\n- **Langflow OSS**\n    - **macOS/Windows/Linux/WSL with `uv pip install`**: `<path_to_venv>/lib/python3.12/site-packages/langflow/langflow.db` (Python version can vary. Database isn't shared between virtual environments because it is tied to the venv path.)\n    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/langflow.db`\n\nLangflow offers a few alternatives to the default database path:\n\n* **Config directory**: Set `LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True` to store the database in your Langflow config directory as set in [`LANGFLOW_CONFIG_DIR`](/logging).\n\n* **External PostgreSQL database**: You can use an external PostgreSQL database for all of your Langflow storage.\nFor more information, see [Configure external memory](#configure-external-memory)\n\n    External storage can be useful if you want to preserve the data after uninstalling Langflow or to share the same database between multiple virtual environments.\n\n* **Separate chat memory**: You can selectively use external storage for chat memory only, separate from other Langflow storage.\nFor more information, see [Store chat memory](#store-chat-memory).\n\n* **No database**: To disable all database operations and run a no-op session, set `LANGFLOW_USE_NOOP_DATABASE=True` in your [Langflow environment variables](/environment-variables).\nThis is useful for testing when you don't want to persist any data.\n\n## Langflow database tables\n\nThe following tables are stored in `langflow.db`:\n\n **ApiKey**: Manages Langflow API authentication keys. Component API keys are stored in the **Variables** table. For more information, see [API keys and authentication](/api-keys-and-authentication).\n\n **File**: Stores metadata for files uploaded to Langflow's file management system, including file names, paths, sizes, and storage providers. For more information, see [Manage files](/concepts-file-management).\n\n **Flow**: Contains flow definitions, including nodes, edges, and components, stored as JSON or database records. For more information, see [Build flows](/concepts-flows).\n\n    \n**Note:** To automatically remove API keys and tokens from flow data before saving a flow to the database, set `LANGFLOW_REMOVE_API_KEYS=True` in your [Langflow environment variables](/environment-variables).\n    When `true`, any field marked as a password field that _also_ has `api`, `key`, or `token` in its name is set to `null` before the flow is saved.\n    This helps prevent credentials from being stored in the database.\n    \n\n **Folder**: Provides a structure for flow storage, including single-user folders and shared folders accessed by multiple users. For more information, see [Manage flows in projects](/concepts-flows#projects).\n\n **Message**: Stores chat messages and interactions that occur between components. For more information, see [Message objects](/data-types#message) and [Store chat memory](#store-chat-memory).\n\n **Transactions**: Records execution history and results of flow runs. This information is used for [logging](/logging).\n\n **User**: Stores user account information including credentials, permissions, profiles, and user management settings. For more information, see [API keys and authentication](/api-keys-and-authentication).\n\n **Variables**: Stores global encrypted values and credentials. For more information, see [Global variables](/configuration-global-variables) and [Component API keys](/api-keys-and-authentication#component-api-keys).\n\n **VertexBuild**: Tracks the build status of individual nodes within flows. For more information, see [Test flows in the Playground](/concepts-playground).\n\nFor more information, see the database models in the [source code](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/services/database/models).\n\n## Configure external memory {#configure-external-memory}\n\nTo replace the default Langflow SQLite database with another database, set the `LANGFLOW_DATABASE_URL` environment variable to your database URL, and then start Langflow with your `.env` file.\nFor more information and examples, see [Configure an external PostgreSQL database](/configuration-custom-database).\n\n```text\nLANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow\n```\n\nTo fine-tune your database connection pool and timeout settings, you can set the following additional environment variables:\n\n* `LANGFLOW_DATABASE_CONNECTION_RETRY`: Whether to retry lost connections to your Langflow database. If `true`, Langflow tries to connect to the database again if the connection fails. Default: `false`.\n\n* `LANGFLOW_DB_CONNECT_TIMEOUT`: The number of seconds to wait before giving up on a lock to be released or establishing a connection to the database. This may be separate from the `pool_timeout` in `LANGFLOW_DB_CONNECTION_SETTINGS`. Default: 30.\n\n* `LANGFLOW_DB_CONNECTION_SETTINGS`: A JSON dictionary containing the following database connection pool settings:\n\n    - `pool_size`: The base number of connections to keep open in the connection pool. Default: 20.\n    - `max_overflow`: Maximum number of connections that can be created in excess of `pool_size` if needed. Default: 30.\n    - `pool_timeout`: Number of seconds to wait for a connection from the pool before timing out. Default: 30.\n    - `pool_pre_ping`: If `true`, the pool tests connections for liveness upon each checkout. Default: `true`.\n    - `pool_recycle`: Number of seconds after which a connection is automatically recycled. Default: 1800 (30 minutes).\n    - `echo`: If `true`, SQL queries are logged for debugging purposes. Default: `false`.\n\n    For example:\n\n    ```text\n    LANGFLOW_DB_CONNECTION_SETTINGS='{\"pool_size\": 20, \"max_overflow\": 30, \"pool_timeout\": 30, \"pool_pre_ping\": true, \"pool_recycle\": 1800, \"echo\": false}'\n    ```\n\n    Don't use the deprecated environment variables `LANGFLOW_DB_POOL_SIZE` or `LANGFLOW_DB_MAX_OVERFLOW`.\n    Instead, use `pool_size` and `max_overflow` in `LANGFLOW_DB_CONNECTION_SETTINGS`.\n\n## Configure cache memory\n\nThe default Langflow caching behavior is an asynchronous, in-memory cache:\n\n```text\nLANGFLOW_LANGCHAIN_CACHE=InMemoryCache\nLANGFLOW_CACHE_TYPE=async\n```\n\nLangflow officially supports only the default asynchronous, in-memory cache, which is suitable for most use cases.\nOther cache options, such as Redis, are experimental and can change without notice.\nIf you want to use a non-default cache setting, you can use the following environment variables:\n\n| Variable | Type | Default | Description |\n|----------|------|---------|-------------|\n| `LANGFLOW_CACHE_TYPE` | String | `async` | Set the cache type for Langflow's internal caching system. Possible values: `async`, `redis`, `memory`, `disk`. If you set the type to `redis`, then you must also set the `LANGFLOW_REDIS_*` environment variables. |\n| `LANGFLOW_LANGCHAIN_CACHE` | String | `InMemoryCache` | Set the cache storage type for the LangChain caching system (a Langflow dependency), either `InMemoryCache` or `SQLiteCache`. |\n| `LANGFLOW_REDIS_HOST` | String | `localhost` | Redis server hostname if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_PORT` | Integer | `6379` | Redis server port if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_DB` | Integer | `0` | Redis database number if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_CACHE_EXPIRE` | Integer | `3600` | Cache expiration time in seconds if `LANGFLOW_CACHE_TYPE=redis`. |\n| `LANGFLOW_REDIS_PASSWORD` | String | Not set | Optional password for Redis authentication if `LANGFLOW_CACHE_TYPE=redis`. |\n\n## Store chat memory\n\nChat-based flows with a **Chat Input** or **Chat Output** component produce chat history that is stored in the Langflow `messages` table.\nAt minimum, this serves as a chat log, but it isn't functionally the same as chat memory that provides historical context to an LLM.\n\nTo store and retrieve chat memories in flows, you can use a **Message History** component or the **Agent** component's built-in chat memory.\n\n<details>\n<summary>How does chat memory work?</summary>\n\nChat memory is a cache for an LLM or agent to preserve past conversations to retain and reference that context in future interactions.\nFor example, if a user has already told the LLM their name, the LLM can retrieve that information from chat memory rather than asking the user to repeat themselves in future conversations or messages.\n\nChat memory is distinct from vector store memory because it is built specifically for storing and retrieving chat messages from databases.\n\nComponents that support chat memory (such as the **Agent** and **Message History** components) provide access to their respective databases _as memory_.\nRetrieval as memory is an important distinction for LLMs and agents because this storage and retrieval mechanism is specifically designed to recall context from past conversations.\nUnlike vector stores, which are designed for semantic search and retrieval of text chunks, chat memory is designed to store and retrieve chat messages in a way that is optimized for conversation history.\n\n</details>\n\n### Session ID and chat memory\n\nChat history and memories are grouped by [session ID (`session_id`)](/session-id).\n\nThe default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one large chat session.\n\nFor better segregation of chat memory, especially in flows used by multiple users, consider using custom session IDs.\nFor example, if you use user IDs as session IDs, then each user's chat history is stored separately, isolating the context of their chats from other users' chats.\n\n### Chat memory options\n\nWhere and how chat memory is stored depends on the components used in your flow:\n\n* **Agent component**: This component has built-in chat memory that is enabled by default.\nThis memory allows the agent to retrieve and reference messages from previous conversations associated with the same session ID.\nAll messages are stored in [Langflow storage](#storage-options-and-paths), and the component provides minimal memory configuration options, such as the number of messages to retrieve.\n\n    The **Agent** component's built-in chat memory is sufficient for most use cases.\n\n    If you want to use external chat memory storage, retrieve memories outside the context of a chat, or use chat memory with a language model component (not an agent), you must use the **Message History** component (with or without a third-party chat memory component).\n\n* **Message History component**: By default, this component stores and retrieves memories from Langflow storage, unless you attach a third-party chat memory component. It provides a few more options for sorting and filtering memories, although most of these options are also built-in to the **Agent** component as configurable or fixed parameters.\n\n    You can use the **Message History** component with or without a language model or agent.\n    For example, if you need to retrieve data from memories outside of chat, you can use the **Message History** component to fetch that data directly from your chat memory database without feeding it into a chat.\n\n* **Third-party chat memory components**: Use one of these components only if you need to store or retrieve chat memories from a dedicated external chat memory database.\nTypically, this is necessary only if you have specific storage needs that aren't met by Langflow storage.\nFor example, if you want to manage chat memory data by directly working with the database, or if you want to use a different database than the default Langflow storage.\n\nFor more information and examples, see [**Message History** component](/components-helpers#message-history) and [Agent memory](/agents#agent-memory).\n\n## See also\n\n* [Langflow file management](/concepts-file-management)\n* [Langflow logs](/logging)\n* [Langflow environment variables](/environment-variables)",
      "summary": "Langflow provides flexible memory management options for storage and retrieval of data relevant to your flows and your Langflow server.\nThis includes essential Langflow database tables, file management, and caching, as well as chat memory.",
      "keywords": [
        "agent",
        "apikey",
        "cache",
        "chat",
        "clone",
        "component",
        "components",
        "config",
        "configure",
        "database",
        "desktop",
        "directory",
        "external",
        "file",
        "flow",
        "folder",
        "git",
        "history",
        "input",
        "install"
      ]
    },
    "/session-id": {
      "title": "Use session ID to manage communication between components",
      "slug": "/session-id",
      "category": "develop",
      "content": "Session ID is a unique identifier for client/server connections. A single session equals the duration of a client's connection to a server.\n\nIn the Langflow **Playground**, current sessions are listed on the left side of the pane.\n\nLangflow uses session IDs to track different chat interactions within flows. This allows multiple chat sessions to exist in a single flow. Messages are stored in the database with session IDs as a reference.\n\nThis differentiation between users per session is helpful in managing client/server connections, but is also important in maintaining separate conversational contexts within a single flow. LLMs rely on past interactions to generate responses to queries, and if these conversations aren't separated, the responses becomes less useful, or even confused.\n\n## Customize session ID\n\nCustom session IDs can be set as part of the payload in API calls, or as advanced settings in individual components. The API session ID value takes precedence. If no session ID is specified, the flow ID is assigned.\n\nIf you set a custom session ID in a payload, all downstream components use the upstream component's session ID value.\nReplace `LANGFLOW_SERVER_ADDRESS`, `FLOW_ID`, and `LANGFLOW_API_KEY` with the values from your Langflow deployment.\n```\ncurl --request POST \\\n  --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Hello\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"session_id\": \"my_custom_session_value\"\n}'\n```\n\nThe `my_custom_session_value` value is used in components that accept it, and the stored messages from this flow are stored in `langflow.db` with their respective `session_id` values.\n\n## Retrieval of messages from memory by session ID\n\nTo retrieve messages from local Langflow memory, add a [**Message History** component](/components-helpers#message-history) to your flow.\nThe component accepts `sessionID` as a filter parameter, and uses the session ID value from upstream automatically to retrieve message history by session ID from storage.\n\nMessages can be retrieved by `session_id` from the Langflow API at `GET /v1/monitor/messages`. For more information, see [Monitor endpoints](https://docs.langflow.org/api-monitor).\n\nFor an example of session ID in action, see [Use Session IDs in Langflow](https://www.youtube.com/watch?v=nJiF_eF21MY).",
      "summary": "Session ID is a unique identifier for client/server connections. A single session equals the duration of a client's connection to a server.",
      "keywords": [
        "between",
        "communication",
        "components",
        "customize",
        "history",
        "manage",
        "memory",
        "message",
        "messages",
        "playground",
        "retrieval",
        "session"
      ]
    },
    "session-id": {
      "title": "Use session ID to manage communication between components",
      "slug": "/session-id",
      "category": "develop",
      "content": "Session ID is a unique identifier for client/server connections. A single session equals the duration of a client's connection to a server.\n\nIn the Langflow **Playground**, current sessions are listed on the left side of the pane.\n\nLangflow uses session IDs to track different chat interactions within flows. This allows multiple chat sessions to exist in a single flow. Messages are stored in the database with session IDs as a reference.\n\nThis differentiation between users per session is helpful in managing client/server connections, but is also important in maintaining separate conversational contexts within a single flow. LLMs rely on past interactions to generate responses to queries, and if these conversations aren't separated, the responses becomes less useful, or even confused.\n\n## Customize session ID\n\nCustom session IDs can be set as part of the payload in API calls, or as advanced settings in individual components. The API session ID value takes precedence. If no session ID is specified, the flow ID is assigned.\n\nIf you set a custom session ID in a payload, all downstream components use the upstream component's session ID value.\nReplace `LANGFLOW_SERVER_ADDRESS`, `FLOW_ID`, and `LANGFLOW_API_KEY` with the values from your Langflow deployment.\n```\ncurl --request POST \\\n  --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Hello\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"session_id\": \"my_custom_session_value\"\n}'\n```\n\nThe `my_custom_session_value` value is used in components that accept it, and the stored messages from this flow are stored in `langflow.db` with their respective `session_id` values.\n\n## Retrieval of messages from memory by session ID\n\nTo retrieve messages from local Langflow memory, add a [**Message History** component](/components-helpers#message-history) to your flow.\nThe component accepts `sessionID` as a filter parameter, and uses the session ID value from upstream automatically to retrieve message history by session ID from storage.\n\nMessages can be retrieved by `session_id` from the Langflow API at `GET /v1/monitor/messages`. For more information, see [Monitor endpoints](https://docs.langflow.org/api-monitor).\n\nFor an example of session ID in action, see [Use Session IDs in Langflow](https://www.youtube.com/watch?v=nJiF_eF21MY).",
      "summary": "Session ID is a unique identifier for client/server connections. A single session equals the duration of a client's connection to a server.",
      "keywords": [
        "between",
        "communication",
        "components",
        "customize",
        "history",
        "manage",
        "memory",
        "message",
        "messages",
        "playground",
        "retrieval",
        "session"
      ]
    },
    "/concepts-flows-import": {
      "title": "Import and export flows",
      "slug": "/concepts-flows-import",
      "category": "flows",
      "content": "You can export flows to transfer them between Langflow instances, share them with others, or create backups.\n\n## Export a flow\n\nThere are three ways to export flows:\n\n* **Export from projects**: On the [**Projects** page](/concepts-flows#projects), find the flow you want to export, click  **More**, and then select **Export**. To export all flows in a project, click  **Options** on the **Projects** list, and then select **Download**.\n\n* **Export by sharing**: When editing a flow, click **Share**, and then click **Export**.\n\n* **Export with the Langflow API**: To export one flow, use the [`/flows/download`](/api-flows#export-flows) endpoint.\nTo export all flows in a project, use the [`/projects/download`](/api-projects#export-a-project) endpoint.\n\nExported flows are downloaded to your local machine as JSON files named `FLOW_NAME.json`.\nIf you export an entire project, the JSON files are packaged in a zip archive.\nFor more information, see [Langflow JSON file contents](#langflow-json-file-contents).\n\n### Save with my API keys\n\nWhen exporting from the **Projects** page or **Share** menu, you can select **Save with my API keys** to export the flow _and_ any defined API key variables.\nNon-API key variables are included in the export regardless of the **Save with my API keys** setting.\n\n**Note:** If you enter the literal key into a component's API key field, then **Save with my API keys** exports the literal key value.\n\nIf your key is stored in a Langflow global variable, **Save with my API keys** exports only the variable name.\n\nWhen you or another user import the flow to another Langflow instance, that instance must have Langflow global variables with the same names and valid values in order to run the flow successfully.\nIf any variables are missing or invalid, those variables must be created or edited after importing the flow.\n\n## Import a flow\n\nYou can import Langflow JSON files from your local machine in the following ways:\n\n* **Import to projects**: On the **Projects** page, click  **Upload a flow**, and then select the Langflow JSON file to import.\n* **Import anywhere**: Drag and drop Langflow JSON files from your file explorer into your Langflow window to import a flow from any Langflow page.\n* **Import with the Langflow API**: To import one Langflow JSON file, use the [`/flows/upload/`](/api-flows#import-flows) endpoint.\nTo import a zip archive of Langflow JSON files, use the [`/projects/upload`](/api-projects#import-a-project) endpoint.\n\n### Run an imported flow\n\nOnce imported, your flow is ready to use.\nIf the flow contains any global variables, make sure your Langflow instance has global variables with the same names and valid values.\nFor more information, see [Save with my API keys](/concepts-flows-import#save-with-my-api-keys).\n\n## Langflow JSON file contents\n\nAn exported flow is downloaded to your local machine as a JSON file named `FLOW_NAME.json`.\n\nLangflow JSON files contain the following:\n\n* [Nodes](#nodes) and [edges](#edges) that describe components and connections in the flow.\n* [Additional metadata](#additional-metadata-and-project-information) that describes the flow and the project it belongs to.\n\nFor example Langflow JSON files, you can examine any of the https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects[templates in the Langflow repository], or you can create a flow from a template in Langflow, export it, and then open the exported JSON file in a text editor.\n\n### Nodes\n\nNodes represent the components that make up the flow.\nFor example, this object represents a **Chat Input** component:\n\n```json\n{\n  \"data\": {\n    \"description\": \"Get chat inputs from the Playground.\",\n    \"display_name\": \"Chat Input\",\n    \"id\": \"ChatInput-jFwUm\",\n    \"node\": {\n      \"base_classes\": [\"Message\"],\n      \"description\": \"Get chat inputs from the Playground.\",\n      \"display_name\": \"Chat Input\",\n      \"icon\": \"MessagesSquare\",\n      \"template\": {\n        \"input_value\": {\n          \"display_name\": \"Text\",\n          \"info\": \"Message to be passed as input.\",\n          \"value\": \"Hello\"\n        },\n        \"sender\": {\n          \"value\": \"User\",\n          \"options\": [\"Machine\", \"User\"]\n        },\n        \"sender_name\": {\n          \"value\": \"User\"\n        },\n        \"should_store_message\": {\n          \"value\": true\n        }\n      }\n    },\n    \"type\": \"ChatInput\"\n  },\n  \"position\": {\n    \"x\": 689.5720422421635,\n    \"y\": 765.155834131403\n  }\n}\n```\n\nEach node has a unique identifier in the format of `NODE_NAME-UUID`, such as `ChatInput-jFwUm`.\n\nEntrypoint nodes, such as the `ChatInput` node, are the first node executed when running a flow.\n\n### Edges\n\nEdges represent the connections between nodes.\n\nThe following example represents the edge (or connection) between the `ChatInput` node and the `OpenAIModel` node:\n\n```json\n{\n  \"className\": \"\",\n  \"data\": {\n    \"sourceHandle\": {\n      \"dataType\": \"ChatInput\",\n      \"id\": \"ChatInput-jFwUm\",\n      \"name\": \"message\",\n      \"output_types\": [\"Message\"]\n    },\n    \"targetHandle\": {\n      \"fieldName\": \"input_value\",\n      \"id\": \"OpenAIModel-OcXkl\",\n      \"inputTypes\": [\"Message\"],\n      \"type\": \"str\"\n    }\n  },\n  \"id\": \"reactflow__edge-ChatInput-jFwUm{dataType:ChatInput,id:ChatInput-jFwUm,name:message,output_types:[Message]}-OpenAIModel-OcXkl{fieldName:input_value,id:OpenAIModel-OcXkl,inputTypes:[Message],type:str}\",\n  \"source\": \"ChatInput-jFwUm\",\n  \"sourceHandle\": \"{dataType: ChatInput, id: ChatInput-jFwUm, name: message, output_types: [Message]}\",\n  \"target\": \"OpenAIModel-OcXkl\",\n  \"targetHandle\": \"{fieldName: input_value, id: OpenAIModel-OcXkl, inputTypes: [Message], type: str}\"\n}\n```\n\nThis edge shows that the `ChatInput` component outputs a `Message` type to the `target` node, which is the `OpenAIModel` node.\nThe `OpenAIModel` component accepts the `Message` type at the `input_value` field.\n\n### Additional metadata and project information\n\nAdditional information about the flow is stored in the root `data` object:\n\n* Metadata and project information including the name, description, and `last_tested_version` of the flow.\nFor example:\n\n    ```json\n    {\n      \"name\": \"Basic Prompting\",\n      \"description\": \"Perform basic prompting with an OpenAI model.\",\n      \"tags\": [\"chatbots\"],\n      \"id\": \"1511c230-d446-43a7-bfc3-539e69ce05b8\",\n      \"last_tested_version\": \"1.0.19.post2\",\n      \"gradient\": \"2\",\n      \"icon\": \"Braces\"\n    }\n    ```\n\n* Visual information about the flow defining the position of the viewport when you open the flow in the workspace:\n\n    ```json\n    \"viewport\": {\n      \"x\": -37.61270157375441,\n      \"y\": -155.91266341888854,\n      \"zoom\": 0.7575251406952855\n    }\n    ```\n\n*  **Notes** are comments that help explain the flow's purpose, configuration details, and any other information relevant to users who might be editing the flow.\nThey can contain text, links, code snippets, and other information.\nThey are encoded in Markdown format and stored as `node` objects.\n\n    ```json\n    {\n      \"id\": \"undefined-kVLkG\",\n      \"node\": {\n        \"description\": \"##  README\\nPerform basic prompting with an OpenAI model.\\n\\n#### Quick Start\\n- Add your **OpenAI API key** to the **OpenAI Model**\\n- Open the **Playground** to chat with your bot.\\n...\"\n      }\n    }\n    ```\n\n## See also\n\n* [Build flows](/concepts-flows)\n* [Share and embed flows](/concepts-publish)",
      "summary": "You can export flows to transfer them between Langflow instances, share them with others, or create backups.",
      "keywords": [
        "additional",
        "anywhere",
        "api",
        "chat",
        "contents",
        "download",
        "edges",
        "export",
        "file",
        "flow",
        "flows",
        "import",
        "imported",
        "input",
        "json",
        "key",
        "keys",
        "langflow",
        "metadata",
        "model"
      ]
    },
    "concepts-flows-import": {
      "title": "Import and export flows",
      "slug": "/concepts-flows-import",
      "category": "flows",
      "content": "You can export flows to transfer them between Langflow instances, share them with others, or create backups.\n\n## Export a flow\n\nThere are three ways to export flows:\n\n* **Export from projects**: On the [**Projects** page](/concepts-flows#projects), find the flow you want to export, click  **More**, and then select **Export**. To export all flows in a project, click  **Options** on the **Projects** list, and then select **Download**.\n\n* **Export by sharing**: When editing a flow, click **Share**, and then click **Export**.\n\n* **Export with the Langflow API**: To export one flow, use the [`/flows/download`](/api-flows#export-flows) endpoint.\nTo export all flows in a project, use the [`/projects/download`](/api-projects#export-a-project) endpoint.\n\nExported flows are downloaded to your local machine as JSON files named `FLOW_NAME.json`.\nIf you export an entire project, the JSON files are packaged in a zip archive.\nFor more information, see [Langflow JSON file contents](#langflow-json-file-contents).\n\n### Save with my API keys\n\nWhen exporting from the **Projects** page or **Share** menu, you can select **Save with my API keys** to export the flow _and_ any defined API key variables.\nNon-API key variables are included in the export regardless of the **Save with my API keys** setting.\n\n**Note:** If you enter the literal key into a component's API key field, then **Save with my API keys** exports the literal key value.\n\nIf your key is stored in a Langflow global variable, **Save with my API keys** exports only the variable name.\n\nWhen you or another user import the flow to another Langflow instance, that instance must have Langflow global variables with the same names and valid values in order to run the flow successfully.\nIf any variables are missing or invalid, those variables must be created or edited after importing the flow.\n\n## Import a flow\n\nYou can import Langflow JSON files from your local machine in the following ways:\n\n* **Import to projects**: On the **Projects** page, click  **Upload a flow**, and then select the Langflow JSON file to import.\n* **Import anywhere**: Drag and drop Langflow JSON files from your file explorer into your Langflow window to import a flow from any Langflow page.\n* **Import with the Langflow API**: To import one Langflow JSON file, use the [`/flows/upload/`](/api-flows#import-flows) endpoint.\nTo import a zip archive of Langflow JSON files, use the [`/projects/upload`](/api-projects#import-a-project) endpoint.\n\n### Run an imported flow\n\nOnce imported, your flow is ready to use.\nIf the flow contains any global variables, make sure your Langflow instance has global variables with the same names and valid values.\nFor more information, see [Save with my API keys](/concepts-flows-import#save-with-my-api-keys).\n\n## Langflow JSON file contents\n\nAn exported flow is downloaded to your local machine as a JSON file named `FLOW_NAME.json`.\n\nLangflow JSON files contain the following:\n\n* [Nodes](#nodes) and [edges](#edges) that describe components and connections in the flow.\n* [Additional metadata](#additional-metadata-and-project-information) that describes the flow and the project it belongs to.\n\nFor example Langflow JSON files, you can examine any of the https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects[templates in the Langflow repository], or you can create a flow from a template in Langflow, export it, and then open the exported JSON file in a text editor.\n\n### Nodes\n\nNodes represent the components that make up the flow.\nFor example, this object represents a **Chat Input** component:\n\n```json\n{\n  \"data\": {\n    \"description\": \"Get chat inputs from the Playground.\",\n    \"display_name\": \"Chat Input\",\n    \"id\": \"ChatInput-jFwUm\",\n    \"node\": {\n      \"base_classes\": [\"Message\"],\n      \"description\": \"Get chat inputs from the Playground.\",\n      \"display_name\": \"Chat Input\",\n      \"icon\": \"MessagesSquare\",\n      \"template\": {\n        \"input_value\": {\n          \"display_name\": \"Text\",\n          \"info\": \"Message to be passed as input.\",\n          \"value\": \"Hello\"\n        },\n        \"sender\": {\n          \"value\": \"User\",\n          \"options\": [\"Machine\", \"User\"]\n        },\n        \"sender_name\": {\n          \"value\": \"User\"\n        },\n        \"should_store_message\": {\n          \"value\": true\n        }\n      }\n    },\n    \"type\": \"ChatInput\"\n  },\n  \"position\": {\n    \"x\": 689.5720422421635,\n    \"y\": 765.155834131403\n  }\n}\n```\n\nEach node has a unique identifier in the format of `NODE_NAME-UUID`, such as `ChatInput-jFwUm`.\n\nEntrypoint nodes, such as the `ChatInput` node, are the first node executed when running a flow.\n\n### Edges\n\nEdges represent the connections between nodes.\n\nThe following example represents the edge (or connection) between the `ChatInput` node and the `OpenAIModel` node:\n\n```json\n{\n  \"className\": \"\",\n  \"data\": {\n    \"sourceHandle\": {\n      \"dataType\": \"ChatInput\",\n      \"id\": \"ChatInput-jFwUm\",\n      \"name\": \"message\",\n      \"output_types\": [\"Message\"]\n    },\n    \"targetHandle\": {\n      \"fieldName\": \"input_value\",\n      \"id\": \"OpenAIModel-OcXkl\",\n      \"inputTypes\": [\"Message\"],\n      \"type\": \"str\"\n    }\n  },\n  \"id\": \"reactflow__edge-ChatInput-jFwUm{dataType:ChatInput,id:ChatInput-jFwUm,name:message,output_types:[Message]}-OpenAIModel-OcXkl{fieldName:input_value,id:OpenAIModel-OcXkl,inputTypes:[Message],type:str}\",\n  \"source\": \"ChatInput-jFwUm\",\n  \"sourceHandle\": \"{dataType: ChatInput, id: ChatInput-jFwUm, name: message, output_types: [Message]}\",\n  \"target\": \"OpenAIModel-OcXkl\",\n  \"targetHandle\": \"{fieldName: input_value, id: OpenAIModel-OcXkl, inputTypes: [Message], type: str}\"\n}\n```\n\nThis edge shows that the `ChatInput` component outputs a `Message` type to the `target` node, which is the `OpenAIModel` node.\nThe `OpenAIModel` component accepts the `Message` type at the `input_value` field.\n\n### Additional metadata and project information\n\nAdditional information about the flow is stored in the root `data` object:\n\n* Metadata and project information including the name, description, and `last_tested_version` of the flow.\nFor example:\n\n    ```json\n    {\n      \"name\": \"Basic Prompting\",\n      \"description\": \"Perform basic prompting with an OpenAI model.\",\n      \"tags\": [\"chatbots\"],\n      \"id\": \"1511c230-d446-43a7-bfc3-539e69ce05b8\",\n      \"last_tested_version\": \"1.0.19.post2\",\n      \"gradient\": \"2\",\n      \"icon\": \"Braces\"\n    }\n    ```\n\n* Visual information about the flow defining the position of the viewport when you open the flow in the workspace:\n\n    ```json\n    \"viewport\": {\n      \"x\": -37.61270157375441,\n      \"y\": -155.91266341888854,\n      \"zoom\": 0.7575251406952855\n    }\n    ```\n\n*  **Notes** are comments that help explain the flow's purpose, configuration details, and any other information relevant to users who might be editing the flow.\nThey can contain text, links, code snippets, and other information.\nThey are encoded in Markdown format and stored as `node` objects.\n\n    ```json\n    {\n      \"id\": \"undefined-kVLkG\",\n      \"node\": {\n        \"description\": \"##  README\\nPerform basic prompting with an OpenAI model.\\n\\n#### Quick Start\\n- Add your **OpenAI API key** to the **OpenAI Model**\\n- Open the **Playground** to chat with your bot.\\n...\"\n      }\n    }\n    ```\n\n## See also\n\n* [Build flows](/concepts-flows)\n* [Share and embed flows](/concepts-publish)",
      "summary": "You can export flows to transfer them between Langflow instances, share them with others, or create backups.",
      "keywords": [
        "additional",
        "anywhere",
        "api",
        "chat",
        "contents",
        "download",
        "edges",
        "export",
        "file",
        "flow",
        "flows",
        "import",
        "imported",
        "input",
        "json",
        "key",
        "keys",
        "langflow",
        "metadata",
        "model"
      ]
    },
    "/concepts-flows": {
      "title": "Build flows",
      "slug": "/concepts-flows",
      "category": "flows",
      "content": "A _flow_ is a functional representation of an application workflow.\nFlows receive input, process it, and produce output.\n\nFlows consist of _components_ that represent individual steps in your application's workflow.\n\n![Basic Prompting flow in the workspace](/img/workspace-basic-prompting.png)\n\nLangflow flows are fully serializable and can be saved and loaded from the file system where Langflow is installed.\n\n**Note:** To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).\n\n## Create a flow\n\nFrom the [**Projects** page](#projects), there are four ways to create a flow:\n\n* **Create a blank flow**: Select a project, click **New Flow**, and then click **Blank Flow**.\n\n* **Create a flow from a template**: Select a project, click **New Flow**, and then click the template that you want to use.\n\n    <details>\n    <summary>What are templates?</summary>\n\n    Templates are pre-built flows that you can use as a starting point for your own flow.\n    They range from basic flows with a few components to complex flows with many components and sub-flows.\n\n    For example, the **Basic Prompting** template demonstrates a small flow that passes both chat input and pre-defined instructions (as a prompt) to an LLM.\n    In contrast, the **Vector Store RAG** template consists of two sub-flows that demonstrate how to create a Retrieval Augmented Generation (RAG) chatbot. One sub-flow populates the vector store with contextually relevant data and embeddings, and the other sub-flow queries the vector store for similar data to answer user questions.\n\n    You can also [contribute templates](/contributing-templates) to the Langflow codebase.\n\n    </details>\n\n* **Duplicate an existing flow**: Locate the flow you want to copy, click  **More**, and then select **Duplicate**.\n\n* **Import a flow**: See [Import and export flows](/concepts-flows-import).\n\nYou can also create a flow with the [Langflow API](/api-flows), but the Langflow team recommends using the [visual editor](/concepts-overview) until you are familiar with flow creation.\n\n### Add components\n\nFlows consist of [components](/concepts-components), which are nodes that you configure and connect in the [workspace](/concepts-overview#workspace).\nEach component performs a specific task, like serving an AI model or connecting a data source.\n\nDrag and drop components from the  **Core components** and  **Bundles** menus to add them to your flow.\nThen, configure the component settings and connect the components together.\n\n![Chat Input and Output connected to a Language Model component](/img/connect-component.png)\n\nEach component has configuration settings and options. Some of these are common to all components, and some are unique to specific components.\n\nTo form a cohesive flow, you connect components by _edges_ or _ports_, which have a specific data type they receive or send.\nFor example, message ports send text strings between components.\n\nFor more information about component configuration, including port types and underlying component code, see [Components overview](/concepts-components).\n\n### Run a flow\n\nAfter you build a prototype flow, you can test it in the [**Playground**](/concepts-playground).\nWhen you're ready to use Langflow for application development, learn how to [trigger flows with the Langflow API](/concepts-publish), explore more advanced configuration options like [custom dependencies](/install-custom-dependencies), and, eventually, [containerize your Langflow application](/develop-application).\n\nWhen you're ready to go to production or deploy a Langflow MCP server for access over the public internet, see [Langflow deployment overview](/deployment-overview).\n\n#### Flow graphs\n\nWhen a flow runs, Langflow builds a Directed Acyclic Graph (DAG) object from the nodes (components) and edges (connections), and the nodes are sorted to determine the order of execution.\n\nThe graph build calls each component's `def_build` function to validate and prepare the nodes.\nThis graph is then processed in dependency order.\nEach node is built and executed sequentially, with results from each built node being passed to nodes that are dependent on that node's results.\n\n## Manage flows in projects {#projects}\n\nThe **Projects** page is where you arrive when you launch Langflow.\nFrom here, you can manage flows and your projects' [MCP servers](/mcp-server).\n\nLangflow projects are like folders that you can use to organize related flows.\nThe default project is **Starter Project**, and your flows are stored here unless you create another project.\nTo create a project, click  **Create new project**.\n\n![Projects page with multiple flows in a project](/img/my-projects.png)\n\n**Note:** To get back to the **Projects** page after editing a flow, click the project name or Langflow icon in the Langflow header.\n\n### Edit flow details\n\n1. On the **Projects** page, locate the flow you want to edit.\n2. Click  **More**, and then select **Edit details**.\n3. Edit the **Name** and **Description**, and then click **Save**.\n\n### Lock a flow\n\nTo prevent changes to a flow, you can lock it:\n\n1. On the **Projects** page, locate the flow you want to lock.\n2. Click  **More**, and then select **Edit details**.\n3. Enable **Lock Flow**, and then click **Save**.\n\nRepeat these steps to unlock the flow by disabling **Lock Flow**.\n\nWhen editing a flow, the **Lock Status** indicates whether the flow is  **Locked** or  **Unlocked**.\nYou cannot change the lock status while editing the flow.\n\n### Move a flow\n\nTo move a flow from one project to another, do the following:\n\n1. On the **Projects** page, locate the flow you want to move.\n2. Click and drag the flow from the list of flows to the target project name in the list of projects.\n\n### Delete a flow\n\n1. On the **Projects** page, locate the flow you want to delete.\n2. Click  **More**, and then select **Delete**.\n\n## Flow storage and logs\n\nBy default, flows and flow execution data are stored in the Langflow database, and flow logs are stored with other Langflow logs in the Langflow config directory.\nFor more information, see [Memory management options](/memory) and [Logging](/logging).\n\n## See also\n\n* [Share and embed flows](/concepts-publish)\n* [Import and export flows](/concepts-flows-import)\n* [Langflow environment variables](/environment-variables)",
      "summary": "A _flow_ is a functional representation of an application workflow.\nFlows receive input, process it, and produce output.",
      "keywords": [
        "add",
        "basic",
        "blank",
        "build",
        "bundles",
        "components",
        "core",
        "create",
        "delete",
        "description",
        "details",
        "duplicate",
        "edit",
        "existing",
        "flow",
        "flows",
        "graphs",
        "import",
        "lock",
        "locked"
      ]
    },
    "concepts-flows": {
      "title": "Build flows",
      "slug": "/concepts-flows",
      "category": "flows",
      "content": "A _flow_ is a functional representation of an application workflow.\nFlows receive input, process it, and produce output.\n\nFlows consist of _components_ that represent individual steps in your application's workflow.\n\n![Basic Prompting flow in the workspace](/img/workspace-basic-prompting.png)\n\nLangflow flows are fully serializable and can be saved and loaded from the file system where Langflow is installed.\n\n**Note:** To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).\n\n## Create a flow\n\nFrom the [**Projects** page](#projects), there are four ways to create a flow:\n\n* **Create a blank flow**: Select a project, click **New Flow**, and then click **Blank Flow**.\n\n* **Create a flow from a template**: Select a project, click **New Flow**, and then click the template that you want to use.\n\n    <details>\n    <summary>What are templates?</summary>\n\n    Templates are pre-built flows that you can use as a starting point for your own flow.\n    They range from basic flows with a few components to complex flows with many components and sub-flows.\n\n    For example, the **Basic Prompting** template demonstrates a small flow that passes both chat input and pre-defined instructions (as a prompt) to an LLM.\n    In contrast, the **Vector Store RAG** template consists of two sub-flows that demonstrate how to create a Retrieval Augmented Generation (RAG) chatbot. One sub-flow populates the vector store with contextually relevant data and embeddings, and the other sub-flow queries the vector store for similar data to answer user questions.\n\n    You can also [contribute templates](/contributing-templates) to the Langflow codebase.\n\n    </details>\n\n* **Duplicate an existing flow**: Locate the flow you want to copy, click  **More**, and then select **Duplicate**.\n\n* **Import a flow**: See [Import and export flows](/concepts-flows-import).\n\nYou can also create a flow with the [Langflow API](/api-flows), but the Langflow team recommends using the [visual editor](/concepts-overview) until you are familiar with flow creation.\n\n### Add components\n\nFlows consist of [components](/concepts-components), which are nodes that you configure and connect in the [workspace](/concepts-overview#workspace).\nEach component performs a specific task, like serving an AI model or connecting a data source.\n\nDrag and drop components from the  **Core components** and  **Bundles** menus to add them to your flow.\nThen, configure the component settings and connect the components together.\n\n![Chat Input and Output connected to a Language Model component](/img/connect-component.png)\n\nEach component has configuration settings and options. Some of these are common to all components, and some are unique to specific components.\n\nTo form a cohesive flow, you connect components by _edges_ or _ports_, which have a specific data type they receive or send.\nFor example, message ports send text strings between components.\n\nFor more information about component configuration, including port types and underlying component code, see [Components overview](/concepts-components).\n\n### Run a flow\n\nAfter you build a prototype flow, you can test it in the [**Playground**](/concepts-playground).\nWhen you're ready to use Langflow for application development, learn how to [trigger flows with the Langflow API](/concepts-publish), explore more advanced configuration options like [custom dependencies](/install-custom-dependencies), and, eventually, [containerize your Langflow application](/develop-application).\n\nWhen you're ready to go to production or deploy a Langflow MCP server for access over the public internet, see [Langflow deployment overview](/deployment-overview).\n\n#### Flow graphs\n\nWhen a flow runs, Langflow builds a Directed Acyclic Graph (DAG) object from the nodes (components) and edges (connections), and the nodes are sorted to determine the order of execution.\n\nThe graph build calls each component's `def_build` function to validate and prepare the nodes.\nThis graph is then processed in dependency order.\nEach node is built and executed sequentially, with results from each built node being passed to nodes that are dependent on that node's results.\n\n## Manage flows in projects {#projects}\n\nThe **Projects** page is where you arrive when you launch Langflow.\nFrom here, you can manage flows and your projects' [MCP servers](/mcp-server).\n\nLangflow projects are like folders that you can use to organize related flows.\nThe default project is **Starter Project**, and your flows are stored here unless you create another project.\nTo create a project, click  **Create new project**.\n\n![Projects page with multiple flows in a project](/img/my-projects.png)\n\n**Note:** To get back to the **Projects** page after editing a flow, click the project name or Langflow icon in the Langflow header.\n\n### Edit flow details\n\n1. On the **Projects** page, locate the flow you want to edit.\n2. Click  **More**, and then select **Edit details**.\n3. Edit the **Name** and **Description**, and then click **Save**.\n\n### Lock a flow\n\nTo prevent changes to a flow, you can lock it:\n\n1. On the **Projects** page, locate the flow you want to lock.\n2. Click  **More**, and then select **Edit details**.\n3. Enable **Lock Flow**, and then click **Save**.\n\nRepeat these steps to unlock the flow by disabling **Lock Flow**.\n\nWhen editing a flow, the **Lock Status** indicates whether the flow is  **Locked** or  **Unlocked**.\nYou cannot change the lock status while editing the flow.\n\n### Move a flow\n\nTo move a flow from one project to another, do the following:\n\n1. On the **Projects** page, locate the flow you want to move.\n2. Click and drag the flow from the list of flows to the target project name in the list of projects.\n\n### Delete a flow\n\n1. On the **Projects** page, locate the flow you want to delete.\n2. Click  **More**, and then select **Delete**.\n\n## Flow storage and logs\n\nBy default, flows and flow execution data are stored in the Langflow database, and flow logs are stored with other Langflow logs in the Langflow config directory.\nFor more information, see [Memory management options](/memory) and [Logging](/logging).\n\n## See also\n\n* [Share and embed flows](/concepts-publish)\n* [Import and export flows](/concepts-flows-import)\n* [Langflow environment variables](/environment-variables)",
      "summary": "A _flow_ is a functional representation of an application workflow.\nFlows receive input, process it, and produce output.",
      "keywords": [
        "add",
        "basic",
        "blank",
        "build",
        "bundles",
        "components",
        "core",
        "create",
        "delete",
        "description",
        "details",
        "duplicate",
        "edit",
        "existing",
        "flow",
        "flows",
        "graphs",
        "import",
        "lock",
        "locked"
      ]
    },
    "/concepts-overview": {
      "title": "Use the visual editor",
      "slug": "/concepts-overview",
      "category": "flows",
      "content": "You use Langflow's visual editor to create, test, and share flows, which are functional representations of application workflows.\nFlows consist of components that represent individual steps in your application's workflow.\n\nLangflow's drag-and-drop interface allows you to create complex AI workflows without writing extensive code.\nYou can connect different resources, including prompts, large language models (LLMs), data sources, agents, MCP servers, and other tools and integrations.\n\n**Note:** To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).\n\n## Workspace\n\nWhen building a [flow](/concepts-flows), you primarily interact with the workspace.\nThis is where you add [components](/concepts-components), configure them, and attach them together.\n\n![Empty Langflow workspace](/img/workspace.png)\n\nFrom the workspace, you can also access the [**Playground**](#playground), [**Share** menu](#share-menu), and [**Logs**](/logging).\n\n### Workspace gestures and interactions\n\nUse these shortcuts, gestures, and functionality to navigate the workspace:\n\n- **Pan horizontally and vertically**: Click and drag an empty area of the workspace.\n\n- **Rearrange components**: Click and drag the components anywhere on the workspace.\n\n    To change the programmatic relationship between components, you must manipulate the component _edges_ or _ports_. For more information, see [Components overview](/concepts-components).\n\n    To enable guide lines, click  **Help**, and then toggle **Enable smart guides**.\n\n    If you can't edit any components, make sure the flow is [unlocked](/concepts-flows#lock-a-flow).\n\n- **Zoom**: Scroll on the mouse or trackpad, or click  **Canvas controls** next to the zoom percentage more zoom options: **Zoom In**, **Zoom Out**, **Zoom To 100%**, and **Zoom To Fit**.\n\n- **Add notes and comments**: Click  **Add Note**.\n\n- **Keyboard shortcuts**: To view available shortcuts, click  **Help**, and then select **Shortcuts**.\n\n## Playground\n\nIf your flow has a **Chat Input** component, you can use the **Playground** to run your flow, chat with your flow, view inputs and outputs, and modify the LLM's memories to tune the flow's responses in real time.\n\nTo try this for yourself, create a flow based on the **Basic Prompting** template, and then click  **Playground** when editing the flow in the workspace.\n\n![Playground](/img/playground.png)\n\nIf you have an **Agent** component in your flow, the **Playground** displays its tool calls and outputs so you can monitor the agent's tool use and understand the reasoning behind its responses.\nTo try an agent flow in the **Playground**, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).\n\n![Playground with agent response](/img/playground-with-agent.png)\n\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n## Share {#share-menu}\n\nThe **Share** menu provides the following options for integrating your flow into external applications:\n\n* [**API access**](/concepts-publish#api-access): Integrate your flow into your applications with automatically-generated Python, JavaScript, and curl code snippets.\n* [**Export**](/concepts-flows-import#export-a-flow): Export your flow to your local machine as a JSON file.\n* [**MCP Server**](/mcp-server): Expose your flow as a tool for MCP-compatible clients.\n* [**Embed into site**](/concepts-publish#embedded-chat-widget): Embed your flow in HTML, React, or Angular applications.\n* [**Shareable Playground**](/concepts-playground#share-a-flows-playground): Share your **Playground** interface with another user.\n\n    This is specifically for sharing the **Playground** experience; it isn't for running a flow in a production application.\n\n    The **Sharable Playground** isn't available for Langflow Desktop.\n\n## See also\n\n* [Manage files in Langflow](/concepts-file-management)\n* [Global variables](/configuration-global-variables)\n* [API keys and authentication](/api-keys-and-authentication)",
      "summary": "You use Langflow's visual editor to create, test, and share flows, which are functional representations of application workflows.\nFlows consist of components that represent individual steps in your application's workflow.",
      "keywords": [
        "access",
        "add",
        "agent",
        "api",
        "basic",
        "canvas",
        "chat",
        "comments",
        "components",
        "controls",
        "editor",
        "embed",
        "enable",
        "export",
        "fit",
        "gestures",
        "guides",
        "help",
        "horizontally",
        "input"
      ]
    },
    "concepts-overview": {
      "title": "Use the visual editor",
      "slug": "/concepts-overview",
      "category": "flows",
      "content": "You use Langflow's visual editor to create, test, and share flows, which are functional representations of application workflows.\nFlows consist of components that represent individual steps in your application's workflow.\n\nLangflow's drag-and-drop interface allows you to create complex AI workflows without writing extensive code.\nYou can connect different resources, including prompts, large language models (LLMs), data sources, agents, MCP servers, and other tools and integrations.\n\n**Note:** To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).\n\n## Workspace\n\nWhen building a [flow](/concepts-flows), you primarily interact with the workspace.\nThis is where you add [components](/concepts-components), configure them, and attach them together.\n\n![Empty Langflow workspace](/img/workspace.png)\n\nFrom the workspace, you can also access the [**Playground**](#playground), [**Share** menu](#share-menu), and [**Logs**](/logging).\n\n### Workspace gestures and interactions\n\nUse these shortcuts, gestures, and functionality to navigate the workspace:\n\n- **Pan horizontally and vertically**: Click and drag an empty area of the workspace.\n\n- **Rearrange components**: Click and drag the components anywhere on the workspace.\n\n    To change the programmatic relationship between components, you must manipulate the component _edges_ or _ports_. For more information, see [Components overview](/concepts-components).\n\n    To enable guide lines, click  **Help**, and then toggle **Enable smart guides**.\n\n    If you can't edit any components, make sure the flow is [unlocked](/concepts-flows#lock-a-flow).\n\n- **Zoom**: Scroll on the mouse or trackpad, or click  **Canvas controls** next to the zoom percentage more zoom options: **Zoom In**, **Zoom Out**, **Zoom To 100%**, and **Zoom To Fit**.\n\n- **Add notes and comments**: Click  **Add Note**.\n\n- **Keyboard shortcuts**: To view available shortcuts, click  **Help**, and then select **Shortcuts**.\n\n## Playground\n\nIf your flow has a **Chat Input** component, you can use the **Playground** to run your flow, chat with your flow, view inputs and outputs, and modify the LLM's memories to tune the flow's responses in real time.\n\nTo try this for yourself, create a flow based on the **Basic Prompting** template, and then click  **Playground** when editing the flow in the workspace.\n\n![Playground](/img/playground.png)\n\nIf you have an **Agent** component in your flow, the **Playground** displays its tool calls and outputs so you can monitor the agent's tool use and understand the reasoning behind its responses.\nTo try an agent flow in the **Playground**, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).\n\n![Playground with agent response](/img/playground-with-agent.png)\n\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n## Share {#share-menu}\n\nThe **Share** menu provides the following options for integrating your flow into external applications:\n\n* [**API access**](/concepts-publish#api-access): Integrate your flow into your applications with automatically-generated Python, JavaScript, and curl code snippets.\n* [**Export**](/concepts-flows-import#export-a-flow): Export your flow to your local machine as a JSON file.\n* [**MCP Server**](/mcp-server): Expose your flow as a tool for MCP-compatible clients.\n* [**Embed into site**](/concepts-publish#embedded-chat-widget): Embed your flow in HTML, React, or Angular applications.\n* [**Shareable Playground**](/concepts-playground#share-a-flows-playground): Share your **Playground** interface with another user.\n\n    This is specifically for sharing the **Playground** experience; it isn't for running a flow in a production application.\n\n    The **Sharable Playground** isn't available for Langflow Desktop.\n\n## See also\n\n* [Manage files in Langflow](/concepts-file-management)\n* [Global variables](/configuration-global-variables)\n* [API keys and authentication](/api-keys-and-authentication)",
      "summary": "You use Langflow's visual editor to create, test, and share flows, which are functional representations of application workflows.\nFlows consist of components that represent individual steps in your application's workflow.",
      "keywords": [
        "access",
        "add",
        "agent",
        "api",
        "basic",
        "canvas",
        "chat",
        "comments",
        "components",
        "controls",
        "editor",
        "embed",
        "enable",
        "export",
        "fit",
        "gestures",
        "guides",
        "help",
        "horizontally",
        "input"
      ]
    },
    "/concepts-playground": {
      "title": "Test flows in the Playground",
      "slug": "/concepts-playground",
      "category": "flows",
      "content": "Langflow's **Playground** is a dynamic interface you can use to test your LLM-based flows in real-time.\n\nYou can test how a flow responds to different inputs, review and modify memories, and monitor flow output and logic.\nFor example, you can make sure agent flows use the appropriate tools to respond to different inputs.\n\nThe **Playground** allows you to quickly iterate over your flow's logic and behavior, making it easier to prototype and refine your applications.\n\n## Run a flow in the Playground\n\nTo run a flow in the **Playground**, open the flow, and then click  **Playground**.\nThen, if your flow has a [**Chat Input** component](/components-io), enter a prompt or [use voice mode](/concepts-voice-mode) to trigger the flow and start a chat session.\n\n**Note:** If there is no message input field in the **Playground**, make sure your flow has a **Chat Input** component that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.\n\nBecause the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface\n\nFor flows that require another type of input, such as a webhook event, file upload, or text input, you can [use the Langflow API to trigger the flow](/api-flows-run), and then open the **Playground** to review the LLM activity for the flow run, if applicable.\n\n![Playground window](/img/playground.png)\n\nFor technical details about how the **Playground** works, see [Monitor endpoints](/api-monitor).\n\n### Review agent logic\n\nIf your flow has an **Agent** component, the **Playground** prints the tools used by the agent and the output from each tool.\nThis helps you monitor the agent's tool use and understand the logic behind the agent's responses.\nFor example, the following agent used a connected `fetch_content` tool to perform a web search:\n\n![Playground with agent response](/img/playground-with-agent.png)\n\n### View chat history {#view-chat-history}\n\nIn the **Playground**, you can view message logs for each of your flow's chat sessions, including timestamps, content, and senders.\n\nIn the **Playground** sidebar, find the chat session you want to review, click  **Options**, and then select **Message Logs**.\n\n![Playground logs](/img/messages-logs.png)\n\nMessage logs break apart the [`Message` data](/data-types#message) for each chat message.\nClick any cell in the message logs to view the full contents of that cell.\n\n### Modify memories in the Playground\n\nTo help debug and test your flows, you can edit or delete individual messages in [message logs](#view-chat-history).\nFor example, you might want to delete messages that you sent while testing a component that is no longer part of your flow.\n\nYou can also delete entire chat sessions from the sidebar: click  **Options**, and then select **Delete**.\n\nModifying memories influences the behavior of the chatbot responses if you continue the chat session or if you preserve memories over multiple chat sessions.\n\n**Editing message logs edits Langflow's internal `messages` table, which is the default chat memory storage.**\nFor more information about managing sessions and chat memory in Langflow, see [Use custom session IDs](#session-ids) and [Memory management options](/memory).\n\n## Set custom session IDs {#session-ids}\n\nChat sessions are identified by session ID (`session_id`), which is a unique identifier for a flow run.\n\nThe default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one enormous chat session.\n\nIf you need to preserve chat context over multiple flow runs or differentiate chat sessions when debugging flows, you can set a custom `session_id`.\n\nCustom session IDs are helpful for multiple reasons:\n\n* Separate chat sessions in situations where one flow has multiple chat sessions, such as a chatbot that can have multiple simultaneous user interactions.\n* Preserve memory when continuing a chat session across multiple flow runs or when passing context from one flow to another.\n* Differentiate activity from multiple users within the same flow.\n* Identify your own chat sessions when debugging and testing flows.\n\nYou can set custom session IDs in the visual editor and programmatically.\n\n</Tabs>\n\n**Note:** In a production environment, consider using a variable for the session ID rather than a hardcoded value.\n\nFor example, if you want to preserve context for authenticated users, user ID could be a useful variable for the session ID.\nAlternatively, if you want every chat to be unique, you might want to automatically generate a UUID for each session ID.\n\nFor more information, see [Use session ID to manage communication between components](/session-id).\n\n## Share a flow's Playground\n\n**Note:** The **Shareable Playground** is for testing purposes only.\nThe **Playground** isn't meant for embedding flows in applications. For information about running flows in applications or websites, see [Trigger flows with the Langflow API](/concepts-publish).\n\nThe **Shareable Playground** isn't available in Langflow Desktop.\n\nThe **Shareable Playground** option exposes the **Playground** for a single flow at the `/public_flow/$FLOW_ID` endpoint.\n\nAfter you [deploy a public Langflow server](/deployment-overview), you can share this public URL with another user to allow them to access the specified flow's **Playground** only.\nThe user can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.\n\nTo share a flow's **Playground** with another user, do the following:\n\n1. In Langflow, open the flow you want share.\n2. In the [workspace](/concepts-overview#workspace), click **Share**, and then enable **Shareable Playground**.\n3. Click **Shareable Playground** again to open the **Playground** window.\nThis window's URL is the flow's **Shareable Playground** address, such as `https://3f7c-73-64-93-151.ngrok-free.app/playground/d764c4b8-5cec-4c0f-9de0-4b419b11901a`.\n4. Send the URL to another user to give them access to the flow's **Playground**.\n\n## See also\n\n- [Upload images](/concepts-file-management#upload-images)\n- [Use voice mode](/concepts-voice-mode)\n- [Trigger flows with the Langflow API](/concepts-publish)\n- [Session ID](/session-id)",
      "summary": "Langflow's **Playground** is a dynamic interface you can use to test your LLM-based flows in real-time.",
      "keywords": [
        "agent",
        "chat",
        "custom",
        "default",
        "delete",
        "editing",
        "edits",
        "flow",
        "flows",
        "history",
        "ids",
        "input",
        "internal",
        "langflow",
        "language",
        "logic",
        "logs",
        "memories",
        "memory",
        "message"
      ]
    },
    "concepts-playground": {
      "title": "Test flows in the Playground",
      "slug": "/concepts-playground",
      "category": "flows",
      "content": "Langflow's **Playground** is a dynamic interface you can use to test your LLM-based flows in real-time.\n\nYou can test how a flow responds to different inputs, review and modify memories, and monitor flow output and logic.\nFor example, you can make sure agent flows use the appropriate tools to respond to different inputs.\n\nThe **Playground** allows you to quickly iterate over your flow's logic and behavior, making it easier to prototype and refine your applications.\n\n## Run a flow in the Playground\n\nTo run a flow in the **Playground**, open the flow, and then click  **Playground**.\nThen, if your flow has a [**Chat Input** component](/components-io), enter a prompt or [use voice mode](/concepts-voice-mode) to trigger the flow and start a chat session.\n\n**Note:** If there is no message input field in the **Playground**, make sure your flow has a **Chat Input** component that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.\n\nBecause the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface\n\nFor flows that require another type of input, such as a webhook event, file upload, or text input, you can [use the Langflow API to trigger the flow](/api-flows-run), and then open the **Playground** to review the LLM activity for the flow run, if applicable.\n\n![Playground window](/img/playground.png)\n\nFor technical details about how the **Playground** works, see [Monitor endpoints](/api-monitor).\n\n### Review agent logic\n\nIf your flow has an **Agent** component, the **Playground** prints the tools used by the agent and the output from each tool.\nThis helps you monitor the agent's tool use and understand the logic behind the agent's responses.\nFor example, the following agent used a connected `fetch_content` tool to perform a web search:\n\n![Playground with agent response](/img/playground-with-agent.png)\n\n### View chat history {#view-chat-history}\n\nIn the **Playground**, you can view message logs for each of your flow's chat sessions, including timestamps, content, and senders.\n\nIn the **Playground** sidebar, find the chat session you want to review, click  **Options**, and then select **Message Logs**.\n\n![Playground logs](/img/messages-logs.png)\n\nMessage logs break apart the [`Message` data](/data-types#message) for each chat message.\nClick any cell in the message logs to view the full contents of that cell.\n\n### Modify memories in the Playground\n\nTo help debug and test your flows, you can edit or delete individual messages in [message logs](#view-chat-history).\nFor example, you might want to delete messages that you sent while testing a component that is no longer part of your flow.\n\nYou can also delete entire chat sessions from the sidebar: click  **Options**, and then select **Delete**.\n\nModifying memories influences the behavior of the chatbot responses if you continue the chat session or if you preserve memories over multiple chat sessions.\n\n**Editing message logs edits Langflow's internal `messages` table, which is the default chat memory storage.**\nFor more information about managing sessions and chat memory in Langflow, see [Use custom session IDs](#session-ids) and [Memory management options](/memory).\n\n## Set custom session IDs {#session-ids}\n\nChat sessions are identified by session ID (`session_id`), which is a unique identifier for a flow run.\n\nThe default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one enormous chat session.\n\nIf you need to preserve chat context over multiple flow runs or differentiate chat sessions when debugging flows, you can set a custom `session_id`.\n\nCustom session IDs are helpful for multiple reasons:\n\n* Separate chat sessions in situations where one flow has multiple chat sessions, such as a chatbot that can have multiple simultaneous user interactions.\n* Preserve memory when continuing a chat session across multiple flow runs or when passing context from one flow to another.\n* Differentiate activity from multiple users within the same flow.\n* Identify your own chat sessions when debugging and testing flows.\n\nYou can set custom session IDs in the visual editor and programmatically.\n\n</Tabs>\n\n**Note:** In a production environment, consider using a variable for the session ID rather than a hardcoded value.\n\nFor example, if you want to preserve context for authenticated users, user ID could be a useful variable for the session ID.\nAlternatively, if you want every chat to be unique, you might want to automatically generate a UUID for each session ID.\n\nFor more information, see [Use session ID to manage communication between components](/session-id).\n\n## Share a flow's Playground\n\n**Note:** The **Shareable Playground** is for testing purposes only.\nThe **Playground** isn't meant for embedding flows in applications. For information about running flows in applications or websites, see [Trigger flows with the Langflow API](/concepts-publish).\n\nThe **Shareable Playground** isn't available in Langflow Desktop.\n\nThe **Shareable Playground** option exposes the **Playground** for a single flow at the `/public_flow/$FLOW_ID` endpoint.\n\nAfter you [deploy a public Langflow server](/deployment-overview), you can share this public URL with another user to allow them to access the specified flow's **Playground** only.\nThe user can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.\n\nTo share a flow's **Playground** with another user, do the following:\n\n1. In Langflow, open the flow you want share.\n2. In the [workspace](/concepts-overview#workspace), click **Share**, and then enable **Shareable Playground**.\n3. Click **Shareable Playground** again to open the **Playground** window.\nThis window's URL is the flow's **Shareable Playground** address, such as `https://3f7c-73-64-93-151.ngrok-free.app/playground/d764c4b8-5cec-4c0f-9de0-4b419b11901a`.\n4. Send the URL to another user to give them access to the flow's **Playground**.\n\n## See also\n\n- [Upload images](/concepts-file-management#upload-images)\n- [Use voice mode](/concepts-voice-mode)\n- [Trigger flows with the Langflow API](/concepts-publish)\n- [Session ID](/session-id)",
      "summary": "Langflow's **Playground** is a dynamic interface you can use to test your LLM-based flows in real-time.",
      "keywords": [
        "agent",
        "chat",
        "custom",
        "default",
        "delete",
        "editing",
        "edits",
        "flow",
        "flows",
        "history",
        "ids",
        "input",
        "internal",
        "langflow",
        "language",
        "logic",
        "logs",
        "memories",
        "memory",
        "message"
      ]
    },
    "/concepts-publish": {
      "title": "Trigger flows with the Langflow API",
      "slug": "/concepts-publish",
      "category": "flows",
      "content": "After you build a flow, you probably want to run it within an application, such as a chatbot within a mobile app or website.\n\nLangflow provides several ways to run flows from external applications:\n\n* [Trigger flows with the Langflow API](#api-access)\n* [Add an embedded chat widget to a website](#embedded-chat-widget)\n* [Serve flows through a Langflow MCP server](#serve-flows-through-a-langflow-mcp-server)\n* [Run flows with the OpenAI Responses compatible endpoint](#openai-responses-api)\n\nAlthough you can use these options with an isolated, local Langflow instance, they are typically more valuable when you have [deployed a Langflow server](/deployment-overview) or [packaged Langflow as a dependency of an application](/develop-application).\n\n## Use the Langflow API to run flows {#api-access}\n\nThe Langflow API is the primary way to access your flows and Langflow servers programmatically.\n\n**Note:** Try it\nFor an example of a script that calls the Langflow API, see the [Quickstart](/get-started-quickstart).\n\n### Generate API code snippets\n\nTo help you embed Langflow API requests in your scripts, Langflow automatically generates Python, JavaScript, and curl code snippets for your flows.\nTo get these code snippets, do the following:\n\n1. In Langflow, open the flow that you want to embed in your application.\n2. Click **Share**, and then select **API access**.\n\n    These code snippets call the `/v1/run/$FLOW_ID` endpoint, and they automatically populate minimum values, like the Langflow server URL, flow ID, headers, and request parameters.\n\n    \n**Note:** Windows\n    The paths generated by the API access pane assume a *nix environment.\n    If you use Microsoft Windows or WSL, you might need to adjust the filepaths given in the code snippets.\n    \n\n    ![API access pane](/img/api-pane.png)\n\n3. Optional: Click [**Input Schema**](#input-schema) to modify component parameters in the code snippets without changing the flow itself.\n\n4. Copy the snippet for the language that you want to use.\n\n5. Run the snippet as is, or use the snippet in the context of a larger script.\n\nFor more information and examples of other Langflow API endpoints, see [Get started with the Langflow API](/api-reference-api-examples).\n\n### Langflow API authentication\n\nIn Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key.\n\nCode snippets generated in the **API access** pane include a script that checks for a `LANGFLOW_API_KEY` environment variable set in the local terminal session.\nThis script doesn't check for Langflow API keys set anywhere besides the local terminal session.\n\nFor this script to work, you must set a `LANGFLOW_API_KEY` variable in the terminal session where you intend to run the code snippet, such as `export LANGFLOW_API_KEY=\"sk...\"`.\n\nAlternatively, you can edit the code snippet to include an `x-api-key` header and ensure that the request can authenticate to the Langflow API.\n\nFor more information, see [API keys and authentication](/api-keys-and-authentication) and [Get started with the Langflow API](/api-reference-api-examples).\n\n### Input Schema (tweaks) {#input-schema}\n\nTweaks are one-time overrides that modify component parameters at runtime, rather than permanently modifying the flow itself.\nFor an example of tweaks in a script, see the [Quickstart](/get-started-quickstart).\n\n**Note:** Tweaks make your flows more dynamic and reusable.\n\nYou can create one flow and use it for multiple applications by passing application-specific tweaks in each application's Langflow API requests.\n\nIn the **API access** pane, click **Input Schema** to add `tweaks` to the request payload in a flow's code snippets.\n\nChanges to a flow's **Input Schema** are saved exclusively as tweaks for that flow's **API access** code snippets.\nThese tweaks don't change the flow parameters set in the [workspace](/concepts-overview#workspace), and they don't apply to other flows.\n\nAdding tweaks through the **Input Schema** can help you troubleshoot formatting issues with tweaks that you manually added to Langflow API requests.\n\nFor example, the following curl command includes a tweak that disables the **Store Messages** setting in a flow's **Chat Input** component:\n\n```bash\ncurl --request POST \\\n  --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Text to input to the flow\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatInput-4WKag\": {\n      \"should_store_message\": false\n    }\n  }\n}'\n```\n\n### Use a flow ID alias\n\nIf you want your requests to use an alias instead of the actual flow ID, you can rename the flow's `/v1/run/$FLOW_ID` endpoint:\n\n1. In Langflow, open the flow, click **Share**, and then select **API access**.\n2. Click **Input Schema**.\n3. In the **Endpoint Name** field, enter an alias for your flow's ID, such as a memorable, human-readable name.\n\n    The name can contain only letters, numbers, hyphens, and underscores, such as `flow-customer-database-agent`.\n\n4. To save the change, close the **Input Schema** pane.\n\nThe automatically generated code snippets now use your new endpoint name instead of the original flow ID, such as `url = \"http://localhost:7868/api/v1/run/flow-customer-database-agent\"`.\n\n## Embed a flow into a website {#embedded-chat-widget}\n\nFor each flow, Langflow provides a code snippet that you can insert into the `<body>` of your website's HTML to interact with your flow through an embedded chat widget.\n\n**Note:** Required components\nThe chat widget only supports flows that have **Chat Input** and **Chat Output** components, which are required for the chat experience.\n**Text Input** and **Text Output** components can send and receive messages, but they don't include ongoing LLM chat context.\n\nAttempting to chat with a flow that doesn't have [**Chat Input** component](/components-io) will trigger the flow, but the response only indicates that the input was empty.\n\n### Get a langflow-chat snippet\n\nTo get a flow's embedded chat widget code snippet, do the following:\n\n1. In Langflow, open the flow you want to embed.\n2. Click **Share**, and then select **Embed into site**.\n3. Copy the code snippet and use it in the `<body>` of your website's HTML.\nFor more information, see [Embed the chat widget with React, Angular, or HTML](#embed-the-chat-widget).\n4. Add the `api_key` prop to ensure the widget has permission to run the flow, as explained in [Configure the langflow-chat web component](#configure-the-langflow-chat-web-component).\n\nThe chat widget is implemented as a web component called `langflow-chat` that is loaded from a CDN. For more information, see the [langflow-embedded-chat repository](https://github.com/langflow-ai/langflow-embedded-chat).\n\nFor example, the following HTML embeds a chat widget for a **Basic Prompting** template flow hosted on a Langflow server deployed on ngrok:\n\n```html\n<html>\n  <head>\n    <script src=\"https://cdn.jsdelivr.net/gh/langflow-ai/langflow-embedded-chat@main/dist/build/static/js/bundle.min.js\"></script>\n  </head>\n  <body>\n    <langflow-chat\n      host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n      flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n      api_key=\"$LANGFLOW_API_KEY\"\n    ></langflow-chat>\n  </body>\n</html>\n```\n\nWhen this code is deployed to a live site, it renders as a responsive chatbot.\nIf a user interacts with the chatbot, the input triggers the specified flow, and then the chatbot returns the output from the flow run.\n\n![Default chat widget](/img/chat-widget-default.png)\n\n**Note:** Try it\nUse the [Langflow embedded chat CodeSandbox](https://codesandbox.io/p/sandbox/langflow-embedded-chat-example-dv9zpx) for an interactive live demo of the embedded chat widget that uses your own flow.\nFor more information, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#live-example).\n\n### Embed the chat widget with React, Angular, or HTML {#embed-the-chat-widget}\n\nThe following examples show how to use embedded chat widget in React, Angular, and plain HTML.\n\n</Tabs>\n\n### Configure the langflow-chat web component {#configure-the-langflow-chat-web-component}\n\nTo use the embedded chat widget in your HTML, the `langflow-chat` web component must include the following minimum inputs (also known as _props_ in React):\n\n* `host_url`: Your Langflow server URL. Must be `HTTPS`. Don't include a trailing slash (`/`).\n* `flow_id`: The ID of the flow you want to embed.\n* `api_key`: A [Langflow API key](/api-keys-and-authentication).\nThis prop is recommended to ensure the widget has permission to run the flow.\n\nThe minimum inputs are automatically populated in the [**Embed into site** code snippet](#get-a-langflow-chat-snippet) that is generated by Langflow.\n\nYou can use additional inputs (props) to modify the embedded chat widget.\nFor a list of all props, types, and descriptions, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#configuration).\n\n<details>\n<summary>Example: Langflow API key prop</summary>\n\nThe `api_key` prop stores a Langflow API key that the chat widget can use to authenticate the underlying Langflow API request.\n\nThe Langflow team recommends following industry best practices for handling sensitive credentials.\nFor example, securely store your API key, and then retrieve with an environment variable:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n></langflow-chat>\n```\n\n</details>\n\n<details>\n<summary>Example: Style props</summary>\n\nThere are many props you can use to customize the style and positioning of the embedded chat widget.\nMany of these props are of type JSON, and they require specific formatting, depending on where you embed the `langflow-chat` web component.\n\nIn React and plain HTML, JSON props are expressed as JSON objects or stringified JSON, such as `\\{\"key\":\"value\"\\}`:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    chat_window_style='{\n        \"backgroundColor\": \"#1a0d0d\",\n        \"border\": \"4px solid #b30000\",\n        \"borderRadius\": \"16px\",\n        \"boxShadow\": \"0 8px 32px #b30000\",\n        \"color\": \"#fff\",\n        \"fontFamily\": \"Georgia, serif\",\n        \"padding\": \"16px\"\n    }'\n    window_title=\"Custom Styled Chat\"\n    height=\"600\"\n    width=\"400\"\n></langflow-chat>\n```\n\nFor Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.\nFor example:\n\n```javascript\n\n@Component({\n  selector: 'app-root',\n  template: `\n    <div class=\"container\">\n      <h1>Langflow Chat Test</h1>\n      <langflow-chat\n        host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n        flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n        api_key=\"$LANGFLOW_API_KEY\"\n        [chat_window_style]='{\"backgroundColor\": \"#ffffff\"}'\n        [bot_message_style]='{\"color\": \"#000000\"}'\n        [user_message_style]='{\"color\": \"#000000\"}'\n        height=\"600\"\n        width=\"400\"\n        chat_position=\"bottom-right\"\n      ></langflow-chat>\n    </div>\n  `,\n  styles: [`\n    .container {\n      padding: 20px;\n      text-align: center;\n    }\n  `]\n})\nexport class AppComponent {\n  title = 'Langflow Chat Test';\n}\n```\n\n</details>\n\n<details>\n<summary>Example: Session ID prop</summary>\n\nThe following example adds a custom [session ID](/session-id) to help identify flow runs started by the embedded chat widget:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    session_id=\"$SESSION_ID\"\n></langflow-chat>\n```\n\n</details>\n\n<details>\n<summary>Example: Tweaks prop</summary>\n\nUse the `tweaks` prop to modify flow parameters at runtime.\nThe available keys for the `tweaks` object depend on the flow you are serving through the embedded chat widget.\n\nIn React and plain HTML, `tweaks` are declared as a JSON object, similar to how you would pass them to a Langflow API endpoint like [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow).\nFor example:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    tweaks='{\n        \"model_name\": \"llama-3.1-8b-instant\"\n    }'\n></langflow-chat>\n```\n\nFor Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.\nFor example:\n\n```javascript\n\n@Component({\n  selector: 'app-root',\n  template: `\n    <div class=\"container\">\n      <h1>Langflow Chat Test</h1>\n      <langflow-chat\n        host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n        flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n        api_key=\"$LANGFLOW_API_KEY\"\n        [tweaks]='{\"model_name\": \"llama-3.1-8b-instant\"}'\n      ></langflow-chat>\n    </div>\n  `,\n  styles: [`\n    .container {\n      padding: 20px;\n      text-align: center;\n    }\n  `]\n})\nexport class AppComponent {\n  title = 'Langflow Chat Test';\n}\n```\n\n</details>\n\n## Serve flows through a Langflow MCP server\n\nEach [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use to generate responses.\n\nIn addition to serving flows through Langflow MCP servers, you can use Langflow as an MCP client to access any MCP server, including your Langflow MCP servers.\n\nInteractions with Langflow MCP servers happen through the Langflow API's `/mcp` endpoints.\n\nFor more information, see [Use Langflow as an MCP server](/mcp-server) and [Use Langflow as an MCP client](/mcp-client).\n\n## Run flows with the OpenAI Responses compatible endpoint {#openai-responses-api}\n\nLangflow includes an OpenAI Responses API-compatible endpoint at `/api/v1/responses` that allows you to use existing OpenAI client libraries and code with minimal modifications.\n\nFor more information, see [OpenAI Responses API](/api-openai-responses).\n\n## See also\n\n* [Import and export flows](/concepts-flows-import)\n* [Files endpoints](/api-files)\n* [Test flows in the Playground](/concepts-playground)",
      "summary": "After you build a flow, you probably want to run it within an application, such as a chatbot within a mobile app or website.",
      "keywords": [
        "access",
        "alias",
        "angular",
        "api",
        "authentication",
        "basic",
        "chat",
        "code",
        "compatible",
        "component",
        "configure",
        "embed",
        "embedded",
        "endpoint",
        "flow",
        "flows",
        "generate",
        "get",
        "html",
        "input"
      ]
    },
    "concepts-publish": {
      "title": "Trigger flows with the Langflow API",
      "slug": "/concepts-publish",
      "category": "flows",
      "content": "After you build a flow, you probably want to run it within an application, such as a chatbot within a mobile app or website.\n\nLangflow provides several ways to run flows from external applications:\n\n* [Trigger flows with the Langflow API](#api-access)\n* [Add an embedded chat widget to a website](#embedded-chat-widget)\n* [Serve flows through a Langflow MCP server](#serve-flows-through-a-langflow-mcp-server)\n* [Run flows with the OpenAI Responses compatible endpoint](#openai-responses-api)\n\nAlthough you can use these options with an isolated, local Langflow instance, they are typically more valuable when you have [deployed a Langflow server](/deployment-overview) or [packaged Langflow as a dependency of an application](/develop-application).\n\n## Use the Langflow API to run flows {#api-access}\n\nThe Langflow API is the primary way to access your flows and Langflow servers programmatically.\n\n**Note:** Try it\nFor an example of a script that calls the Langflow API, see the [Quickstart](/get-started-quickstart).\n\n### Generate API code snippets\n\nTo help you embed Langflow API requests in your scripts, Langflow automatically generates Python, JavaScript, and curl code snippets for your flows.\nTo get these code snippets, do the following:\n\n1. In Langflow, open the flow that you want to embed in your application.\n2. Click **Share**, and then select **API access**.\n\n    These code snippets call the `/v1/run/$FLOW_ID` endpoint, and they automatically populate minimum values, like the Langflow server URL, flow ID, headers, and request parameters.\n\n    \n**Note:** Windows\n    The paths generated by the API access pane assume a *nix environment.\n    If you use Microsoft Windows or WSL, you might need to adjust the filepaths given in the code snippets.\n    \n\n    ![API access pane](/img/api-pane.png)\n\n3. Optional: Click [**Input Schema**](#input-schema) to modify component parameters in the code snippets without changing the flow itself.\n\n4. Copy the snippet for the language that you want to use.\n\n5. Run the snippet as is, or use the snippet in the context of a larger script.\n\nFor more information and examples of other Langflow API endpoints, see [Get started with the Langflow API](/api-reference-api-examples).\n\n### Langflow API authentication\n\nIn Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key.\n\nCode snippets generated in the **API access** pane include a script that checks for a `LANGFLOW_API_KEY` environment variable set in the local terminal session.\nThis script doesn't check for Langflow API keys set anywhere besides the local terminal session.\n\nFor this script to work, you must set a `LANGFLOW_API_KEY` variable in the terminal session where you intend to run the code snippet, such as `export LANGFLOW_API_KEY=\"sk...\"`.\n\nAlternatively, you can edit the code snippet to include an `x-api-key` header and ensure that the request can authenticate to the Langflow API.\n\nFor more information, see [API keys and authentication](/api-keys-and-authentication) and [Get started with the Langflow API](/api-reference-api-examples).\n\n### Input Schema (tweaks) {#input-schema}\n\nTweaks are one-time overrides that modify component parameters at runtime, rather than permanently modifying the flow itself.\nFor an example of tweaks in a script, see the [Quickstart](/get-started-quickstart).\n\n**Note:** Tweaks make your flows more dynamic and reusable.\n\nYou can create one flow and use it for multiple applications by passing application-specific tweaks in each application's Langflow API requests.\n\nIn the **API access** pane, click **Input Schema** to add `tweaks` to the request payload in a flow's code snippets.\n\nChanges to a flow's **Input Schema** are saved exclusively as tweaks for that flow's **API access** code snippets.\nThese tweaks don't change the flow parameters set in the [workspace](/concepts-overview#workspace), and they don't apply to other flows.\n\nAdding tweaks through the **Input Schema** can help you troubleshoot formatting issues with tweaks that you manually added to Langflow API requests.\n\nFor example, the following curl command includes a tweak that disables the **Store Messages** setting in a flow's **Chat Input** component:\n\n```bash\ncurl --request POST \\\n  --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"x-api-key: LANGFLOW_API_KEY\" \\\n  --data '{\n  \"input_value\": \"Text to input to the flow\",\n  \"output_type\": \"chat\",\n  \"input_type\": \"chat\",\n  \"tweaks\": {\n    \"ChatInput-4WKag\": {\n      \"should_store_message\": false\n    }\n  }\n}'\n```\n\n### Use a flow ID alias\n\nIf you want your requests to use an alias instead of the actual flow ID, you can rename the flow's `/v1/run/$FLOW_ID` endpoint:\n\n1. In Langflow, open the flow, click **Share**, and then select **API access**.\n2. Click **Input Schema**.\n3. In the **Endpoint Name** field, enter an alias for your flow's ID, such as a memorable, human-readable name.\n\n    The name can contain only letters, numbers, hyphens, and underscores, such as `flow-customer-database-agent`.\n\n4. To save the change, close the **Input Schema** pane.\n\nThe automatically generated code snippets now use your new endpoint name instead of the original flow ID, such as `url = \"http://localhost:7868/api/v1/run/flow-customer-database-agent\"`.\n\n## Embed a flow into a website {#embedded-chat-widget}\n\nFor each flow, Langflow provides a code snippet that you can insert into the `<body>` of your website's HTML to interact with your flow through an embedded chat widget.\n\n**Note:** Required components\nThe chat widget only supports flows that have **Chat Input** and **Chat Output** components, which are required for the chat experience.\n**Text Input** and **Text Output** components can send and receive messages, but they don't include ongoing LLM chat context.\n\nAttempting to chat with a flow that doesn't have [**Chat Input** component](/components-io) will trigger the flow, but the response only indicates that the input was empty.\n\n### Get a langflow-chat snippet\n\nTo get a flow's embedded chat widget code snippet, do the following:\n\n1. In Langflow, open the flow you want to embed.\n2. Click **Share**, and then select **Embed into site**.\n3. Copy the code snippet and use it in the `<body>` of your website's HTML.\nFor more information, see [Embed the chat widget with React, Angular, or HTML](#embed-the-chat-widget).\n4. Add the `api_key` prop to ensure the widget has permission to run the flow, as explained in [Configure the langflow-chat web component](#configure-the-langflow-chat-web-component).\n\nThe chat widget is implemented as a web component called `langflow-chat` that is loaded from a CDN. For more information, see the [langflow-embedded-chat repository](https://github.com/langflow-ai/langflow-embedded-chat).\n\nFor example, the following HTML embeds a chat widget for a **Basic Prompting** template flow hosted on a Langflow server deployed on ngrok:\n\n```html\n<html>\n  <head>\n    <script src=\"https://cdn.jsdelivr.net/gh/langflow-ai/langflow-embedded-chat@main/dist/build/static/js/bundle.min.js\"></script>\n  </head>\n  <body>\n    <langflow-chat\n      host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n      flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n      api_key=\"$LANGFLOW_API_KEY\"\n    ></langflow-chat>\n  </body>\n</html>\n```\n\nWhen this code is deployed to a live site, it renders as a responsive chatbot.\nIf a user interacts with the chatbot, the input triggers the specified flow, and then the chatbot returns the output from the flow run.\n\n![Default chat widget](/img/chat-widget-default.png)\n\n**Note:** Try it\nUse the [Langflow embedded chat CodeSandbox](https://codesandbox.io/p/sandbox/langflow-embedded-chat-example-dv9zpx) for an interactive live demo of the embedded chat widget that uses your own flow.\nFor more information, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#live-example).\n\n### Embed the chat widget with React, Angular, or HTML {#embed-the-chat-widget}\n\nThe following examples show how to use embedded chat widget in React, Angular, and plain HTML.\n\n</Tabs>\n\n### Configure the langflow-chat web component {#configure-the-langflow-chat-web-component}\n\nTo use the embedded chat widget in your HTML, the `langflow-chat` web component must include the following minimum inputs (also known as _props_ in React):\n\n* `host_url`: Your Langflow server URL. Must be `HTTPS`. Don't include a trailing slash (`/`).\n* `flow_id`: The ID of the flow you want to embed.\n* `api_key`: A [Langflow API key](/api-keys-and-authentication).\nThis prop is recommended to ensure the widget has permission to run the flow.\n\nThe minimum inputs are automatically populated in the [**Embed into site** code snippet](#get-a-langflow-chat-snippet) that is generated by Langflow.\n\nYou can use additional inputs (props) to modify the embedded chat widget.\nFor a list of all props, types, and descriptions, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#configuration).\n\n<details>\n<summary>Example: Langflow API key prop</summary>\n\nThe `api_key` prop stores a Langflow API key that the chat widget can use to authenticate the underlying Langflow API request.\n\nThe Langflow team recommends following industry best practices for handling sensitive credentials.\nFor example, securely store your API key, and then retrieve with an environment variable:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n></langflow-chat>\n```\n\n</details>\n\n<details>\n<summary>Example: Style props</summary>\n\nThere are many props you can use to customize the style and positioning of the embedded chat widget.\nMany of these props are of type JSON, and they require specific formatting, depending on where you embed the `langflow-chat` web component.\n\nIn React and plain HTML, JSON props are expressed as JSON objects or stringified JSON, such as `\\{\"key\":\"value\"\\}`:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    chat_window_style='{\n        \"backgroundColor\": \"#1a0d0d\",\n        \"border\": \"4px solid #b30000\",\n        \"borderRadius\": \"16px\",\n        \"boxShadow\": \"0 8px 32px #b30000\",\n        \"color\": \"#fff\",\n        \"fontFamily\": \"Georgia, serif\",\n        \"padding\": \"16px\"\n    }'\n    window_title=\"Custom Styled Chat\"\n    height=\"600\"\n    width=\"400\"\n></langflow-chat>\n```\n\nFor Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.\nFor example:\n\n```javascript\n\n@Component({\n  selector: 'app-root',\n  template: `\n    <div class=\"container\">\n      <h1>Langflow Chat Test</h1>\n      <langflow-chat\n        host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n        flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n        api_key=\"$LANGFLOW_API_KEY\"\n        [chat_window_style]='{\"backgroundColor\": \"#ffffff\"}'\n        [bot_message_style]='{\"color\": \"#000000\"}'\n        [user_message_style]='{\"color\": \"#000000\"}'\n        height=\"600\"\n        width=\"400\"\n        chat_position=\"bottom-right\"\n      ></langflow-chat>\n    </div>\n  `,\n  styles: [`\n    .container {\n      padding: 20px;\n      text-align: center;\n    }\n  `]\n})\nexport class AppComponent {\n  title = 'Langflow Chat Test';\n}\n```\n\n</details>\n\n<details>\n<summary>Example: Session ID prop</summary>\n\nThe following example adds a custom [session ID](/session-id) to help identify flow runs started by the embedded chat widget:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    session_id=\"$SESSION_ID\"\n></langflow-chat>\n```\n\n</details>\n\n<details>\n<summary>Example: Tweaks prop</summary>\n\nUse the `tweaks` prop to modify flow parameters at runtime.\nThe available keys for the `tweaks` object depend on the flow you are serving through the embedded chat widget.\n\nIn React and plain HTML, `tweaks` are declared as a JSON object, similar to how you would pass them to a Langflow API endpoint like [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow).\nFor example:\n\n```html\n<langflow-chat\n    host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n    flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n    api_key=\"$LANGFLOW_API_KEY\"\n    tweaks='{\n        \"model_name\": \"llama-3.1-8b-instant\"\n    }'\n></langflow-chat>\n```\n\nFor Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.\nFor example:\n\n```javascript\n\n@Component({\n  selector: 'app-root',\n  template: `\n    <div class=\"container\">\n      <h1>Langflow Chat Test</h1>\n      <langflow-chat\n        host_url=\"https://c822-73-64-93-151.ngrok-free.app\"\n        flow_id=\"dcbed533-859f-4b99-b1f5-16fce884f28f\"\n        api_key=\"$LANGFLOW_API_KEY\"\n        [tweaks]='{\"model_name\": \"llama-3.1-8b-instant\"}'\n      ></langflow-chat>\n    </div>\n  `,\n  styles: [`\n    .container {\n      padding: 20px;\n      text-align: center;\n    }\n  `]\n})\nexport class AppComponent {\n  title = 'Langflow Chat Test';\n}\n```\n\n</details>\n\n## Serve flows through a Langflow MCP server\n\nEach [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use to generate responses.\n\nIn addition to serving flows through Langflow MCP servers, you can use Langflow as an MCP client to access any MCP server, including your Langflow MCP servers.\n\nInteractions with Langflow MCP servers happen through the Langflow API's `/mcp` endpoints.\n\nFor more information, see [Use Langflow as an MCP server](/mcp-server) and [Use Langflow as an MCP client](/mcp-client).\n\n## Run flows with the OpenAI Responses compatible endpoint {#openai-responses-api}\n\nLangflow includes an OpenAI Responses API-compatible endpoint at `/api/v1/responses` that allows you to use existing OpenAI client libraries and code with minimal modifications.\n\nFor more information, see [OpenAI Responses API](/api-openai-responses).\n\n## See also\n\n* [Import and export flows](/concepts-flows-import)\n* [Files endpoints](/api-files)\n* [Test flows in the Playground](/concepts-playground)",
      "summary": "After you build a flow, you probably want to run it within an application, such as a chatbot within a mobile app or website.",
      "keywords": [
        "access",
        "alias",
        "angular",
        "api",
        "authentication",
        "basic",
        "chat",
        "code",
        "compatible",
        "component",
        "configure",
        "embed",
        "embedded",
        "endpoint",
        "flow",
        "flows",
        "generate",
        "get",
        "html",
        "input"
      ]
    },
    "/webhook": {
      "title": "Trigger flows with webhooks",
      "slug": "/webhook",
      "category": "flows",
      "content": "You can use the **Webhook** component to start a flow run in response to an external event.\n\nWith the **Webhook** component, a flow can receive data directly from external sources. Then, the flow can parse the data and pass it to other components in the flow to initiate other actions, such as calling APIs, writing to databases, and chatting with LLMs.\nIf the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow.\n\nThe **Webhook** component provides a versatile entrypoint that can make your flows more event-driven and integrated with your entire stack of applications and services.\nFor example:\n\n* Use an LLM to analyze the sentiment and content of customer feedback or survey responses.\n* Receive notifications from a monitoring system, and then trigger automated responses based on alert type and severity.\n* Integrate with e-commerce platforms to process orders and update inventory.\n\n## Configure the Webhook component\n\nTo use the **Webhook** component in a flow, do the following:\n\n1. In Langflow, open the flow where you want to use the **Webhook** component.\n\n2. Add a [**Webhook** component](/components-data#webhook) and a [**Parser** component](/components-processing#parser) to your flow.\n\n    These two components are commonly paired together because the **Parser** component extracts relevant data from the raw payload received by the **Webhook** component.\n\n3. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.\n\n4. In the **Parser** component's **Template** field, enter a template to parse the raw payload into structured text.\n\n    In the template, use variables for payload keys in the same way you would define variables in a [**Prompt Template** component](/components-prompts).\n\n    For example, assume that you expect your **Webhook** component to receive the following JSON data:\n\n    ```json\n    {\n      \"id\": \"\",\n      \"name\": \"\",\n      \"email\": \"\"\n    }\n    ```\n\n    Then, you can [use curly braces](/components-prompts#define-variables-in-prompts) to reference the JSON keys anywhere in your parser template:\n\n    ```text\n    ID: {id} - Name: {name} - Email: {email}\n    ```\n\n5. Connect the **Parser** component's **Parsed Text** output to the next logical component in your flow, such as a **Chat Input** component.\n\n    If you want to test only the **Webhook** and **Parser** components, you can connect the **Parsed Text** output directly to a **Chat Output** component's **Text** input. Then, you can see the parsed data in the **Playground** after you run the flow.\n\n6. From the **Webhook** component's **Endpoint** field, copy the API endpoint that you will use to send data to the **Webhook** component and trigger the flow.\n\n    Alternatively, to get a complete `POST /v1/webhook/$FLOW_ID` code snippet, open the flow's [**API access** pane](/concepts-publish#api-access), and then click the **Webhook curl** tab.\n    You can also modify the default curl command in the **Webhook** component's **curl** field.\n    If this field isn't visible by default, click the **Webhook** component, and then click  **Controls** in the [component's header menu](/concepts-components#component-menus).\n\n7. Send a POST request with `data` to the flow's `webhook` endpoint to trigger the flow.\n\n    The following example sends a payload containing `id`, `name`, and `email` strings:\n\n    ```bash\n    curl -X POST \"http://localhost:7860/api/v1/webhook/FLOW_ID\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"x-api-key: LANGFLOW_API_KEY\" \\\n        -d '{\"id\": \"12345\", \"name\": \"alex\", \"email\": \"alex@email.com\"}'\n    ```\n\n    A successful response indicates that Langflow started the flow.\n    The response doesn't include the output for the entire flow, only an indication that the flow started.\n\n    ```json\n    {\n      \"message\": \"Task started in the background\",\n      \"status\": \"in progress\"\n    }\n    ```\n\n8. To view the flow's most recent parsed payload, click the **Parser** component, and then click  **Inspect output**.\nFor the preceding example, the parsed payload would be a string like `ID: 12345 - Name: alex - Email: alex@email.com`.\n\n## Trigger flows with Composio webhooks\n\nTypically, you won't manually trigger the **Webhook** component.\nTo learn about triggering flows with payloads from external applications, see the video tutorial [How to Use Webhooks in Langflow](https://www.youtube.com/watch?v=IC1CAtzFRE0).\n\n## Troubleshoot flows with Webhook components\n\nUse the following information to help address common issues that can occur with the **Webhook** component.\n\n### Validate data received by the Webhook component\n\nTo troubleshoot a flow with a **Webhook** component and verify that the component is receiving data, you can create a small flow that outputs only the parsed payload:\n\n1. Create a flow with **Webhook**, **Parser**, and **Chat Output** components.\n2. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.\n3. Connect the **Parser** component's **Parsed Text** output to the **Chat Output** component's **Text** input.\n4. Edit the **Parser** component to set **Mode** to **Stringify**.\n\n    This mode passes the data received by the **Webhook** component as a string that is printed by the **Chat Output** component.\n\n5. Click **Share**, select **API access**, and then copy the **Webhook curl** code snippet.\n6. Optional: Edit the `data` in the code snippet if you want to pass a different payload.\n7. Send the POST request to trigger the flow.\n8. Click **Playground** to verify that the **Chat Output** component printed the JSON data from your POST request.\n\n### Parser component build failure\n\nThe **Parser** component can fail to build if it doesn't receive data from the **Webhook** component or if there is a problem with the incoming data.\n\nIf this occurs, try changing the **Parser** component's **Mode** to **Stringify** so that the component outputs the parsed payload as a single string.\nThen, you can examine the string output and troubleshoot your parsing template, or work with the parsed data in string form.\n\n## See also\n\n- [Get started with the Langflow API](/api-reference-api-examples)\n- [**Webhook** component](/components-data#webhook)\n- [Flow trigger endpoints](/api-flows-run)",
      "summary": "You can use the **Webhook** component to start a flow run in response to an external event.",
      "keywords": [
        "access",
        "api",
        "build",
        "chat",
        "component",
        "components",
        "composio",
        "configure",
        "controls",
        "curl",
        "data",
        "endpoint",
        "failure",
        "flows",
        "input",
        "inspect",
        "mode",
        "output",
        "parsed",
        "parser"
      ]
    },
    "webhook": {
      "title": "Trigger flows with webhooks",
      "slug": "/webhook",
      "category": "flows",
      "content": "You can use the **Webhook** component to start a flow run in response to an external event.\n\nWith the **Webhook** component, a flow can receive data directly from external sources. Then, the flow can parse the data and pass it to other components in the flow to initiate other actions, such as calling APIs, writing to databases, and chatting with LLMs.\nIf the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow.\n\nThe **Webhook** component provides a versatile entrypoint that can make your flows more event-driven and integrated with your entire stack of applications and services.\nFor example:\n\n* Use an LLM to analyze the sentiment and content of customer feedback or survey responses.\n* Receive notifications from a monitoring system, and then trigger automated responses based on alert type and severity.\n* Integrate with e-commerce platforms to process orders and update inventory.\n\n## Configure the Webhook component\n\nTo use the **Webhook** component in a flow, do the following:\n\n1. In Langflow, open the flow where you want to use the **Webhook** component.\n\n2. Add a [**Webhook** component](/components-data#webhook) and a [**Parser** component](/components-processing#parser) to your flow.\n\n    These two components are commonly paired together because the **Parser** component extracts relevant data from the raw payload received by the **Webhook** component.\n\n3. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.\n\n4. In the **Parser** component's **Template** field, enter a template to parse the raw payload into structured text.\n\n    In the template, use variables for payload keys in the same way you would define variables in a [**Prompt Template** component](/components-prompts).\n\n    For example, assume that you expect your **Webhook** component to receive the following JSON data:\n\n    ```json\n    {\n      \"id\": \"\",\n      \"name\": \"\",\n      \"email\": \"\"\n    }\n    ```\n\n    Then, you can [use curly braces](/components-prompts#define-variables-in-prompts) to reference the JSON keys anywhere in your parser template:\n\n    ```text\n    ID: {id} - Name: {name} - Email: {email}\n    ```\n\n5. Connect the **Parser** component's **Parsed Text** output to the next logical component in your flow, such as a **Chat Input** component.\n\n    If you want to test only the **Webhook** and **Parser** components, you can connect the **Parsed Text** output directly to a **Chat Output** component's **Text** input. Then, you can see the parsed data in the **Playground** after you run the flow.\n\n6. From the **Webhook** component's **Endpoint** field, copy the API endpoint that you will use to send data to the **Webhook** component and trigger the flow.\n\n    Alternatively, to get a complete `POST /v1/webhook/$FLOW_ID` code snippet, open the flow's [**API access** pane](/concepts-publish#api-access), and then click the **Webhook curl** tab.\n    You can also modify the default curl command in the **Webhook** component's **curl** field.\n    If this field isn't visible by default, click the **Webhook** component, and then click  **Controls** in the [component's header menu](/concepts-components#component-menus).\n\n7. Send a POST request with `data` to the flow's `webhook` endpoint to trigger the flow.\n\n    The following example sends a payload containing `id`, `name`, and `email` strings:\n\n    ```bash\n    curl -X POST \"http://localhost:7860/api/v1/webhook/FLOW_ID\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"x-api-key: LANGFLOW_API_KEY\" \\\n        -d '{\"id\": \"12345\", \"name\": \"alex\", \"email\": \"alex@email.com\"}'\n    ```\n\n    A successful response indicates that Langflow started the flow.\n    The response doesn't include the output for the entire flow, only an indication that the flow started.\n\n    ```json\n    {\n      \"message\": \"Task started in the background\",\n      \"status\": \"in progress\"\n    }\n    ```\n\n8. To view the flow's most recent parsed payload, click the **Parser** component, and then click  **Inspect output**.\nFor the preceding example, the parsed payload would be a string like `ID: 12345 - Name: alex - Email: alex@email.com`.\n\n## Trigger flows with Composio webhooks\n\nTypically, you won't manually trigger the **Webhook** component.\nTo learn about triggering flows with payloads from external applications, see the video tutorial [How to Use Webhooks in Langflow](https://www.youtube.com/watch?v=IC1CAtzFRE0).\n\n## Troubleshoot flows with Webhook components\n\nUse the following information to help address common issues that can occur with the **Webhook** component.\n\n### Validate data received by the Webhook component\n\nTo troubleshoot a flow with a **Webhook** component and verify that the component is receiving data, you can create a small flow that outputs only the parsed payload:\n\n1. Create a flow with **Webhook**, **Parser**, and **Chat Output** components.\n2. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.\n3. Connect the **Parser** component's **Parsed Text** output to the **Chat Output** component's **Text** input.\n4. Edit the **Parser** component to set **Mode** to **Stringify**.\n\n    This mode passes the data received by the **Webhook** component as a string that is printed by the **Chat Output** component.\n\n5. Click **Share**, select **API access**, and then copy the **Webhook curl** code snippet.\n6. Optional: Edit the `data` in the code snippet if you want to pass a different payload.\n7. Send the POST request to trigger the flow.\n8. Click **Playground** to verify that the **Chat Output** component printed the JSON data from your POST request.\n\n### Parser component build failure\n\nThe **Parser** component can fail to build if it doesn't receive data from the **Webhook** component or if there is a problem with the incoming data.\n\nIf this occurs, try changing the **Parser** component's **Mode** to **Stringify** so that the component outputs the parsed payload as a single string.\nThen, you can examine the string output and troubleshoot your parsing template, or work with the parsed data in string form.\n\n## See also\n\n- [Get started with the Langflow API](/api-reference-api-examples)\n- [**Webhook** component](/components-data#webhook)\n- [Flow trigger endpoints](/api-flows-run)",
      "summary": "You can use the **Webhook** component to start a flow run in response to an external event.",
      "keywords": [
        "access",
        "api",
        "build",
        "chat",
        "component",
        "components",
        "composio",
        "configure",
        "controls",
        "curl",
        "data",
        "endpoint",
        "failure",
        "flows",
        "input",
        "inspect",
        "mode",
        "output",
        "parsed",
        "parser"
      ]
    },
    "/": {
      "title": "What is Langflow?",
      "slug": "/",
      "category": "get_started",
      "content": "Langflow is an open-source, Python-based, customizable framework for building AI applications.\nIt supports important AI functionality like agents and the Model Context Protocol (MCP), and it doesn't require you to use specific large language models (LLMs) or vector stores.\n\nThe visual editor simplifies prototyping of application workflows, enabling developers to quickly turn their ideas into powerful, real-world solutions.\n\n**Note:** Try it\nBuild and run your first flow in minutes: [Install Langflow](/get-started-installation), and then try the [Quickstart](/get-started-quickstart).\n\n## Application development and prototyping\n\nLangflow can help you develop a wide variety of AI applications, such as chatbots, document analysis systems, content generators, and agentic applications.\n\nLangflow includes several pre-built templates that are ready to use or customize to your needs.\n\n### Create flows in minutes\n\nThe primary purpose of Langflow is to create and serve flows, which are functional representations of application workflows.\n\nTo [build a flow](/concepts-flows), you connect and configure component nodes. Each component is a single step in the workflow.\n\nWith Langflow's [visual editor](/concepts-overview), you can drag and drop components to quickly build and test a functional AI application workflow.\nFor example, you could build a chatbot flow for an e-commerce store that uses an LLM and a product data store to allow customers to ask questions about the store's products.\n\n![Basic Prompting flow in Langflow](/img/workspace-basic-prompting.png)\n\n### Test flows in real-time\n\nYou can use the [**Playground**](/concepts-playground) to test flows without having to build your entire application stack.\nYou can interact with your flows and get real-time feedback about flow logic and response generation.\n\nYou can also run individual components to test dependencies in isolation.\n\n### Run and serve flows\n\nYou can use your flows as prototypes for more formal application development, or you can use the Langflow API to embed your flows into your application code.\n\nFor more extensive development, you can build Langflow as a dependency or deploy a Langflow server to serve flows over the public internet.\n\nFor more information, see the following:\n\n* [Trigger flows with the Langflow API](/concepts-publish)\n* [Containerize a Langflow application](/develop-application)\n\n## Endless modifications and integrations\n\nLangflow provides [components](/concepts-components) that support many services, tools, and functionality that are required for AI applications.\n\nSome components are generalized, such as inputs, outputs, and data stores.\nOthers are specialized, such as agents, language models, and embedding providers.\n\nAll components offer parameters that you can set to fixed or variable values. You can also use tweaks to temporarily override flow settings at runtime.\n\n### Agent and MCP support\n\nIn addition to building agent flows with Langflow, you can leverage Langflow's built-in agent and MCP features:\n\n* [Use Langflow Agents](/agents)\n* [Use components and flows as agent tools](/agents-tools)\n* [Use Langflow as an MCP server](/mcp-server)\n* [Use Langflow as an MCP client](/mcp-client)\n\n### Extensibility\n\nIn addition to the core components, Langflow supports custom components.\n\nYou can use custom components developed by others, and you can develop your own custom components for personal use or to share with other Langflow users.\n\nFor more information, see the following:\n\n* [Contribute to Langflow](/contributing-how-to-contribute)\n* [Create custom Python components](/components-custom-components)\n* [Get help and request enhancements](/contributing-github-issues)\n\n## Next steps\n\n* [Install Langflow](/get-started-installation)\n* [Quickstart](/get-started-quickstart)",
      "summary": "Langflow is an open-source, Python-based, customizable framework for building AI applications.\nIt supports important AI functionality like agents and the Model Context Protocol (MCP), and it doesn't require you to use specific large language models (LLMs) or vector stores.",
      "keywords": [
        "agent",
        "application",
        "create",
        "development",
        "endless",
        "extensibility",
        "flows",
        "integrations",
        "langflow",
        "mcp",
        "minutes",
        "modifications",
        "next",
        "note",
        "playground",
        "prototyping",
        "real",
        "run",
        "serve",
        "steps"
      ]
    },
    "about-langflow": {
      "title": "What is Langflow?",
      "slug": "/",
      "category": "get_started",
      "content": "Langflow is an open-source, Python-based, customizable framework for building AI applications.\nIt supports important AI functionality like agents and the Model Context Protocol (MCP), and it doesn't require you to use specific large language models (LLMs) or vector stores.\n\nThe visual editor simplifies prototyping of application workflows, enabling developers to quickly turn their ideas into powerful, real-world solutions.\n\n**Note:** Try it\nBuild and run your first flow in minutes: [Install Langflow](/get-started-installation), and then try the [Quickstart](/get-started-quickstart).\n\n## Application development and prototyping\n\nLangflow can help you develop a wide variety of AI applications, such as chatbots, document analysis systems, content generators, and agentic applications.\n\nLangflow includes several pre-built templates that are ready to use or customize to your needs.\n\n### Create flows in minutes\n\nThe primary purpose of Langflow is to create and serve flows, which are functional representations of application workflows.\n\nTo [build a flow](/concepts-flows), you connect and configure component nodes. Each component is a single step in the workflow.\n\nWith Langflow's [visual editor](/concepts-overview), you can drag and drop components to quickly build and test a functional AI application workflow.\nFor example, you could build a chatbot flow for an e-commerce store that uses an LLM and a product data store to allow customers to ask questions about the store's products.\n\n![Basic Prompting flow in Langflow](/img/workspace-basic-prompting.png)\n\n### Test flows in real-time\n\nYou can use the [**Playground**](/concepts-playground) to test flows without having to build your entire application stack.\nYou can interact with your flows and get real-time feedback about flow logic and response generation.\n\nYou can also run individual components to test dependencies in isolation.\n\n### Run and serve flows\n\nYou can use your flows as prototypes for more formal application development, or you can use the Langflow API to embed your flows into your application code.\n\nFor more extensive development, you can build Langflow as a dependency or deploy a Langflow server to serve flows over the public internet.\n\nFor more information, see the following:\n\n* [Trigger flows with the Langflow API](/concepts-publish)\n* [Containerize a Langflow application](/develop-application)\n\n## Endless modifications and integrations\n\nLangflow provides [components](/concepts-components) that support many services, tools, and functionality that are required for AI applications.\n\nSome components are generalized, such as inputs, outputs, and data stores.\nOthers are specialized, such as agents, language models, and embedding providers.\n\nAll components offer parameters that you can set to fixed or variable values. You can also use tweaks to temporarily override flow settings at runtime.\n\n### Agent and MCP support\n\nIn addition to building agent flows with Langflow, you can leverage Langflow's built-in agent and MCP features:\n\n* [Use Langflow Agents](/agents)\n* [Use components and flows as agent tools](/agents-tools)\n* [Use Langflow as an MCP server](/mcp-server)\n* [Use Langflow as an MCP client](/mcp-client)\n\n### Extensibility\n\nIn addition to the core components, Langflow supports custom components.\n\nYou can use custom components developed by others, and you can develop your own custom components for personal use or to share with other Langflow users.\n\nFor more information, see the following:\n\n* [Contribute to Langflow](/contributing-how-to-contribute)\n* [Create custom Python components](/components-custom-components)\n* [Get help and request enhancements](/contributing-github-issues)\n\n## Next steps\n\n* [Install Langflow](/get-started-installation)\n* [Quickstart](/get-started-quickstart)",
      "summary": "Langflow is an open-source, Python-based, customizable framework for building AI applications.\nIt supports important AI functionality like agents and the Model Context Protocol (MCP), and it doesn't require you to use specific large language models (LLMs) or vector stores.",
      "keywords": [
        "agent",
        "application",
        "create",
        "development",
        "endless",
        "extensibility",
        "flows",
        "integrations",
        "langflow",
        "mcp",
        "minutes",
        "modifications",
        "next",
        "note",
        "playground",
        "prototyping",
        "real",
        "run",
        "serve",
        "steps"
      ]
    },
    "/get-started-installation": {
      "title": "Install Langflow",
      "slug": "/get-started-installation",
      "category": "get_started",
      "content": "Langflow can be installed in multiple ways:\n\n* [**Langflow Desktop (Recommended)**](#install-and-run-langflow-desktop): Download and install the standalone desktop application for the least complicated setup experience.\nThis option includes dependency management and facilitated upgrades.\n\n* [**Docker**](#install-and-run-langflow-docker): Pull and run the Langflow Docker image to start a Langflow container and run Langflow in isolation.\n\n* [**Python package**](#install-and-run-the-langflow-oss-python-package): Install and run the Langflow OSS Python package.\nThis option offers more control over the environment, dependencies, and versioning.\n\n* [**Install from source**](/contributing-how-to-contribute#install-langflow-from-source): Use this option if you want to contribute to the Langflow codebase or documentation.\n\n## Install and run Langflow Desktop\n\nLangflow Desktop is a desktop version of Langflow that simplifies dependency management and upgrades.\nHowever, some features aren't available for Langflow Desktop, such as the **Shareable Playground** and **Voice Mode**.\n\n</Tabs>\n\nFor upgrade information, see the [Release notes](/release-notes).\n\nTo manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).\n\n## Install and run Langflow with Docker {#install-and-run-langflow-docker}\n\nYou can use the Langflow Docker image to start a Langflow container.\nFor more information, see [Deploy Langflow on Docker](/deployment-docker).\n\n1. Install and start [Docker](https://docs.docker.com/).\n\n2. Pull the latest [Langflow Docker image](https://hub.docker.com/r/langflowai/langflow) and start it:\n\n    ```bash\n    docker run -p 7860:7860 langflowai/langflow:latest\n    ```\n\n3. To access Langflow, navigate to `http://localhost:7860/`.\n\n4. Create your first flow with the [Quickstart](/get-started-quickstart).\n\n## Install and run the Langflow OSS Python package\n\n1. Make sure you have the required dependencies and infrastructure:\n\n    - [Python](https://www.python.org/downloads/release/python-3100/)\n       - macOS and Linux: Version 3.10 to 3.13\n       - Windows: Version 3.10 to 3.12\n    - [uv](https://docs.astral.sh/uv/getting-started/installation/)\n    - Sufficient infrastructure:\n       - Minimum: Dual-core CPU and 2 GB RAM\n       - Recommended: Multi-core CPU and at least 4 GB RAM\n\n2. Create a virtual environment with [uv](https://docs.astral.sh/uv/pip/environments).\n\n    <details>\n    <summary>Need help with virtual environments?</summary>\n\n    Virtual environments ensure Langflow is installed in an isolated, fresh environment.\n    To create a new virtual environment, do the following.\n\n        \n        \n        </Tabs>\n\n    </details>\n\n3. In your virtual environment, install Langflow:\n\n    ```bash\n    uv pip install langflow\n    ```\n\n    To install a specific version of the Langflow package, add the required version to the command, such as `uv pip install langflow==1.4.22`.\n\n    <details>\n    <summary>Reinstall or upgrade Langflow</summary>\n\n    To reinstall Langflow and all of its dependencies, run `uv pip install langflow --force-reinstall`.\n\n    To upgrade Langflow to the latest version, run `uv pip install langflow -U`.\n    However, the Langflow team recommends taking steps to backup your existing installation before you upgrade Langflow.\n    For more information, see [Prepare to upgrade](/release-notes#prepare-to-upgrade).\n\n    </details>\n\n4. Start Langflow:\n\n    ```bash\n    uv run langflow run\n    ```\n\n    It can take a few minutes for Langflow to start.\n\n5. To confirm that a local Langflow instance is running, navigate to the default Langflow URL `http://127.0.0.1:7860`.\n\n6. Create your first flow with the [Quickstart](/get-started-quickstart).\n\nFor upgrade information, see the [Release notes](/release-notes).\n\nFor information about optional dependency groups and support for custom dependencies to extend Langflow OSS functionality, see [Install custom dependencies](/install-custom-dependencies).\n\n## Next steps\n\n* [Quickstart](/get-started-quickstart): Build and run your first flow in minutes.\n* [Build flows](/concepts-flows): Learn about building flows.\n* [Troubleshoot Langflow](/troubleshoot): Get help with common Langflow install and startup issues.",
      "summary": "Langflow can be installed in multiple ways:",
      "keywords": [
        "desktop",
        "docker",
        "install",
        "langflow",
        "mode",
        "next",
        "oss",
        "package",
        "playground",
        "python",
        "recommended",
        "run",
        "shareable",
        "source",
        "steps",
        "voice"
      ]
    },
    "get-started-installation": {
      "title": "Install Langflow",
      "slug": "/get-started-installation",
      "category": "get_started",
      "content": "Langflow can be installed in multiple ways:\n\n* [**Langflow Desktop (Recommended)**](#install-and-run-langflow-desktop): Download and install the standalone desktop application for the least complicated setup experience.\nThis option includes dependency management and facilitated upgrades.\n\n* [**Docker**](#install-and-run-langflow-docker): Pull and run the Langflow Docker image to start a Langflow container and run Langflow in isolation.\n\n* [**Python package**](#install-and-run-the-langflow-oss-python-package): Install and run the Langflow OSS Python package.\nThis option offers more control over the environment, dependencies, and versioning.\n\n* [**Install from source**](/contributing-how-to-contribute#install-langflow-from-source): Use this option if you want to contribute to the Langflow codebase or documentation.\n\n## Install and run Langflow Desktop\n\nLangflow Desktop is a desktop version of Langflow that simplifies dependency management and upgrades.\nHowever, some features aren't available for Langflow Desktop, such as the **Shareable Playground** and **Voice Mode**.\n\n</Tabs>\n\nFor upgrade information, see the [Release notes](/release-notes).\n\nTo manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).\n\n## Install and run Langflow with Docker {#install-and-run-langflow-docker}\n\nYou can use the Langflow Docker image to start a Langflow container.\nFor more information, see [Deploy Langflow on Docker](/deployment-docker).\n\n1. Install and start [Docker](https://docs.docker.com/).\n\n2. Pull the latest [Langflow Docker image](https://hub.docker.com/r/langflowai/langflow) and start it:\n\n    ```bash\n    docker run -p 7860:7860 langflowai/langflow:latest\n    ```\n\n3. To access Langflow, navigate to `http://localhost:7860/`.\n\n4. Create your first flow with the [Quickstart](/get-started-quickstart).\n\n## Install and run the Langflow OSS Python package\n\n1. Make sure you have the required dependencies and infrastructure:\n\n    - [Python](https://www.python.org/downloads/release/python-3100/)\n       - macOS and Linux: Version 3.10 to 3.13\n       - Windows: Version 3.10 to 3.12\n    - [uv](https://docs.astral.sh/uv/getting-started/installation/)\n    - Sufficient infrastructure:\n       - Minimum: Dual-core CPU and 2 GB RAM\n       - Recommended: Multi-core CPU and at least 4 GB RAM\n\n2. Create a virtual environment with [uv](https://docs.astral.sh/uv/pip/environments).\n\n    <details>\n    <summary>Need help with virtual environments?</summary>\n\n    Virtual environments ensure Langflow is installed in an isolated, fresh environment.\n    To create a new virtual environment, do the following.\n\n        \n        \n        </Tabs>\n\n    </details>\n\n3. In your virtual environment, install Langflow:\n\n    ```bash\n    uv pip install langflow\n    ```\n\n    To install a specific version of the Langflow package, add the required version to the command, such as `uv pip install langflow==1.4.22`.\n\n    <details>\n    <summary>Reinstall or upgrade Langflow</summary>\n\n    To reinstall Langflow and all of its dependencies, run `uv pip install langflow --force-reinstall`.\n\n    To upgrade Langflow to the latest version, run `uv pip install langflow -U`.\n    However, the Langflow team recommends taking steps to backup your existing installation before you upgrade Langflow.\n    For more information, see [Prepare to upgrade](/release-notes#prepare-to-upgrade).\n\n    </details>\n\n4. Start Langflow:\n\n    ```bash\n    uv run langflow run\n    ```\n\n    It can take a few minutes for Langflow to start.\n\n5. To confirm that a local Langflow instance is running, navigate to the default Langflow URL `http://127.0.0.1:7860`.\n\n6. Create your first flow with the [Quickstart](/get-started-quickstart).\n\nFor upgrade information, see the [Release notes](/release-notes).\n\nFor information about optional dependency groups and support for custom dependencies to extend Langflow OSS functionality, see [Install custom dependencies](/install-custom-dependencies).\n\n## Next steps\n\n* [Quickstart](/get-started-quickstart): Build and run your first flow in minutes.\n* [Build flows](/concepts-flows): Learn about building flows.\n* [Troubleshoot Langflow](/troubleshoot): Get help with common Langflow install and startup issues.",
      "summary": "Langflow can be installed in multiple ways:",
      "keywords": [
        "desktop",
        "docker",
        "install",
        "langflow",
        "mode",
        "next",
        "oss",
        "package",
        "playground",
        "python",
        "recommended",
        "run",
        "shareable",
        "source",
        "steps",
        "voice"
      ]
    },
    "/get-started-quickstart": {
      "title": "Quickstart",
      "slug": "/get-started-quickstart",
      "category": "get_started",
      "content": "Get started with Langflow by loading a template flow, running it, and then serving it at the `/run` API endpoint.\n\n## Prerequisites\n\n- [Install and start Langflow](/get-started-installation)\n- Create an [OpenAI API key](https://platform.openai.com/api-keys)\n- Create a [Langflow API key](/api-keys-and-authentication)\n\n    <details>\n    <summary>Create a Langflow API key</summary>\n\n    A Langflow API key is a user-specific token you can use with Langflow.\n\n    To create a Langflow API key, do the following:\n\n    1. In Langflow, click your user icon, and then select **Settings**.\n    2. Click **Langflow API Keys**, and then click  **Add New**.\n    3. Name your key, and then click **Create API Key**.\n    4. Copy the API key and store it securely.\n    5. To use your Langflow API key in a request, set a `LANGFLOW_API_KEY` environment variable in your terminal, and then include an `x-api-key` header or query parameter with your request.\n    For example:\n\n        ```bash\n        # Set variable\n        export LANGFLOW_API_KEY=\"sk...\"\n\n        # Send request\n        curl --request POST \\\n        --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n        --header \"Content-Type: application/json\" \\\n        --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n        --data '{\n          \"output_type\": \"chat\",\n          \"input_type\": \"chat\",\n          \"input_value\": \"Hello\"\n        }'\n        ```\n\n    </details>\n\n## Run the Simple Agent template flow\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n\n![Simple Agent template](/img/quickstart-simple-agent-flow.png)\n\nThe **Simple Agent** template consists of an [**Agent** component](/agents) connected to [**Chat Input** and **Chat Output** components](/components-io), a [**Calculator** component](/components-helpers#calculator), and a [**URL** component](/components-data#url). When you run this flow, you submit a query to the agent through the **Chat Input** component, the agent uses the **Calculator** and **URL** tools to generate a response, and then returns the response through the **Chat Output** component.\n\nMany components can be tools for agents, including [Model Context Protocol (MCP) servers](/mcp-server). The agent decides which tools to call based on the context of a given query.\n\n2. In the **Agent** component, enter your OpenAI API key directly or use a  [global variable](/configuration-global-variables).\n\n    This example uses the **Agent** component's built-in OpenAI model.\n    If you want to use a different provider, edit the model provider, model name, and credentials accordingly.\n    If your preferred provider or model isn't listed, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models#additional-language-models).\n\n3. To run the flow, click  **Playground**.\n\n4. To test the **Calculator** tool, ask the agent a simple math question, such as `I want to add 4 and 4.`\nTo help you test and evaluate your flows, the **Playground** shows the agent's reasoning process as it analyzes the prompt, selects a tool, and then uses the tool to generate a response.\nIn this case, a math question causes the agent to select the **Calculator** tool and use an action like `evaluate_expression`.\n\n![Playground with Agent tool](/img/quickstart-simple-agent-playground.png)\n\n5. To test the **URL** tool, ask the agent about current events.\nFor this request, the agent selects the **URL** tool's `fetch_content` action, and then returns a summary of current news headlines.\n\n6. When you are done testing the flow, click **Close**.\n\n**Note:** Next steps\nNow that you've run your first flow, try these next steps:\n\n- Edit your **Simple Agent** flow by attaching different tools or adding more [components](/concepts-components) to the flow.\n- [Build your own flows](/concepts-flows) from scratch or by modifying other template flows.\n- Integrate flows into your applications, as explained in [Run your flows from external applications](#run-your-flows-from-external-applications).\n\n## Run your flows from external applications\n\nLangflow is an IDE, but it's also a runtime you can call through the [Langflow API](/api-reference-api-examples) with Python, JavaScript, or HTTP.\n\nWhen you start Langflow locally, you can send requests to the local Langflow server.\nFor production applications, you need to [deploy a stable Langflow instance](/deployment-overview) to handle API calls.\n\nFor example, you can use the `/run` endpoint to run a flow and get the result.\n\nLangflow provides code snippets to help you get started with the Langflow API.\n\n1. When editing a flow, click **Share**, and then click **API access**.\n\n    The default code in the API access pane constructs a request with the Langflow server `url`, `headers`, and a `payload` of request data.\n    The code snippets automatically include the `LANGFLOW_SERVER_ADDRESS` and `FLOW_ID` values for the flow, and a script to include your `LANGFLOW_API_KEY` if you've set it as an environment variable in your terminal session.\n    Replace these values if you're using the code for a different server or flow.\n    The default Langflow server address is `http://localhost:7860`.\n\n    \n    \n    \n    </Tabs>\n\n2. Copy the snippet, paste it in a script file, and then run the script to send the request.\nIf you are using the curl snippet, you can run the command directly in your terminal.\n\nIf the request is successful, the response includes many details about the flow run, including the session ID, inputs, outputs, components, durations, and more.\nThe following is an example of a response from running the **Simple Agent** template flow:\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n  \"outputs\": [\n    {\n      \"inputs\": {\n        \"input_value\": \"hello world!\"\n      },\n      \"outputs\": [\n        {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2025-06-16 19:58:23 UTC\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n                \"text\": \"Hello world!  How can I assist you today?\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": \"Agent-ZOknz\",\n                    \"display_name\": \"Agent\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"bot\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [\n                  {\n                    \"title\": \"Agent Steps\",\n                    \"contents\": [\n                      {\n                        \"type\": \"text\",\n                        \"duration\": 2,\n                        \"header\": {\n                          \"title\": \"Input\",\n                          \"icon\": \"MessageSquare\"\n                        },\n                        \"text\": \"**Input**: hello world!\"\n                      },\n                      {\n                        \"type\": \"text\",\n                        \"duration\": 226,\n                        \"header\": {\n                          \"title\": \"Output\",\n                          \"icon\": \"MessageSquare\"\n                        },\n                        \"text\": \"Hello world!  How can I assist you today?\"\n                      }\n                    ],\n                    \"allow_markdown\": true,\n                    \"media_url\": null\n                  }\n                ],\n                \"id\": \"f3d85d9a-261c-4325-b004-95a1bf5de7ca\",\n                \"flow_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n                \"duration\": null\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello world!  How can I assist you today?\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"files\": [],\n              \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"timestamp\": \"2025-06-16T19:58:23+00:00\",\n              \"flow_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"error\": false,\n              \"edit\": false,\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": false,\n                \"source\": {\n                  \"id\": \"Agent-ZOknz\",\n                  \"display_name\": \"Agent\",\n                  \"source\": \"gpt-4o-mini\"\n                },\n                \"icon\": \"bot\",\n                \"allow_markdown\": false,\n                \"positive_feedback\": null,\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": [\n                {\n                  \"title\": \"Agent Steps\",\n                  \"contents\": [\n                    {\n                      \"type\": \"text\",\n                      \"duration\": 2,\n                      \"header\": {\n                        \"title\": \"Input\",\n                        \"icon\": \"MessageSquare\"\n                      },\n                      \"text\": \"**Input**: hello world!\"\n                    },\n                    {\n                      \"type\": \"text\",\n                      \"duration\": 226,\n                      \"header\": {\n                        \"title\": \"Output\",\n                        \"icon\": \"MessageSquare\"\n                      },\n                      \"text\": \"Hello world!  How can I assist you today?\"\n                    }\n                  ],\n                  \"allow_markdown\": true,\n                  \"media_url\": null\n                }\n              ],\n              \"duration\": null\n            }\n          },\n          \"artifacts\": {\n            \"message\": \"Hello world!  How can I assist you today?\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": \"Hello world!  How can I assist you today?\",\n              \"type\": \"text\"\n            }\n          },\n          \"logs\": {\n            \"message\": []\n          },\n          \"messages\": [\n            {\n              \"message\": \"Hello world!  How can I assist you today?\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"stream_url\": null,\n              \"component_id\": \"ChatOutput-aF5lw\",\n              \"files\": [],\n              \"type\": \"text\"\n            }\n          ],\n          \"timedelta\": null,\n          \"duration\": null,\n          \"component_display_name\": \"Chat Output\",\n          \"component_id\": \"ChatOutput-aF5lw\",\n          \"used_frozen_result\": false\n        }\n      ]\n    }\n  ]\n}\n```\n\n</details>\n\nIn a production application, you probably want to select parts of this response to return to the user, store in logs, and so on. The next steps demonstrate how you can extract data from a Langflow API response to use in your application.\n\n### Extract data from the response\n\nThe following example builds on the API pane's example code to create a question-and-answer chat in your terminal that stores the agent's previous answer.\n\n1. Incorporate your **Simple Agent** flow's `/run` snippet into the following script.\nThis script runs a question-and-answer chat in your terminal and stores the agent's previous answer so you can compare them.\n\n    \n    \n    </Tabs>\n\n2. To view the agent's previous answer, type `compare`. To close the terminal chat, type `exit`.\n\n### Use tweaks to apply temporary overrides to a flow run\n\nYou can include tweaks with your requests to temporarily modify flow parameters.\nTweaks are added to the API request, and temporarily change component parameters within your flow.\nTweaks override the flow's components' settings for a single run only.\nThey don't modify the underlying flow configuration or persist between runs.\n\nTweaks are added to the `/run` endpoint's `payload`.\nTo assist with formatting, you can define tweaks in Langflow's **Input Schema** pane before copying the code snippet.\n\n1. To open the **Input Schema** pane, from the **API access** pane, click **Input Schema**.\n2. In the **Input Schema** pane, select the parameter you want to modify in your next request.\nEnabling parameters in the **Input Schema** pane doesn't permanently change the listed parameters. It only adds them to the sample code snippets.\n3. For example, to change the LLM provider from OpenAI to Groq, and include your Groq API key with the request, select the values **Model Providers**, **Model**, and **Groq API Key**.\nLangflow updates the `tweaks` object in the code snippets based on your input parameters, and includes default values to guide you.\nUse the updated code snippets in your script to run your flow with your overrides.\n\n```json\npayload = {\n    \"output_type\": \"chat\",\n    \"input_type\": \"chat\",\n    \"input_value\": \"hello world!\",\n    \"tweaks\": {\n        \"Agent-ZOknz\": {\n            \"agent_llm\": \"Groq\",\n            \"api_key\": \"GROQ_API_KEY\",\n            \"model_name\": \"llama-3.1-8b-instant\"\n        }\n    }\n}\n```\n\n## Next steps\n\n* [Trigger flows with the Langflow API](/concepts-publish)\n* [Use Langflow as a Model Context Protocol (MCP) server](/mcp-server)\n* [Containerize a Langflow application](/develop-application)\n* [File management](/concepts-file-management)",
      "summary": "Get started with Langflow by loading a template flow, running it, and then serving it at the `/run` API endpoint.",
      "keywords": [
        "access",
        "add",
        "agent",
        "api",
        "applications",
        "apply",
        "calculator",
        "chat",
        "close",
        "connect",
        "create",
        "data",
        "external",
        "extract",
        "flow",
        "flows",
        "groq",
        "input",
        "key",
        "keys"
      ]
    },
    "get-started-quickstart": {
      "title": "Quickstart",
      "slug": "/get-started-quickstart",
      "category": "get_started",
      "content": "Get started with Langflow by loading a template flow, running it, and then serving it at the `/run` API endpoint.\n\n## Prerequisites\n\n- [Install and start Langflow](/get-started-installation)\n- Create an [OpenAI API key](https://platform.openai.com/api-keys)\n- Create a [Langflow API key](/api-keys-and-authentication)\n\n    <details>\n    <summary>Create a Langflow API key</summary>\n\n    A Langflow API key is a user-specific token you can use with Langflow.\n\n    To create a Langflow API key, do the following:\n\n    1. In Langflow, click your user icon, and then select **Settings**.\n    2. Click **Langflow API Keys**, and then click  **Add New**.\n    3. Name your key, and then click **Create API Key**.\n    4. Copy the API key and store it securely.\n    5. To use your Langflow API key in a request, set a `LANGFLOW_API_KEY` environment variable in your terminal, and then include an `x-api-key` header or query parameter with your request.\n    For example:\n\n        ```bash\n        # Set variable\n        export LANGFLOW_API_KEY=\"sk...\"\n\n        # Send request\n        curl --request POST \\\n        --url \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\" \\\n        --header \"Content-Type: application/json\" \\\n        --header \"x-api-key: $LANGFLOW_API_KEY\" \\\n        --data '{\n          \"output_type\": \"chat\",\n          \"input_type\": \"chat\",\n          \"input_value\": \"Hello\"\n        }'\n        ```\n\n    </details>\n\n## Run the Simple Agent template flow\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n\n![Simple Agent template](/img/quickstart-simple-agent-flow.png)\n\nThe **Simple Agent** template consists of an [**Agent** component](/agents) connected to [**Chat Input** and **Chat Output** components](/components-io), a [**Calculator** component](/components-helpers#calculator), and a [**URL** component](/components-data#url). When you run this flow, you submit a query to the agent through the **Chat Input** component, the agent uses the **Calculator** and **URL** tools to generate a response, and then returns the response through the **Chat Output** component.\n\nMany components can be tools for agents, including [Model Context Protocol (MCP) servers](/mcp-server). The agent decides which tools to call based on the context of a given query.\n\n2. In the **Agent** component, enter your OpenAI API key directly or use a  [global variable](/configuration-global-variables).\n\n    This example uses the **Agent** component's built-in OpenAI model.\n    If you want to use a different provider, edit the model provider, model name, and credentials accordingly.\n    If your preferred provider or model isn't listed, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models#additional-language-models).\n\n3. To run the flow, click  **Playground**.\n\n4. To test the **Calculator** tool, ask the agent a simple math question, such as `I want to add 4 and 4.`\nTo help you test and evaluate your flows, the **Playground** shows the agent's reasoning process as it analyzes the prompt, selects a tool, and then uses the tool to generate a response.\nIn this case, a math question causes the agent to select the **Calculator** tool and use an action like `evaluate_expression`.\n\n![Playground with Agent tool](/img/quickstart-simple-agent-playground.png)\n\n5. To test the **URL** tool, ask the agent about current events.\nFor this request, the agent selects the **URL** tool's `fetch_content` action, and then returns a summary of current news headlines.\n\n6. When you are done testing the flow, click **Close**.\n\n**Note:** Next steps\nNow that you've run your first flow, try these next steps:\n\n- Edit your **Simple Agent** flow by attaching different tools or adding more [components](/concepts-components) to the flow.\n- [Build your own flows](/concepts-flows) from scratch or by modifying other template flows.\n- Integrate flows into your applications, as explained in [Run your flows from external applications](#run-your-flows-from-external-applications).\n\n## Run your flows from external applications\n\nLangflow is an IDE, but it's also a runtime you can call through the [Langflow API](/api-reference-api-examples) with Python, JavaScript, or HTTP.\n\nWhen you start Langflow locally, you can send requests to the local Langflow server.\nFor production applications, you need to [deploy a stable Langflow instance](/deployment-overview) to handle API calls.\n\nFor example, you can use the `/run` endpoint to run a flow and get the result.\n\nLangflow provides code snippets to help you get started with the Langflow API.\n\n1. When editing a flow, click **Share**, and then click **API access**.\n\n    The default code in the API access pane constructs a request with the Langflow server `url`, `headers`, and a `payload` of request data.\n    The code snippets automatically include the `LANGFLOW_SERVER_ADDRESS` and `FLOW_ID` values for the flow, and a script to include your `LANGFLOW_API_KEY` if you've set it as an environment variable in your terminal session.\n    Replace these values if you're using the code for a different server or flow.\n    The default Langflow server address is `http://localhost:7860`.\n\n    \n    \n    \n    </Tabs>\n\n2. Copy the snippet, paste it in a script file, and then run the script to send the request.\nIf you are using the curl snippet, you can run the command directly in your terminal.\n\nIf the request is successful, the response includes many details about the flow run, including the session ID, inputs, outputs, components, durations, and more.\nThe following is an example of a response from running the **Simple Agent** template flow:\n\n<details>\n<summary>Result</summary>\n\n```json\n{\n  \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n  \"outputs\": [\n    {\n      \"inputs\": {\n        \"input_value\": \"hello world!\"\n      },\n      \"outputs\": [\n        {\n          \"results\": {\n            \"message\": {\n              \"text_key\": \"text\",\n              \"data\": {\n                \"timestamp\": \"2025-06-16 19:58:23 UTC\",\n                \"sender\": \"Machine\",\n                \"sender_name\": \"AI\",\n                \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n                \"text\": \"Hello world!  How can I assist you today?\",\n                \"files\": [],\n                \"error\": false,\n                \"edit\": false,\n                \"properties\": {\n                  \"text_color\": \"\",\n                  \"background_color\": \"\",\n                  \"edited\": false,\n                  \"source\": {\n                    \"id\": \"Agent-ZOknz\",\n                    \"display_name\": \"Agent\",\n                    \"source\": \"gpt-4o-mini\"\n                  },\n                  \"icon\": \"bot\",\n                  \"allow_markdown\": false,\n                  \"positive_feedback\": null,\n                  \"state\": \"complete\",\n                  \"targets\": []\n                },\n                \"category\": \"message\",\n                \"content_blocks\": [\n                  {\n                    \"title\": \"Agent Steps\",\n                    \"contents\": [\n                      {\n                        \"type\": \"text\",\n                        \"duration\": 2,\n                        \"header\": {\n                          \"title\": \"Input\",\n                          \"icon\": \"MessageSquare\"\n                        },\n                        \"text\": \"**Input**: hello world!\"\n                      },\n                      {\n                        \"type\": \"text\",\n                        \"duration\": 226,\n                        \"header\": {\n                          \"title\": \"Output\",\n                          \"icon\": \"MessageSquare\"\n                        },\n                        \"text\": \"Hello world!  How can I assist you today?\"\n                      }\n                    ],\n                    \"allow_markdown\": true,\n                    \"media_url\": null\n                  }\n                ],\n                \"id\": \"f3d85d9a-261c-4325-b004-95a1bf5de7ca\",\n                \"flow_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n                \"duration\": null\n              },\n              \"default_value\": \"\",\n              \"text\": \"Hello world!  How can I assist you today?\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"files\": [],\n              \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"timestamp\": \"2025-06-16T19:58:23+00:00\",\n              \"flow_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"error\": false,\n              \"edit\": false,\n              \"properties\": {\n                \"text_color\": \"\",\n                \"background_color\": \"\",\n                \"edited\": false,\n                \"source\": {\n                  \"id\": \"Agent-ZOknz\",\n                  \"display_name\": \"Agent\",\n                  \"source\": \"gpt-4o-mini\"\n                },\n                \"icon\": \"bot\",\n                \"allow_markdown\": false,\n                \"positive_feedback\": null,\n                \"state\": \"complete\",\n                \"targets\": []\n              },\n              \"category\": \"message\",\n              \"content_blocks\": [\n                {\n                  \"title\": \"Agent Steps\",\n                  \"contents\": [\n                    {\n                      \"type\": \"text\",\n                      \"duration\": 2,\n                      \"header\": {\n                        \"title\": \"Input\",\n                        \"icon\": \"MessageSquare\"\n                      },\n                      \"text\": \"**Input**: hello world!\"\n                    },\n                    {\n                      \"type\": \"text\",\n                      \"duration\": 226,\n                      \"header\": {\n                        \"title\": \"Output\",\n                        \"icon\": \"MessageSquare\"\n                      },\n                      \"text\": \"Hello world!  How can I assist you today?\"\n                    }\n                  ],\n                  \"allow_markdown\": true,\n                  \"media_url\": null\n                }\n              ],\n              \"duration\": null\n            }\n          },\n          \"artifacts\": {\n            \"message\": \"Hello world!  How can I assist you today?\",\n            \"sender\": \"Machine\",\n            \"sender_name\": \"AI\",\n            \"files\": [],\n            \"type\": \"object\"\n          },\n          \"outputs\": {\n            \"message\": {\n              \"message\": \"Hello world!  How can I assist you today?\",\n              \"type\": \"text\"\n            }\n          },\n          \"logs\": {\n            \"message\": []\n          },\n          \"messages\": [\n            {\n              \"message\": \"Hello world!  How can I assist you today?\",\n              \"sender\": \"Machine\",\n              \"sender_name\": \"AI\",\n              \"session_id\": \"29deb764-af3f-4d7d-94a0-47491ed241d6\",\n              \"stream_url\": null,\n              \"component_id\": \"ChatOutput-aF5lw\",\n              \"files\": [],\n              \"type\": \"text\"\n            }\n          ],\n          \"timedelta\": null,\n          \"duration\": null,\n          \"component_display_name\": \"Chat Output\",\n          \"component_id\": \"ChatOutput-aF5lw\",\n          \"used_frozen_result\": false\n        }\n      ]\n    }\n  ]\n}\n```\n\n</details>\n\nIn a production application, you probably want to select parts of this response to return to the user, store in logs, and so on. The next steps demonstrate how you can extract data from a Langflow API response to use in your application.\n\n### Extract data from the response\n\nThe following example builds on the API pane's example code to create a question-and-answer chat in your terminal that stores the agent's previous answer.\n\n1. Incorporate your **Simple Agent** flow's `/run` snippet into the following script.\nThis script runs a question-and-answer chat in your terminal and stores the agent's previous answer so you can compare them.\n\n    \n    \n    </Tabs>\n\n2. To view the agent's previous answer, type `compare`. To close the terminal chat, type `exit`.\n\n### Use tweaks to apply temporary overrides to a flow run\n\nYou can include tweaks with your requests to temporarily modify flow parameters.\nTweaks are added to the API request, and temporarily change component parameters within your flow.\nTweaks override the flow's components' settings for a single run only.\nThey don't modify the underlying flow configuration or persist between runs.\n\nTweaks are added to the `/run` endpoint's `payload`.\nTo assist with formatting, you can define tweaks in Langflow's **Input Schema** pane before copying the code snippet.\n\n1. To open the **Input Schema** pane, from the **API access** pane, click **Input Schema**.\n2. In the **Input Schema** pane, select the parameter you want to modify in your next request.\nEnabling parameters in the **Input Schema** pane doesn't permanently change the listed parameters. It only adds them to the sample code snippets.\n3. For example, to change the LLM provider from OpenAI to Groq, and include your Groq API key with the request, select the values **Model Providers**, **Model**, and **Groq API Key**.\nLangflow updates the `tweaks` object in the code snippets based on your input parameters, and includes default values to guide you.\nUse the updated code snippets in your script to run your flow with your overrides.\n\n```json\npayload = {\n    \"output_type\": \"chat\",\n    \"input_type\": \"chat\",\n    \"input_value\": \"hello world!\",\n    \"tweaks\": {\n        \"Agent-ZOknz\": {\n            \"agent_llm\": \"Groq\",\n            \"api_key\": \"GROQ_API_KEY\",\n            \"model_name\": \"llama-3.1-8b-instant\"\n        }\n    }\n}\n```\n\n## Next steps\n\n* [Trigger flows with the Langflow API](/concepts-publish)\n* [Use Langflow as a Model Context Protocol (MCP) server](/mcp-server)\n* [Containerize a Langflow application](/develop-application)\n* [File management](/concepts-file-management)",
      "summary": "Get started with Langflow by loading a template flow, running it, and then serving it at the `/run` API endpoint.",
      "keywords": [
        "access",
        "add",
        "agent",
        "api",
        "applications",
        "apply",
        "calculator",
        "chat",
        "close",
        "connect",
        "create",
        "data",
        "external",
        "extract",
        "flow",
        "flows",
        "groq",
        "input",
        "key",
        "keys"
      ]
    },
    "/contributing-github-issues": {
      "title": "Get help and request enhancements",
      "slug": "/contributing-github-issues",
      "category": "support",
      "content": "The Langflow GitHub repository is an integral part of the [Langflow community](/contributing-community).\n\nIn addition to general assistance with Langflow, the repository is the best place to report bugs and request enhancements to ensure that they are tracked by Langflow maintainers.\n\n## GitHub issues\n\nThe [Issues page in the Langflow repository](https://github.com/langflow-ai/langflow/issues) is actively updated with bugs and feature requests.\n\n**Note:** The best way to promote a request or bug is to comment on an existing issue.\nHighly active issues are more likely to receive attention from contributors.\n\nBefore you report a bug or submit a feature request, search for existing similar issues.\nUse the [Langflow repository's labels](https://github.com/langflow-ai/langflow/labels) to help filter your search.\n\n## GitHub discussions\n\nIf you need help with your code or Langflow in general, you can visit the [Langflow GitHub Discussions page](https://github.com/langflow-ai/langflow/discussions) or reach out through other [Langflow community](/contributing-community) channels.\n\nThe Langflow team doesn't provide individual support over email, and the team believes that public discussions help more users by virtue of their discoverability.\n\n## Community guidelines and tips\n\nBecause the Issues and Discussion pages are public, the Langflow team asks that you follow these guidelines when submitting questions and issues:\n\n* **Provide as many details as possible**: Simply stating that a feature doesn't work isn't helpful. The Langflow team needs details in order to recreate and find the issue.\n* **Explain what exactly went wrong**: Including error messages and descriptions of _how_ your code failed, not just the fact that it failed.\n* **Retrace your steps**: Explain what happened before the error, what you expected to happen instead of the error, and any recent changes you made, such as upgrading Langflow or a dependency.\n* **Describe your environment**: Include your operating system, Langflow version, Python version, and any other environment-related details that could have contributed to the issue.\n* **Include snippets of the code that failed**: Be sure to omit any sensitive values, and only provide parts relevant to the failure, rather than the entire script. Providing code snippets makes it much easier to reproduce errors, troubleshoot, and provide specific advice.\n  * If your submission includes long sections of code, logs, or tracebacks, wrap them in [details tags](https://developer.mozilla.org/en/docs/Web/HTML/Element/details) (`<details> PASTE CODE HERE </details>`) to collapse the content and make it easier to read your submission.\n* **Omit sensitive information**: Other than the information available on your public GitHub profile, don't include sensitive or personally identifying data, such as security keys, full names, personal identification numbers, addresses, and phone numbers.\n* **Be kind**: Although bugs can be frustrating with any software, remember that your messages are read by real people who want to help. While you don't have to be saccharine, there's no need to be rude to get support.\n  * Your issues and discussions are attached to your GitHub account, and they can be read by anyone on the internet, including current and potential employers and colleagues.\n  * The Langlow repository is a public GitHub repository and, therefore, subject to the [GitHub Code of Conduct](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct).",
      "summary": "The Langflow GitHub repository is an integral part of the [Langflow community](/contributing-community).",
      "keywords": [
        "code",
        "community",
        "describe",
        "details",
        "discussions",
        "enhancements",
        "environment",
        "exactly",
        "explain",
        "failed",
        "get",
        "github",
        "guidelines",
        "help",
        "include",
        "issues",
        "kind",
        "many",
        "note",
        "omit"
      ]
    },
    "contributing-github-issues": {
      "title": "Get help and request enhancements",
      "slug": "/contributing-github-issues",
      "category": "support",
      "content": "The Langflow GitHub repository is an integral part of the [Langflow community](/contributing-community).\n\nIn addition to general assistance with Langflow, the repository is the best place to report bugs and request enhancements to ensure that they are tracked by Langflow maintainers.\n\n## GitHub issues\n\nThe [Issues page in the Langflow repository](https://github.com/langflow-ai/langflow/issues) is actively updated with bugs and feature requests.\n\n**Note:** The best way to promote a request or bug is to comment on an existing issue.\nHighly active issues are more likely to receive attention from contributors.\n\nBefore you report a bug or submit a feature request, search for existing similar issues.\nUse the [Langflow repository's labels](https://github.com/langflow-ai/langflow/labels) to help filter your search.\n\n## GitHub discussions\n\nIf you need help with your code or Langflow in general, you can visit the [Langflow GitHub Discussions page](https://github.com/langflow-ai/langflow/discussions) or reach out through other [Langflow community](/contributing-community) channels.\n\nThe Langflow team doesn't provide individual support over email, and the team believes that public discussions help more users by virtue of their discoverability.\n\n## Community guidelines and tips\n\nBecause the Issues and Discussion pages are public, the Langflow team asks that you follow these guidelines when submitting questions and issues:\n\n* **Provide as many details as possible**: Simply stating that a feature doesn't work isn't helpful. The Langflow team needs details in order to recreate and find the issue.\n* **Explain what exactly went wrong**: Including error messages and descriptions of _how_ your code failed, not just the fact that it failed.\n* **Retrace your steps**: Explain what happened before the error, what you expected to happen instead of the error, and any recent changes you made, such as upgrading Langflow or a dependency.\n* **Describe your environment**: Include your operating system, Langflow version, Python version, and any other environment-related details that could have contributed to the issue.\n* **Include snippets of the code that failed**: Be sure to omit any sensitive values, and only provide parts relevant to the failure, rather than the entire script. Providing code snippets makes it much easier to reproduce errors, troubleshoot, and provide specific advice.\n  * If your submission includes long sections of code, logs, or tracebacks, wrap them in [details tags](https://developer.mozilla.org/en/docs/Web/HTML/Element/details) (`<details> PASTE CODE HERE </details>`) to collapse the content and make it easier to read your submission.\n* **Omit sensitive information**: Other than the information available on your public GitHub profile, don't include sensitive or personally identifying data, such as security keys, full names, personal identification numbers, addresses, and phone numbers.\n* **Be kind**: Although bugs can be frustrating with any software, remember that your messages are read by real people who want to help. While you don't have to be saccharine, there's no need to be rude to get support.\n  * Your issues and discussions are attached to your GitHub account, and they can be read by anyone on the internet, including current and potential employers and colleagues.\n  * The Langlow repository is a public GitHub repository and, therefore, subject to the [GitHub Code of Conduct](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct).",
      "summary": "The Langflow GitHub repository is an integral part of the [Langflow community](/contributing-community).",
      "keywords": [
        "code",
        "community",
        "describe",
        "details",
        "discussions",
        "enhancements",
        "environment",
        "exactly",
        "explain",
        "failed",
        "get",
        "github",
        "guidelines",
        "help",
        "include",
        "issues",
        "kind",
        "many",
        "note",
        "omit"
      ]
    },
    "/luna-for-langflow": {
      "title": "IBM Elite Support for Langflow",
      "slug": "/luna-for-langflow",
      "category": "support",
      "content": "IBM Elite Support offers enterprise support for Langflow to help you develop and deploy Langflow applications with confidence.\n\nIBM Elite Support for Langflow is a subscription to the Langflow expertise at IBM.\nIt's meant for Langflow users who want all the benefits of running their own open-source deployments, as well as the peace of mind that comes with having direct access to the team that has authored the majority of the Langflow code.\n\nIBM Elite Support subscribers can get help with general-purpose and technical questions for their open-source Langflow deployments.\nIf an issue is encountered, IBM is there to help.\n\nTo subscribe or learn more, see [IBM Elite Support for Langflow](https://ibm.com/docs/esfl?topic=elite-support-langflow-specification).",
      "summary": "IBM Elite Support offers enterprise support for Langflow to help you develop and deploy Langflow applications with confidence.",
      "keywords": [
        "elite",
        "ibm",
        "langflow",
        "support"
      ]
    },
    "luna-for-langflow": {
      "title": "IBM Elite Support for Langflow",
      "slug": "/luna-for-langflow",
      "category": "support",
      "content": "IBM Elite Support offers enterprise support for Langflow to help you develop and deploy Langflow applications with confidence.\n\nIBM Elite Support for Langflow is a subscription to the Langflow expertise at IBM.\nIt's meant for Langflow users who want all the benefits of running their own open-source deployments, as well as the peace of mind that comes with having direct access to the team that has authored the majority of the Langflow code.\n\nIBM Elite Support subscribers can get help with general-purpose and technical questions for their open-source Langflow deployments.\nIf an issue is encountered, IBM is there to help.\n\nTo subscribe or learn more, see [IBM Elite Support for Langflow](https://ibm.com/docs/esfl?topic=elite-support-langflow-specification).",
      "summary": "IBM Elite Support offers enterprise support for Langflow to help you develop and deploy Langflow applications with confidence.",
      "keywords": [
        "elite",
        "ibm",
        "langflow",
        "support"
      ]
    },
    "/release-notes": {
      "title": "Langflow release notes",
      "slug": "/release-notes",
      "category": "support",
      "content": "This page summarizes significant changes to Langflow in each release.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).\n\nDue to strict SemVer requirements, Langflow Desktop can have different patch versions than the core Langflow OSS Python package, but the major and minor versions are aligned.\n\n## Prepare to upgrade\n\n**Note:** Whenever possible, the Langflow team recommends installing new Langflow versions in a new virtual environment or VM before upgrading your primary installation.\nThis allows you to [import flows](/concepts-flows-import#import-a-flow) from your existing installation and test them in the new version without disrupting your existing installation.\nIn the event of breaking changes or bugs, your existing installation is preserved in a stable state.\nIf you are upgrading Windows Desktop to 1.6.0, don't auto-upgrade with the in-app **Update** button. Instead, follow the instructions in [Known issue: Don't auto-upgrade Windows Desktop](#windows-desktop-update-issue).\n\nTo avoid the impact of potential breaking changes and test new versions, the Langflow team recommends the following upgrade process:\n\n1. Recommended: [Export your projects](/api-projects#export-a-project) to create backups of your flows:\n\n    ```bash\n    curl -X GET \\\n    \"$LANGFLOW_SERVER_URL/api/v1/projects/download/$PROJECT_ID\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n   To export flows from the visual editor, see [Import and export flows](/concepts-flows-import).\n\n2. Install the new version:\n\n   * **Langflow OSS Python package**: Install the new version in a new virtual environment. For instructions, see [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n   * **Langflow Docker image**: Run the new image in a separate container.\n   * **Langflow Desktop**: To upgrade in place, open Langflow Desktop, and then click **Upgrade Available** in the Langflow header. If you want to isolate the new version, you must install Langflow Desktop on a separate physical or virtual machine, and then [import your flows](/concepts-flows-import) to the new installation.\n\n3. [Import your flows](/concepts-flows-import) to test them in the new version, [upgrading components](/concepts-components#component-versions) as needed.\n\n    When upgrading components, you can use the **Create backup flow before updating** option if you didn't previously export your flows.\n\n4. If you installed the new version in isolation, upgrade your primary installation after testing the new version.\n\n    If you made changes to your flows in the isolated installation, you might want to export and import those flows back to your upgraded primary installation so you don't have to repeat the component upgrade process.\n\n## 1.6.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### Known issue, potential security vulnerability: .env file not loaded in versions 1.6.0 through 1.6.3 {#env-file-bug}\n\nLangflow versions 1.6.0 through 1.6.3 have a critical bug where environment variables from `.env` files aren't read.\nThis affects all deployments using environment variables for configuration, including security settings.\n\n**Note:** Potential security vulnerability\nIf your `.env` file includes `AUTO_LOGIN=false`, upgrading to the impacted versions causes Langflow to fall back to default settings, potentially giving all users superuser access immediately upon upgrade.\nAdditionally, database credentials, API keys, and other sensitive configurations can't be loaded from `.env` files.\n\n_Don't_ upgrade to any Langflow version from 1.6.0 through 1.6.3 if you use `.env` files for configuration.\nInstead, upgrade to 1.6.4, which includes a fix for this bug.\n\n### Known issue: Don't auto-upgrade Windows Desktop {#windows-desktop-update-issue}\n\n**Note:** Windows users of Langflow Desktop should **not** use the in-app update feature to upgrade to Langflow version 1.6.0.\nOnly Windows Desktop users upgrading to Langflow version 1.6.0 are affected.\n\nThe **Update** button in Langflow Desktop will not work for Windows users, and may result in data loss.\n\nInstead, download a fresh installation from the [Langflow website](https://langflow.org) when a new version is available.\n\nFollow the instructions below to minimize the risk of losing flows.\n\nThese instructions assume the default path for the Langflow database of `C:\\Users\\YOUR_USERNAME\\AppData\\Roaming\\com.langflow\\data\\database.db` or `C:\\Users\\%YOUR_USERNAME%\\AppData\\Roaming\\com.langflow\\data\\database.db`. Replace `YOUR_USERNAME` with your Windows username.\n\n1. Ensure you have Administrator privileges.\n2. Ensure you have enough disk space for a second installation of Langflow.\n3. Close Langflow, and ensure no Langflow process is running in Task Manager.\n4. The Langflow 1.6.0 installer automatically performs a database backup in a later step, but an additional manual backup provides additional redundancy and is recommended.\nTo manually back up your Langflow database file, do the following:\n   1. In Windows Explorer, navigate to `C:\\Users\\YOUR_USERNAME\\AppData\\Roaming\\com.langflow\\data\\`. Replace `YOUR_USERNAME` with your Windows username.\n   2. Copy the `database.db` at this location, and paste it to a safe location.\n5. Download the Windows installer from the [Langflow website](https://langflow.org).\n6. Run the Windows installer as an Administrator. To run the installer as an Administrator, right-click the executable and select **Run as administrator**.\n7. Follow the Windows installer's guided steps.\nThe Langflow 1.6.0 installer automatically performs a database backup.\nThese steps install Langflow from scratch, and result in two Langflow installations: the previously installed version, and version 1.6.0.\nThis is expected behavior.\n8. Start version 1.6.0 of Langflow, and confirm your flows behave as expected.\n9. **If flows are missing**, restore your flows from your manual backup by doing the following:\n    1. Close Langflow.\n    2. Navigate to your backup location, and copy the `database.db` file.\n    3. Replace the database file in the new installation's `data` directory.\n    4. Start Langflow, and confirm your flows behave as expected.\n    5. After confirmation, uninstall the previous version of Langflow, and keep version 1.6.0.\n\n### Breaking changes\n\n- Authentication enforced for Langflow API requests by default\n\n    In Langflow version 1.6, `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False` by default.\n    This enforces authentication for Langflow API requests while still automatically authenticating all users as superusers in the visual editor.\n\n    **This is a breaking change from 1.5 where both of these environment variables were `true` by default, bypassing all authentication.**\n\n    For temporary backwards compatibility, you can revert to the earlier unauthenticated behavior by setting both variables to `true`.\n    However, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.\n    At that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.\n\n    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).\n\n### New features and enhancements\n\n- OpenAI Responses API compatibility\n\n  Langflow now includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) at `POST /api/v1/responses`.\n  This allows you to use existing OpenAI client libraries with minimal code changes by replacing the `model` name with your `flow_id`.\n  The endpoint supports streaming responses, conversation continuity, tool call results, and global variable passing through headers.\n  For more information, see [OpenAI Responses API](/api-openai-responses).\n\n- Advanced document parsing with built-in Docling support\n\n  The **File** component supports advanced parsing with the Docling library.\n\n  To make it easier to use the [**Docling** components](/bundles-docling) and the **File** component's new advanced parsing feature, the Docling dependency is now included with Langflow for all operating systems except macOS Intel (x86_64).\n\n  For more information, see [Advanced parsing](/components-data#advanced-parsing).\n\n- Reorganized component menus and visual editor controls\n\n  - The [workspace](/concepts-overview#workspace) sidebar is divided into separate sections for  **Search**,  **Core components**,  [**MCP servers**](/mcp-server),  [**Bundles**](/components-bundle-components), and  **Add Note**.\n  - Lock/unlock controls moved to flow details in [**Projects**](/concepts-flows#projects).\n  - Zoom and help controls moved to the lower-right corner of the workspace.\n  - Vector store components moved to provider-specific  [**Bundles**](/components-bundle-components)\n  - **Serper Google Search API** component moved to the **Serper** bundle\n\n- Increased the default maximum file upload size from `100 MB` to `1024 MB`.\n\n- New integrations and bundles:\n\n  - [Traceloop](/integrations-instana-traceloop)\n  - [Cohere Rerank](/bundles-cohere)\n  - [S3 Bucket Uploader](/bundles-amazon)\n  - [Composio single-service components](/bundles-composio)\n\n### Deprecations\n\n- The **Local DB** component is now in legacy status.\nReplace this component with the [**Chroma DB** component](/bundles-chroma).\n\n## 1.5.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Langflow API requests can require authentication\n\n    To enhance security and ensure proper authentication for automatic login features, Langflow API endpoints now require authentication with a Langflow API key, even when `LANGFLOW_AUTO_LOGIN=True`.\n    This change will be enforced in a future release.\n    For temporary backwards compatibility, this release adds the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable.\n    The default value is `true`, which disables API authentication enforcement.\n    To enforce API authentication, set `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.\n    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).\n\n- Centralized **Language Model** and **Embedding Model** components\n\n    The [**Language Model** component](/components-models) and [**Embedding Model** component](/components-embedding-models) are now core components for your LLM and embeddings flows. They support multiple models and model providers, and allow you to experiment with different models without swapping out single-provider components.\n    Find them in the visual editor in the **Models** category.\n\n    The single-provider components moved to the [**Bundles**](/components-bundle-components) section.\n    You can use them to replace the **Language Model** and **Embedding Model** core components, or connect them to the **Agent** component with the **Connect other models** provider option.\n\n- MCP server one-click installation\n\n    On your Langflow project's **MCP server** page, click **Auto install** to install your Langflow MCP server to MCP clients with just one click.\n    The option to install with a JSON configuration file is available for macOS, Windows, and WSL.\n    For more information, see [Use Langflow as an MCP server](/mcp-server).\n\n- MCP server management\n\n    You can now add, remove, and edit your MCP servers in the **MCP Tools** components and through your Langflow **Settings** page.\n    For more information, see [Use Langflow as an MCP client](/mcp-client).\n\n- Input schema replaces temporary overrides\n\n    The **Input schema** pane replaces the need to manage tweak values in the **API access** pane. When you enable a parameter in the **Input schema** pane, the parameter is automatically added to your flow's code snippets, providing ready-to-use templates for making requests in your preferred programming language.\n\n- Tools components are redistributed\n\n    All components in the **Tools** category were moved to other component categories, such as **Helpers** and [**Bundles**](/components-bundle-components), or marked as legacy.\n\n    The [**MCP Tools** component](/mcp-client) is now under the **Agents** category.\n\n    Tools that performed the same function were combined into single components that support multiple providers, such as the [**Web Search** component](/components-data#web-search) and the [**News Search** component](/components-data#news-search).\n\n    For more information, see [Tools components](/components-tools).\n\n- Stability improvements\n\n    General stability improvements and bug fixes for enhanced reliability.\n    See an issue? [Raise it on GitHub](https://github.com/langflow-ai/langflow/issues).\n\n- New integrations and bundles\n\n   - [**Cleanlab** bundle](/bundles-cleanlab)\n\n## 1.4.2\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Enhanced file and flow management system with improved bulk capabilities.\n- Added the **BigQuery** component\n- Added the **Twelve Labs** bundle\n- Added the **NVIDIA System-Assist** component\n\n### Deprecations\n\n- Deprecated the **Combine Text** component.\n\n## 1.4.1\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Added an enhanced **Breaking Changes** feature to help update components without breaking flows after updating Langflow.\n\n## 1.4.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Introduced MCP server functionality to serve Langflow tools to MCP-compatible clients.\n- Renamed **Folders** to **Projects** in the visual editor.\n- The `/folders` endpoints now redirect to `/projects`.\n\n### Deprecations\n\n- Deprecated the **Gmail**, **Google Drive**, and **Google Search** components.\nFor alternatives, see the [**Google** bundle](/bundles-google).\n\n## Earlier releases\n\nSee the [Changelog](https://github.com/langflow-ai/langflow/releases).",
      "summary": "This page summarizes significant changes to Langflow in each release.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).",
      "keywords": [
        "access",
        "add",
        "administrator",
        "agent",
        "agents",
        "all",
        "api",
        "assist",
        "authentication",
        "auto",
        "available",
        "backup",
        "before",
        "bigquery",
        "both",
        "breaking",
        "bug",
        "bundles",
        "bypassing",
        "change"
      ]
    },
    "release-notes": {
      "title": "Langflow release notes",
      "slug": "/release-notes",
      "category": "support",
      "content": "This page summarizes significant changes to Langflow in each release.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).\n\nDue to strict SemVer requirements, Langflow Desktop can have different patch versions than the core Langflow OSS Python package, but the major and minor versions are aligned.\n\n## Prepare to upgrade\n\n**Note:** Whenever possible, the Langflow team recommends installing new Langflow versions in a new virtual environment or VM before upgrading your primary installation.\nThis allows you to [import flows](/concepts-flows-import#import-a-flow) from your existing installation and test them in the new version without disrupting your existing installation.\nIn the event of breaking changes or bugs, your existing installation is preserved in a stable state.\nIf you are upgrading Windows Desktop to 1.6.0, don't auto-upgrade with the in-app **Update** button. Instead, follow the instructions in [Known issue: Don't auto-upgrade Windows Desktop](#windows-desktop-update-issue).\n\nTo avoid the impact of potential breaking changes and test new versions, the Langflow team recommends the following upgrade process:\n\n1. Recommended: [Export your projects](/api-projects#export-a-project) to create backups of your flows:\n\n    ```bash\n    curl -X GET \\\n    \"$LANGFLOW_SERVER_URL/api/v1/projects/download/$PROJECT_ID\" \\\n      -H \"accept: application/json\" \\\n      -H \"x-api-key: $LANGFLOW_API_KEY\"\n    ```\n\n   To export flows from the visual editor, see [Import and export flows](/concepts-flows-import).\n\n2. Install the new version:\n\n   * **Langflow OSS Python package**: Install the new version in a new virtual environment. For instructions, see [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n   * **Langflow Docker image**: Run the new image in a separate container.\n   * **Langflow Desktop**: To upgrade in place, open Langflow Desktop, and then click **Upgrade Available** in the Langflow header. If you want to isolate the new version, you must install Langflow Desktop on a separate physical or virtual machine, and then [import your flows](/concepts-flows-import) to the new installation.\n\n3. [Import your flows](/concepts-flows-import) to test them in the new version, [upgrading components](/concepts-components#component-versions) as needed.\n\n    When upgrading components, you can use the **Create backup flow before updating** option if you didn't previously export your flows.\n\n4. If you installed the new version in isolation, upgrade your primary installation after testing the new version.\n\n    If you made changes to your flows in the isolated installation, you might want to export and import those flows back to your upgraded primary installation so you don't have to repeat the component upgrade process.\n\n## 1.6.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### Known issue, potential security vulnerability: .env file not loaded in versions 1.6.0 through 1.6.3 {#env-file-bug}\n\nLangflow versions 1.6.0 through 1.6.3 have a critical bug where environment variables from `.env` files aren't read.\nThis affects all deployments using environment variables for configuration, including security settings.\n\n**Note:** Potential security vulnerability\nIf your `.env` file includes `AUTO_LOGIN=false`, upgrading to the impacted versions causes Langflow to fall back to default settings, potentially giving all users superuser access immediately upon upgrade.\nAdditionally, database credentials, API keys, and other sensitive configurations can't be loaded from `.env` files.\n\n_Don't_ upgrade to any Langflow version from 1.6.0 through 1.6.3 if you use `.env` files for configuration.\nInstead, upgrade to 1.6.4, which includes a fix for this bug.\n\n### Known issue: Don't auto-upgrade Windows Desktop {#windows-desktop-update-issue}\n\n**Note:** Windows users of Langflow Desktop should **not** use the in-app update feature to upgrade to Langflow version 1.6.0.\nOnly Windows Desktop users upgrading to Langflow version 1.6.0 are affected.\n\nThe **Update** button in Langflow Desktop will not work for Windows users, and may result in data loss.\n\nInstead, download a fresh installation from the [Langflow website](https://langflow.org) when a new version is available.\n\nFollow the instructions below to minimize the risk of losing flows.\n\nThese instructions assume the default path for the Langflow database of `C:\\Users\\YOUR_USERNAME\\AppData\\Roaming\\com.langflow\\data\\database.db` or `C:\\Users\\%YOUR_USERNAME%\\AppData\\Roaming\\com.langflow\\data\\database.db`. Replace `YOUR_USERNAME` with your Windows username.\n\n1. Ensure you have Administrator privileges.\n2. Ensure you have enough disk space for a second installation of Langflow.\n3. Close Langflow, and ensure no Langflow process is running in Task Manager.\n4. The Langflow 1.6.0 installer automatically performs a database backup in a later step, but an additional manual backup provides additional redundancy and is recommended.\nTo manually back up your Langflow database file, do the following:\n   1. In Windows Explorer, navigate to `C:\\Users\\YOUR_USERNAME\\AppData\\Roaming\\com.langflow\\data\\`. Replace `YOUR_USERNAME` with your Windows username.\n   2. Copy the `database.db` at this location, and paste it to a safe location.\n5. Download the Windows installer from the [Langflow website](https://langflow.org).\n6. Run the Windows installer as an Administrator. To run the installer as an Administrator, right-click the executable and select **Run as administrator**.\n7. Follow the Windows installer's guided steps.\nThe Langflow 1.6.0 installer automatically performs a database backup.\nThese steps install Langflow from scratch, and result in two Langflow installations: the previously installed version, and version 1.6.0.\nThis is expected behavior.\n8. Start version 1.6.0 of Langflow, and confirm your flows behave as expected.\n9. **If flows are missing**, restore your flows from your manual backup by doing the following:\n    1. Close Langflow.\n    2. Navigate to your backup location, and copy the `database.db` file.\n    3. Replace the database file in the new installation's `data` directory.\n    4. Start Langflow, and confirm your flows behave as expected.\n    5. After confirmation, uninstall the previous version of Langflow, and keep version 1.6.0.\n\n### Breaking changes\n\n- Authentication enforced for Langflow API requests by default\n\n    In Langflow version 1.6, `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False` by default.\n    This enforces authentication for Langflow API requests while still automatically authenticating all users as superusers in the visual editor.\n\n    **This is a breaking change from 1.5 where both of these environment variables were `true` by default, bypassing all authentication.**\n\n    For temporary backwards compatibility, you can revert to the earlier unauthenticated behavior by setting both variables to `true`.\n    However, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.\n    At that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.\n\n    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).\n\n### New features and enhancements\n\n- OpenAI Responses API compatibility\n\n  Langflow now includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) at `POST /api/v1/responses`.\n  This allows you to use existing OpenAI client libraries with minimal code changes by replacing the `model` name with your `flow_id`.\n  The endpoint supports streaming responses, conversation continuity, tool call results, and global variable passing through headers.\n  For more information, see [OpenAI Responses API](/api-openai-responses).\n\n- Advanced document parsing with built-in Docling support\n\n  The **File** component supports advanced parsing with the Docling library.\n\n  To make it easier to use the [**Docling** components](/bundles-docling) and the **File** component's new advanced parsing feature, the Docling dependency is now included with Langflow for all operating systems except macOS Intel (x86_64).\n\n  For more information, see [Advanced parsing](/components-data#advanced-parsing).\n\n- Reorganized component menus and visual editor controls\n\n  - The [workspace](/concepts-overview#workspace) sidebar is divided into separate sections for  **Search**,  **Core components**,  [**MCP servers**](/mcp-server),  [**Bundles**](/components-bundle-components), and  **Add Note**.\n  - Lock/unlock controls moved to flow details in [**Projects**](/concepts-flows#projects).\n  - Zoom and help controls moved to the lower-right corner of the workspace.\n  - Vector store components moved to provider-specific  [**Bundles**](/components-bundle-components)\n  - **Serper Google Search API** component moved to the **Serper** bundle\n\n- Increased the default maximum file upload size from `100 MB` to `1024 MB`.\n\n- New integrations and bundles:\n\n  - [Traceloop](/integrations-instana-traceloop)\n  - [Cohere Rerank](/bundles-cohere)\n  - [S3 Bucket Uploader](/bundles-amazon)\n  - [Composio single-service components](/bundles-composio)\n\n### Deprecations\n\n- The **Local DB** component is now in legacy status.\nReplace this component with the [**Chroma DB** component](/bundles-chroma).\n\n## 1.5.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Langflow API requests can require authentication\n\n    To enhance security and ensure proper authentication for automatic login features, Langflow API endpoints now require authentication with a Langflow API key, even when `LANGFLOW_AUTO_LOGIN=True`.\n    This change will be enforced in a future release.\n    For temporary backwards compatibility, this release adds the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable.\n    The default value is `true`, which disables API authentication enforcement.\n    To enforce API authentication, set `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.\n    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).\n\n- Centralized **Language Model** and **Embedding Model** components\n\n    The [**Language Model** component](/components-models) and [**Embedding Model** component](/components-embedding-models) are now core components for your LLM and embeddings flows. They support multiple models and model providers, and allow you to experiment with different models without swapping out single-provider components.\n    Find them in the visual editor in the **Models** category.\n\n    The single-provider components moved to the [**Bundles**](/components-bundle-components) section.\n    You can use them to replace the **Language Model** and **Embedding Model** core components, or connect them to the **Agent** component with the **Connect other models** provider option.\n\n- MCP server one-click installation\n\n    On your Langflow project's **MCP server** page, click **Auto install** to install your Langflow MCP server to MCP clients with just one click.\n    The option to install with a JSON configuration file is available for macOS, Windows, and WSL.\n    For more information, see [Use Langflow as an MCP server](/mcp-server).\n\n- MCP server management\n\n    You can now add, remove, and edit your MCP servers in the **MCP Tools** components and through your Langflow **Settings** page.\n    For more information, see [Use Langflow as an MCP client](/mcp-client).\n\n- Input schema replaces temporary overrides\n\n    The **Input schema** pane replaces the need to manage tweak values in the **API access** pane. When you enable a parameter in the **Input schema** pane, the parameter is automatically added to your flow's code snippets, providing ready-to-use templates for making requests in your preferred programming language.\n\n- Tools components are redistributed\n\n    All components in the **Tools** category were moved to other component categories, such as **Helpers** and [**Bundles**](/components-bundle-components), or marked as legacy.\n\n    The [**MCP Tools** component](/mcp-client) is now under the **Agents** category.\n\n    Tools that performed the same function were combined into single components that support multiple providers, such as the [**Web Search** component](/components-data#web-search) and the [**News Search** component](/components-data#news-search).\n\n    For more information, see [Tools components](/components-tools).\n\n- Stability improvements\n\n    General stability improvements and bug fixes for enhanced reliability.\n    See an issue? [Raise it on GitHub](https://github.com/langflow-ai/langflow/issues).\n\n- New integrations and bundles\n\n   - [**Cleanlab** bundle](/bundles-cleanlab)\n\n## 1.4.2\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Enhanced file and flow management system with improved bulk capabilities.\n- Added the **BigQuery** component\n- Added the **Twelve Labs** bundle\n- Added the **NVIDIA System-Assist** component\n\n### Deprecations\n\n- Deprecated the **Combine Text** component.\n\n## 1.4.1\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Added an enhanced **Breaking Changes** feature to help update components without breaking flows after updating Langflow.\n\n## 1.4.0\n\nHighlights of this release include the following changes.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).\n\n### New features and enhancements\n\n- Introduced MCP server functionality to serve Langflow tools to MCP-compatible clients.\n- Renamed **Folders** to **Projects** in the visual editor.\n- The `/folders` endpoints now redirect to `/projects`.\n\n### Deprecations\n\n- Deprecated the **Gmail**, **Google Drive**, and **Google Search** components.\nFor alternatives, see the [**Google** bundle](/bundles-google).\n\n## Earlier releases\n\nSee the [Changelog](https://github.com/langflow-ai/langflow/releases).",
      "summary": "This page summarizes significant changes to Langflow in each release.\nFor all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).",
      "keywords": [
        "access",
        "add",
        "administrator",
        "agent",
        "agents",
        "all",
        "api",
        "assist",
        "authentication",
        "auto",
        "available",
        "backup",
        "before",
        "bigquery",
        "both",
        "breaking",
        "bug",
        "bundles",
        "bypassing",
        "change"
      ]
    },
    "/troubleshoot": {
      "title": "Troubleshoot Langflow",
      "slug": "/troubleshoot",
      "category": "support",
      "content": "This page provides troubleshooting advice for issues you might encounter when using Langflow or contributing to Langflow.\n\n## Missing components\n\nAs Langflow development continues, components are often recategorized or deprecated for better alignment or to prepare for new components.\n\nIf a component appears to be missing from the  **Core components** and  menus, try the following:\n\n*  **Search** for the component.\n* Check other component categories and  [**Bundles**](/components-bundle-components).\n* Check [legacy components](/concepts-components#legacy-components), which are hidden by default.\n* Check the [Changelog](https://github.com/langflow-ai/langflow/releases/latest) for component changes in recent releases.\n* Make sure the component isn't already present in your flow if it is a single-use component.\n\nIf you still cannot locate the component, see [Langflow GitHub Issues and Discussions](/contributing-github-issues).\n\n## No input in the Playground\n\nIf there is no message input field in the **Playground**, make sure your flow has a [**Chat Input** component](/components-io#chat-io) that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.\n\nBecause the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface.\n\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n## Missing key, no key found, or invalid API key\n\nIf you get an API key error when running a flow, try the following:\n\n* For all components that require credentials, make sure those components have a valid credential in the component's settings, such as the **API Key** field.\n* If you store your credentials in [Langflow global variables](/configuration-global-variables), make sure you selected the correct global variable and that the variable contains a valid credential.\n* Make sure the provided credentials are active, have the required permissions, and, if applicable, have sufficient funds in the account to execute the required action. For example, model providers require credits to use their LLMs.\n\n## Langflow installation issues\n\nThe following issues can occur when installing Langflow.\n\n### Langflow installation freezes at pip dependency resolution\n\nInstalling Langflow OSS with `pip install langflow` slowly fails with this error message:\n\n```text\npip is looking at multiple versions of <<library>> to determine which version is compatible with other requirements. This could take a while.\n```\n\nTo work around this issue, install Langflow with `uv` instead of `pip`, as explained in [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n\n### Linux installation fails to build required package\n\nWhen you try to install Langflow OSS on Linux, installation fails because of outdated or missing packages:\n\n```bash\nResolved 455 packages in 18.92s\n   Failed to build `webrtcvad==2.0.10`\n   The build backend returned an error\n   Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)\n```\n\nTo resolve this error, install the required build dependencies, and then retry the Langflow installation:\n\n```bash\nsudo apt-get update\nsudo apt-get install build-essential python3-dev\n```\n\nIf upgrading your packages doesn't fix the issue, install `gcc` separately, and then retry the Langflow installation:\n\n```bash\nsudo apt-get install gcc\n```\n\n### Installation failure from `webrtcvad` package\n\nIf you experience an error from the `webrtcvad` package, run `uv pip install webrtcvad-wheels` in your virtual environment, and then retry the Langflow installation.\n\n### C++ build tools required for Langflow Desktop on Windows\n\nMicrosoft Windows installations of Langflow Desktop require a C++ compiler that may not be present on your system. If you receive a `C++ Build Tools Required!` error, follow the on-screen prompt to install Microsoft C++ Build Tools, or [install Microsoft Visual Studio](https://visualstudio.microsoft.com/downloads/).\n\n## Langflow startup issues\n\nThe following issues can occur when attempting to start Langflow.\n\n### No `langflow.__main__` module\n\nWhen you try to run Langflow with the command `langflow run`, you encounter the following error:\n\n```bash\n> No module named 'langflow.__main__'\n```\n\nTo resolve this issue, try the following:\n\n1. Run `uv run langflow run` instead of `langflow run`.\n2. If that doesn't work, reinstall the latest Langflow version with `uv pip install langflow -U`.\n3. If that doesn't work, reinstall Langflow and its dependencies with `uv pip install langflow --pre -U --force-reinstall`.\n\n### Langflow runTraceback\n\nWhen you try to run Langflow using the command `langflow run`, you encounter the following error:\n\n```bash\n> langflow runTraceback (most recent call last): File \".../langflow\", line 5, in <module>  from langflow.__main__ import mainModuleNotFoundError: No module named 'langflow.__main__'\n```\n\nThere are two possible reasons for this error:\n\n* **Multiple Langflow installations**: You installed Langflow using `pip install langflow` but you already had a previous version of Langflow installed in your system. In this case, you might be running the wrong executable.\n\n    To solve this issue, run the correct executable by running `python -m langflow run` instead of `langflow run`.\n\n    If that doesn't work, try uninstalling and reinstalling Langflow with `uv pip install langflow --pre -U`.\n\n* **Version conflict during installation**: Some version conflicts might have occurred during the installation process. To resolve this issue, reinstall Langflow and its dependencies by running `python -m pip install langflow --pre -U --force-reinstall`.\n\n### Environment variables not available from terminal\n\nEnvironment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when launched through the Finder or the Start Menu.\nTo set environment variables for Langflow Desktop, see [Set environment variables for Langflow Desktop](/environment-variables#set-environment-variables-for-langflow-desktop).\n\n### Access Langflow Desktop startup logs\n\nIf you encounter issues with Langflow Desktop, you might need to [access Langflow Desktop startup logs](/logging#desktop-logs) for debugging.\n\n### User not found or inactive when running multiple flows\n\nWhen running multiple local Langflow OSS instances on different ports, such as `localhost:7860` and `localhost:7861`, you might see authentication errors in the logs.\nFor example:\n\n```text\n[07/22/25 10:57:07] INFO     2025-07-22 10:57:07 - INFO     - utils - User not found or inactive.\n```\n\nTo resolve this error, use separate browser instances or browser profiles to access each Langflow instance.\n\n### Package is not installed\n\nIn Langflow OSS, you can follow the error message's instructions to install the missing dependency.\n\nTo manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).\n\n## Langflow upgrade issues\n\nThe following issues can occur when upgrading your Langflow version.\n\nFor information about managing Langflow versions, see [Install Langflow](/get-started-installation).\n\n### Something went wrong running migrations\n\nThe following error can occur during Langflow upgrades when the new version can't override `langflow-pre.db` in the Langflow cache folder:\n\n```bash\n> Something went wrong running migrations. Please, run 'langflow migration --fix'\n```\n\nTo resolve this error, clear the cache by deleting the contents of your Langflow cache folder.\nThe filepath depends on your operating system, installation type, and configuration options.\nFor more information and default filepaths, see [Memory management options](/memory).\n\n**Note:** Clearing the cache erases your settings.\nIf you want to retain your settings files, create a backup of those files before clearing the cache folder.\n\n### Langflow Desktop says it is running the latest version, but it is actually behind\n\nIf you are running Langflow Desktop version 1.4.2 or earlier, the UI might incorrectly report that you are on the latest version when a newer version is available.\n\nThis happens because the automatic update feature in the UI was introduced in version 1.4.2.\nEarlier versions can't automatically detect or apply updates.\n\nTo resolve this issue, uninstall Langflow Desktop, and then [download and install the latest version of Langflow Desktop](https://langflow.org/desktop).\n\n### 422 error when freezing components after upgrading Pydantic/FastAPI dependencies\n\nIf you locally upgrade your Pydantic and FastAPI dependencies, you may encounter a 422 error when trying to freeze components.\n\nThis error occurs due to changes in how request bodies are handled in newer versions of these dependencies.\n\nIf you're experiencing this issue, update your Langflow installation to version 1.6.5 or later, which includes a fix for this issue.\n\n```bash\nuv pip install langflow -U\n```\n\n## Langflow uninstall issues\n\nThe following issues can occur when uninstalling Langflow.\n\n### Dot directory isn't deleted when uninstalling Langflow Desktop on macOS\n\nOn macOS, uninstalling Langflow Desktop deletes the `.app` file but doesn't delete files in `~/.langflow`, which includes files generated during usage like cache and settings.\n\nIf you reinstall Langflow Desktop, it starts with the existing data from the previous installation.\n\nTo fully remove a Langflow Desktop macOS installation, you must also delete `~/.langflow`:\n\n    ```bash\n    rm -rf .langflow\n    ```\n\n## Langflow MCP issues {#mcp}\n\nThe following issues can occur when using Langflow as an MCP server or client.\n\n### Claude for Desktop doesn't use MCP server tools correctly\n\nIf Claude for Desktop doesn't use your server's tools correctly, try explicitly defining the path to your local `uvx` or `npx` executable file in the `claude_desktop_config.json` configuration file:\n\n1. To find your `uvx` path, run `which uvx`.\n\n    To find your `npx` path, run `which npx`.\n\n2. In your `claude_desktop_config.json` file, add the path to your Langflow MCP server configuration, as shown in the following examples.\n\n    \n    \n    </Tabs>\n\n### MCP browser-based flows don't open a browser on Windows\n\nThis is a known issue when using MCP Tools with browser navigation actions, such as Playwright, on Windows: The agent can execute the tool successfully but the browser tab or window doesn't open.\n\nThis issue occurs because the MCP server runs from a Python process, which prevents it from opening browser windows in WSL or Windows.\n\nTo work around this issue, use the standalone MCP server approach documented in the [Playwright MCP repository](https://github.com/microsoft/playwright-mcp?tab=readme-ov-file#standalone-mcp-server).\nAfter the server is up and running, you can add it as an SSE server in Langflow.\nFor other browser navigation tools, see the provider's documentation for specific troubleshooting guidance.\n\n### \"No tools or prompts connected\" on MCP servers in mixed Windows/WSL environments\n\nIf you encounter \"No tools or prompts connected\" errors or connection failures when using Langflow Desktop as an MCP server with clients running in different environments, such as Langflow on a Windows host and an MCP client in WSL, this is due to network isolation between Windows and WSL environments.\n\nWSL cannot directly access Windows localhost services, and Langflow running on a Windows host is not accessible from WSL clients at `localhost:7860`.\n\nTo work around this issue, run the server and host in the same operating environment.\n\nAlternatively, configure Langflow Desktop to accept connections from WSL at the default Windows IP address of `10.255.255.254:7860` instead of `localhost`.\n\n## Token length limit errors in Embedding Model components\n\nToken length errors can happen if your chunking strategy doesn't align with your embedding model's tokenization limits.\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\n\n## Custom components and integrations issues\n\nFor troubleshooting advice for a third-party integration, see the information about that integration in the Langflow documentation and the provider's documentation.\n\nIf you are building a custom component, see [Error handling and logging for custom Python components](/components-custom-components#error-handling-and-logging).\n\n### Custom components not appearing in the visual editor\n\nIf your custom components are not appearing in the Langflow visual editor, try the following troubleshooting steps:\n\n1. Ensure your components follow the [required directory structure](https://docs.langflow.org/components-custom-components#directory-structure-requirements).\n    ```\n    /your/custom/components/path/       # Base directory set by LANGFLOW_COMPONENTS_PATH\n         category_name/              # Required category subfolder that determines menu name\n             __init__.py             # Required\n             custom_component.py     # Component file\n    ```\n\n2. Verify each category directory includes an `__init__.py` file.\nThis is required for Python to recognize the directory as a module.\n\n3. Use the command line argument instead of the environment variable for `LANGFLOW_COMPONENTS_PATH`.\nIf you're using the `LANGFLOW_COMPONENTS_PATH` environment variable and components aren't loading, try the `--components-path` command line argument instead:\n\n    ```bash\n    uv run langflow run --components-path /path/to/your/custom/components\n    ```\n\nIf you continue to experience issues, please [report them on GitHub](https://github.com/langflow-ai/langflow/issues) with details about your directory structure and component setup.\n\n## See also\n\n- [Langflow GitHub Issues and Discussions](/contributing-github-issues)\n- [Langflow telemetry](/contributing-telemetry)",
      "summary": "This page provides troubleshooting advice for issues you might encounter when using Langflow or contributing to Langflow.",
      "keywords": [
        "access",
        "actually",
        "after",
        "agent",
        "api",
        "appearing",
        "available",
        "based",
        "behind",
        "browser",
        "build",
        "bundles",
        "but",
        "chat",
        "claude",
        "components",
        "conflict",
        "connected",
        "core",
        "correctly"
      ]
    },
    "troubleshooting": {
      "title": "Troubleshoot Langflow",
      "slug": "/troubleshoot",
      "category": "support",
      "content": "This page provides troubleshooting advice for issues you might encounter when using Langflow or contributing to Langflow.\n\n## Missing components\n\nAs Langflow development continues, components are often recategorized or deprecated for better alignment or to prepare for new components.\n\nIf a component appears to be missing from the  **Core components** and  menus, try the following:\n\n*  **Search** for the component.\n* Check other component categories and  [**Bundles**](/components-bundle-components).\n* Check [legacy components](/concepts-components#legacy-components), which are hidden by default.\n* Check the [Changelog](https://github.com/langflow-ai/langflow/releases/latest) for component changes in recent releases.\n* Make sure the component isn't already present in your flow if it is a single-use component.\n\nIf you still cannot locate the component, see [Langflow GitHub Issues and Discussions](/contributing-github-issues).\n\n## No input in the Playground\n\nIf there is no message input field in the **Playground**, make sure your flow has a [**Chat Input** component](/components-io#chat-io) that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.\n\nBecause the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface.\n\nFor more information, see [Test flows in the Playground](/concepts-playground).\n\n## Missing key, no key found, or invalid API key\n\nIf you get an API key error when running a flow, try the following:\n\n* For all components that require credentials, make sure those components have a valid credential in the component's settings, such as the **API Key** field.\n* If you store your credentials in [Langflow global variables](/configuration-global-variables), make sure you selected the correct global variable and that the variable contains a valid credential.\n* Make sure the provided credentials are active, have the required permissions, and, if applicable, have sufficient funds in the account to execute the required action. For example, model providers require credits to use their LLMs.\n\n## Langflow installation issues\n\nThe following issues can occur when installing Langflow.\n\n### Langflow installation freezes at pip dependency resolution\n\nInstalling Langflow OSS with `pip install langflow` slowly fails with this error message:\n\n```text\npip is looking at multiple versions of <<library>> to determine which version is compatible with other requirements. This could take a while.\n```\n\nTo work around this issue, install Langflow with `uv` instead of `pip`, as explained in [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).\n\n### Linux installation fails to build required package\n\nWhen you try to install Langflow OSS on Linux, installation fails because of outdated or missing packages:\n\n```bash\nResolved 455 packages in 18.92s\n   Failed to build `webrtcvad==2.0.10`\n   The build backend returned an error\n   Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)\n```\n\nTo resolve this error, install the required build dependencies, and then retry the Langflow installation:\n\n```bash\nsudo apt-get update\nsudo apt-get install build-essential python3-dev\n```\n\nIf upgrading your packages doesn't fix the issue, install `gcc` separately, and then retry the Langflow installation:\n\n```bash\nsudo apt-get install gcc\n```\n\n### Installation failure from `webrtcvad` package\n\nIf you experience an error from the `webrtcvad` package, run `uv pip install webrtcvad-wheels` in your virtual environment, and then retry the Langflow installation.\n\n### C++ build tools required for Langflow Desktop on Windows\n\nMicrosoft Windows installations of Langflow Desktop require a C++ compiler that may not be present on your system. If you receive a `C++ Build Tools Required!` error, follow the on-screen prompt to install Microsoft C++ Build Tools, or [install Microsoft Visual Studio](https://visualstudio.microsoft.com/downloads/).\n\n## Langflow startup issues\n\nThe following issues can occur when attempting to start Langflow.\n\n### No `langflow.__main__` module\n\nWhen you try to run Langflow with the command `langflow run`, you encounter the following error:\n\n```bash\n> No module named 'langflow.__main__'\n```\n\nTo resolve this issue, try the following:\n\n1. Run `uv run langflow run` instead of `langflow run`.\n2. If that doesn't work, reinstall the latest Langflow version with `uv pip install langflow -U`.\n3. If that doesn't work, reinstall Langflow and its dependencies with `uv pip install langflow --pre -U --force-reinstall`.\n\n### Langflow runTraceback\n\nWhen you try to run Langflow using the command `langflow run`, you encounter the following error:\n\n```bash\n> langflow runTraceback (most recent call last): File \".../langflow\", line 5, in <module>  from langflow.__main__ import mainModuleNotFoundError: No module named 'langflow.__main__'\n```\n\nThere are two possible reasons for this error:\n\n* **Multiple Langflow installations**: You installed Langflow using `pip install langflow` but you already had a previous version of Langflow installed in your system. In this case, you might be running the wrong executable.\n\n    To solve this issue, run the correct executable by running `python -m langflow run` instead of `langflow run`.\n\n    If that doesn't work, try uninstalling and reinstalling Langflow with `uv pip install langflow --pre -U`.\n\n* **Version conflict during installation**: Some version conflicts might have occurred during the installation process. To resolve this issue, reinstall Langflow and its dependencies by running `python -m pip install langflow --pre -U --force-reinstall`.\n\n### Environment variables not available from terminal\n\nEnvironment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when launched through the Finder or the Start Menu.\nTo set environment variables for Langflow Desktop, see [Set environment variables for Langflow Desktop](/environment-variables#set-environment-variables-for-langflow-desktop).\n\n### Access Langflow Desktop startup logs\n\nIf you encounter issues with Langflow Desktop, you might need to [access Langflow Desktop startup logs](/logging#desktop-logs) for debugging.\n\n### User not found or inactive when running multiple flows\n\nWhen running multiple local Langflow OSS instances on different ports, such as `localhost:7860` and `localhost:7861`, you might see authentication errors in the logs.\nFor example:\n\n```text\n[07/22/25 10:57:07] INFO     2025-07-22 10:57:07 - INFO     - utils - User not found or inactive.\n```\n\nTo resolve this error, use separate browser instances or browser profiles to access each Langflow instance.\n\n### Package is not installed\n\nIn Langflow OSS, you can follow the error message's instructions to install the missing dependency.\n\nTo manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).\n\n## Langflow upgrade issues\n\nThe following issues can occur when upgrading your Langflow version.\n\nFor information about managing Langflow versions, see [Install Langflow](/get-started-installation).\n\n### Something went wrong running migrations\n\nThe following error can occur during Langflow upgrades when the new version can't override `langflow-pre.db` in the Langflow cache folder:\n\n```bash\n> Something went wrong running migrations. Please, run 'langflow migration --fix'\n```\n\nTo resolve this error, clear the cache by deleting the contents of your Langflow cache folder.\nThe filepath depends on your operating system, installation type, and configuration options.\nFor more information and default filepaths, see [Memory management options](/memory).\n\n**Note:** Clearing the cache erases your settings.\nIf you want to retain your settings files, create a backup of those files before clearing the cache folder.\n\n### Langflow Desktop says it is running the latest version, but it is actually behind\n\nIf you are running Langflow Desktop version 1.4.2 or earlier, the UI might incorrectly report that you are on the latest version when a newer version is available.\n\nThis happens because the automatic update feature in the UI was introduced in version 1.4.2.\nEarlier versions can't automatically detect or apply updates.\n\nTo resolve this issue, uninstall Langflow Desktop, and then [download and install the latest version of Langflow Desktop](https://langflow.org/desktop).\n\n### 422 error when freezing components after upgrading Pydantic/FastAPI dependencies\n\nIf you locally upgrade your Pydantic and FastAPI dependencies, you may encounter a 422 error when trying to freeze components.\n\nThis error occurs due to changes in how request bodies are handled in newer versions of these dependencies.\n\nIf you're experiencing this issue, update your Langflow installation to version 1.6.5 or later, which includes a fix for this issue.\n\n```bash\nuv pip install langflow -U\n```\n\n## Langflow uninstall issues\n\nThe following issues can occur when uninstalling Langflow.\n\n### Dot directory isn't deleted when uninstalling Langflow Desktop on macOS\n\nOn macOS, uninstalling Langflow Desktop deletes the `.app` file but doesn't delete files in `~/.langflow`, which includes files generated during usage like cache and settings.\n\nIf you reinstall Langflow Desktop, it starts with the existing data from the previous installation.\n\nTo fully remove a Langflow Desktop macOS installation, you must also delete `~/.langflow`:\n\n    ```bash\n    rm -rf .langflow\n    ```\n\n## Langflow MCP issues {#mcp}\n\nThe following issues can occur when using Langflow as an MCP server or client.\n\n### Claude for Desktop doesn't use MCP server tools correctly\n\nIf Claude for Desktop doesn't use your server's tools correctly, try explicitly defining the path to your local `uvx` or `npx` executable file in the `claude_desktop_config.json` configuration file:\n\n1. To find your `uvx` path, run `which uvx`.\n\n    To find your `npx` path, run `which npx`.\n\n2. In your `claude_desktop_config.json` file, add the path to your Langflow MCP server configuration, as shown in the following examples.\n\n    \n    \n    </Tabs>\n\n### MCP browser-based flows don't open a browser on Windows\n\nThis is a known issue when using MCP Tools with browser navigation actions, such as Playwright, on Windows: The agent can execute the tool successfully but the browser tab or window doesn't open.\n\nThis issue occurs because the MCP server runs from a Python process, which prevents it from opening browser windows in WSL or Windows.\n\nTo work around this issue, use the standalone MCP server approach documented in the [Playwright MCP repository](https://github.com/microsoft/playwright-mcp?tab=readme-ov-file#standalone-mcp-server).\nAfter the server is up and running, you can add it as an SSE server in Langflow.\nFor other browser navigation tools, see the provider's documentation for specific troubleshooting guidance.\n\n### \"No tools or prompts connected\" on MCP servers in mixed Windows/WSL environments\n\nIf you encounter \"No tools or prompts connected\" errors or connection failures when using Langflow Desktop as an MCP server with clients running in different environments, such as Langflow on a Windows host and an MCP client in WSL, this is due to network isolation between Windows and WSL environments.\n\nWSL cannot directly access Windows localhost services, and Langflow running on a Windows host is not accessible from WSL clients at `localhost:7860`.\n\nTo work around this issue, run the server and host in the same operating environment.\n\nAlternatively, configure Langflow Desktop to accept connections from WSL at the default Windows IP address of `10.255.255.254:7860` instead of `localhost`.\n\n## Token length limit errors in Embedding Model components\n\nToken length errors can happen if your chunking strategy doesn't align with your embedding model's tokenization limits.\nFor more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).\n\n## Custom components and integrations issues\n\nFor troubleshooting advice for a third-party integration, see the information about that integration in the Langflow documentation and the provider's documentation.\n\nIf you are building a custom component, see [Error handling and logging for custom Python components](/components-custom-components#error-handling-and-logging).\n\n### Custom components not appearing in the visual editor\n\nIf your custom components are not appearing in the Langflow visual editor, try the following troubleshooting steps:\n\n1. Ensure your components follow the [required directory structure](https://docs.langflow.org/components-custom-components#directory-structure-requirements).\n    ```\n    /your/custom/components/path/       # Base directory set by LANGFLOW_COMPONENTS_PATH\n         category_name/              # Required category subfolder that determines menu name\n             __init__.py             # Required\n             custom_component.py     # Component file\n    ```\n\n2. Verify each category directory includes an `__init__.py` file.\nThis is required for Python to recognize the directory as a module.\n\n3. Use the command line argument instead of the environment variable for `LANGFLOW_COMPONENTS_PATH`.\nIf you're using the `LANGFLOW_COMPONENTS_PATH` environment variable and components aren't loading, try the `--components-path` command line argument instead:\n\n    ```bash\n    uv run langflow run --components-path /path/to/your/custom/components\n    ```\n\nIf you continue to experience issues, please [report them on GitHub](https://github.com/langflow-ai/langflow/issues) with details about your directory structure and component setup.\n\n## See also\n\n- [Langflow GitHub Issues and Discussions](/contributing-github-issues)\n- [Langflow telemetry](/contributing-telemetry)",
      "summary": "This page provides troubleshooting advice for issues you might encounter when using Langflow or contributing to Langflow.",
      "keywords": [
        "access",
        "actually",
        "after",
        "agent",
        "api",
        "appearing",
        "available",
        "based",
        "behind",
        "browser",
        "build",
        "bundles",
        "but",
        "chat",
        "claude",
        "components",
        "conflict",
        "connected",
        "core",
        "correctly"
      ]
    },
    "/agent-tutorial": {
      "title": "Connect applications to agents",
      "slug": "/agent-tutorial",
      "category": "tutorials",
      "content": "This tutorial shows you how to connect a JavaScript application to a [Langflow agent](/agents).\n\nWith an agent, your application can use any connected tools to retrieve more contextual and timely data without changing any application code. The tools are selected by the agent's internal LLM to solve problems and answer questions.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Install the [Langflow JavaScript client](/typescript-client)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create an agent flow\n\nThe following steps modify the **Simple Agent** template to connect a [**Directory** component](/components-data#directory) and a [**Web Search** component](/components-data#web-search) as tools for an **Agent** component.\nThe **Directory** component loads all files of a given type from a target directory on your local machine, and the **Web Search** component performs a DuckDuckGo search.\nWhen connected to an **Agent** component as tools, the agent has the option to use these components when handling requests.\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n2. Remove the **URL** and **Calculator** tools, and then add **Directory** and **Web Search** components to your flow.\n3. In the **Directory** component's **Path** field, enter the directory path and file types that you want to make available to the **Agent** component.\n\n    In this tutorial, the agent needs access to a record of customer purchases, so the directory name is `customer_orders` and the file type is `.csv`. Later in this tutorial, the agent will be prompted to find `email` values in the customer data.\n\n    You can adapt the tutorial to suit your data, or, to follow along with the tutorial, you can download [`customer-orders.csv`](/files/customer_orders.csv) and save it in a `customer_orders` folder on your local machine.\n\n4. In the **Directory** and **Web Search** [components' header menus](/concepts-components#component-menus), enable **Tool Mode** so you can use the components with an agent.\n\n5. Connect the **Directory** and **Web Search** components' **Toolset** ports to the **Agent** component's **Tools** port.\n6. In the **Agent** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n7. To test the flow, click  **Playground**, and then ask the LLM a question, such as `Recommend 3 used items for carol.davis@example.com, based on previous orders.`\n\n    Given the example prompt, the LLM would respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\n\n    The **Playground** prints the agent's chain of thought as it selects tools to use and interacts with functionality provided by those tools.\n    For example, the agent can use the **Directory** component's `as_dataframe` tool to retrieve a [DataFrame](/data-types#dataframe), and the **Web Search** component's `perform_search` tool to find links to related items.\n\n## Add a Prompt Template component to the flow\n\nIn this example, the application sends a customer's email address to the Langflow agent. The agent compares the customer's previous orders within the **Directory** component, searches the web for used versions of those items, and returns three results.\n\n1. To include the email address as a value in your flow, add a [**Prompt Template** component](/components-prompts) to your flow between the **Chat Input** and **Agent** components.\n2. In the **Prompt Template** component's **Template** field, enter `Recommend 3 used items for {email}, based on previous orders.`\nAdding the `{email}` value in curly braces creates a new input in the **Prompt Template** component, and the component connected to the `{email}` port is supplying the value for that variable.\nThis creates a point for the user's email to enter the flow from your request.\nIf you aren't using the `customer_orders.csv` example file, modify the input to search for a value in your dataset.\n\n    At this point your flow has six components. The **Chat Input** component is connected to the **Prompt Template** component's **email** input port. Then, the **Prompt Template** component's output is connected to the **Agent** component's **System Message** input port. The **Directory** and **Web Search** components are connected to the **Agent** component's **Tools** port. Finally, the **Agent** component's output is connected to the **Chat Output** component, which returns the final response to the application.\n\n    ![An Agent component connected to Web Search and Directory components as tools](/img/tutorial-agent-with-directory.png)\n\n## Send requests to your flow from a JavaScript application\n\nWith your flow operational, connect it to a JavaScript application to use the agent's responses.\n\n1. To construct a JavaScript application to connect to your flow, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step.\nIf you're using the `customer_orders.csv` example file, you can run this example as-is with the example email address in the code sample.\nIf not, modify the `const email = \"isabella.rodriguez@example.com\"` to search for a value in your dataset.\n\n    ```js\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const LANGFLOW_SERVER_ADDRESS = 'LANGFLOW_SERVER_ADDRESS';\n    const FLOW_ID = 'FLOW_ID';\n    const LANGFLOW_API_KEY = 'LANGFLOW_API_KEY';\n    const email = \"isabella.rodriguez@example.com\";\n\n    async function runAgentFlow(): Promise<void> {\n        try {\n            // Initialize the Langflow client\n            const client = new LangflowClient({\n                baseUrl: LANGFLOW_SERVER_ADDRESS,\n                apiKey: LANGFLOW_API_KEY\n            });\n\n            console.log(`Connecting to Langflow server at: ${LANGFLOW_SERVER_ADDRESS} `);\n            console.log(`Flow ID: ${FLOW_ID}`);\n            console.log(`Email: ${email}`);\n\n            // Get the flow instance\n            const flow = client.flow(FLOW_ID);\n\n            // Run the flow with the email as input\n            console.log('\\nSending request to agent...');\n            const response = await flow.run(email, {\n                session_id: email // Use email as session ID for context\n            });\n\n            console.log('\\n=== Response from Langflow ===');\n            console.log('Session ID:', response.sessionId);\n\n            // Extract URLs from the chat message\n            const chatMessage = response.chatOutputText();\n            console.log('\\n=== URLs from Chat Message ===');\n            const messageUrls = chatMessage.match(/https?:\\/\\/[^\\s\"')\\]]+/g) || [];\n            const cleanMessageUrls = [...new Set(messageUrls)].map(url => url.trim());\n            console.log('URLs from message:');\n            cleanMessageUrls.slice(0, 3).forEach(url => console.log(url));\n\n        } catch (error) {\n            console.error('Error running flow:', error);\n\n            // Provide error messages\n            if (error instanceof Error) {\n                if (error.message.includes('fetch')) {\n                    console.error('\\nMake sure your Langflow server is running and accessible at:', LANGFLOW_SERVER_ADDRESS);\n                }\n                if (error.message.includes('401') || error.message.includes('403')) {\n                    console.error('\\nCheck your API key configuration');\n                }\n                if (error.message.includes('404')) {\n                    console.error('\\nCheck your Flow ID - make sure it exists and is correct');\n                }\n            }\n        }\n    }\n\n    // Run the function\n    console.log('Starting Langflow Agent...\\n');\n    runAgentFlow().catch(console.error);\n    ```\n\n3.  Save and run the script to send the request and test the flow.\n\n    Your application receives three URLs for recommended used items based on a customer's previous orders in your local CSV, all without changing any code.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n    Starting Langflow Agent...\n\n    Connecting to Langflow server at: http://localhost:7860\n    Flow ID: local-db-search\n    Email: isabella.rodriguez@example.com\n\n    Sending request to agent...\n\n    === Response from Langflow ===\n    Session ID: isabella.rodriguez@example.com\n\n    URLs found:\n    https://www.facebook.com/marketplace/258164108225783/electronics/\n    https://www.facebook.com/marketplace/458332108944152/furniture/\n    https://www.facebook.com/marketplace/613732137493719/kitchen-cabinets/\n    ```\n\n    </details>\n\n4.  To quickly check traffic to your flow, open the **Playground**.\n    New sessions are named after the user's email address.\n    Keeping sessions distinct helps the agent maintain context. For more on session IDs, see [Session ID](/session-id).\n\n## Next steps\n\nFor more information on building or extending this tutorial, see the following:\n\n* [Model Context Protocol (MCP) servers](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial shows you how to connect a JavaScript application to a [Langflow agent](/agents).",
      "keywords": [
        "access",
        "add",
        "agent",
        "agents",
        "api",
        "application",
        "applications",
        "calculator",
        "chat",
        "component",
        "connect",
        "create",
        "directory",
        "email",
        "flow",
        "input",
        "javascript",
        "key",
        "message",
        "mode"
      ]
    },
    "agent": {
      "title": "Connect applications to agents",
      "slug": "/agent-tutorial",
      "category": "tutorials",
      "content": "This tutorial shows you how to connect a JavaScript application to a [Langflow agent](/agents).\n\nWith an agent, your application can use any connected tools to retrieve more contextual and timely data without changing any application code. The tools are selected by the agent's internal LLM to solve problems and answer questions.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Install the [Langflow JavaScript client](/typescript-client)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create an agent flow\n\nThe following steps modify the **Simple Agent** template to connect a [**Directory** component](/components-data#directory) and a [**Web Search** component](/components-data#web-search) as tools for an **Agent** component.\nThe **Directory** component loads all files of a given type from a target directory on your local machine, and the **Web Search** component performs a DuckDuckGo search.\nWhen connected to an **Agent** component as tools, the agent has the option to use these components when handling requests.\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n2. Remove the **URL** and **Calculator** tools, and then add **Directory** and **Web Search** components to your flow.\n3. In the **Directory** component's **Path** field, enter the directory path and file types that you want to make available to the **Agent** component.\n\n    In this tutorial, the agent needs access to a record of customer purchases, so the directory name is `customer_orders` and the file type is `.csv`. Later in this tutorial, the agent will be prompted to find `email` values in the customer data.\n\n    You can adapt the tutorial to suit your data, or, to follow along with the tutorial, you can download [`customer-orders.csv`](/files/customer_orders.csv) and save it in a `customer_orders` folder on your local machine.\n\n4. In the **Directory** and **Web Search** [components' header menus](/concepts-components#component-menus), enable **Tool Mode** so you can use the components with an agent.\n\n5. Connect the **Directory** and **Web Search** components' **Toolset** ports to the **Agent** component's **Tools** port.\n6. In the **Agent** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n7. To test the flow, click  **Playground**, and then ask the LLM a question, such as `Recommend 3 used items for carol.davis@example.com, based on previous orders.`\n\n    Given the example prompt, the LLM would respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.\n\n    The **Playground** prints the agent's chain of thought as it selects tools to use and interacts with functionality provided by those tools.\n    For example, the agent can use the **Directory** component's `as_dataframe` tool to retrieve a [DataFrame](/data-types#dataframe), and the **Web Search** component's `perform_search` tool to find links to related items.\n\n## Add a Prompt Template component to the flow\n\nIn this example, the application sends a customer's email address to the Langflow agent. The agent compares the customer's previous orders within the **Directory** component, searches the web for used versions of those items, and returns three results.\n\n1. To include the email address as a value in your flow, add a [**Prompt Template** component](/components-prompts) to your flow between the **Chat Input** and **Agent** components.\n2. In the **Prompt Template** component's **Template** field, enter `Recommend 3 used items for {email}, based on previous orders.`\nAdding the `{email}` value in curly braces creates a new input in the **Prompt Template** component, and the component connected to the `{email}` port is supplying the value for that variable.\nThis creates a point for the user's email to enter the flow from your request.\nIf you aren't using the `customer_orders.csv` example file, modify the input to search for a value in your dataset.\n\n    At this point your flow has six components. The **Chat Input** component is connected to the **Prompt Template** component's **email** input port. Then, the **Prompt Template** component's output is connected to the **Agent** component's **System Message** input port. The **Directory** and **Web Search** components are connected to the **Agent** component's **Tools** port. Finally, the **Agent** component's output is connected to the **Chat Output** component, which returns the final response to the application.\n\n    ![An Agent component connected to Web Search and Directory components as tools](/img/tutorial-agent-with-directory.png)\n\n## Send requests to your flow from a JavaScript application\n\nWith your flow operational, connect it to a JavaScript application to use the agent's responses.\n\n1. To construct a JavaScript application to connect to your flow, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step.\nIf you're using the `customer_orders.csv` example file, you can run this example as-is with the example email address in the code sample.\nIf not, modify the `const email = \"isabella.rodriguez@example.com\"` to search for a value in your dataset.\n\n    ```js\n    import { LangflowClient } from \"@datastax/langflow-client\";\n\n    const LANGFLOW_SERVER_ADDRESS = 'LANGFLOW_SERVER_ADDRESS';\n    const FLOW_ID = 'FLOW_ID';\n    const LANGFLOW_API_KEY = 'LANGFLOW_API_KEY';\n    const email = \"isabella.rodriguez@example.com\";\n\n    async function runAgentFlow(): Promise<void> {\n        try {\n            // Initialize the Langflow client\n            const client = new LangflowClient({\n                baseUrl: LANGFLOW_SERVER_ADDRESS,\n                apiKey: LANGFLOW_API_KEY\n            });\n\n            console.log(`Connecting to Langflow server at: ${LANGFLOW_SERVER_ADDRESS} `);\n            console.log(`Flow ID: ${FLOW_ID}`);\n            console.log(`Email: ${email}`);\n\n            // Get the flow instance\n            const flow = client.flow(FLOW_ID);\n\n            // Run the flow with the email as input\n            console.log('\\nSending request to agent...');\n            const response = await flow.run(email, {\n                session_id: email // Use email as session ID for context\n            });\n\n            console.log('\\n=== Response from Langflow ===');\n            console.log('Session ID:', response.sessionId);\n\n            // Extract URLs from the chat message\n            const chatMessage = response.chatOutputText();\n            console.log('\\n=== URLs from Chat Message ===');\n            const messageUrls = chatMessage.match(/https?:\\/\\/[^\\s\"')\\]]+/g) || [];\n            const cleanMessageUrls = [...new Set(messageUrls)].map(url => url.trim());\n            console.log('URLs from message:');\n            cleanMessageUrls.slice(0, 3).forEach(url => console.log(url));\n\n        } catch (error) {\n            console.error('Error running flow:', error);\n\n            // Provide error messages\n            if (error instanceof Error) {\n                if (error.message.includes('fetch')) {\n                    console.error('\\nMake sure your Langflow server is running and accessible at:', LANGFLOW_SERVER_ADDRESS);\n                }\n                if (error.message.includes('401') || error.message.includes('403')) {\n                    console.error('\\nCheck your API key configuration');\n                }\n                if (error.message.includes('404')) {\n                    console.error('\\nCheck your Flow ID - make sure it exists and is correct');\n                }\n            }\n        }\n    }\n\n    // Run the function\n    console.log('Starting Langflow Agent...\\n');\n    runAgentFlow().catch(console.error);\n    ```\n\n3.  Save and run the script to send the request and test the flow.\n\n    Your application receives three URLs for recommended used items based on a customer's previous orders in your local CSV, all without changing any code.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n    Starting Langflow Agent...\n\n    Connecting to Langflow server at: http://localhost:7860\n    Flow ID: local-db-search\n    Email: isabella.rodriguez@example.com\n\n    Sending request to agent...\n\n    === Response from Langflow ===\n    Session ID: isabella.rodriguez@example.com\n\n    URLs found:\n    https://www.facebook.com/marketplace/258164108225783/electronics/\n    https://www.facebook.com/marketplace/458332108944152/furniture/\n    https://www.facebook.com/marketplace/613732137493719/kitchen-cabinets/\n    ```\n\n    </details>\n\n4.  To quickly check traffic to your flow, open the **Playground**.\n    New sessions are named after the user's email address.\n    Keeping sessions distinct helps the agent maintain context. For more on session IDs, see [Session ID](/session-id).\n\n## Next steps\n\nFor more information on building or extending this tutorial, see the following:\n\n* [Model Context Protocol (MCP) servers](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial shows you how to connect a JavaScript application to a [Langflow agent](/agents).",
      "keywords": [
        "access",
        "add",
        "agent",
        "agents",
        "api",
        "application",
        "applications",
        "calculator",
        "chat",
        "component",
        "connect",
        "create",
        "directory",
        "email",
        "flow",
        "input",
        "javascript",
        "key",
        "message",
        "mode"
      ]
    },
    "/chat-with-files": {
      "title": "Create a chatbot that can ingest files",
      "slug": "/chat-with-files",
      "category": "tutorials",
      "content": "This tutorial shows you how to build a chatbot that can read and answer questions about files you upload, such as meeting notes or job applications.\n\nFor example, you could upload a contract and ask, What are the termination clauses in this agreement? Or upload a resume and ask, Does this candidate have experience with marketing analytics?\n\nThe main focus of this tutorial is to show you how to provide files as input to a Langflow flow, so your chatbot can use the content of those files in its responses.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create a flow that accepts file input\n\nTo ingest files, your flow must have a **File** component attached to a component that receives input, such as a **Prompt Template** or **Agent** component.\n\nThe following steps modify the **Basic Prompting** template to accept file input:\n\n1. In Langflow, click **New Flow**, and then select the **Basic Prompting** template.\n2. In the **Language Model** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n3. To verify that your API key is valid, click  **Playground**, and then ask the LLM a question.\nThe LLM should respond according to the specifications in the **Prompt Template** component's **Template** field.\n\n4. Exit the **Playground**, and then modify the **Prompt Template** component to accept file input in addition to chat input.\nTo do this, edit the **Template** field, and then replace the default prompt with the following text:\n\n    ```text\n    ChatInput:\n    {chat-input}\n    File:\n    {file}\n    ```\n\n    \n**Note:** You can use any string to name your template variables.\n    These strings become the names of the fields (input ports) on the **Prompt Template** component.\n\n    For this tutorial, the variables are named after the components that connect to them: **chat-input** for the **Chat Input** component and **file** for the **File** component.\n    \n\n5. Add a [**File** component](/components-data#file) to the flow, and then connect the **Raw Content** output port to the **Prompt Template** component's **file** input port.\nTo connect ports, click and drag from one port to the other.\n\n    You can add files directly to the **File** component to pre-load input before running the flow, or you can load files at runtime. The next section of this tutorial covers runtime file uploads.\n\n    At this point your flow has five components. The **Chat Input** and **File** components are connected to the **Prompt Template** component's input ports. Then, the **Prompt Template** component's output port is connected to the **Language Model** component's input port. Finally, the **Language Model** component's output port is connected to the **Chat Output** component, which returns the final response to the user.\n\n    ![File loader chat flow](/img/tutorial-chat-file-loader.png)\n\n## Send requests to your flow from a Python application\n\nThis section of the tutorial demonstrates how you can send file input to a flow from an application.\n\nTo do this, your application must send a `POST /run` request to your Langflow server with the file you want to upload and a text prompt.\nThe result includes the outcome of the flow run and the LLM's response.\n\nThis example uses a local Langflow instance, and it asks the LLM to evaluate a sample resume.\nIf you don't have a resume on hand, you can download [fake-resume.txt](/files/fake-resume.txt).\n\n**Note:** For help with constructing file upload requests in Python, JavaScript, and curl, see the [Langflow File Upload Utility](https://langflow-file-upload-examples.onrender.com).\n\n1. To construct the request, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FILE_COMPONENT_ID`: The UUID of the **File** component in your flow, such as `File-KZP68`. To find the component ID, open your flow in Langflow, click the **File** component, and then click **Controls**. The component ID is at the top of the **Controls** pane.\n    * `CHAT_INPUT`: The message you want to send to the Chat Input of your flow, such as `Evaluate this resume for a job opening in my Marketing department.`\n    * `FILE_NAME` and `FILE_PATH`: The name and path to the local file that you want to send to your flow.\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```python\n    # Python example using requests\n    import requests\n    import json\n\n    # 1. Set the upload URL\n    url = \"http://LANGFLOW_SERVER_ADDRESS/api/v2/files/\"\n\n    # 2. Prepare the file and payload\n    payload = {}\n    files = [\n      ('file', ('FILE_PATH', open('FILE_NAME', 'rb'), 'application/octet-stream'))\n    ]\n    headers = {\n      'Accept': 'application/json',\n      'x-api-key': 'LANGFLOW_API_KEY'\n    }\n\n    # 3. Upload the file to Langflow\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n    print(response.text)\n\n    # 4. Get the uploaded file path from the response\n    uploaded_data = response.json()\n    uploaded_path = uploaded_data.get('path')\n\n    # 5. Call the Langflow run endpoint with the uploaded file path\n    run_url = \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\"\n    run_payload = {\n        \"input_value\": \"CHAT_INPUT\",\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"tweaks\": {\n            \"FILE_COMPONENT_ID\": {\n                \"path\": uploaded_path\n            }\n        }\n    }\n    run_headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n        'x-api-key': 'LANGFLOW_API_KEY'\n    }\n    run_response = requests.post(run_url, headers=run_headers, data=json.dumps(run_payload))\n    langflow_data = run_response.json()\n    # Output only the message\n    message = None\n    try:\n        message = langflow_data['outputs'][0]['outputs'][0]['results']['message']['data']['text']\n    except (KeyError, IndexError, TypeError):\n        pass\n    print(message)\n\n    ```\n\n    This script contains two requests.\n\n    The first request uploads a file, such as `fake-resume.txt`, to your Langflow server at the `/v2/files` endpoint. This request returns a file path that can be referenced in subsequent Langflow requests, such as `02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt`\n\n    The second request sends a chat message to the Langflow flow at the `/v1/run/` endpoint.\n    The `tweaks` parameter includes the path to the uploaded file as the variable `uploaded_path`, and sends this file directly to the **File** component.\n\n3. Save and run the script to send the requests and test the flow.\n\n    The initial output contains the JSON response object from the file upload endpoint, including the internal path where Langflow stores the file.\n    Then, the LLM retrieves the file and evaluates its content, in this case the suitability of the resume for a job position.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n    {\"id\":\"793ba3d8-5e7a-4499-8b89-d9a7b6325fee\",\"name\":\"fake-resume (1)\",\"path\":\"02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt\",\"size\":1779,\"provider\":null}\n    The resume for Emily J. Wilson presents a strong candidate for a position in your Marketing department. Here are some key points to consider:\n\n    ### Strengths:\n    1. **Experience**: With over 8 years in marketing, Emily has held progressively responsible positions, culminating in her current role as Marketing Director. This indicates a solid foundation in the field.\n\n    2. **Quantifiable Achievements**: The resume highlights specific accomplishments, such as a 25% increase in brand recognition and a 30% sales increase after launching new product lines. These metrics demonstrate her ability to drive results.\n\n    3. **Diverse Skill Set**: Emily's skills encompass various aspects of marketing, including strategy development, team management, social media marketing, event planning, and data analysis. This versatility can be beneficial in a dynamic marketing environment.\n\n    4. **Educational Background**: Her MBA and a Bachelor's degree in Marketing provide a strong academic foundation, which is often valued in marketing roles.\n\n    5. **Certifications**: The Certified Marketing Professional (CMP) and Google Analytics Certification indicate a commitment to professional development and staying current with industry standards.\n\n    ### Areas for Improvement:\n    1. **Specificity in Skills**: While the skills listed are relevant, providing examples of how she has applied these skills in her previous roles could strengthen her resume further.\n\n    2. **References**: While stating that references are available upon request is standard, including a couple of testimonials or notable endorsements could enhance credibility.\n\n    3. **Formatting**: Ensure that the resume is visually appealing and easy to read. Clear headings and bullet points help in quickly identifying key information.\n\n    ### Conclusion:\n    Overall, Emily J. Wilson's resume reflects a well-rounded marketing professional with a proven track record of success. If her experience aligns with the specific needs of your Marketing department, she could be a valuable addition to your team. Consider inviting her for an interview to further assess her fit for the role.\n    ```\n\n    </details>\n\n## Next steps\n\nTo continue building on this tutorial, try these next steps.\n\n### Process multiple files loaded at runtime\n\nTo process multiple files in a single flow run, add a separate **File** component for each file you want to ingest. Then, modify your script to upload each file, retrieve each returned file path, and then pass a unique file path to each **File** component ID.\n\nFor example, you can modify `tweaks` to accept multiple **File** components.\nThe following code is just an example; it isn't working code:\n\n```python\n## set multiple file paths\nfile_paths = {\n    FILE_COMPONENT_1: uploaded_path_1,\n    FILE_COMPONENT_2: uploaded_path_2\n}\n\ndef chat_with_flow(input_message, file_paths):\n    \"\"\"Compare the contents of these two files.\"\"\"\n    run_url = f\"{LANGFLOW_SERVER_ADDRESS}/api/v1/run/{FLOW_ID}\"\n    # Prepare tweaks with both file paths\n    tweaks = {}\n    for component_id, file_path in file_paths.items():\n        tweaks[component_id] = {\"path\": file_path}\n```\n\nYou can also use a [**Directory** component](/components-data#directory) to load all files in a directory or pass an archive file to the **File** component.\n\n### Upload external files at runtime\n\nTo upload files from another machine that isn't your local environment, your Langflow server must first be accessible over the internet. Then, authenticated users can upload files to your public Langflow server's `/v2/files/` endpoint, as shown in the tutorial. For more information, see [Langflow deployment overview](/deployment-overview).\n\n### Preload files outside the chat session\n\nYou can use the **File** component to load files anywhere in a flow, not just in a chat session.\n\nIn the visual editor, you can preload files to the **File** component by selecting them from your local machine or [Langflow file management](/concepts-file-management).\n\nFor example, you can preload an instructions file for a prompt template, or you can preload a vector store with documents that you want to query in a Retrieval Augmented Generation (RAG) flow.\n\nFor more information about the **File** component and other data loading components, see [Data components](/components-data).",
      "summary": "This tutorial shows you how to build a chatbot that can read and answer questions about files you upload, such as meeting notes or job applications.",
      "keywords": [
        "accepts",
        "access",
        "achievements",
        "agent",
        "api",
        "application",
        "background",
        "basic",
        "certifications",
        "chat",
        "chatbot",
        "content",
        "controls",
        "create",
        "directory",
        "diverse",
        "educational",
        "experience",
        "external",
        "file"
      ]
    },
    "chat-with-files": {
      "title": "Create a chatbot that can ingest files",
      "slug": "/chat-with-files",
      "category": "tutorials",
      "content": "This tutorial shows you how to build a chatbot that can read and answer questions about files you upload, such as meeting notes or job applications.\n\nFor example, you could upload a contract and ask, What are the termination clauses in this agreement? Or upload a resume and ask, Does this candidate have experience with marketing analytics?\n\nThe main focus of this tutorial is to show you how to provide files as input to a Langflow flow, so your chatbot can use the content of those files in its responses.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create a flow that accepts file input\n\nTo ingest files, your flow must have a **File** component attached to a component that receives input, such as a **Prompt Template** or **Agent** component.\n\nThe following steps modify the **Basic Prompting** template to accept file input:\n\n1. In Langflow, click **New Flow**, and then select the **Basic Prompting** template.\n2. In the **Language Model** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n3. To verify that your API key is valid, click  **Playground**, and then ask the LLM a question.\nThe LLM should respond according to the specifications in the **Prompt Template** component's **Template** field.\n\n4. Exit the **Playground**, and then modify the **Prompt Template** component to accept file input in addition to chat input.\nTo do this, edit the **Template** field, and then replace the default prompt with the following text:\n\n    ```text\n    ChatInput:\n    {chat-input}\n    File:\n    {file}\n    ```\n\n    \n**Note:** You can use any string to name your template variables.\n    These strings become the names of the fields (input ports) on the **Prompt Template** component.\n\n    For this tutorial, the variables are named after the components that connect to them: **chat-input** for the **Chat Input** component and **file** for the **File** component.\n    \n\n5. Add a [**File** component](/components-data#file) to the flow, and then connect the **Raw Content** output port to the **Prompt Template** component's **file** input port.\nTo connect ports, click and drag from one port to the other.\n\n    You can add files directly to the **File** component to pre-load input before running the flow, or you can load files at runtime. The next section of this tutorial covers runtime file uploads.\n\n    At this point your flow has five components. The **Chat Input** and **File** components are connected to the **Prompt Template** component's input ports. Then, the **Prompt Template** component's output port is connected to the **Language Model** component's input port. Finally, the **Language Model** component's output port is connected to the **Chat Output** component, which returns the final response to the user.\n\n    ![File loader chat flow](/img/tutorial-chat-file-loader.png)\n\n## Send requests to your flow from a Python application\n\nThis section of the tutorial demonstrates how you can send file input to a flow from an application.\n\nTo do this, your application must send a `POST /run` request to your Langflow server with the file you want to upload and a text prompt.\nThe result includes the outcome of the flow run and the LLM's response.\n\nThis example uses a local Langflow instance, and it asks the LLM to evaluate a sample resume.\nIf you don't have a resume on hand, you can download [fake-resume.txt](/files/fake-resume.txt).\n\n**Note:** For help with constructing file upload requests in Python, JavaScript, and curl, see the [Langflow File Upload Utility](https://langflow-file-upload-examples.onrender.com).\n\n1. To construct the request, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FILE_COMPONENT_ID`: The UUID of the **File** component in your flow, such as `File-KZP68`. To find the component ID, open your flow in Langflow, click the **File** component, and then click **Controls**. The component ID is at the top of the **Controls** pane.\n    * `CHAT_INPUT`: The message you want to send to the Chat Input of your flow, such as `Evaluate this resume for a job opening in my Marketing department.`\n    * `FILE_NAME` and `FILE_PATH`: The name and path to the local file that you want to send to your flow.\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```python\n    # Python example using requests\n    import requests\n    import json\n\n    # 1. Set the upload URL\n    url = \"http://LANGFLOW_SERVER_ADDRESS/api/v2/files/\"\n\n    # 2. Prepare the file and payload\n    payload = {}\n    files = [\n      ('file', ('FILE_PATH', open('FILE_NAME', 'rb'), 'application/octet-stream'))\n    ]\n    headers = {\n      'Accept': 'application/json',\n      'x-api-key': 'LANGFLOW_API_KEY'\n    }\n\n    # 3. Upload the file to Langflow\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n    print(response.text)\n\n    # 4. Get the uploaded file path from the response\n    uploaded_data = response.json()\n    uploaded_path = uploaded_data.get('path')\n\n    # 5. Call the Langflow run endpoint with the uploaded file path\n    run_url = \"http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\"\n    run_payload = {\n        \"input_value\": \"CHAT_INPUT\",\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"tweaks\": {\n            \"FILE_COMPONENT_ID\": {\n                \"path\": uploaded_path\n            }\n        }\n    }\n    run_headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json',\n        'x-api-key': 'LANGFLOW_API_KEY'\n    }\n    run_response = requests.post(run_url, headers=run_headers, data=json.dumps(run_payload))\n    langflow_data = run_response.json()\n    # Output only the message\n    message = None\n    try:\n        message = langflow_data['outputs'][0]['outputs'][0]['results']['message']['data']['text']\n    except (KeyError, IndexError, TypeError):\n        pass\n    print(message)\n\n    ```\n\n    This script contains two requests.\n\n    The first request uploads a file, such as `fake-resume.txt`, to your Langflow server at the `/v2/files` endpoint. This request returns a file path that can be referenced in subsequent Langflow requests, such as `02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt`\n\n    The second request sends a chat message to the Langflow flow at the `/v1/run/` endpoint.\n    The `tweaks` parameter includes the path to the uploaded file as the variable `uploaded_path`, and sends this file directly to the **File** component.\n\n3. Save and run the script to send the requests and test the flow.\n\n    The initial output contains the JSON response object from the file upload endpoint, including the internal path where Langflow stores the file.\n    Then, the LLM retrieves the file and evaluates its content, in this case the suitability of the resume for a job position.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n    {\"id\":\"793ba3d8-5e7a-4499-8b89-d9a7b6325fee\",\"name\":\"fake-resume (1)\",\"path\":\"02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt\",\"size\":1779,\"provider\":null}\n    The resume for Emily J. Wilson presents a strong candidate for a position in your Marketing department. Here are some key points to consider:\n\n    ### Strengths:\n    1. **Experience**: With over 8 years in marketing, Emily has held progressively responsible positions, culminating in her current role as Marketing Director. This indicates a solid foundation in the field.\n\n    2. **Quantifiable Achievements**: The resume highlights specific accomplishments, such as a 25% increase in brand recognition and a 30% sales increase after launching new product lines. These metrics demonstrate her ability to drive results.\n\n    3. **Diverse Skill Set**: Emily's skills encompass various aspects of marketing, including strategy development, team management, social media marketing, event planning, and data analysis. This versatility can be beneficial in a dynamic marketing environment.\n\n    4. **Educational Background**: Her MBA and a Bachelor's degree in Marketing provide a strong academic foundation, which is often valued in marketing roles.\n\n    5. **Certifications**: The Certified Marketing Professional (CMP) and Google Analytics Certification indicate a commitment to professional development and staying current with industry standards.\n\n    ### Areas for Improvement:\n    1. **Specificity in Skills**: While the skills listed are relevant, providing examples of how she has applied these skills in her previous roles could strengthen her resume further.\n\n    2. **References**: While stating that references are available upon request is standard, including a couple of testimonials or notable endorsements could enhance credibility.\n\n    3. **Formatting**: Ensure that the resume is visually appealing and easy to read. Clear headings and bullet points help in quickly identifying key information.\n\n    ### Conclusion:\n    Overall, Emily J. Wilson's resume reflects a well-rounded marketing professional with a proven track record of success. If her experience aligns with the specific needs of your Marketing department, she could be a valuable addition to your team. Consider inviting her for an interview to further assess her fit for the role.\n    ```\n\n    </details>\n\n## Next steps\n\nTo continue building on this tutorial, try these next steps.\n\n### Process multiple files loaded at runtime\n\nTo process multiple files in a single flow run, add a separate **File** component for each file you want to ingest. Then, modify your script to upload each file, retrieve each returned file path, and then pass a unique file path to each **File** component ID.\n\nFor example, you can modify `tweaks` to accept multiple **File** components.\nThe following code is just an example; it isn't working code:\n\n```python\n## set multiple file paths\nfile_paths = {\n    FILE_COMPONENT_1: uploaded_path_1,\n    FILE_COMPONENT_2: uploaded_path_2\n}\n\ndef chat_with_flow(input_message, file_paths):\n    \"\"\"Compare the contents of these two files.\"\"\"\n    run_url = f\"{LANGFLOW_SERVER_ADDRESS}/api/v1/run/{FLOW_ID}\"\n    # Prepare tweaks with both file paths\n    tweaks = {}\n    for component_id, file_path in file_paths.items():\n        tweaks[component_id] = {\"path\": file_path}\n```\n\nYou can also use a [**Directory** component](/components-data#directory) to load all files in a directory or pass an archive file to the **File** component.\n\n### Upload external files at runtime\n\nTo upload files from another machine that isn't your local environment, your Langflow server must first be accessible over the internet. Then, authenticated users can upload files to your public Langflow server's `/v2/files/` endpoint, as shown in the tutorial. For more information, see [Langflow deployment overview](/deployment-overview).\n\n### Preload files outside the chat session\n\nYou can use the **File** component to load files anywhere in a flow, not just in a chat session.\n\nIn the visual editor, you can preload files to the **File** component by selecting them from your local machine or [Langflow file management](/concepts-file-management).\n\nFor example, you can preload an instructions file for a prompt template, or you can preload a vector store with documents that you want to query in a Retrieval Augmented Generation (RAG) flow.\n\nFor more information about the **File** component and other data loading components, see [Data components](/components-data).",
      "summary": "This tutorial shows you how to build a chatbot that can read and answer questions about files you upload, such as meeting notes or job applications.",
      "keywords": [
        "accepts",
        "access",
        "achievements",
        "agent",
        "api",
        "application",
        "background",
        "basic",
        "certifications",
        "chat",
        "chatbot",
        "content",
        "controls",
        "create",
        "directory",
        "diverse",
        "educational",
        "experience",
        "external",
        "file"
      ]
    },
    "/chat-with-rag": {
      "title": "Create a vector RAG chatbot",
      "slug": "/chat-with-rag",
      "category": "tutorials",
      "content": "This tutorial demonstrates how you can use Langflow to create a chatbot application that uses Retrieval Augmented Generation (RAG) to embed your data as vectors in a vector database, and then chat with the data.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n* Install the [Langflow JavaScript client](/typescript-client)\n* Be familiar with vector search concepts and applications, such as vector databases and RAG\n\n## Create a vector RAG flow\n\n1. In Langflow, click **New Flow**, and then select the **Vector Store RAG** template.\n\n    <details>\n    <summary>About the Vector Store RAG template</summary>\n\n    This template has two flows.\n\n    The **Load Data Flow** populates a vector store with data from a file.\n    This data is used to respond to queries submitted to the **Retriever Flow**.\n\n    Specifically, the **Load Data Flow** ingests data from a local file, splits the data into chunks, loads and indexes the data in your vector database, and then computes embeddings for the chunks. The embeddings are also stored with the loaded data. This flow only needs to run when you need to load data into your vector database.\n\n    The **Retriever Flow** receives chat input, generates an embedding for the input, and then uses several components to reconstruct chunks into text and generate a response by comparing the new embedding to the stored embeddings to find similar data.\n\n    </details>\n\n2. Add your **OpenAI** API key to both **OpenAI Embeddings** components.\n\n3. Optional: Replace both **Astra DB** vector store components with a **Chroma DB** or another vector store component of your choice.\nThis tutorial uses Chroma DB.\n\n    The **Load Data Flow** should have **File**, **Split Text**, **Embedding Model**, vector store (such as **Chroma DB**), and **Chat Output** components:\n\n    ![File loader chat flow](/img/tutorial-chatbot-embed-files.png)\n\n    The **Retriever Flow** should have **Chat Input**, **Embedding Model**, vector store, **Parser**, **Prompt**, **Language Model**, and **Chat Output** components:\n\n    ![Chat with RAG flow](/img/tutorial-chatbot-chat-flow.png)\n\n    The flows are ready to use.\n    Continue the tutorial to learn how to use the loading flow to load data into your vector store, and then call the chat flow in a chatbot application.\n\n## Load data and generate embeddings\n\nTo load data and generate embeddings, you can use the visual editor or the `/v2/files` endpoint.\n\nThe visual editor option is simpler, but it is only recommended for scenarios where the user who created the flow is the same user who loads data into the database.\n\nIn situations where many users load data or you need to load data programmatically, use the Langflow API option.\n\n</Tabs>\n\nWhen the flow runs, the flow ingests the selected file, chunks the data, loads the data into the vector store database, and then generates embeddings for the chunks, which are also stored in the vector store.\n\nYour database now contains data with vector embeddings that an LLM can use as context to respond to queries, as demonstrated in the next section of the tutorial.\n\n## Chat with your flow from a JavaScript application\n\nTo chat with the data in your vector database, create a chatbot application that runs the **Retriever Flow** programmatically.\n\nThis tutorial uses JavaScript for demonstration purposes.\n\n1. To construct the chatbot, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```js\n    const readline = require('readline');\n    const { LangflowClient } = require('@datastax/langflow-client');\n\n    const API_KEY = 'LANGFLOW_API_KEY';\n    const SERVER = 'LANGFLOW_SERVER_ADDRESS';\n    const FLOW_ID = 'FLOW_ID';\n\n    const rl = readline.createInterface({ input: process.stdin, output: process.stdout });\n\n    // Initialize the Langflow client\n    const client = new LangflowClient({\n        baseUrl: SERVER,\n        apiKey: API_KEY\n    });\n\n    async function sendMessage(message) {\n        try {\n            const response = await client.flow(FLOW_ID).run(message, {\n                session_id: 'user_1'\n            });\n\n            // Use the convenience method to get the chat output text\n            return response.chatOutputText() || 'No response';\n        } catch (error) {\n            return `Error: ${error.message}`;\n        }\n    }\n\n    function chat() {\n        console.log(' Langflow RAG Chatbot (type \"quit\" to exit)\\n');\n\n        const ask = () => {\n            rl.question(' You: ', async (input) => {\n                if (['quit', 'exit', 'bye'].includes(input.trim().toLowerCase())) {\n                    console.log(' Goodbye!');\n                    rl.close();\n                    return;\n                }\n\n                const response = await sendMessage(input.trim());\n                console.log(` Assistant: ${response}\\n`);\n                ask();\n            });\n        };\n\n        ask();\n    }\n\n    chat();\n    ```\n\n    The script creates a Node application that chats with the content in your vector database, using the `chat` input and output types to communicate with your flow.\n    Chat maintains ongoing conversation context across multiple messages. If you used `text` type inputs and outputs, each request is a standalone text string.\n\n    \n**Note:** The [Langflow TypeScript client](/typescript-client) has a `chatOutputText()` convenience method that simplifies working with Langflow's complex JSON response structure.\n    Instead of manually navigating through multiple levels of nested objects with `data.outputs[0].outputs[0].results.message.data.text`, the client automatically extracts the message text and handles potentially undefined values gracefully.\n    \n\n3. Save and run the script to send the requests and test the flow.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n     You: Do you have any documents about engines?\n     Assistant: Yes, the provided text contains several warnings and guidelines related to engine installation, maintenance, and selection. It emphasizes the importance of using the correct engine for specific applications, ensuring all components are in good condition, and following safety precautions to prevent fire or explosion. If you need more specific information or details, please let me know!\n\n     You: It should be about a Briggs and Stratton engine.\n     Assistant: The text provides important safety and installation guidelines for Briggs & Stratton engines. It emphasizes that these engines should not be used on 3-wheel All-Terrain Vehicles (ATVs), motor bikes, aircraft products, or vehicles intended for competitive events, as such uses are not approved by Briggs & Stratton.\n\n    If you have any specific questions about Briggs & Stratton engines or need further information, feel free to ask!\n    ```\n\n    </details>\n\n## Next steps\n\nFor more information on building or extending this tutorial, see the following:\n\n* [Model Context Protocol (MCP) servers](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial demonstrates how you can use Langflow to create a chatbot application that uses Retrieval Augmented Generation (RAG) to embed your data as vectors in a vector database, and then chat with the data.",
      "keywords": [
        "access",
        "api",
        "application",
        "astra",
        "chat",
        "chatbot",
        "chroma",
        "create",
        "data",
        "embedding",
        "embeddings",
        "file",
        "flow",
        "generate",
        "input",
        "javascript",
        "language",
        "load",
        "model",
        "new"
      ]
    },
    "chat-with-rag": {
      "title": "Create a vector RAG chatbot",
      "slug": "/chat-with-rag",
      "category": "tutorials",
      "content": "This tutorial demonstrates how you can use Langflow to create a chatbot application that uses Retrieval Augmented Generation (RAG) to embed your data as vectors in a vector database, and then chat with the data.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n* Install the [Langflow JavaScript client](/typescript-client)\n* Be familiar with vector search concepts and applications, such as vector databases and RAG\n\n## Create a vector RAG flow\n\n1. In Langflow, click **New Flow**, and then select the **Vector Store RAG** template.\n\n    <details>\n    <summary>About the Vector Store RAG template</summary>\n\n    This template has two flows.\n\n    The **Load Data Flow** populates a vector store with data from a file.\n    This data is used to respond to queries submitted to the **Retriever Flow**.\n\n    Specifically, the **Load Data Flow** ingests data from a local file, splits the data into chunks, loads and indexes the data in your vector database, and then computes embeddings for the chunks. The embeddings are also stored with the loaded data. This flow only needs to run when you need to load data into your vector database.\n\n    The **Retriever Flow** receives chat input, generates an embedding for the input, and then uses several components to reconstruct chunks into text and generate a response by comparing the new embedding to the stored embeddings to find similar data.\n\n    </details>\n\n2. Add your **OpenAI** API key to both **OpenAI Embeddings** components.\n\n3. Optional: Replace both **Astra DB** vector store components with a **Chroma DB** or another vector store component of your choice.\nThis tutorial uses Chroma DB.\n\n    The **Load Data Flow** should have **File**, **Split Text**, **Embedding Model**, vector store (such as **Chroma DB**), and **Chat Output** components:\n\n    ![File loader chat flow](/img/tutorial-chatbot-embed-files.png)\n\n    The **Retriever Flow** should have **Chat Input**, **Embedding Model**, vector store, **Parser**, **Prompt**, **Language Model**, and **Chat Output** components:\n\n    ![Chat with RAG flow](/img/tutorial-chatbot-chat-flow.png)\n\n    The flows are ready to use.\n    Continue the tutorial to learn how to use the loading flow to load data into your vector store, and then call the chat flow in a chatbot application.\n\n## Load data and generate embeddings\n\nTo load data and generate embeddings, you can use the visual editor or the `/v2/files` endpoint.\n\nThe visual editor option is simpler, but it is only recommended for scenarios where the user who created the flow is the same user who loads data into the database.\n\nIn situations where many users load data or you need to load data programmatically, use the Langflow API option.\n\n</Tabs>\n\nWhen the flow runs, the flow ingests the selected file, chunks the data, loads the data into the vector store database, and then generates embeddings for the chunks, which are also stored in the vector store.\n\nYour database now contains data with vector embeddings that an LLM can use as context to respond to queries, as demonstrated in the next section of the tutorial.\n\n## Chat with your flow from a JavaScript application\n\nTo chat with the data in your vector database, create a chatbot application that runs the **Retriever Flow** programmatically.\n\nThis tutorial uses JavaScript for demonstration purposes.\n\n1. To construct the chatbot, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```js\n    const readline = require('readline');\n    const { LangflowClient } = require('@datastax/langflow-client');\n\n    const API_KEY = 'LANGFLOW_API_KEY';\n    const SERVER = 'LANGFLOW_SERVER_ADDRESS';\n    const FLOW_ID = 'FLOW_ID';\n\n    const rl = readline.createInterface({ input: process.stdin, output: process.stdout });\n\n    // Initialize the Langflow client\n    const client = new LangflowClient({\n        baseUrl: SERVER,\n        apiKey: API_KEY\n    });\n\n    async function sendMessage(message) {\n        try {\n            const response = await client.flow(FLOW_ID).run(message, {\n                session_id: 'user_1'\n            });\n\n            // Use the convenience method to get the chat output text\n            return response.chatOutputText() || 'No response';\n        } catch (error) {\n            return `Error: ${error.message}`;\n        }\n    }\n\n    function chat() {\n        console.log(' Langflow RAG Chatbot (type \"quit\" to exit)\\n');\n\n        const ask = () => {\n            rl.question(' You: ', async (input) => {\n                if (['quit', 'exit', 'bye'].includes(input.trim().toLowerCase())) {\n                    console.log(' Goodbye!');\n                    rl.close();\n                    return;\n                }\n\n                const response = await sendMessage(input.trim());\n                console.log(` Assistant: ${response}\\n`);\n                ask();\n            });\n        };\n\n        ask();\n    }\n\n    chat();\n    ```\n\n    The script creates a Node application that chats with the content in your vector database, using the `chat` input and output types to communicate with your flow.\n    Chat maintains ongoing conversation context across multiple messages. If you used `text` type inputs and outputs, each request is a standalone text string.\n\n    \n**Note:** The [Langflow TypeScript client](/typescript-client) has a `chatOutputText()` convenience method that simplifies working with Langflow's complex JSON response structure.\n    Instead of manually navigating through multiple levels of nested objects with `data.outputs[0].outputs[0].results.message.data.text`, the client automatically extracts the message text and handles potentially undefined values gracefully.\n    \n\n3. Save and run the script to send the requests and test the flow.\n\n    <details>\n    <summary>Result</summary>\n\n    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.\n\n    ```\n     You: Do you have any documents about engines?\n     Assistant: Yes, the provided text contains several warnings and guidelines related to engine installation, maintenance, and selection. It emphasizes the importance of using the correct engine for specific applications, ensuring all components are in good condition, and following safety precautions to prevent fire or explosion. If you need more specific information or details, please let me know!\n\n     You: It should be about a Briggs and Stratton engine.\n     Assistant: The text provides important safety and installation guidelines for Briggs & Stratton engines. It emphasizes that these engines should not be used on 3-wheel All-Terrain Vehicles (ATVs), motor bikes, aircraft products, or vehicles intended for competitive events, as such uses are not approved by Briggs & Stratton.\n\n    If you have any specific questions about Briggs & Stratton engines or need further information, feel free to ask!\n    ```\n\n    </details>\n\n## Next steps\n\nFor more information on building or extending this tutorial, see the following:\n\n* [Model Context Protocol (MCP) servers](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial demonstrates how you can use Langflow to create a chatbot application that uses Retrieval Augmented Generation (RAG) to embed your data as vectors in a vector database, and then chat with the data.",
      "keywords": [
        "access",
        "api",
        "application",
        "astra",
        "chat",
        "chatbot",
        "chroma",
        "create",
        "data",
        "embedding",
        "embeddings",
        "file",
        "flow",
        "generate",
        "input",
        "javascript",
        "language",
        "load",
        "model",
        "new"
      ]
    },
    "/mcp-tutorial": {
      "title": "Connect to MCP servers from your application",
      "slug": "/mcp-tutorial",
      "category": "tutorials",
      "content": "This tutorial shows you how to connect MCP servers to your applications using Langflow's [**MCP Tools** component](/mcp-client).\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) helps agents integrate with LLMs through _MCP clients_ and _MCP servers_.\nSpecifically, MCP servers host tools that agents (MCP clients) use to complete specialized tasks.\nMCP servers are connected to MCP clients like Cursor.\nThen, you interact with the client, and the client uses tools from the connected servers as needed to complete your requests.\n\nYou can run Langflow as an MCP client and an MCP server:\n\n* [Use Langflow as an MCP client](/mcp-client): When run as an MCP client, an **Agent** component in a Langflow flow can use connected components as tools to handle requests.\nYou can use existing components as tools, and you can connect any MCP server to your flow to make that server's tools available to the agent.\n\n* [Use Langflow as an MCP server](/mcp-server): When run as an MCP server, your flows become tools that can be used by an MCP client, which could be an external client or another Langflow flow.\n\nIn this tutorial, you will use the Langflow **MCP Tools** component to connect multiple MCP servers to your flow, and then you'll use a Python application to run your flow and chat with the agent programmatically.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create an agent flow\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n\n2. In the **Agent** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n3. To test the flow, click  **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n\n    This query demonstrates how an LLM, by itself, might not have access to information or functions designed to address specialized queries. In this example, the default OpenAI model provides a vague response, although the agent does know the current date by using its internal `get_current_date` function.\n\n    ```text\n    Today is July 11, 2025.\n    To determine if it's safe to go hiking in the Adirondacks today, you should check the current weather conditions, trail advisories, and any local alerts (such as bear activity or flooding).\n    Would you like a detailed weather forecast or information on trail conditions for the Adirondacks today?\n    ```\n\n    To improve the response, you can connect MCP servers to your flow that provide specialized tools for the agent to use when generating responses. In the next part of this tutorial, you'll connect an MCP server that provides the agent with real-time weather information so that it can generate a more specific response.\n\n## Add an MCP Tools component\n\nThere are many MCP servers available online that offer different tools for different tasks.\nTo use an MCP server with an MCP client, you have to make the server available to the client.\nWith all MCP clients, there are several ways to do this:\n\n* Install the server locally.\n* Use `uvx` or `npx` to fetch and run a server package.\n* Call a server running remotely, like those available on Smithery.\n\nThis tutorial demonstrates how to install a weather server locally with `uv pip install`, and how to use `npx` to run the geolocation server package.\nYour particular MCP server's requirements may vary.\n\nIn Langflow, you use the **MCP Tools** component to connect a specific MCP server to a flow.\nYou need one **MCP Tools** component for each MCP server that you want your flow to use.\n\n1. For this tutorial, install a [weather MCP server](https://github.com/isdaniel/mcp_weather_server) on your local machine with uv and Python:\n\n    ```shell\n    uv pip install mcp_weather_server\n    ```\n\n    Make sure you install the server in the same Python environment where Langflow is running:\n\n    * Langflow in a virtual environment: Activate the environment before installing the server.\n    * Langflow Docker image: Install the server inside the Docker container.\n    * Langflow Desktop or system-wide Langflow OSS: Install the server globally or in the same user environment where you run Langflow.\n\n2. In your **Simple Agent** flow, remove the **URL** and **Calculator** tools, and then add an [**MCP Tools**](/mcp-client) component.\n\n3. Click the **MCP Tools** component, and then click  **Add MCP Server**.\n\n    \n**Note:** To manage all of your MCP server connections, click  **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.\n    \n\n4. In the **Add MCP Server** pane, provide the server startup command and arguments to connect the weather MCP server to your flow. For this tutorial, use either the **JSON** or **STDIO** option.\n\n    Langflow runs the command to launch the server when the agent determines that it needs to use a tool provided by that server.\n\n    Notice that both configurations provide the same information but in different formats.\n    This means that if your MCP server repository only provides a JSON file for the server, you can still use those values with the STDIO option.\n\n    \n    \n    </Tabs>\n\n5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.\n\n    With this weather server, the **MCP Tools** component also adds an optional **City** field.\n    For this tutorial, don't enter anything in this field.\n    Instead, you will add a geolocation MCP server in the next step, which the agent will use to detect your location.\n\n6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.\n\n    At this point your flow has four connected components:\n\n    * The **Chat Input** component is connected to the **Agent** component's **Input** port. This allows to flow to be triggered by an incoming prompt from a user or application.\n    * The **MCP Tools** component with the weather MCP server is connected to the **Agent** component's **Tools** port. The agent may not use this server for every request; the agent only uses this connection if it decides the server can help respond to the prompt.\n    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.\n\n    ![An Agent component connected to an MCP weather server](/img/tutorial-mcp-weather.png)\n\n7. To test the weather MCP server, click  **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n\n    The  **Playground** shows you the agent's logic as it analyzes the request and select tools to use.\n\n    Ideally, the agent's response will be more specific than the previous response because of the additional context provided by the weather MCP server.\n    For example:\n\n    ```text\n    The current weather in Lake Placid, a central location in the Adirondacks,\n    is foggy with a temperature of 17.2C (about 63F).\n    If you plan to go hiking today, be cautious as fog can reduce visibility\n    on trails and make navigation more difficult.\n    ```\n\n    This is a better response, but what makes this MCP server more valuable than just calling a weather API?\n\n    First, MCP servers are often customized for specific tasks, such as highly specialized actions or chained tools for complex, multi-step problem solving.\n    Typically, you would have to write a custom script for a specific task, possibly including multiple API calls in a single script, and then you would have to either execute this script outside the context of the agent or provide it to your agent in some way.\n\n    Instead, the MCP ensures that all MCP servers are added to agents in the same way, without having to know each server's specific endpoint structures or write custom integrations.\n    The MCP is a standardized way to integrate many diverse tools into agentic applications.\n    You don't have to learn a new API or write custom code every time you want to use a new MCP server.\n\n    Additionally, you can attach many MCP servers to one agent, depending on the problems you want your application to solve.\n    The more servers you add, the more specialized context the agent can use in its responses.\n    In this tutorial, adding the weather MCP server already improved the quality of the LLM's response.\n    In the next section of the tutorial, you will add a `ip_geolocation` MCP server so the agent can detect the user's location if they don't specify a location in their prompt.\n\n## Add a geolocation server\n\nThe [Toolkit MCP server](https://github.com/cyanheads/toolkit-mcp-server) includes multiple MCP tools for network monitoring, including IP geolocation. It isn't extremely precise, but it doesn't require an API key.\n\nNote that this tool returns the IP geolocation of your Langflow server, so if your server is deployed remotely, consider alternative approaches for getting user-specific location data, such as browser geolocation APIs.\n\nThis MCP server can be started with one [npx](https://docs.npmjs.com/cli/v8/commands/npx) command, which downloads and runs the [Toolkit MCP server Node registry package](https://www.npmjs.com/package/@cyanheads/toolkit-mcp-server) without installing the package locally.\n\nTo add the Toolkip MCP server to your flow, do the following:\n\n1. Add another **MCP Tools** component to your flow, click the component, and then click  **Add MCP Server**.\n\n2. Select **STDIO**.\n\n3. For **Name**, enter `ip_geolocation`.\n\n    \n**Note:** The tool name and description help the agent select tools.\n    If your agent is struggling to select tools, make sure the names and descriptions are clear and human-readable.\n    \n\n4. For **Command**, enter `npx @cyanheads/toolkit-mcp-server`.\n\n5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.\n\n6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.\n\n    Your flow now has an additional **MCP Tools** component for a total of five components.\n\n    ![An Agent component connected to MCP weather and geolocation servers](/img/tutorial-mcp-geolocation.png)\n\n## Create a Python application that connects to Langflow\n\nAt this point, you can open the **Playground** and ask about the weather in your current location to test the IP geolocation tool.\nHowever, geolocation tools are most useful in applications where you or your users want to ask about the weather from different places around the world.\n\nIn the last part of this tutorial, you'll learn how to use the Langflow API to run a flow in a script.\nThis could be part of a larger application, such as a mobile app where users want to know if the weather is good for a particular sport.\n\nWhen you use the Langflow API to run a flow, you can change some aspects of the flow without changing the code.\nFor example, you can add more MCP servers to your flow in Langflow, and then use the same script to run the flow.\nYou can use the same input or a new input that prompts the agent to use other tools.\n\n1. For this tutorial's Python script, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```python\n    import requests\n    import os\n\n    url = \"LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\"  # The complete API endpoint URL for this flow\n\n    # Request payload configuration\n    payload = {\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"input_value\": \"What's the weather like where I am right now?\"\n    }\n\n    # Request headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": \"LANGFLOW_API_KEY\"\n    }\n\n    try:\n        # Send API request\n        response = requests.request(\"POST\", url, json=payload, headers=headers)\n        response.raise_for_status()  # Raise exception for bad status codes\n\n        # Parse and print only the message text\n        data = response.json()\n        message = data[\"outputs\"][0][\"outputs\"][0][\"results\"][\"message\"][\"text\"]\n        print(message)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error making API request: {e}\")\n    except ValueError as e:\n        print(f\"Error parsing response: {e}\")\n    except (KeyError, IndexError) as e:\n        print(f\"Error extracting message from response: {e}\")\n    ```\n\n    Notice that this script uses a different prompt than the previous **Playground** examples.\n    In this script, the `input_value` asks about the weather in the user's current location without providing any hints about the user's location, such as a particular city.\n\n    Additionally, this script includes parsing code to extract the LLM's reply from the entire Langflow API response.\n    You will want to use similar extraction in your own applications because the Langflow API response includes metadata and other information that isn't relevant to the reply passed to the user.\n\n3.  Save and run the script to send the request and test the flow.\n\n    The agent uses the `ip_geolocation` tool to detect the requester's location, and then it uses the `weather` tool to retrieve weather information for that location.\n    For example:\n\n    ```text\n    The weather in Waynesboro, Pennsylvania, is currently overcast with a temperature of 23.0C (about 73.4F).\n    If you need more details or have any other questions, feel free to ask!\n    ```\n\n    Remember, the `ip_geolocation` tool used in this tutorial uses your Langflow server's location, which can be different from your actual location.\n\n## Next steps\n\nTo continue building on the concepts introduced in this tutorial, see the following:\n\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use Langflow Agents](/agents)\n* [Use Langflow as an MCP server](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial shows you how to connect MCP servers to your applications using Langflow's [**MCP Tools** component](/mcp-client).",
      "keywords": [
        "access",
        "actions",
        "add",
        "agent",
        "api",
        "application",
        "calculator",
        "chat",
        "city",
        "command",
        "component",
        "connect",
        "connects",
        "create",
        "flow",
        "geolocation",
        "input",
        "json",
        "key",
        "langflow"
      ]
    },
    "mcp-tutorial": {
      "title": "Connect to MCP servers from your application",
      "slug": "/mcp-tutorial",
      "category": "tutorials",
      "content": "This tutorial shows you how to connect MCP servers to your applications using Langflow's [**MCP Tools** component](/mcp-client).\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) helps agents integrate with LLMs through _MCP clients_ and _MCP servers_.\nSpecifically, MCP servers host tools that agents (MCP clients) use to complete specialized tasks.\nMCP servers are connected to MCP clients like Cursor.\nThen, you interact with the client, and the client uses tools from the connected servers as needed to complete your requests.\n\nYou can run Langflow as an MCP client and an MCP server:\n\n* [Use Langflow as an MCP client](/mcp-client): When run as an MCP client, an **Agent** component in a Langflow flow can use connected components as tools to handle requests.\nYou can use existing components as tools, and you can connect any MCP server to your flow to make that server's tools available to the agent.\n\n* [Use Langflow as an MCP server](/mcp-server): When run as an MCP server, your flows become tools that can be used by an MCP client, which could be an external client or another Langflow flow.\n\nIn this tutorial, you will use the Langflow **MCP Tools** component to connect multiple MCP servers to your flow, and then you'll use a Python application to run your flow and chat with the agent programmatically.\n\n## Prerequisites\n\n* [Install and start Langflow](/get-started-installation)\n* Create a [Langflow API key](/api-keys-and-authentication)\n* Create an [OpenAI API key](https://platform.openai.com/api-keys)\n\nThis tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.\n\n## Create an agent flow\n\n1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.\n\n2. In the **Agent** component, enter your OpenAI API key.\n\n    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.\n\n3. To test the flow, click  **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n\n    This query demonstrates how an LLM, by itself, might not have access to information or functions designed to address specialized queries. In this example, the default OpenAI model provides a vague response, although the agent does know the current date by using its internal `get_current_date` function.\n\n    ```text\n    Today is July 11, 2025.\n    To determine if it's safe to go hiking in the Adirondacks today, you should check the current weather conditions, trail advisories, and any local alerts (such as bear activity or flooding).\n    Would you like a detailed weather forecast or information on trail conditions for the Adirondacks today?\n    ```\n\n    To improve the response, you can connect MCP servers to your flow that provide specialized tools for the agent to use when generating responses. In the next part of this tutorial, you'll connect an MCP server that provides the agent with real-time weather information so that it can generate a more specific response.\n\n## Add an MCP Tools component\n\nThere are many MCP servers available online that offer different tools for different tasks.\nTo use an MCP server with an MCP client, you have to make the server available to the client.\nWith all MCP clients, there are several ways to do this:\n\n* Install the server locally.\n* Use `uvx` or `npx` to fetch and run a server package.\n* Call a server running remotely, like those available on Smithery.\n\nThis tutorial demonstrates how to install a weather server locally with `uv pip install`, and how to use `npx` to run the geolocation server package.\nYour particular MCP server's requirements may vary.\n\nIn Langflow, you use the **MCP Tools** component to connect a specific MCP server to a flow.\nYou need one **MCP Tools** component for each MCP server that you want your flow to use.\n\n1. For this tutorial, install a [weather MCP server](https://github.com/isdaniel/mcp_weather_server) on your local machine with uv and Python:\n\n    ```shell\n    uv pip install mcp_weather_server\n    ```\n\n    Make sure you install the server in the same Python environment where Langflow is running:\n\n    * Langflow in a virtual environment: Activate the environment before installing the server.\n    * Langflow Docker image: Install the server inside the Docker container.\n    * Langflow Desktop or system-wide Langflow OSS: Install the server globally or in the same user environment where you run Langflow.\n\n2. In your **Simple Agent** flow, remove the **URL** and **Calculator** tools, and then add an [**MCP Tools**](/mcp-client) component.\n\n3. Click the **MCP Tools** component, and then click  **Add MCP Server**.\n\n    \n**Note:** To manage all of your MCP server connections, click  **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.\n    \n\n4. In the **Add MCP Server** pane, provide the server startup command and arguments to connect the weather MCP server to your flow. For this tutorial, use either the **JSON** or **STDIO** option.\n\n    Langflow runs the command to launch the server when the agent determines that it needs to use a tool provided by that server.\n\n    Notice that both configurations provide the same information but in different formats.\n    This means that if your MCP server repository only provides a JSON file for the server, you can still use those values with the STDIO option.\n\n    \n    \n    </Tabs>\n\n5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.\n\n    With this weather server, the **MCP Tools** component also adds an optional **City** field.\n    For this tutorial, don't enter anything in this field.\n    Instead, you will add a geolocation MCP server in the next step, which the agent will use to detect your location.\n\n6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.\n\n    At this point your flow has four connected components:\n\n    * The **Chat Input** component is connected to the **Agent** component's **Input** port. This allows to flow to be triggered by an incoming prompt from a user or application.\n    * The **MCP Tools** component with the weather MCP server is connected to the **Agent** component's **Tools** port. The agent may not use this server for every request; the agent only uses this connection if it decides the server can help respond to the prompt.\n    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.\n\n    ![An Agent component connected to an MCP weather server](/img/tutorial-mcp-weather.png)\n\n7. To test the weather MCP server, click  **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`\n\n    The  **Playground** shows you the agent's logic as it analyzes the request and select tools to use.\n\n    Ideally, the agent's response will be more specific than the previous response because of the additional context provided by the weather MCP server.\n    For example:\n\n    ```text\n    The current weather in Lake Placid, a central location in the Adirondacks,\n    is foggy with a temperature of 17.2C (about 63F).\n    If you plan to go hiking today, be cautious as fog can reduce visibility\n    on trails and make navigation more difficult.\n    ```\n\n    This is a better response, but what makes this MCP server more valuable than just calling a weather API?\n\n    First, MCP servers are often customized for specific tasks, such as highly specialized actions or chained tools for complex, multi-step problem solving.\n    Typically, you would have to write a custom script for a specific task, possibly including multiple API calls in a single script, and then you would have to either execute this script outside the context of the agent or provide it to your agent in some way.\n\n    Instead, the MCP ensures that all MCP servers are added to agents in the same way, without having to know each server's specific endpoint structures or write custom integrations.\n    The MCP is a standardized way to integrate many diverse tools into agentic applications.\n    You don't have to learn a new API or write custom code every time you want to use a new MCP server.\n\n    Additionally, you can attach many MCP servers to one agent, depending on the problems you want your application to solve.\n    The more servers you add, the more specialized context the agent can use in its responses.\n    In this tutorial, adding the weather MCP server already improved the quality of the LLM's response.\n    In the next section of the tutorial, you will add a `ip_geolocation` MCP server so the agent can detect the user's location if they don't specify a location in their prompt.\n\n## Add a geolocation server\n\nThe [Toolkit MCP server](https://github.com/cyanheads/toolkit-mcp-server) includes multiple MCP tools for network monitoring, including IP geolocation. It isn't extremely precise, but it doesn't require an API key.\n\nNote that this tool returns the IP geolocation of your Langflow server, so if your server is deployed remotely, consider alternative approaches for getting user-specific location data, such as browser geolocation APIs.\n\nThis MCP server can be started with one [npx](https://docs.npmjs.com/cli/v8/commands/npx) command, which downloads and runs the [Toolkit MCP server Node registry package](https://www.npmjs.com/package/@cyanheads/toolkit-mcp-server) without installing the package locally.\n\nTo add the Toolkip MCP server to your flow, do the following:\n\n1. Add another **MCP Tools** component to your flow, click the component, and then click  **Add MCP Server**.\n\n2. Select **STDIO**.\n\n3. For **Name**, enter `ip_geolocation`.\n\n    \n**Note:** The tool name and description help the agent select tools.\n    If your agent is struggling to select tools, make sure the names and descriptions are clear and human-readable.\n    \n\n4. For **Command**, enter `npx @cyanheads/toolkit-mcp-server`.\n\n5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.\n\n6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.\n\n    Your flow now has an additional **MCP Tools** component for a total of five components.\n\n    ![An Agent component connected to MCP weather and geolocation servers](/img/tutorial-mcp-geolocation.png)\n\n## Create a Python application that connects to Langflow\n\nAt this point, you can open the **Playground** and ask about the weather in your current location to test the IP geolocation tool.\nHowever, geolocation tools are most useful in applications where you or your users want to ask about the weather from different places around the world.\n\nIn the last part of this tutorial, you'll learn how to use the Langflow API to run a flow in a script.\nThis could be part of a larger application, such as a mobile app where users want to know if the weather is good for a particular sport.\n\nWhen you use the Langflow API to run a flow, you can change some aspects of the flow without changing the code.\nFor example, you can add more MCP servers to your flow in Langflow, and then use the same script to run the flow.\nYou can use the same input or a new input that prompts the agent to use other tools.\n\n1. For this tutorial's Python script, gather the following information:\n\n    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).\n    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).\n\n2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:\n\n    ```python\n    import requests\n    import os\n\n    url = \"LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID\"  # The complete API endpoint URL for this flow\n\n    # Request payload configuration\n    payload = {\n        \"output_type\": \"chat\",\n        \"input_type\": \"chat\",\n        \"input_value\": \"What's the weather like where I am right now?\"\n    }\n\n    # Request headers\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"x-api-key\": \"LANGFLOW_API_KEY\"\n    }\n\n    try:\n        # Send API request\n        response = requests.request(\"POST\", url, json=payload, headers=headers)\n        response.raise_for_status()  # Raise exception for bad status codes\n\n        # Parse and print only the message text\n        data = response.json()\n        message = data[\"outputs\"][0][\"outputs\"][0][\"results\"][\"message\"][\"text\"]\n        print(message)\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error making API request: {e}\")\n    except ValueError as e:\n        print(f\"Error parsing response: {e}\")\n    except (KeyError, IndexError) as e:\n        print(f\"Error extracting message from response: {e}\")\n    ```\n\n    Notice that this script uses a different prompt than the previous **Playground** examples.\n    In this script, the `input_value` asks about the weather in the user's current location without providing any hints about the user's location, such as a particular city.\n\n    Additionally, this script includes parsing code to extract the LLM's reply from the entire Langflow API response.\n    You will want to use similar extraction in your own applications because the Langflow API response includes metadata and other information that isn't relevant to the reply passed to the user.\n\n3.  Save and run the script to send the request and test the flow.\n\n    The agent uses the `ip_geolocation` tool to detect the requester's location, and then it uses the `weather` tool to retrieve weather information for that location.\n    For example:\n\n    ```text\n    The weather in Waynesboro, Pennsylvania, is currently overcast with a temperature of 23.0C (about 73.4F).\n    If you need more details or have any other questions, feel free to ask!\n    ```\n\n    Remember, the `ip_geolocation` tool used in this tutorial uses your Langflow server's location, which can be different from your actual location.\n\n## Next steps\n\nTo continue building on the concepts introduced in this tutorial, see the following:\n\n* [Use Langflow as an MCP client](/mcp-client)\n* [Use Langflow Agents](/agents)\n* [Use Langflow as an MCP server](/mcp-server)\n* [Langflow deployment overview](/deployment-overview)",
      "summary": "This tutorial shows you how to connect MCP servers to your applications using Langflow's [**MCP Tools** component](/mcp-client).",
      "keywords": [
        "access",
        "actions",
        "add",
        "agent",
        "api",
        "application",
        "calculator",
        "chat",
        "city",
        "command",
        "component",
        "connect",
        "connects",
        "create",
        "flow",
        "geolocation",
        "input",
        "json",
        "key",
        "langflow"
      ]
    }
  }
}