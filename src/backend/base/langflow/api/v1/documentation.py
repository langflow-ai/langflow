"""Documentation module for Langflow Assistant.

This module provides access to Langflow documentation for the Flow Assistant.
Documentation is loaded from a pre-built JSON bundle (agent_docs.json).

The JSON bundle is generated by running:
    make build_agent_docs
or:
    python scripts/build_agent_docs.py

This ensures documentation is available in production builds where the
docs/ directory may not be present.
"""

from __future__ import annotations

import json
import re
from functools import lru_cache
from pathlib import Path
from typing import Any

# Path to the bundled documentation JSON (in initial_setup with other static resources)
AGENT_DOCS_FILE = Path(__file__).parent.parent.parent / "initial_setup" / "agent_docs.json"


class DocumentationNotAvailableError(Exception):
    """Raised when documentation bundle is not available."""


@lru_cache(maxsize=1)
def _load_documentation_bundle() -> dict[str, Any]:
    """Load the documentation bundle from JSON file.

    The bundle is cached for performance.

    Returns:
        Dictionary containing:
        - version: Bundle version
        - generated_at: ISO timestamp of generation
        - total_pages: Number of documentation pages
        - categories: Dict mapping category names to lists of slugs
        - pages: Dict mapping slugs/filenames to page data

    Raises:
        DocumentationNotAvailableError: If bundle file is not found
    """
    if not AGENT_DOCS_FILE.exists():
        msg = f"Documentation bundle not found at {AGENT_DOCS_FILE}. Run 'make build_agent_docs' to generate it."
        raise DocumentationNotAvailableError(msg)

    with AGENT_DOCS_FILE.open("r", encoding="utf-8") as f:
        return json.load(f)


def is_documentation_available() -> bool:
    """Check if documentation bundle is available."""
    return AGENT_DOCS_FILE.exists()


def get_documentation_index() -> dict[str, list[dict[str, str]]]:
    """Get an index of all documentation organized by category.

    Returns:
        Dictionary mapping category names to lists of page info:
        {
            "get_started": [
                {"title": "...", "slug": "/...", "summary": "..."},
                ...
            ],
            ...
        }
    """
    try:
        bundle = _load_documentation_bundle()
    except DocumentationNotAvailableError:
        return {}

    pages = bundle.get("pages", {})
    categories = bundle.get("categories", {})

    result: dict[str, list[dict[str, str]]] = {}
    seen_slugs: set[str] = set()

    for category, slugs in categories.items():
        result[category] = []
        for slug in slugs:
            if slug in seen_slugs:
                continue
            seen_slugs.add(slug)

            page = pages.get(slug, {})
            if page:
                summary = page.get("summary", "")
                truncated = len(summary) > _SUMMARY_TRUNCATE_LENGTH
                if truncated:
                    summary = summary[:_SUMMARY_TRUNCATE_LENGTH] + "..."

                result[category].append(
                    {
                        "title": page.get("title", slug),
                        "slug": slug,
                        "summary": summary,
                    }
                )

    return result


_SUMMARY_TRUNCATE_LENGTH = 150


def search_documentation(query: str, max_results: int = 5) -> list[dict[str, Any]]:
    """Search documentation by query string.

    Performs keyword-based search across titles, summaries, and content.

    Args:
        query: Search query string
        max_results: Maximum number of results to return (1-20)

    Returns:
        List of matching pages with scores:
        [
            {"title": "...", "slug": "...", "category": "...", "summary": "...", "score": 10.0},
            ...
        ]
    """
    try:
        bundle = _load_documentation_bundle()
    except DocumentationNotAvailableError:
        return []

    pages = bundle.get("pages", {})
    query_lower = query.lower()
    query_words = set(re.findall(r"\b[a-zA-Z]{2,}\b", query_lower))

    results: list[tuple[float, dict[str, Any]]] = []
    seen_slugs: set[str] = set()

    for page in pages.values():
        slug = page.get("slug", "")
        if slug in seen_slugs:
            continue
        seen_slugs.add(slug)

        score = 0.0

        # Title match (highest weight)
        title_lower = page.get("title", "").lower()
        if query_lower in title_lower:
            score += 10.0
        for word in query_words:
            if word in title_lower:
                score += 3.0

        # Keyword match
        page_keywords = set(page.get("keywords", []))
        keyword_matches = query_words & page_keywords
        score += len(keyword_matches) * 2.0

        # Summary match
        summary_lower = page.get("summary", "").lower()
        for word in query_words:
            if word in summary_lower:
                score += 1.0

        # Content match (lower weight due to size)
        content_lower = page.get("content", "").lower()
        for word in query_words:
            if word in content_lower:
                score += 0.5

        if score > 0:
            results.append((score, page))

    # Sort by score descending
    results.sort(key=lambda x: x[0], reverse=True)

    max_results = min(max(1, max_results), 20)
    return [
        {
            "title": page.get("title", ""),
            "slug": page.get("slug", ""),
            "category": page.get("category", ""),
            "summary": page.get("summary", ""),
            "score": score,
        }
        for score, page in results[:max_results]
    ]


def get_documentation_page(identifier: str) -> dict[str, Any] | None:
    """Get a specific documentation page by slug or filename.

    Args:
        identifier: The slug (e.g., '/concepts-flows') or filename (e.g., 'concepts-flows')

    Returns:
        Page data dictionary or None if not found:
        {
            "title": "...",
            "slug": "/...",
            "category": "...",
            "content": "...",
        }
    """
    try:
        bundle = _load_documentation_bundle()
    except DocumentationNotAvailableError:
        return None

    pages = bundle.get("pages", {})

    # Try direct lookup
    if identifier in pages:
        page = pages[identifier]
        return {
            "title": page.get("title", ""),
            "slug": page.get("slug", ""),
            "category": page.get("category", ""),
            "content": page.get("content", ""),
        }

    # Try with/without leading slash
    alt_identifier = identifier.lstrip("/") if identifier.startswith("/") else f"/{identifier}"
    if alt_identifier in pages:
        page = pages[alt_identifier]
        return {
            "title": page.get("title", ""),
            "slug": page.get("slug", ""),
            "category": page.get("category", ""),
            "content": page.get("content", ""),
        }

    return None


def get_documentation_overview() -> str:
    """Get a high-level overview of Langflow for system prompt inclusion."""
    return """# Langflow Documentation Overview

Langflow is an open-source, Python-based, customizable framework for building AI applications.
It supports agents, MCP (Model Context Protocol), and doesn't require specific LLMs or vector stores.

## Core Concepts

### Flows
- Flows are functional representations of application workflows
- Flows consist of components connected by edges (data ports)
- Flows receive input, process it, and produce output
- Test flows in the Playground before deploying

### Components
- Building blocks of flows - like classes in an application
- Each component performs a specific task
- Components have inputs, outputs, and configurable parameters
- Component ports indicate data types (color-coded)

### Data Types
- **Message** (Indigo): Chat messages with text, sender, session info
- **Data** (Red): Structured key-value pairs (JSON-like)
- **DataFrame** (Pink): Tabular data (pandas-compatible)
- **LanguageModel** (Fuchsia): LLM instances for inference
- **Tool** (Cyan): Functions for agents
- **Embeddings** (Emerald): Vector embeddings
- **Memory** (Orange): Conversation history

## Component Categories

### Input/Output
- ChatInput/ChatOutput: Chat interface components
- TextInput/TextOutput: Simple text handling
- Webhook: HTTP request receiver

### Models
- QueryRouterModel (RECOMMENDED): Unified access to 100+ AI models
- OpenAI, Anthropic, Google, Groq, Ollama model components

### Processing
- Prompt Template: Dynamic text templates with {variables}
- Text Splitter: Chunk documents
- Parse Data: Extract fields
- Type Convert: Convert between data types

### Agents
- Agent: AI agent with tools and memory
- Supports tool calling and multi-step reasoning
- Connect LanguageModel and Tool components

### Vector Stores
- Chroma, Pinecone, Qdrant, Weaviate, FAISS, etc.
- Require Embeddings component connection

## Common Patterns

1. **Simple Chat**: ChatInput → Agent → ChatOutput
2. **RAG Pipeline**: Document → Splitter → Embeddings → VectorStore → Retriever → Agent
3. **Multi-Tool Agent**: ChatInput → Agent (with multiple tools) → ChatOutput

## Use lf_documentation tool for detailed information on specific topics!
"""


# Category descriptions for better context
CATEGORY_DESCRIPTIONS: dict[str, str] = {
    "get_started": "Installation, quickstart, and introduction to Langflow",
    "flows": "Creating, managing, and running flows",
    "components": "Component types, bundles, and integrations",
    "develop": "Data types, API keys, configuration, environment variables",
    "agents": "Agent components, tools, MCP integration",
    "deployment": "Docker, Kubernetes, cloud deployment options",
    "api_reference": "REST API documentation",
    "tutorials": "Step-by-step guides and examples",
    "contributing": "How to contribute to Langflow",
    "support": "Getting help and troubleshooting",
}


def get_category_description(category: str) -> str:
    """Get a description for a documentation category."""
    return CATEGORY_DESCRIPTIONS.get(category, f"Documentation about {category}")
