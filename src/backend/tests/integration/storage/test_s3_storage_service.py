"""Integration tests for S3StorageService using real AWS S3.

These tests use actual AWS credentials and interact with a real S3 bucket.
They are designed to be safe and clean up after themselves.

AWS credentials must be set as environment variables:
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_DEFAULT_REGION (optional, defaults to us-west-2)
"""

import contextlib
import json
import os
import tempfile
from pathlib import Path
from unittest.mock import Mock

import pytest
from langflow.services.storage.s3 import S3StorageService

# Mark all tests in this module as requiring API keys
pytestmark = pytest.mark.api_key_required


@pytest.fixture
def aws_credentials():
    """Verify AWS credentials are set via environment variables."""
    required_vars = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
    missing_vars = [var for var in required_vars if not os.environ.get(var)]

    if missing_vars:
        pytest.skip(f"Missing required environment variables: {', '.join(missing_vars)}")

    # Set default region if not provided
    if not os.environ.get("AWS_DEFAULT_REGION"):
        os.environ["AWS_DEFAULT_REGION"] = "us-west-2"

    # No cleanup needed - we're using existing env vars


@pytest.fixture
def mock_settings_service():
    """Create a mock settings service with S3 configuration.

    Configuration via environment variables:
    - LANGFLOW_OBJECT_STORAGE_BUCKET_NAME: S3 bucket name (default: langflow-ci)
    - LANGFLOW_OBJECT_STORAGE_PREFIX: S3 prefix (default: test-files-1)
    - LANGFLOW_OBJECT_STORAGE_TAGS: S3 tags as JSON string (default: {"env": "test-1"})

    Note: All settings use LANGFLOW_OBJECT_STORAGE_* names to test that
    the S3StorageService properly respects these settings.
    """
    settings_service = Mock()
    settings_service.settings.config_dir = str(Path(tempfile.gettempdir()) / "langflow_test")

    # Bucket name from env or default
    settings_service.settings.object_storage_bucket_name = os.environ.get(
        "LANGFLOW_OBJECT_STORAGE_BUCKET_NAME", "langflow-ci"
    )

    # Prefix from env - using standard LANGFLOW env var name
    settings_service.settings.object_storage_prefix = os.environ.get("LANGFLOW_OBJECT_STORAGE_PREFIX", "test-files-1")

    # Tags from env - using standard LANGFLOW env var name
    default_tags = {"env": "test-1"}
    tags_str = os.environ.get("LANGFLOW_OBJECT_STORAGE_TAGS")
    if tags_str:
        try:
            settings_service.settings.object_storage_tags = json.loads(tags_str)
        except json.JSONDecodeError:
            settings_service.settings.object_storage_tags = default_tags
    else:
        settings_service.settings.object_storage_tags = default_tags

    return settings_service


@pytest.fixture
def mock_session_service():
    """Create a mock session service."""
    return Mock()


@pytest.fixture
async def s3_storage_service(mock_session_service, mock_settings_service, _aws_credentials):
    """Create an S3StorageService instance for testing with real AWS."""
    service = S3StorageService(mock_session_service, mock_settings_service)
    yield service
    await service.teardown()


@pytest.fixture
def test_flow_id():
    """Unique flow ID for testing to avoid conflicts."""
    import uuid

    return f"test_flow_{uuid.uuid4().hex[:8]}"


@pytest.mark.asyncio
class TestS3StorageServiceInitialization:
    """Test S3StorageService initialization."""

    async def test_initialization(self, s3_storage_service):
        """Test that the service initializes correctly and respects settings."""
        assert s3_storage_service.ready is True

        # Verify bucket name matches env or default
        expected_bucket = os.environ.get("LANGFLOW_OBJECT_STORAGE_BUCKET_NAME", "langflow-ci")
        assert s3_storage_service.bucket_name == expected_bucket

        # Verify prefix matches env or default (with trailing slash)
        # This tests that S3StorageService respects LANGFLOW_OBJECT_STORAGE_PREFIX
        expected_prefix = os.environ.get("LANGFLOW_OBJECT_STORAGE_PREFIX", "test-files-1")
        assert s3_storage_service.prefix == f"{expected_prefix}/"

        # Verify tags match env or default
        # This tests that S3StorageService respects LANGFLOW_OBJECT_STORAGE_TAGS
        default_tags = {"env": "test-1"}
        tags_str = os.environ.get("LANGFLOW_OBJECT_STORAGE_TAGS")
        expected_tags = json.loads(tags_str) if tags_str else default_tags
        assert s3_storage_service.tags == expected_tags

    async def test_build_full_path(self, s3_storage_service):
        """Test building full S3 key with configured prefix."""
        expected_prefix = os.environ.get("LANGFLOW_OBJECT_STORAGE_PREFIX", "test-files-1")
        key = s3_storage_service.build_full_path("flow_123", "test.txt")
        assert key == f"{expected_prefix}/flow_123/test.txt"

    async def test_resolve_component_path(self, s3_storage_service):
        """Test that resolve_component_path returns logical path as-is."""
        logical_path = "flow_123/myfile.txt"
        resolved = s3_storage_service.resolve_component_path(logical_path)
        assert resolved == logical_path


@pytest.mark.asyncio
class TestS3StorageServiceFileOperations:
    """Test file operations in S3StorageService with real S3."""

    async def test_save_and_get_file(self, s3_storage_service, test_flow_id):
        """Test saving and retrieving a file."""
        file_name = "test.txt"
        data = b"Hello, S3!"

        try:
            # Save file
            await s3_storage_service.save_file(test_flow_id, file_name, data)

            # Retrieve file
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)

            assert retrieved == data
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_save_file_overwrites_existing(self, s3_storage_service, test_flow_id):
        """Test that saving a file overwrites existing content."""
        file_name = "overwrite.txt"

        try:
            # Save initial file
            await s3_storage_service.save_file(test_flow_id, file_name, b"original")

            # Overwrite with new data
            new_data = b"updated content"
            await s3_storage_service.save_file(test_flow_id, file_name, new_data)

            # Verify new data
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)
            assert retrieved == new_data
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_get_file_not_found(self, s3_storage_service, test_flow_id):
        """Test getting a non-existent file raises FileNotFoundError."""
        with pytest.raises(FileNotFoundError, match="not found"):
            await s3_storage_service.get_file(test_flow_id, "nonexistent.txt")

    async def test_save_binary_file(self, s3_storage_service, test_flow_id):
        """Test saving and retrieving binary data."""
        file_name = "binary.bin"
        data = bytes(range(256))

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)

            assert retrieved == data
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_save_large_file(self, s3_storage_service, test_flow_id):
        """Test saving and retrieving a larger file (1MB)."""
        file_name = "large.bin"
        data = b"X" * (1024 * 1024)  # 1 MB

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)

            # Verify size
            size = await s3_storage_service.get_file_size(test_flow_id, file_name)
            assert size == 1024 * 1024

            # Verify content
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)
            assert retrieved == data
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)


@pytest.mark.asyncio
class TestS3StorageServiceStreamOperations:
    """Test streaming operations in S3StorageService."""

    async def test_get_file_stream(self, s3_storage_service, test_flow_id):
        """Test streaming a file from S3."""
        file_name = "stream.txt"
        data = b"A" * 10000  # 10KB

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)

            # Stream the file
            chunks = [
                chunk async for chunk in s3_storage_service.get_file_stream(test_flow_id, file_name, chunk_size=1024)
            ]

            # Verify content
            streamed_data = b"".join(chunks)
            assert streamed_data == data
            assert len(chunks) > 1  # Should be multiple chunks
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_get_file_stream_not_found(self, s3_storage_service, test_flow_id):
        """Test streaming a non-existent file raises FileNotFoundError."""
        with pytest.raises(FileNotFoundError):
            async for _ in s3_storage_service.get_file_stream(test_flow_id, "no_file.txt"):
                pass


@pytest.mark.asyncio
class TestS3StorageServiceListOperations:
    """Test list operations in S3StorageService."""

    async def test_list_files_empty(self, s3_storage_service, test_flow_id):
        """Test listing files in an empty flow."""
        files = await s3_storage_service.list_files(test_flow_id)
        assert files == []

    async def test_list_files_with_files(self, s3_storage_service, test_flow_id):
        """Test listing files in a flow with multiple files."""
        file_names = ["file1.txt", "file2.csv", "file3.json"]

        try:
            # Create files
            for file_name in file_names:
                await s3_storage_service.save_file(test_flow_id, file_name, b"content")

            # List files
            listed = await s3_storage_service.list_files(test_flow_id)

            assert len(listed) == 3
            assert set(listed) == set(file_names)
        finally:
            # Cleanup
            for file_name in file_names:
                with contextlib.suppress(Exception):
                    await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_list_files_excludes_other_flows(self, s3_storage_service, test_flow_id):
        """Test that list_files only returns files from the specified flow."""
        import uuid

        other_flow_id = f"test_flow_{uuid.uuid4().hex[:8]}"

        try:
            # Create file in test_flow_id
            await s3_storage_service.save_file(test_flow_id, "file1.txt", b"content1")

            # Create file in other_flow_id
            await s3_storage_service.save_file(other_flow_id, "file2.txt", b"content2")

            # List files for each flow
            files_flow1 = await s3_storage_service.list_files(test_flow_id)
            files_flow2 = await s3_storage_service.list_files(other_flow_id)

            # Verify isolation
            assert "file1.txt" in files_flow1
            assert "file1.txt" not in files_flow2
            assert "file2.txt" in files_flow2
            assert "file2.txt" not in files_flow1
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, "file1.txt")
                await s3_storage_service.delete_file(other_flow_id, "file2.txt")


@pytest.mark.asyncio
class TestS3StorageServiceDeleteOperations:
    """Test delete operations in S3StorageService."""

    async def test_delete_existing_file(self, s3_storage_service, test_flow_id):
        """Test deleting an existing file."""
        file_name = "to_delete.txt"

        # Create file
        await s3_storage_service.save_file(test_flow_id, file_name, b"delete me")

        # Verify it exists
        files = await s3_storage_service.list_files(test_flow_id)
        assert file_name in files

        # Delete
        await s3_storage_service.delete_file(test_flow_id, file_name)

        # Verify it's gone
        with pytest.raises(FileNotFoundError):
            await s3_storage_service.get_file(test_flow_id, file_name)

    async def test_delete_nonexistent_file(self, s3_storage_service, test_flow_id):
        """Test deleting a non-existent file doesn't raise an error."""
        # S3 delete_object doesn't raise for non-existent files
        await s3_storage_service.delete_file(test_flow_id, "no_file.txt")

    async def test_delete_multiple_files(self, s3_storage_service, test_flow_id):
        """Test deleting multiple files."""
        files = ["file1.txt", "file2.txt", "file3.txt"]

        try:
            # Create files
            for file_name in files:
                await s3_storage_service.save_file(test_flow_id, file_name, b"content")

            # Delete all
            for file_name in files:
                await s3_storage_service.delete_file(test_flow_id, file_name)

            # Verify all gone
            listed = await s3_storage_service.list_files(test_flow_id)
            assert len(listed) == 0
        finally:
            # Cleanup any remaining
            for file_name in files:
                with contextlib.suppress(Exception):
                    await s3_storage_service.delete_file(test_flow_id, file_name)


@pytest.mark.asyncio
class TestS3StorageServiceFileSizeOperations:
    """Test file size operations in S3StorageService."""

    async def test_get_file_size(self, s3_storage_service, test_flow_id):
        """Test getting the size of a file."""
        file_name = "sized.txt"
        data = b"X" * 1234

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)

            size = await s3_storage_service.get_file_size(test_flow_id, file_name)
            assert size == 1234
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_get_file_size_empty_file(self, s3_storage_service, test_flow_id):
        """Test getting size of empty file."""
        file_name = "empty.txt"

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, b"")

            size = await s3_storage_service.get_file_size(test_flow_id, file_name)
            assert size == 0
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_get_file_size_nonexistent(self, s3_storage_service, test_flow_id):
        """Test getting size of non-existent file raises FileNotFoundError."""
        with pytest.raises(FileNotFoundError):
            await s3_storage_service.get_file_size(test_flow_id, "no_file.txt")


@pytest.mark.asyncio
class TestS3StorageServiceEdgeCases:
    """Test edge cases and special scenarios."""

    async def test_save_file_with_unicode_content(self, s3_storage_service, test_flow_id):
        """Test saving files with unicode content."""
        file_name = "unicode.txt"
        data = "Hello ‰∏ñÁïå üåç".encode()

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)

            assert retrieved == data
            assert retrieved.decode("utf-8") == "Hello ‰∏ñÁïå üåç"
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_file_name_with_special_characters(self, s3_storage_service, test_flow_id):
        """Test files with special characters in names."""
        file_name = "test-file_2024.txt"
        data = b"special content"

        try:
            await s3_storage_service.save_file(test_flow_id, file_name, data)
            retrieved = await s3_storage_service.get_file(test_flow_id, file_name)

            assert retrieved == data
        finally:
            # Cleanup
            with contextlib.suppress(Exception):
                await s3_storage_service.delete_file(test_flow_id, file_name)

    async def test_concurrent_file_operations(self, s3_storage_service, test_flow_id):
        """Test concurrent file operations."""
        import asyncio

        file_names = [f"concurrent_{i}.txt" for i in range(5)]

        async def save_file(file_name):
            data = f"content_{file_name}".encode()
            await s3_storage_service.save_file(test_flow_id, file_name, data)

        try:
            # Save files concurrently
            await asyncio.gather(*[save_file(fn) for fn in file_names])

            # Verify all files exist
            listed = await s3_storage_service.list_files(test_flow_id)
            assert len(listed) == 5
            for file_name in file_names:
                assert file_name in listed
        finally:
            # Cleanup
            for file_name in file_names:
                with contextlib.suppress(Exception):
                    await s3_storage_service.delete_file(test_flow_id, file_name)


@pytest.mark.asyncio
class TestS3StorageServiceTeardown:
    """Test teardown operations in S3StorageService."""

    async def test_teardown(self, s3_storage_service):
        """Test that teardown completes without errors."""
        await s3_storage_service.teardown()
